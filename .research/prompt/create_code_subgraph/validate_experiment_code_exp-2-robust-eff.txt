
Input:
You are an AI code reviewer validating production-ready experiment code for research papers.

Your task is to compare the derived experiment_code with the original base_code to ensure that:
1. No important functionality has been omitted or truncated
2. All placeholders have been completely replaced with working implementations (no TODO, PLACEHOLDER, pass, or ... allowed)
3. The code is immediately executable and ready for research paper experiments
4. The derived code maintains the quality and completeness of the base foundation

# Instructions

## Core Validation Criteria
Check if the derived experiment code meets ALL of the following requirements:

1. **Complete Implementation Preservation**:
   - All functionality from base_code is preserved or properly enhanced
   - No code sections have been omitted or significantly shortened
   - Core algorithms and logic remain intact and functional
   - No reduction in code quality or completeness

2. **Complete Placeholder Replacement and Variation Implementation**:
   - All `DATASET_PLACEHOLDER` entries replaced with complete, working Hugging Face dataset loading
   - All `MODEL_PLACEHOLDER` entries replaced with complete, working model architectures
   - All `SPECIFIC_CONFIG_PLACEHOLDER` entries replaced with actual parameters
   - All run_variations are defined in both `config/smoke_test.yaml` and `config/full_experiment.yaml`
   - All run_variations are implemented in `src/model.py`
   - `config/smoke_test.yaml` contains ALL run variations in lightweight form
   - No TODO, PLACEHOLDER, pass, ..., or any incomplete implementations remain

3. **Functional Enhancement**:
   - Dataset-specific preprocessing is properly implemented
   - Model-specific configurations are correctly applied
   - Evaluation metrics are adapted for the specific experimental setup
   - All external resources are properly integrated

4. **Code Completeness**:
   - No truncated functions or incomplete implementations
   - All imports and dependencies are properly specified
   - Configuration files contain real experimental parameters
   - No "[UNCHANGED]" markers or similar placeholders remain

5. **Consistency with Base Code**:
   - Same file structure and organization
   - Consistent coding style and patterns
   - Proper error handling and logging maintained
   - All base functionality enhanced, not removed

## Detection of Common Issues
Flag the following problems if found:

- **Truncation**: Code sections that are significantly shorter than base_code equivalents
- **Omission**: Missing functions, classes, or important code blocks from base_code
- **Incomplete Replacement**: TODO, PLACEHOLDER, pass, ..., or any placeholder patterns that haven't been fully replaced with working code
- **Quality Degradation**: Simplified logic that reduces functionality
- **Structural Changes**: Unexpected modifications to the core architecture
- **Not Executable**: Code that cannot be run immediately due to missing implementations

## Output Format
Respond with a JSON object containing:
- `is_experiment_code_ready`: boolean - true if ALL criteria are met, false otherwise
- `experiment_code_issue`: string - specific issues found if any criteria are not met

# Current Research Method
{
    "Open Problems": "Token-level strictly–proper scoring rules such as the Brier or Spherical score improve generation quality when used for fine-tuning, but they converge slowly and under-perform when models are trained from scratch.  The root cause is that bounded scores give much smaller early gradients than the unbounded logarithmic score, so learning stalls in the high-entropy regime.  A minimal change that keeps the advantages of bounded scores while restoring strong early gradients is still missing.",
    "Methods": "We propose Hybrid Proper Scoring (HyProS).\n1.  Loss definition\n    L(θ) = (1−λ) · Slog(θ)  +  λ · SBrier(θ)\n    where Slog is the usual token-level negative log-likelihood and SBrier is the token-level Brier score.\n2.  Dynamic weight λ\n    λ starts near 0 (pure log-loss) and is annealed to 1 (pure Brier) over training steps t:\n        λ(t) = sigmoid((t−T0)/τ)\n    with T0≈0.3·Ttotal and τ≈0.1·Ttotal.\n3.  Motivation\n    • Large early gradients from Slog drive rapid entropy reduction.\n    • As the model becomes competent, λ→1 lets the bounded Brier score shape a better calibration / generation trajectory.\n    • The schedule needs no extra hyper-parameters beyond (T0,τ) and introduces <10 lines of code.",
    "Experimental Setup": "Model: Transformer-base trained from scratch on WMT14 En-De.\nBaselines: (a) pure Log loss, (b) pure Brier loss, (c) HyProS.\nTraining: identical optimiser / lr schedule for all runs, 100 k steps.\nEvaluation: BLEU on newstest14, validation perplexity every 5 k steps to measure convergence speed.\nAblation: fixed λ=0.5 to show the benefit of the annealing schedule.",
    "Experimental Code": "class HybridBrierLoss(nn.Module):\n    def __init__(self, vocab_size, t0, tau, total_steps):\n        super().__init__()\n        self.v = vocab_size\n        self.t0, self.tau = t0, tau\n        self.total = total_steps\n        self.register_buffer('step', torch.tensor(0))\n\n    def forward(self, logits, targets):\n        # logits: (B,T,V), targets: (B,T)\n        p = torch.softmax(logits, -1)\n        one_hot = torch.nn.functional.one_hot(targets, self.v).float()\n        sbrier = ((p - one_hot)**2).sum(-1)          # Brier per token\n        slog   = torch.nn.functional.cross_entropy(\n                    logits.view(-1, self.v), targets.view(-1), reduction='none')\n        slog   = slog.view_as(sbrier)\n        # weight\n        t = self.step.item()\n        lam = torch.sigmoid((t - self.t0) / self.tau)\n        self.step += 1\n        loss = (1 - lam) * slog + lam * sbrier\n        return loss.mean()\n\n# Usage inside training loop\ncriterion = HybridBrierLoss(vocab_size, t0=0.3*TOTAL_STEPS, tau=0.1*TOTAL_STEPS, total_steps=TOTAL_STEPS)\nfor step, (inp, tgt) in enumerate(loader):\n    logits = model(inp)\n    loss = criterion(logits, tgt)\n    loss.backward(); opt.step(); opt.zero_grad()",
    "Expected Result": "•   Convergence: HyProS matches the log-loss perplexity curve for the first ~30 k steps and overtakes the pure Brier run by >2× at 20 k steps.\n•   Final BLEU: Log = 27.3, Brier = 26.8, HyProS ≈ 27.8 (+0.5 over log, +1.0 over Brier).\n•   Calibration (Brier score on dev): HyProS equals Brier and beats Log.\nThus the hybrid schedule removes Brier’s slow-start disadvantage while retaining its generation benefits.",
    "Expected Conclusion": "A simple, two-line weighted combination of log-loss and Brier loss with a sigmoid schedule eliminates the slow-convergence problem of bounded strictly proper scores.  Because no additional model parameters or complex algorithms are introduced, HyProS can be dropped into any existing training script.  The result is faster training, equal-or-better BLEU, and improved probability calibration, demonstrating that minimal objective tweaks can yield meaningful gains."
}

# Experimental Design
## Experiment Strategy
Global goal
Demonstrate that Hybrid Proper Scoring (HyProS) simultaneously (a) preserves the fast early-stage learning of the unbounded log-loss, (b) inherits the calibration / generation quality of bounded scores, and (c) does so with negligible computational overhead, across data scales, model sizes and NLP tasks.

1. Aspects to validate
1.1 Learning dynamics – early-stage gradient magnitude, perplexity/accuracy curves, time-to-reach given performance.
1.2 End-task performance – BLEU, ROUGE-L, F1, perplexity, accuracy, depending on task.
1.3 Calibration – token-level Brier, Expected Calibration Error, reliability diagrams.
1.4 Robustness & generalization – OOD test sets, adversarially perturbed inputs, zero-shot transfer tasks.
1.5 Efficiency – GPU hours, wall-clock time, peak VRAM, energy where available.
1.6 Stability – variance across ≥3 random seeds, sensitivity to λ-schedule hyper-parameters.

2. Comparison families
2.1 Core baselines: (i) pure log-loss (NLL), (ii) pure bounded score (Brier or Spherical, matched to task).
2.2 SOTA scoring-rule hybrids: focal-style λ·log +(1-λ)·Brier with fixed λ, or curriculum schemes from literature.
2.3 Ablations of HyProS: fixed λ values; linear instead of sigmoid schedule; altered (T0,τ) ratios; removing Brier in late stages (λ→0 again).
2.4 Model-size controls: tiny, base, large checkpoints trained with each objective to separate scaling-law effects from loss-function effects.

3. Experimental angles
3.1 Quantitative
    • Learning curves: metric v.s. steps & wall-clock; area-under-curve until convergence.
    • Calibration metrics at multiple checkpoints.
    • Compute profile: TFLOPs, memory, forward/backward time.
3.2 Qualitative
    • Human side-by-side rating of sampled generations (translation adequacy/fluency, summarization faithfulness, open-ended text).
    • Reliability diagrams & entropy histograms illustrating calibration changes over training.
3.3 Stress / robustness
    • Evaluate on noisy inputs (e.g., swapped words, typos), distribution shifts (different domain) and adversarial triggers; report relative degradation.
    • Analyse gradient norms during first 10 k steps to confirm hypothesis about early-stage signal.

4. Multi-task matrix
Train &/or fine-tune identical architectures on:
    MT (WMT14 En-De, low-resource IWSLT15 Viet-En),
    Language modelling (WikiText-103, The Pile-10 B subset),
    Summarization (CNN/DailyMail),
    Text classification (SST-2, AG-News).
Same strategy, dataloader, optimiser; swap task metric accordingly. A100-80 GB memory enables batch sizes ≥256 tokens per GPU for “large” models (≈500 M params) while keeping compute within 2 k GPU-hours.

5. Validation criteria for success
HyProS is judged superior if, averaged over tasks/models/seeds, it fulfils ALL of:
    C1  ≤3 % overhead in wall-clock per step.
    C2  ≥0.3 σ better final task metric than log-loss OR matched performance with ≥25 % faster time-to-target.
    C3  ≥10 % lower ECE or token-level Brier than log-loss with no worse end-task metric.
    C4  No statistically significant degradation vs bounded-score baseline on robustness tests.
    C5  Positive or neutral gains reproduced in ≥80 % of instance comparisons (binomial p<0.05).

6. Execution workflow
6.1 Implement objective as drop-in PyTorch module (provided code). Add callback to log λ, gradient norms.
6.2 For each task x model-size create identical training scripts differing only in loss choice & ablation flags.
6.3 Use deterministic initialisation and three seeds; allocate one A100 per run; log to WandB for unified dashboards.
6.4 Automate metric extraction & statistical analysis (paired t-test, bootstrap) via a shared Jupyter notebook.
6.5 Summarise results in four figure categories: (a) Learning curves, (b) Calibration curves, (c) Robustness bars, (d) Efficiency tables.

This unified experimental strategy ensures that every subsequent experiment—regardless of dataset or model—follows the same comparison groups, evaluation dimensions and success thresholds, providing a holistic, multi-perspective validation of HyProS.

# Base Code (Reference Foundation)
{"evaluate_py": "\"\"\"src/evaluate.py\nAfter all experiment variations have finished, this script consolidates their\nresult files, performs consistent evaluation, and outputs publication-ready\nfigures under \u003cresults_dir\u003e/images/.\n\"\"\"\nfrom __future__ import annotations\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nplt.style.use(\"seaborn-v0_8-paper\")\n\n################################################################################\n# Helpers\n################################################################################\n\ndef _load_results(results_dir: Path) -\u003e List[Dict]:\n    runs = []\n    for run_dir in results_dir.iterdir():\n        if not run_dir.is_dir():\n            continue\n        res_file = run_dir / \"results.json\"\n        if res_file.exists():\n            with res_file.open() as fp:\n                runs.append(json.load(fp))\n    return runs\n\n\ndef _plot_training_curves(runs: List[Dict], results_dir: Path):\n    images_dir = results_dir / \"images\"\n    images_dir.mkdir(parents=True, exist_ok=True)\n\n    # Plot loss curves -----------------------------------------------------\n    fig, ax = plt.subplots(figsize=(6, 4))\n    for run in runs:\n        epochs = [e[\"epoch\"] for e in run[\"metrics\"][\"epoch\"]]\n        losses = [e[\"train\"][\"loss\"] for e in run[\"metrics\"][\"epoch\"]]\n        ax.plot(epochs, losses, label=run[\"run_id\"])\n        # Annotate final value\n        ax.annotate(f\"{losses[-1]:.3f}\", (epochs[-1], losses[-1]), textcoords=\"offset points\", xytext=(5, 0))\n    ax.set_xlabel(\"Epoch\")\n    ax.set_ylabel(\"Training Loss\")\n    ax.set_title(\"Training Loss Curves\")\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    plt.tight_layout()\n    fig_path = images_dir / \"training_loss.pdf\"\n    fig.savefig(fig_path, bbox_inches=\"tight\")\n    plt.close(fig)\n\n    # Plot validation accuracy curves -------------------------------------\n    fig, ax = plt.subplots(figsize=(6, 4))\n    for run in runs:\n        epochs = [e[\"epoch\"] for e in run[\"metrics\"][\"epoch\"]]\n        accs = [e[\"val\"][\"accuracy\"] for e in run[\"metrics\"][\"epoch\"]]\n        ax.plot(epochs, accs, label=run[\"run_id\"])\n        ax.annotate(f\"{accs[-1]*100:.1f}%\", (epochs[-1], accs[-1]), textcoords=\"offset points\", xytext=(5, 0))\n    ax.set_xlabel(\"Epoch\")\n    ax.set_ylabel(\"Validation Accuracy\")\n    ax.set_title(\"Validation Accuracy Curves\")\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    plt.tight_layout()\n    fig_path = images_dir / \"accuracy.pdf\"\n    fig.savefig(fig_path, bbox_inches=\"tight\")\n    plt.close(fig)\n\n\n################################################################################\n# CLI\n################################################################################\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Evaluate and visualise experiment results\")\n    parser.add_argument(\"--results-dir\", type=str, required=True,\n                        help=\"Root directory that contains per-run sub-directories\")\n    args = parser.parse_args()\n    results_dir = Path(args.results_dir)\n\n    runs = _load_results(results_dir)\n    if not runs:\n        raise RuntimeError(f\"No results.json files found in {results_dir}\")\n\n    # Generate figures -----------------------------------------------------\n    _plot_training_curves(runs, results_dir)\n\n    # Consolidated JSON summary -------------------------------------------\n    comparison = {\n        run[\"run_id\"]: {\n            \"final_train_loss\": run[\"metrics\"][\"final\"][\"train\"][\"loss\"],\n            \"final_val_loss\":   run[\"metrics\"][\"final\"][\"val\"][\"loss\"],\n            \"final_val_acc\":    run[\"metrics\"][\"final\"][\"val\"][\"accuracy\"],\n        }\n        for run in runs\n    }\n    print(json.dumps({\"comparison\": comparison}, indent=2))\n\n\nif __name__ == \"__main__\":\n    main()", "full_experiment_yaml": "# config/full_experiment.yaml\n# Placeholder template \u2013 to be populated with actual experiments in the next step.\nexperiments:\n  - run_id: \"RUN_ID_PLACEHOLDER\"\n    model:\n      name: \"MODEL_PLACEHOLDER\"\n      # Example: vocab_size: 32000, hidden_size: 512, num_layers: 6\n    dataset:\n      name: \"DATASET_PLACEHOLDER\"\n      # PLACEHOLDER: Add dataset-specific keys here (e.g., path, seq_len, etc.)\n    training:\n      epochs: EPOCHS_PLACEHOLDER  # e.g., 10\n      batch_size: BATCH_SIZE_PLACEHOLDER  # e.g., 64\n      lr: LR_PLACEHOLDER  # e.g., 3e-4\n    loss:\n      name: \"LOSS_PLACEHOLDER\"  # e.g., nll | brier | hypros\n      # Additional loss-specific hyperparameters here (e.g., t0_ratio, tau_ratio)\n    # Repeat for all desired experimental variations\n    \n# Additional runs can be appended as list items\n# \u2013 ensure each has a unique run_id\n\n# PLACEHOLDER: The derive_specific step will overwrite this file with concrete runs\n", "main_py": "\"\"\"src/main.py\nOrchestrator script.  Executes all experiment variations defined in the YAML\nconfiguration (smoke_test.yaml or full_experiment.yaml), routing logs and results\ninto a common directory structure and invoking evaluation at the end.\n\"\"\"\nfrom __future__ import annotations\nimport argparse\nimport os\nimport sys\nimport subprocess\nimport tempfile\nimport yaml\nimport json\nimport threading\nfrom pathlib import Path\nfrom typing import List, Dict, Any\n\n################################################################################\n# Helpers for tee-like log forwarding\n################################################################################\n\ndef _stream_pipe(pipe, *streams):\n    \"\"\"Forward bytes from pipe to all file-like streams.\"\"\"\n    for line in iter(pipe.readline, b\"\"):\n        for s in streams:\n            try:\n                s.write(line.decode())\n                s.flush()\n            except Exception:\n                pass\n    pipe.close()\n\n################################################################################\n# Subprocess runner\n################################################################################\n\ndef _run_experiment(run_cfg: Dict[str, Any], results_dir: Path):\n    run_id = run_cfg[\"run_id\"]\n    run_dir = results_dir / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    # Write per-run config file ------------------------------------------------\n    tmp_cfg = tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".yaml\", delete=False)\n    yaml.safe_dump(run_cfg, tmp_cfg)\n    tmp_cfg.flush()\n    tmp_cfg_path = tmp_cfg.name\n    tmp_cfg.close()\n\n    # Prepare log files --------------------------------------------------------\n    stdout_file = open(run_dir / \"stdout.log\", \"w\")\n    stderr_file = open(run_dir / \"stderr.log\", \"w\")\n\n    cmd = [sys.executable, \"-m\", \"src.train\", \"--config\", tmp_cfg_path,\n           \"--results-dir\", str(results_dir)]\n    env = os.environ.copy()\n    env[\"PYTHONUNBUFFERED\"] = \"1\"\n    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)\n\n    # Forward output in real-time\n    threads = [\n        threading.Thread(target=_stream_pipe, args=(process.stdout, sys.stdout, stdout_file), daemon=True),\n        threading.Thread(target=_stream_pipe, args=(process.stderr, sys.stderr, stderr_file), daemon=True),\n    ]\n    for t in threads:\n        t.start()\n    for t in threads:\n        t.join()\n\n    retcode = process.wait()\n    stdout_file.close()\n    stderr_file.close()\n    os.remove(tmp_cfg_path)\n\n    if retcode != 0:\n        raise RuntimeError(f\"Run \u0027{run_id}\u0027 exited with code {retcode}\")\n\n################################################################################\n# Main orchestrator\n################################################################################\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Run HyProS experiments\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run smoke_test.yaml\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full_experiment.yaml\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory to store all outputs\")\n    args = parser.parse_args()\n\n    # Resolve configuration file ---------------------------------------------\n    if args.smoke_test:\n        cfg_path = Path(\"config\") / \"smoke_test.yaml\"\n    else:\n        cfg_path = Path(\"config\") / \"full_experiment.yaml\"\n\n    with cfg_path.open() as fp:\n        config = yaml.safe_load(fp)\n\n    experiments: List[Dict[str, Any]] = config.get(\"experiments\", [])\n    if not experiments:\n        raise ValueError(\"No experiments defined in the configuration file\")\n\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    # Sequentially launch runs ----------------------------------------------\n    for run_cfg in experiments:\n        print(json.dumps({\"status\": \"START\", \"run_id\": run_cfg[\"run_id\"]}))\n        _run_experiment(run_cfg, results_dir)\n        print(json.dumps({\"status\": \"END\", \"run_id\": run_cfg[\"run_id\"]}))\n\n    # After all runs, trigger evaluation ------------------------------------\n    eval_cmd = [sys.executable, \"-m\", \"src.evaluate\", \"--results-dir\", str(results_dir)]\n    subprocess.run(eval_cmd, check=True)\n\n\nif __name__ == \"__main__\":\n    main()", "model_py": "\"\"\"src/model.py\nModel architectures and loss functions shared across all experimental runs.\nOnly model-specific hyperparameters will be replaced in the next stage \u2013 the\nimplementations here are *complete* and ready for use.\n\"\"\"\nfrom __future__ import annotations\nimport math\nfrom typing import Dict, Any\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\n################################################################################\n# Model definitions\n################################################################################\n\nclass _BaseLM(nn.Module):\n    \"\"\"A minimal language model: Embedding \u2192 GRU \u2192 Linear.\n    This serves as a stand-in architecture so the infrastructure can run end-to-end.\n    It will be replaced / augmented with task-specific architectures later.\n    \"\"\"\n\n    def __init__(self, vocab_size: int, hidden_size: int = 128, num_layers: int = 1):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, hidden_size)\n        self.rnn   = nn.GRU(hidden_size, hidden_size, num_layers=num_layers, batch_first=True)\n        self.proj  = nn.Linear(hidden_size, vocab_size)\n\n    def forward(self, x):\n        # x: (B, T)\n        h = self.embed(x)\n        h, _ = self.rnn(h)\n        logits = self.proj(h)\n        return logits\n\n\nclass BaselineModel(_BaseLM):\n    pass  # identical for now\n\n\nclass ProposedModel(_BaseLM):\n    pass  # architecture identical \u2013 differs by loss function choice\n\n\nclass AblationModel(_BaseLM):\n    pass  # may be modified later\n\n################################################################################\n# Losses\n################################################################################\n\nclass BrierLoss(nn.Module):\n    \"\"\"Token-level Brier score for language modeling (strictly proper, bounded).\"\"\"\n\n    def __init__(self, vocab_size: int):\n        super().__init__()\n        self.v = vocab_size\n\n    def forward(self, logits: torch.Tensor, targets: torch.Tensor):\n        p = logits.softmax(dim=-1)\n        one_hot = F.one_hot(targets, num_classes=self.v).float()\n        loss = ((p - one_hot) ** 2).sum(dim=-1)  # sum over classes\n        return loss.mean()\n\n\nclass HybridBrierLoss(nn.Module):\n    \"\"\"Hybrid Proper Scoring (HyProS) \u2013 combines log loss \u0026 Brier with annealed \u03bb.\"\"\"\n\n    def __init__(self, vocab_size: int, t0: float, tau: float, total_steps: int):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.t0 = t0\n        self.tau = tau\n        self.total_steps = total_steps\n        self.register_buffer(\"step\", torch.tensor(0, dtype=torch.long))\n\n    def forward(self, logits: torch.Tensor, targets: torch.Tensor):\n        p = logits.softmax(dim=-1)\n        one_hot = F.one_hot(targets, num_classes=self.vocab_size).float()\n        sbrier = ((p - one_hot) ** 2).sum(dim=-1)\n        slog = F.cross_entropy(logits.view(-1, self.vocab_size), targets.view(-1), reduction=\"none\")\n        slog = slog.view_as(sbrier)\n        t = self.step.item()\n        lam = torch.sigmoid((t - self.t0) / self.tau)\n        self.step += 1\n        loss = (1 - lam) * slog + lam * sbrier\n        return loss.mean()\n\n################################################################################\n# Builder helpers\n################################################################################\n\ndef build_model(model_cfg: Dict[str, Any], vocab_size: int) -\u003e nn.Module:\n    name = model_cfg.get(\"name\", \"BaselineModel\")\n    hidden = model_cfg.get(\"hidden_size\", 128)\n    layers = model_cfg.get(\"num_layers\", 1)\n\n    if name == \"BaselineModel\":\n        return BaselineModel(vocab_size, hidden, layers)\n    elif name == \"ProposedModel\":\n        return ProposedModel(vocab_size, hidden, layers)\n    elif name == \"AblationModel\":\n        return AblationModel(vocab_size, hidden, layers)\n    else:\n        raise ValueError(f\"Unknown model name \u0027{name}\u0027\")\n\n\ndef get_loss(loss_cfg: Dict[str, Any], vocab_size: int, total_steps: int) -\u003e nn.Module:\n    name = loss_cfg.get(\"name\", \"nll\").lower()\n    if name in {\"nll\", \"log\", \"cross_entropy\"}:\n        return nn.CrossEntropyLoss()\n    elif name == \"brier\":\n        return BrierLoss(vocab_size)\n    elif name == \"hypros\":\n        hp = loss_cfg.get(\"hypros\", {})\n        t0_ratio = hp.get(\"t0_ratio\", 0.3)\n        tau_ratio = hp.get(\"tau_ratio\", 0.1)\n        t0 = t0_ratio * total_steps\n        tau = max(1.0, tau_ratio * total_steps)\n        return HybridBrierLoss(vocab_size, t0=t0, tau=tau, total_steps=total_steps)\n    else:\n        raise ValueError(f\"Unknown loss name \u0027{name}\u0027\")", "preprocess_py": "\"\"\"src/preprocess.py\nCommon data-loading and preprocessing utilities.\nDataset-specific logic will be *replaced* in the next generation step.  For now we\nprovide a stand-in random dataset so the smoke tests and infrastructure run\nwithout external dependencies.\n\"\"\"\nfrom __future__ import annotations\nimport math\nfrom typing import Tuple, Dict, Any\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\n################################################################################\n# Dummy / placeholder dataset\n################################################################################\n\nclass _RandomLanguageModelingDataset(Dataset):\n    \"\"\"Generates random sequences for language modeling.\n    Each sample is a tuple (input, target) where target is the input shifted left.\n    \"\"\"\n\n    def __init__(self, seq_len: int, vocab_size: int, num_samples: int):\n        super().__init__()\n        self.seq_len = seq_len\n        self.vocab_size = vocab_size\n        self.num_samples = num_samples\n        # Pre-generate to ensure deterministic length/iteration cost\n        self._data = torch.randint(low=0, high=vocab_size, size=(num_samples, seq_len), dtype=torch.long)\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, idx):\n        x = self._data[idx]\n        tgt = torch.roll(x, shifts=-1)\n        return x, tgt\n\n################################################################################\n# Public API\n################################################################################\n\ndef get_dataloaders(config: Dict[str, Any]) -\u003e Tuple[DataLoader, DataLoader, int]:\n    \"\"\"Return (train_loader, val_loader, vocab_size).  Uses placeholder logic unless\n    replaced with dataset-specific implementation.\n    \"\"\"\n    ds_cfg = config[\"dataset\"]\n    name = ds_cfg.get(\"name\", \"DATASET_PLACEHOLDER\")\n\n    if name == \"DATASET_PLACEHOLDER\":\n        # ------------------------------------------------------------------\n        # Placeholder logic \u2013 random synthetic dataset for smoke testing\n        # ------------------------------------------------------------------\n        seq_len     = ds_cfg.get(\"seq_len\", 32)\n        vocab_size  = ds_cfg.get(\"vocab_size\", 100)\n        num_samples = ds_cfg.get(\"num_samples\", 1024)\n        train_size  = math.ceil(num_samples * 0.9)\n        val_size    = num_samples - train_size\n        full_ds     = _RandomLanguageModelingDataset(seq_len, vocab_size, num_samples)\n        train_ds, val_ds = torch.utils.data.random_split(full_ds, [train_size, val_size])\n    else:\n        # ------------------------------------------------------------------\n        # PLACEHOLDER: Replace with dataset-specific loading logic\n        # ------------------------------------------------------------------\n        raise NotImplementedError(\n            \"Dataset loading for \u0027{name}\u0027 not yet implemented \u2013 replace this placeholder\")\n\n    batch_size = config[\"training\"].get(\"batch_size\", 32)\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=0)\n    return train_loader, val_loader, ds_cfg.get(\"vocab_size\", 100)", "pyproject_toml": "[build-system]\nrequires = [\"setuptools\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"hypros-common-core\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for HyProS experimental framework\"\nrequires-python = \"\u003e=3.9\"\ndependencies = [\n    \"torch\u003e=2.0\",\n    \"numpy\",\n    \"pyyaml\",\n    \"matplotlib\",\n    \"seaborn\",\n    \"tqdm\",\n]", "smoke_test_yaml": "# config/smoke_test.yaml\n# Lightweight configuration to verify that the infrastructure executes end-to-end.\nexperiments:\n  - run_id: \"baseline_smoke\"\n    model:\n      name: \"BaselineModel\"\n      vocab_size: 100\n      hidden_size: 32\n    dataset:\n      name: \"DATASET_PLACEHOLDER\"  # will use random synthetic data\n      seq_len: 16\n      vocab_size: 100\n      num_samples: 256\n    training:\n      epochs: 1\n      batch_size: 8\n      lr: 1e-3\n    loss:\n      name: \"nll\"\n\n  - run_id: \"hypros_smoke\"\n    model:\n      name: \"BaselineModel\"\n      vocab_size: 100\n      hidden_size: 32\n    dataset:\n      name: \"DATASET_PLACEHOLDER\"\n      seq_len: 16\n      vocab_size: 100\n      num_samples: 256\n    training:\n      epochs: 1\n      batch_size: 8\n      lr: 1e-3\n    loss:\n      name: \"hypros\"\n      hypros:\n        t0_ratio: 0.3\n        tau_ratio: 0.1\n\n  - run_id: \"brier_smoke\"\n    model:\n      name: \"BaselineModel\"\n      vocab_size: 100\n      hidden_size: 32\n    dataset:\n      name: \"DATASET_PLACEHOLDER\"\n      seq_len: 16\n      vocab_size: 100\n      num_samples: 256\n    training:\n      epochs: 1\n      batch_size: 8\n      lr: 1e-3\n    loss:\n      name: \"brier\"", "train_py": "\"\"\"src/train.py\nTrain a single experiment variation.\nThis script is invoked by src.main as a subprocess. It trains a model, evaluates it on the\nvalidation set, and writes a results.json file containing all relevant metrics.\nThe script is deliberately *self-contained* except for the dataset/model placeholders that will\nbe replaced in a later step.\n\"\"\"\nfrom __future__ import annotations\nimport argparse\nimport json\nimport os\nimport time\nfrom pathlib import Path\nfrom typing import Dict, Any, List\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# Local imports (absolute so that \"python -m src.train\" works)\nfrom src.preprocess import get_dataloaders\nfrom src.model import build_model, get_loss\n\n################################################################################\n# Utility helpers\n################################################################################\n\ndef _token_accuracy(logits: torch.Tensor, targets: torch.Tensor) -\u003e float:\n    \"\"\"Compute token-level accuracy for language-model style logits.\n    logits: (B, T, V); targets: (B, T)\"\"\"\n    preds = logits.argmax(-1)\n    return (preds == targets).float().mean().item()\n\n\ndef _epoch_stats_to_json(epoch: int, train_loss: float, train_acc: float,\n                         val_loss: float, val_acc: float) -\u003e Dict[str, Any]:\n    return {\n        \"epoch\": epoch,\n        \"train\": {\"loss\": train_loss, \"accuracy\": train_acc},\n        \"val\":   {\"loss\": val_loss,   \"accuracy\": val_acc},\n    }\n\n################################################################################\n# Training routine\n################################################################################\n\ndef train(config: Dict[str, Any], results_dir: Path) -\u003e Dict[str, Any]:\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    run_id: str = config[\"run_id\"]\n    training_cfg = config[\"training\"]\n\n    # ---------------------------------------------------------------------\n    # Data\n    # ---------------------------------------------------------------------\n    train_loader, val_loader, vocab_size = get_dataloaders(config)\n\n    # ---------------------------------------------------------------------\n    # Model \u0026 loss\n    # ---------------------------------------------------------------------\n    model: nn.Module = build_model(config[\"model\"], vocab_size=vocab_size).to(device)\n    total_steps_estimate = len(train_loader) * training_cfg[\"epochs\"]\n    criterion: nn.Module = get_loss(config[\"loss\"], vocab_size=vocab_size,\n                                    total_steps=total_steps_estimate).to(device)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=training_cfg.get(\"lr\", 3e-4))\n\n    # ---------------------------------------------------------------------\n    # Training loop\n    # ---------------------------------------------------------------------\n    history: List[Dict[str, Any]] = []\n    global_step = 0\n    model.train()\n    for epoch in range(1, training_cfg[\"epochs\"] + 1):\n        epoch_loss, epoch_acc, n_tokens = 0.0, 0.0, 0\n        pbar = tqdm(train_loader, desc=f\"[{run_id}] Epoch {epoch}/{training_cfg[\u0027epochs\u0027]}\")\n        for inputs, targets in pbar:\n            inputs, targets = inputs.to(device), targets.to(device)\n            logits = model(inputs)\n            loss = criterion(logits, targets)\n            optimizer.zero_grad(set_to_none=True)\n            loss.backward()\n            optimizer.step()\n\n            with torch.no_grad():\n                acc = _token_accuracy(logits, targets)\n            batch_tokens = inputs.numel()\n            epoch_loss += loss.item() * batch_tokens\n            epoch_acc  += acc * batch_tokens\n            n_tokens   += batch_tokens\n            global_step += 1\n            pbar.set_postfix({\"loss\": loss.item(), \"acc\": acc})\n        train_loss = epoch_loss / n_tokens\n        train_acc  = epoch_acc  / n_tokens\n\n        # -----------------------------------------------------------------\n        # Validation\n        # -----------------------------------------------------------------\n        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n        history.append(_epoch_stats_to_json(epoch, train_loss, train_acc, val_loss, val_acc))\n\n    # ---------------------------------------------------------------------\n    # Save artefacts\n    # ---------------------------------------------------------------------\n    run_dir = results_dir / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n    ckpt_path = run_dir / \"model.pt\"\n    torch.save({\n        \"model_state\": model.state_dict(),\n        \"config\": config,\n    }, ckpt_path)\n\n    # Consolidate metrics\n    final_metrics = history[-1]\n    results = {\n        \"run_id\": run_id,\n        \"config\": config,\n        \"metrics\": {\n            \"epoch\": history,\n            \"final\": final_metrics\n        }\n    }\n    with (run_dir / \"results.json\").open(\"w\") as fp:\n        json.dump(results, fp, indent=2)\n    return results\n\n################################################################################\n# Validation routine\n################################################################################\n\ndef evaluate(model: nn.Module, loader: DataLoader, criterion: nn.Module, device: torch.device):\n    model.eval()\n    total_loss, total_acc, n_tokens = 0.0, 0.0, 0\n    with torch.no_grad():\n        for inputs, targets in loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            logits = model(inputs)\n            loss = criterion(logits, targets)\n            acc  = _token_accuracy(logits, targets)\n            batch_tokens = inputs.numel()\n            total_loss += loss.item() * batch_tokens\n            total_acc  += acc * batch_tokens\n            n_tokens   += batch_tokens\n    model.train()\n    return total_loss / n_tokens, total_acc / n_tokens\n\n################################################################################\n# CLI\n################################################################################\n\ndef parse_args() -\u003e argparse.Namespace:\n    parser = argparse.ArgumentParser(description=\"Train a single experimental run\")\n    parser.add_argument(\"--config\", type=str, required=True,\n                        help=\"Path to the YAML configuration file for this run\")\n    parser.add_argument(\"--results-dir\", type=str, required=True,\n                        help=\"Directory where results will be stored (common root)\")\n    return parser.parse_args()\n\n\nif __name__ == \"__main__\":\n    import yaml\n\n    args = parse_args()\n    with open(args.config, \"r\") as fp:\n        run_cfg = yaml.safe_load(fp)\n\n    root_dir = Path(args.results_dir)\n    root_dir.mkdir(parents=True, exist_ok=True)\n\n    start = time.time()\n    outcomes = train(run_cfg, root_dir)\n    duration = time.time() - start\n    outcomes[\"wall_clock_seconds\"] = duration\n\n    # Print structured JSON to stdout for main process \u0026 logs.\n    print(json.dumps(outcomes))"}

# Current Experiment (To be validated)
- Experiment ID: exp-2-robust-eff
- Description: Objective / hypothesis:
Test whether the gains of HyProS generalise to different NLP tasks and under distribution shift while satisfying the efficiency constraint C1.  We focus on summarisation and sentence-level classification, and subject models to input perturbations, domain transfer and adversarial triggers.  Additionally, we measure inference-time cost to verify negligible overhead.

Tasks & Models:
• Summarisation: BART-Base (140 M) on CNN/DailyMail → evaluate ROUGE-1/2/L.
• Text classification: RoBERTa-Base (125 M) fine-tuned on SST-2 → evaluate accuracy / F1.
Loss is applied at the token level for decoder-side (summarisation) and at the masked-token level for CLS-representation training (classification).

Datasets & splits:
1. CNN/DailyMail v3.0 → standard train/val/test.
2. SST-2 (GLUE) → train (67 k), dev (872), test (1.8 k, no labels – use dev for reporting).
   OOD sets:
   • XSum (summarisation domain shift)
   • IMDb movie reviews (classification domain shift)

Pre-processing:
• Original BART/RoBERTa tokenisers.
• Max sequence lengths: 512 (CNN/DM), 128 (SST-2).

Training:
• BART: label-smoothing 0.1, Adam, lr 3e-5, batch 256 tokens, 30k updates.
• RoBERTa: AdamW, lr 1e-5, batch 64 sentences, 10 epochs.
• Early stopping on val ROUGE-L / accuracy.
• Seeds: 3.
• Selection: best val metric checkpoint.

Run variations:
1. log-loss: standard objective.
2. brier: bounded score.
3. hypros: sigmoid schedule as defined.
4. hypros-noanneal: λ schedule ends by returning to 0 after peaking at 0.5 (tests late-stage reliance on Brier).

Evaluation metrics:
Summarisation – Primary: ROUGE-L; Secondary: ROUGE-1/2, Brier, ECE.
Classification – Primary: Accuracy; Secondary: F1, Brier, ECE.
Efficiency – Inference tokens/s (BART) or sentences/s (RoBERTa), average latency, memory footprint.

Robustness protocol:
A. Noise injection – randomly swap 5 % word positions.
B. Adversarial triggers – add universal sentiment shifter phrase (classification) or distractor sentence (summarisation); measure metric drop.
C. OOD – evaluate BART model fine-tuned on CNN/DM directly on XSum; evaluate RoBERTa on IMDb.

Hyper-parameter diagnostics:
• For HyProS runs, sweep λ_max ∈ {0.7,1.0} and τ ∈ {0.05,0.15} T on validation only; plot metric surface.

Compute logging:
• WandB system metrics + NVIDIA-SMI every minute for VRAM & power.
• Torch-profiler for per-batch FLOPs and kernel time.

Example experimental snippet (classification):
```
loss_fn = HybridBrierLoss(vocab, 0.1*total, 0.05*total)
for batch in loader:
    logits = model(batch.input_ids)
    token_loss = loss_fn(logits, batch.labels.expand_as(logits.argmax(-1)))
    cls_loss = token_loss[:,0].mean(); cls_loss.backward()
```

Expected results:
• Summarisation: ROUGE-L – log 44.2, brier 43.6, hypros 44.7 (+0.5), hypros-noanneal 44.0.
• Classification: Accuracy – log 94.1 %, brier 93.8 %, hypros 94.4 %, hypros-noanneal 94.0 %.
• Calibration: HyProS lowers ECE by ≈20 % vs log across both tasks.
• Robustness: HyProS shows ≤1 % extra degradation vs Brier and <½ degradation of log-loss.
• Inference speed change <0.5 % relative to log-loss.

Branch name: exp-2-robust-eff.
- Run Variations: ['log-loss', 'brier', 'hypros', 'hypros-noanneal']

# Derived Experiment Code (To be validated)

{"evaluate_py": "\"\"\"src/evaluate.py\nAfter all experiment variations have finished, this script consolidates their\nresult files, performs consistent evaluation, and outputs publication-ready\nfigures under \u003cresults_dir\u003e/images/.\n\"\"\"\nfrom __future__ import annotations\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nplt.style.use(\"seaborn-v0_8-paper\")\n\n################################################################################\n# Helpers\n################################################################################\n\ndef _load_results(results_dir: Path) -\u003e List[Dict]:\n    runs = []\n    for run_dir in results_dir.iterdir():\n        if not run_dir.is_dir():\n            continue\n        res_file = run_dir / \"results.json\"\n        if res_file.exists():\n            with res_file.open() as fp:\n                runs.append(json.load(fp))\n    return runs\n\n\ndef _plot_training_curves(runs: List[Dict], results_dir: Path):\n    images_dir = results_dir / \"images\"\n    images_dir.mkdir(parents=True, exist_ok=True)\n\n    # Plot loss curves -----------------------------------------------------\n    fig, ax = plt.subplots(figsize=(6, 4))\n    for run in runs:\n        epochs = [e[\"epoch\"] for e in run[\"metrics\"][\"epoch\"]]\n        losses = [e[\"train\"][\"loss\"] for e in run[\"metrics\"][\"epoch\"]]\n        ax.plot(epochs, losses, label=run[\"run_id\"])\n        # Annotate final value\n        ax.annotate(f\"{losses[-1]:.3f}\", (epochs[-1], losses[-1]), textcoords=\"offset points\", xytext=(5, 0))\n    ax.set_xlabel(\"Epoch\")\n    ax.set_ylabel(\"Training Loss\")\n    ax.set_title(\"Training Loss Curves\")\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    plt.tight_layout()\n    fig_path = images_dir / \"training_loss.pdf\"\n    fig.savefig(fig_path, bbox_inches=\"tight\")\n    plt.close(fig)\n\n    # Plot validation accuracy curves -------------------------------------\n    fig, ax = plt.subplots(figsize=(6, 4))\n    for run in runs:\n        epochs = [e[\"epoch\"] for e in run[\"metrics\"][\"epoch\"]]\n        accs = [e[\"val\"][\"accuracy\"] for e in run[\"metrics\"][\"epoch\"]]\n        ax.plot(epochs, accs, label=run[\"run_id\"])\n        ax.annotate(f\"{accs[-1]*100:.1f}%\", (epochs[-1], accs[-1]), textcoords=\"offset points\", xytext=(5, 0))\n    ax.set_xlabel(\"Epoch\")\n    ax.set_ylabel(\"Validation Accuracy\")\n    ax.set_title(\"Validation Accuracy Curves\")\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    plt.tight_layout()\n    fig_path = images_dir / \"accuracy.pdf\"\n    fig.savefig(fig_path, bbox_inches=\"tight\")\n    plt.close(fig)\n\n\n################################################################################\n# CLI\n################################################################################\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Evaluate and visualise experiment results\")\n    parser.add_argument(\"--results-dir\", type=str, required=True,\n                        help=\"Root directory that contains per-run sub-directories\")\n    args = parser.parse_args()\n    results_dir = Path(args.results_dir)\n\n    runs = _load_results(results_dir)\n    if not runs:\n        raise RuntimeError(f\"No results.json files found in {results_dir}\")\n\n    # Generate figures -----------------------------------------------------\n    _plot_training_curves(runs, results_dir)\n\n    # Consolidated JSON summary -------------------------------------------\n    comparison = {\n        run[\"run_id\"]: {\n            \"final_train_loss\": run[\"metrics\"][\"final\"][\"train\"][\"loss\"],\n            \"final_val_loss\":   run[\"metrics\"][\"final\"][\"val\"][\"loss\"],\n            \"final_val_acc\":    run[\"metrics\"][\"final\"][\"val\"][\"accuracy\"],\n        }\n        for run in runs\n    }\n    print(json.dumps({\"comparison\": comparison}, indent=2))\n\n\nif __name__ == \"__main__\":\n    main()", "full_experiment_yaml": "# config/full_experiment.yaml\n# Concrete run list for experiment ID exp-2-robust-eff.\n# We evaluate four loss variants on two tasks, resulting in eight runs.\n\nexperiments:\n  ##############################################################################\n  # Summarisation \u2013 CNN/DailyMail\n  ##############################################################################\n  - run_id: \"cnndm-log-loss\"\n    model:\n      name: \"TransformerLM\"\n      hidden_size: 256\n      num_layers: 4\n      num_heads: 4\n    dataset:\n      name: \"cnn_dailymail\"\n      seq_len: 128\n      max_train_samples: 5000   # keeps runtime manageable\n      max_val_samples: 500\n    training:\n      epochs: 1\n      batch_size: 16\n      lr: 3e-4\n    loss:\n      name: \"nll\"\n\n  - run_id: \"cnndm-brier\"\n    model:\n      name: \"TransformerLM\"\n      hidden_size: 256\n      num_layers: 4\n      num_heads: 4\n    dataset:\n      name: \"cnn_dailymail\"\n      seq_len: 128\n      max_train_samples: 5000\n      max_val_samples: 500\n    training:\n      epochs: 1\n      batch_size: 16\n      lr: 3e-4\n    loss:\n      name: \"brier\"\n\n  - run_id: \"cnndm-hypros\"\n    model:\n      name: \"TransformerLM\"\n      hidden_size: 256\n      num_layers: 4\n      num_heads: 4\n    dataset:\n      name: \"cnn_dailymail\"\n      seq_len: 128\n      max_train_samples: 5000\n      max_val_samples: 500\n    training:\n      epochs: 1\n      batch_size: 16\n      lr: 3e-4\n    loss:\n      name: \"hypros\"\n      t0_ratio: 0.3\n      tau_ratio: 0.1\n\n  - run_id: \"cnndm-hypros-noanneal\"\n    model:\n      name: \"TransformerLM\"\n      hidden_size: 256\n      num_layers: 4\n      num_heads: 4\n    dataset:\n      name: \"cnn_dailymail\"\n      seq_len: 128\n      max_train_samples: 5000\n      max_val_samples: 500\n    training:\n      epochs: 1\n      batch_size: 16\n      lr: 3e-4\n    loss:\n      name: \"hypros-noanneal\"\n      peak_lambda: 0.5\n\n  ##############################################################################\n  # Sentiment classification \u2013 SST-2 (treated as LM over sentences)\n  ##############################################################################\n  - run_id: \"sst2-log-loss\"\n    model:\n      name: \"TransformerLM\"\n      hidden_size: 256\n      num_layers: 4\n      num_heads: 4\n    dataset:\n      name: \"sst2\"\n      seq_len: 128\n      max_train_samples: 10000\n      max_val_samples: 1000\n    training:\n      epochs: 3\n      batch_size: 32\n      lr: 3e-4\n    loss:\n      name: \"nll\"\n\n  - run_id: \"sst2-brier\"\n    model:\n      name: \"TransformerLM\"\n      hidden_size: 256\n      num_layers: 4\n      num_heads: 4\n    dataset:\n      name: \"sst2\"\n      seq_len: 128\n      max_train_samples: 10000\n      max_val_samples: 1000\n    training:\n      epochs: 3\n      batch_size: 32\n      lr: 3e-4\n    loss:\n      name: \"brier\"\n\n  - run_id: \"sst2-hypros\"\n    model:\n      name: \"TransformerLM\"\n      hidden_size: 256\n      num_layers: 4\n      num_heads: 4\n    dataset:\n      name: \"sst2\"\n      seq_len: 128\n      max_train_samples: 10000\n      max_val_samples: 1000\n    training:\n      epochs: 3\n      batch_size: 32\n      lr: 3e-4\n    loss:\n      name: \"hypros\"\n      t0_ratio: 0.3\n      tau_ratio: 0.1\n\n  - run_id: \"sst2-hypros-noanneal\"\n    model:\n      name: \"TransformerLM\"\n      hidden_size: 256\n      num_layers: 4\n      num_heads: 4\n    dataset:\n      name: \"sst2\"\n      seq_len: 128\n      max_train_samples: 10000\n      max_val_samples: 1000\n    training:\n      epochs: 3\n      batch_size: 32\n      lr: 3e-4\n    loss:\n      name: \"hypros-noanneal\"\n      peak_lambda: 0.5\n\n# End of experiment list\n# -----------------------------------------------------------------------------\n# Each run is fully specified and executable without further modification.\n# -----------------------------------------------------------------------------\n", "main_py": "\"\"\"src/main.py\nOrchestrator script.  Executes all experiment variations defined in the YAML\nconfiguration (smoke_test.yaml or full_experiment.yaml), routing logs and results\ninto a common directory structure and invoking evaluation at the end.\n\"\"\"\nfrom __future__ import annotations\nimport argparse\nimport os\nimport sys\nimport subprocess\nimport tempfile\nimport yaml\nimport json\nimport threading\nfrom pathlib import Path\nfrom typing import List, Dict, Any\n\n################################################################################\n# Helpers for tee-like log forwarding\n################################################################################\n\ndef _stream_pipe(pipe, *streams):\n    \"\"\"Forward bytes from pipe to all file-like streams.\"\"\"\n    for line in iter(pipe.readline, b\"\"):\n        for s in streams:\n            try:\n                s.write(line.decode())\n                s.flush()\n            except Exception:\n                pass\n    pipe.close()\n\n################################################################################\n# Subprocess runner\n################################################################################\n\ndef _run_experiment(run_cfg: Dict[str, Any], results_dir: Path):\n    run_id = run_cfg[\"run_id\"]\n    run_dir = results_dir / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    # Write per-run config file ------------------------------------------------\n    tmp_cfg = tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".yaml\", delete=False)\n    yaml.safe_dump(run_cfg, tmp_cfg)\n    tmp_cfg.flush()\n    tmp_cfg_path = tmp_cfg.name\n    tmp_cfg.close()\n\n    # Prepare log files --------------------------------------------------------\n    stdout_file = open(run_dir / \"stdout.log\", \"w\")\n    stderr_file = open(run_dir / \"stderr.log\", \"w\")\n\n    cmd = [sys.executable, \"-m\", \"src.train\", \"--config\", tmp_cfg_path,\n           \"--results-dir\", str(results_dir)]\n    env = os.environ.copy()\n    env[\"PYTHONUNBUFFERED\"] = \"1\"\n    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)\n\n    # Forward output in real-time\n    threads = [\n        threading.Thread(target=_stream_pipe, args=(process.stdout, sys.stdout, stdout_file), daemon=True),\n        threading.Thread(target=_stream_pipe, args=(process.stderr, sys.stderr, stderr_file), daemon=True),\n    ]\n    for t in threads:\n        t.start()\n    for t in threads:\n        t.join()\n\n    retcode = process.wait()\n    stdout_file.close()\n    stderr_file.close()\n    os.remove(tmp_cfg_path)\n\n    if retcode != 0:\n        raise RuntimeError(f\"Run \u0027{run_id}\u0027 exited with code {retcode}\")\n\n################################################################################\n# Main orchestrator\n################################################################################\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Run HyProS experiments\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run smoke_test.yaml\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full_experiment.yaml\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory to store all outputs\")\n    args = parser.parse_args()\n\n    # Resolve configuration file ---------------------------------------------\n    if args.smoke_test:\n        cfg_path = Path(\"config\") / \"smoke_test.yaml\"\n    else:\n        cfg_path = Path(\"config\") / \"full_experiment.yaml\"\n\n    with cfg_path.open() as fp:\n        config = yaml.safe_load(fp)\n\n    experiments: List[Dict[str, Any]] = config.get(\"experiments\", [])\n    if not experiments:\n        raise ValueError(\"No experiments defined in the configuration file\")\n\n    results_dir = Path(args.results_dir)\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    # Sequentially launch runs ----------------------------------------------\n    for run_cfg in experiments:\n        print(json.dumps({\"status\": \"START\", \"run_id\": run_cfg[\"run_id\"]}))\n        _run_experiment(run_cfg, results_dir)\n        print(json.dumps({\"status\": \"END\", \"run_id\": run_cfg[\"run_id\"]}))\n\n    # After all runs, trigger evaluation ------------------------------------\n    eval_cmd = [sys.executable, \"-m\", \"src.evaluate\", \"--results-dir\", str(results_dir)]\n    subprocess.run(eval_cmd, check=True)\n\n\nif __name__ == \"__main__\":\n    main()", "model_py": "\"\"\"src/model.py\nModel architectures and loss functions for all experimental runs.\nThe models are implemented *from scratch* \u2013 no pretrained weights are used to\navoid external dependencies during automated execution.\n\"\"\"\nfrom __future__ import annotations\nfrom typing import Dict, Any\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\n################################################################################\n# Transformer language model \u2013 suitable for both summarisation (on summaries)\n# and generic language-model objectives used in the experiments.\n################################################################################\n\nclass TransformerLM(nn.Module):\n    \"\"\"A small Transformer-decoder language model.\n    Architecture: token embeddings + positional embeddings \u2192 N \u00d7 Transformer\n    decoder blocks \u2192 linear projection.\n    \"\"\"\n\n    def __init__(self, vocab_size: int, hidden_size: int = 256, num_layers: int = 4,\n                 num_heads: int = 4, dropout: float = 0.1):\n        super().__init__()\n        self.embed_tokens = nn.Embedding(vocab_size, hidden_size)\n        self.pos_emb      = nn.Embedding(1024, hidden_size)\n        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_size, nhead=num_heads,\n                                                   dim_feedforward=hidden_size * 4, dropout=dropout,\n                                                   activation=\"gelu\")\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.proj = nn.Linear(hidden_size, vocab_size)\n        self._init_parameters()\n\n    def _init_parameters(self):\n        nn.init.normal_(self.embed_tokens.weight, mean=0.0, std=0.02)\n        nn.init.normal_(self.pos_emb.weight,      mean=0.0, std=0.02)\n        nn.init.normal_(self.proj.weight,        mean=0.0, std=0.02)\n        nn.init.zeros_(self.proj.bias)\n\n    def forward(self, input_ids: torch.Tensor):  # input_ids: (B, T)\n        B, T = input_ids.shape\n        positions = torch.arange(T, device=input_ids.device).unsqueeze(0).expand(B, T)\n        x = self.embed_tokens(input_ids) + self.pos_emb(positions)\n        # Transformer expects (T, B, E)\n        x = self.transformer(x.transpose(0, 1)).transpose(0, 1)\n        logits = self.proj(x)\n        return logits\n\n################################################################################\n# Loss definitions \u2013 strictly proper scores and hybrids\n################################################################################\n\nclass BrierLoss(nn.Module):\n    \"\"\"Token-level Brier (squared error) score.\"\"\"\n\n    def __init__(self, vocab_size: int):\n        super().__init__()\n        self.v = vocab_size\n\n    def forward(self, logits: torch.Tensor, targets: torch.Tensor):\n        probs = logits.softmax(dim=-1)\n        one_hot = F.one_hot(targets, num_classes=self.v).float()\n        loss = ((probs - one_hot) ** 2).sum(dim=-1)\n        return loss.mean()\n\n\nclass HybridBrierLoss(nn.Module):\n    \"\"\"Hybrid Proper Scoring (HyProS) with sigmoid annealing.\"\"\"\n\n    def __init__(self, vocab_size: int, t0: float, tau: float, total_steps: int):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.t0 = float(t0)\n        self.tau = float(tau)\n        self.total_steps = total_steps\n        self.register_buffer(\"step\", torch.tensor(0.0))\n\n    def _lambda(self):\n        t = self.step.item()\n        return torch.sigmoid(torch.tensor((t - self.t0) / self.tau, device=self.step.device))\n\n    def forward(self, logits: torch.Tensor, targets: torch.Tensor):\n        probs = logits.softmax(dim=-1)\n        one_hot = F.one_hot(targets, num_classes=self.vocab_size).float()\n        brier = ((probs - one_hot) ** 2).sum(dim=-1)\n        nll = F.cross_entropy(logits.view(-1, self.vocab_size), targets.view(-1), reduction=\"none\").view_as(brier)\n        lam = self._lambda()\n        self.step += 1.0\n        return ((1 - lam) * nll + lam * brier).mean()\n\n\nclass HybridBrierLossNoAnneal(nn.Module):\n    \"\"\"HyProS variant that returns to log-loss after peaking (ablative).\"\"\"\n\n    def __init__(self, vocab_size: int, total_steps: int, peak_lambda: float = 0.5):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.total = total_steps\n        self.peak = peak_lambda\n        self.register_buffer(\"step\", torch.tensor(0))\n\n    def _lambda(self):\n        # Piece-wise triangular: rises to peak at 0.5 *T, then falls to 0.\n        t = self.step.item()\n        half = self.total / 2\n        if t \u003c= half:\n            lam = self.peak * (t / half)\n        else:\n            lam = self.peak * max(0.0, (self.total - t) / half)\n        return torch.tensor(lam, device=self.step.device)\n\n    def forward(self, logits: torch.Tensor, targets: torch.Tensor):\n        brier = ((logits.softmax(-1) - F.one_hot(targets, self.vocab_size).float()) ** 2).sum(-1)\n        nll = F.cross_entropy(logits.view(-1, self.vocab_size), targets.view(-1), reduction=\"none\").view_as(brier)\n        lam = self._lambda()\n        self.step += 1\n        return ((1 - lam) * nll + lam * brier).mean()\n\n################################################################################\n# Registry helpers\n################################################################################\n\ndef build_model(model_cfg: Dict[str, Any], vocab_size: int) -\u003e nn.Module:\n    name = model_cfg.get(\"name\", \"TransformerLM\")\n    hidden = model_cfg.get(\"hidden_size\", 256)\n    layers = model_cfg.get(\"num_layers\", 4)\n    heads  = model_cfg.get(\"num_heads\", 4)\n\n    if name == \"TransformerLM\":\n        return TransformerLM(vocab_size, hidden_size=hidden, num_layers=layers, num_heads=heads)\n    else:\n        raise ValueError(f\"Unknown model name \u0027{name}\u0027 \u2013 check configuration.\")\n\n\ndef get_loss(loss_cfg: Dict[str, Any], vocab_size: int, total_steps: int):\n    name = loss_cfg.get(\"name\", \"nll\").lower()\n    if name in {\"nll\", \"log\", \"cross_entropy\"}:\n        ignore = loss_cfg.get(\"ignore_index\", -100)\n        return nn.CrossEntropyLoss(ignore_index=ignore)\n    if name == \"brier\":\n        return BrierLoss(vocab_size)\n    if name == \"hypros\":\n        t0_ratio  = loss_cfg.get(\"t0_ratio\", 0.3)\n        tau_ratio = loss_cfg.get(\"tau_ratio\", 0.1)\n        return HybridBrierLoss(vocab_size, t0=t0_ratio * total_steps, tau=max(1, tau_ratio * total_steps),\n                               total_steps=total_steps)\n    if name in {\"hypros-noanneal\", \"hypros_noanneal\"}:\n        peak = loss_cfg.get(\"peak_lambda\", 0.5)\n        return HybridBrierLossNoAnneal(vocab_size, total_steps=total_steps, peak_lambda=peak)\n    raise ValueError(f\"Unknown loss \u0027{name}\u0027.\")", "preprocess_py": "\"\"\"src/preprocess.py\nCommon data-loading and preprocessing utilities.\nDataset-specific logic will be *replaced* in the next generation step.  For now we\nprovide a stand-in random dataset so the smoke tests and infrastructure run\nwithout external dependencies.\n\"\"\"\nfrom __future__ import annotations\nimport math\nfrom typing import Tuple, Dict, Any\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\n################################################################################\n# Dummy / placeholder dataset\n################################################################################\n\nclass _RandomLanguageModelingDataset(Dataset):\n    \"\"\"Generates random sequences for language modeling.\n    Each sample is a tuple (input, target) where target is the input shifted left.\n    \"\"\"\n\n    def __init__(self, seq_len: int, vocab_size: int, num_samples: int):\n        super().__init__()\n        self.seq_len = seq_len\n        self.vocab_size = vocab_size\n        self.num_samples = num_samples\n        # Pre-generate to ensure deterministic length/iteration cost\n        self._data = torch.randint(low=0, high=vocab_size, size=(num_samples, seq_len), dtype=torch.long)\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, idx):\n        x = self._data[idx]\n        tgt = torch.roll(x, shifts=-1)\n        return x, tgt\n\n################################################################################\n# Public API\n################################################################################\n\ndef get_dataloaders(config: Dict[str, Any]) -\u003e Tuple[DataLoader, DataLoader, int]:\n    \"\"\"Return (train_loader, val_loader, vocab_size).  Uses placeholder logic unless\n    replaced with dataset-specific implementation.\n    \"\"\"\n    ds_cfg = config[\"dataset\"]\n    name = ds_cfg.get(\"name\", \"DATASET_PLACEHOLDER\")\n\n    if name == \"DATASET_PLACEHOLDER\":\n        # ------------------------------------------------------------------\n        # Placeholder logic \u2013 random synthetic dataset for smoke testing\n        # ------------------------------------------------------------------\n        seq_len     = ds_cfg.get(\"seq_len\", 32)\n        vocab_size  = ds_cfg.get(\"vocab_size\", 100)\n        num_samples = ds_cfg.get(\"num_samples\", 1024)\n        train_size  = math.ceil(num_samples * 0.9)\n        val_size    = num_samples - train_size\n        full_ds     = _RandomLanguageModelingDataset(seq_len, vocab_size, num_samples)\n        train_ds, val_ds = torch.utils.data.random_split(full_ds, [train_size, val_size])\n    else:\n        # ------------------------------------------------------------------\n        # PLACEHOLDER: Replace with dataset-specific loading logic\n        # ------------------------------------------------------------------\n        raise NotImplementedError(\n            \"Dataset loading for \u0027{name}\u0027 not yet implemented \u2013 replace this placeholder\")\n\n    batch_size = config[\"training\"].get(\"batch_size\", 32)\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=0)\n    return train_loader, val_loader, ds_cfg.get(\"vocab_size\", 100)", "pyproject_toml": "[build-system]\nrequires = [\"setuptools\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"hypros-common-core\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for HyProS experimental framework\"\nrequires-python = \"\u003e=3.9\"\ndependencies = [\n    \"torch\u003e=2.0\",\n    \"numpy\",\n    \"pyyaml\",\n    \"matplotlib\",\n    \"seaborn\",\n    \"tqdm\",\n    \"datasets\u003e=2.17.0\",\n    \"transformers\u003e=4.38.0\",\n]", "smoke_test_yaml": "# config/smoke_test.yaml\n# Lightweight configuration to verify that the infrastructure executes end-to-end.\nexperiments:\n  - run_id: \"baseline_smoke\"\n    model:\n      name: \"BaselineModel\"\n      vocab_size: 100\n      hidden_size: 32\n    dataset:\n      name: \"DATASET_PLACEHOLDER\"  # will use random synthetic data\n      seq_len: 16\n      vocab_size: 100\n      num_samples: 256\n    training:\n      epochs: 1\n      batch_size: 8\n      lr: 1e-3\n    loss:\n      name: \"nll\"\n\n  - run_id: \"hypros_smoke\"\n    model:\n      name: \"BaselineModel\"\n      vocab_size: 100\n      hidden_size: 32\n    dataset:\n      name: \"DATASET_PLACEHOLDER\"\n      seq_len: 16\n      vocab_size: 100\n      num_samples: 256\n    training:\n      epochs: 1\n      batch_size: 8\n      lr: 1e-3\n    loss:\n      name: \"hypros\"\n      hypros:\n        t0_ratio: 0.3\n        tau_ratio: 0.1\n\n  - run_id: \"brier_smoke\"\n    model:\n      name: \"BaselineModel\"\n      vocab_size: 100\n      hidden_size: 32\n    dataset:\n      name: \"DATASET_PLACEHOLDER\"\n      seq_len: 16\n      vocab_size: 100\n      num_samples: 256\n    training:\n      epochs: 1\n      batch_size: 8\n      lr: 1e-3\n    loss:\n      name: \"brier\"", "train_py": "\"\"\"src/train.py\nTrain a single experiment variation.\nThis script is invoked by src.main as a subprocess. It trains a model, evaluates it on the\nvalidation set, and writes a results.json file containing all relevant metrics.\nThe script is deliberately *self-contained* except for the dataset/model placeholders that will\nbe replaced in a later step.\n\"\"\"\nfrom __future__ import annotations\nimport argparse\nimport json\nimport os\nimport time\nfrom pathlib import Path\nfrom typing import Dict, Any, List\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# Local imports (absolute so that \"python -m src.train\" works)\nfrom src.preprocess import get_dataloaders\nfrom src.model import build_model, get_loss\n\n################################################################################\n# Utility helpers\n################################################################################\n\ndef _token_accuracy(logits: torch.Tensor, targets: torch.Tensor) -\u003e float:\n    \"\"\"Compute token-level accuracy for language-model style logits.\n    logits: (B, T, V); targets: (B, T)\"\"\"\n    preds = logits.argmax(-1)\n    return (preds == targets).float().mean().item()\n\n\ndef _epoch_stats_to_json(epoch: int, train_loss: float, train_acc: float,\n                         val_loss: float, val_acc: float) -\u003e Dict[str, Any]:\n    return {\n        \"epoch\": epoch,\n        \"train\": {\"loss\": train_loss, \"accuracy\": train_acc},\n        \"val\":   {\"loss\": val_loss,   \"accuracy\": val_acc},\n    }\n\n################################################################################\n# Training routine\n################################################################################\n\ndef train(config: Dict[str, Any], results_dir: Path) -\u003e Dict[str, Any]:\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    run_id: str = config[\"run_id\"]\n    training_cfg = config[\"training\"]\n\n    # ---------------------------------------------------------------------\n    # Data\n    # ---------------------------------------------------------------------\n    train_loader, val_loader, vocab_size = get_dataloaders(config)\n\n    # ---------------------------------------------------------------------\n    # Model \u0026 loss\n    # ---------------------------------------------------------------------\n    model: nn.Module = build_model(config[\"model\"], vocab_size=vocab_size).to(device)\n    total_steps_estimate = len(train_loader) * training_cfg[\"epochs\"]\n    criterion: nn.Module = get_loss(config[\"loss\"], vocab_size=vocab_size,\n                                    total_steps=total_steps_estimate).to(device)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=training_cfg.get(\"lr\", 3e-4))\n\n    # ---------------------------------------------------------------------\n    # Training loop\n    # ---------------------------------------------------------------------\n    history: List[Dict[str, Any]] = []\n    global_step = 0\n    model.train()\n    for epoch in range(1, training_cfg[\"epochs\"] + 1):\n        epoch_loss, epoch_acc, n_tokens = 0.0, 0.0, 0\n        pbar = tqdm(train_loader, desc=f\"[{run_id}] Epoch {epoch}/{training_cfg[\u0027epochs\u0027]}\")\n        for inputs, targets in pbar:\n            inputs, targets = inputs.to(device), targets.to(device)\n            logits = model(inputs)\n            loss = criterion(logits, targets)\n            optimizer.zero_grad(set_to_none=True)\n            loss.backward()\n            optimizer.step()\n\n            with torch.no_grad():\n                acc = _token_accuracy(logits, targets)\n            batch_tokens = inputs.numel()\n            epoch_loss += loss.item() * batch_tokens\n            epoch_acc  += acc * batch_tokens\n            n_tokens   += batch_tokens\n            global_step += 1\n            pbar.set_postfix({\"loss\": loss.item(), \"acc\": acc})\n        train_loss = epoch_loss / n_tokens\n        train_acc  = epoch_acc  / n_tokens\n\n        # -----------------------------------------------------------------\n        # Validation\n        # -----------------------------------------------------------------\n        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n        history.append(_epoch_stats_to_json(epoch, train_loss, train_acc, val_loss, val_acc))\n\n    # ---------------------------------------------------------------------\n    # Save artefacts\n    # ---------------------------------------------------------------------\n    run_dir = results_dir / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n    ckpt_path = run_dir / \"model.pt\"\n    torch.save({\n        \"model_state\": model.state_dict(),\n        \"config\": config,\n    }, ckpt_path)\n\n    # Consolidate metrics\n    final_metrics = history[-1]\n    results = {\n        \"run_id\": run_id,\n        \"config\": config,\n        \"metrics\": {\n            \"epoch\": history,\n            \"final\": final_metrics\n        }\n    }\n    with (run_dir / \"results.json\").open(\"w\") as fp:\n        json.dump(results, fp, indent=2)\n    return results\n\n################################################################################\n# Validation routine\n################################################################################\n\ndef evaluate(model: nn.Module, loader: DataLoader, criterion: nn.Module, device: torch.device):\n    model.eval()\n    total_loss, total_acc, n_tokens = 0.0, 0.0, 0\n    with torch.no_grad():\n        for inputs, targets in loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            logits = model(inputs)\n            loss = criterion(logits, targets)\n            acc  = _token_accuracy(logits, targets)\n            batch_tokens = inputs.numel()\n            total_loss += loss.item() * batch_tokens\n            total_acc  += acc * batch_tokens\n            n_tokens   += batch_tokens\n    model.train()\n    return total_loss / n_tokens, total_acc / n_tokens\n\n################################################################################\n# CLI\n################################################################################\n\ndef parse_args() -\u003e argparse.Namespace:\n    parser = argparse.ArgumentParser(description=\"Train a single experimental run\")\n    parser.add_argument(\"--config\", type=str, required=True,\n                        help=\"Path to the YAML configuration file for this run\")\n    parser.add_argument(\"--results-dir\", type=str, required=True,\n                        help=\"Directory where results will be stored (common root)\")\n    return parser.parse_args()\n\n\nif __name__ == \"__main__\":\n    import yaml\n\n    args = parse_args()\n    with open(args.config, \"r\") as fp:\n        run_cfg = yaml.safe_load(fp)\n\n    root_dir = Path(args.results_dir)\n    root_dir.mkdir(parents=True, exist_ok=True)\n\n    start = time.time()\n    outcomes = train(run_cfg, root_dir)\n    duration = time.time() - start\n    outcomes[\"wall_clock_seconds\"] = duration\n\n    # Print structured JSON to stdout for main process \u0026 logs.\n    print(json.dumps(outcomes))"}


Compare the Base Code with the Derived Experiment Code for this specific experiment thoroughly. Ensure the derived code maintains all the quality, completeness, and functionality of the base code while properly replacing placeholders with specific implementations.
Output:
{
    "is_experiment_code_ready": false,
    "experiment_code_issue": "1. preprocess.py still contains PLACEHOLDER logic – cnn_dailymail and sst2 loading not implemented; code raises NotImplementedError, experiments not runnable.\n2. BaselineModel class has been removed from src/model.py but smoke_test.yaml still refers to \"BaselineModel\" -> build_model will raise ValueError, smoke-test fails.\n3. DATASET_PLACEHOLDER handling not fully replaced; criterion 2 unmet.\n4. Other placeholders (e.g., BaselineModel reference) indicate incomplete integration.\nHence derived code is not immediately executable and violates completeness requirements."
}
