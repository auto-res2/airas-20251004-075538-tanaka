
Input:
You are a researcher with expertise in engineering in the field of machine learning.

# Instructions
- The content described in “Repository Content” corresponds to the GitHub repository of the method described in “Method.”
- Please extract the following two pieces of information from “Repository Content”:
    - experimental_code：Extract the implementation sections that are directly related to the method described in “Method.”
    - experimental_info：Extract and output the experimental settings related to the method described in “Method.”

# Method
The methodology adapts the Risk-Averse Reinforcement Learning (RARL) approach to the standard Reinforcement Learning with Human Feedback (RLHF) pipeline, optimizing the Conditional Value at Risk (CVaR) of the return instead of the expected value. It employs a soft-risk scheduling mechanism, inspired by the CeSoR algorithm, to gradually introduce risk-aversion. This involves an initial training phase where the full batch of episodes is used, followed by a gradual reduction of the batch size to focus on trajectories with the lowest returns (worst-case scenarios). The approach starts with a supervised fine-tuned (SFT) base policy for positive episode recognition and regularizes against it using a KL-Divergence term in the reward function. Proximal Policy Optimization (PPO) is used for policy updates, operating in an Actor-Critic setting with a base transformer extended by a language modeling head (actor) and a value function head (critic).

# Repository Content
File Path: benchmark/benchmark.py
Content:
import argparse
import math
import os
import shlex
import subprocess
import uuid
from distutils.util import strtobool

import requests


def parse_args():
    # fmt: off
    parser = argparse.ArgumentParser()
    parser.add_argument("--command", type=str, default="",
        help="the command to run")
    parser.add_argument("--num-seeds", type=int, default=3,
        help="the number of random seeds")
    parser.add_argument("--start-seed", type=int, default=1,
        help="the number of the starting seed")
    parser.add_argument("--workers", type=int, default=0,
        help="the number of workers to run benchmark experimenets")
    parser.add_argument("--auto-tag", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
        help="if toggled, the runs will be tagged with git tags, commit, and pull request number if possible")
    parser.add_argument("--slurm-template-path", type=str, default=None,
        help="the path to the slurm template file (see docs for more details)")
    parser.add_argument("--slurm-gpus-per-task", type=int, default=1,
        help="the number of gpus per task to use for slurm jobs")
    parser.add_argument("--slurm-total-cpus", type=int, default=50,
        help="the number of gpus per task to use for slurm jobs")
    parser.add_argument("--slurm-ntasks", type=int, default=1,
        help="the number of tasks to use for slurm jobs")
    parser.add_argument("--slurm-nodes", type=int, default=None,
        help="the number of nodes to use for slurm jobs")
    args = parser.parse_args()
    # fmt: on
    return args


def run_experiment(command: str):
    command_list = shlex.split(command)
    print(f"running {command}")

    # Use subprocess.PIPE to capture the output
    fd = subprocess.Popen(command_list, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    output, errors = fd.communicate()

    return_code = fd.returncode
    assert return_code == 0, f"Command failed with error: {errors.decode('utf-8')}"

    # Convert bytes to string and strip leading/trailing whitespaces
    return output.decode("utf-8").strip()


def autotag() -> str:
    wandb_tag = ""
    print("autotag feature is enabled")
    git_tag = ""
    try:
        git_tag = subprocess.check_output(["git", "describe", "--tags"]).decode("ascii").strip()
        print(f"identified git tag: {git_tag}")
    except subprocess.CalledProcessError as e:
        print(e)
    if len(git_tag) == 0:
        try:
            count = int(subprocess.check_output(["git", "rev-list", "--count", "HEAD"]).decode("ascii").strip())
            hash = subprocess.check_output(["git", "rev-parse", "--short", "HEAD"]).decode("ascii").strip()
            git_tag = f"no-tag-{count}-g{hash}"
            print(f"identified git tag: {git_tag}")
        except subprocess.CalledProcessError as e:
            print(e)
    wandb_tag = f"{git_tag}"

    git_commit = subprocess.check_output(["git", "rev-parse", "--verify", "HEAD"]).decode("ascii").strip()
    try:
        # try finding the pull request number on github
        prs = requests.get(f"https://api.github.com/search/issues?q=repo:huggingface/trl+is:pr+{git_commit}")
        if prs.status_code == 200:
            prs = prs.json()
            if len(prs["items"]) > 0:
                pr = prs["items"][0]
                pr_number = pr["number"]
                wandb_tag += f",pr-{pr_number}"
        print(f"identified github pull request: {pr_number}")
    except Exception as e:
        print(e)

    return wandb_tag


if __name__ == "__main__":
    args = parse_args()
    if args.auto_tag:
        existing_wandb_tag = os.environ.get("WANDB_TAGS", "")
        wandb_tag = autotag()
        if len(wandb_tag) > 0:
            if len(existing_wandb_tag) > 0:
                os.environ["WANDB_TAGS"] = ",".join([existing_wandb_tag, wandb_tag])
            else:
                os.environ["WANDB_TAGS"] = wandb_tag
    print("WANDB_TAGS: ", os.environ.get("WANDB_TAGS", ""))
    commands = []
    for seed in range(0, args.num_seeds):
        commands += [" ".join([args.command, "--seed", str(args.start_seed + seed)])]

    print("======= commands to run:")
    for command in commands:
        print(command)

    if args.workers > 0 and args.slurm_template_path is None:
        from concurrent.futures import ThreadPoolExecutor

        executor = ThreadPoolExecutor(max_workers=args.workers, thread_name_prefix="cleanrl-benchmark-worker-")
        for command in commands:
            executor.submit(run_experiment, command)
        executor.shutdown(wait=True)
    else:
        print("not running the experiments because --workers is set to 0; just printing the commands to run")

    # SLURM logic
    if args.slurm_template_path is not None:
        if not os.path.exists("slurm"):
            os.makedirs("slurm")
        if not os.path.exists("slurm/logs"):
            os.makedirs("slurm/logs")
        print("======= slurm commands to run:")
        with open(args.slurm_template_path) as f:
            slurm_template = f.read()
        slurm_template = slurm_template.replace("{{array}}", f"0-{len(commands) - 1}%{args.workers}")
        slurm_template = slurm_template.replace(
            "{{seeds}}", f"({' '.join([str(args.start_seed + int(seed)) for seed in range(args.num_seeds)])})"
        )
        slurm_template = slurm_template.replace("{{len_seeds}}", f"{args.num_seeds}")
        slurm_template = slurm_template.replace("{{command}}", args.command)
        slurm_template = slurm_template.replace("{{gpus_per_task}}", f"{args.slurm_gpus_per_task}")
        total_gpus = args.slurm_gpus_per_task * args.slurm_ntasks
        slurm_cpus_per_gpu = math.ceil(args.slurm_total_cpus / total_gpus)
        slurm_template = slurm_template.replace("{{cpus_per_gpu}}", f"{slurm_cpus_per_gpu}")
        slurm_template = slurm_template.replace("{{ntasks}}", f"{args.slurm_ntasks}")
        if args.slurm_nodes is not None:
            slurm_template = slurm_template.replace("{{nodes}}", f"#SBATCH --nodes={args.slurm_nodes}")
        else:
            slurm_template = slurm_template.replace("{{nodes}}", "")
        filename = str(uuid.uuid4())
        open(os.path.join("slurm", f"{filename}.slurm"), "w").write(slurm_template)
        slurm_path = os.path.join("slurm", f"{filename}.slurm")
        print(f"saving command in {slurm_path}")
        if args.workers > 0:
            job_id = run_experiment(f"sbatch --parsable {slurm_path}")
            print(f"Job ID: {job_id}")

File Path: benchmark/post_github_comment.py
Content:
import json
import os

from ghapi.all import GhApi


FOLDER_STRING = os.environ.get("FOLDER_STRING", "")
folder = f"benchmark/trl/{FOLDER_STRING}"
host_url = f"https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/images/benchmark/{FOLDER_STRING}"

# Create a GitHub API instance
github_context = json.loads(os.environ["GITHUB_CONTEXT"])
token = os.environ["PERSONAL_ACCESS_TOKEN_GITHUB"]  # this needs to refreshed every 12 months
status_message = "**[COSTA BENCHMARK BOT]**: Here are the results"
body = status_message
repo = github_context["repository"]
owner, repo = repo.split("/")
api = GhApi(owner=owner, repo=repo, token=token)

# for each `.png` file in the folder, add it to the comment
for file in os.listdir(folder):
    if file.endswith(".png"):
        body += f"\n![{file}]({host_url}/{file})"

# Create a comment on the issue
api.issues.create_comment(issue_number=github_context["event"]["issue"]["number"], body=body)

File Path: benchmark/upload_benchmark.py
Content:
from dataclasses import dataclass

import tyro
from huggingface_hub import HfApi


@dataclass
class Args:
    folder_path: str = "benchmark/trl"
    path_in_repo: str = "images/benchmark"
    repo_id: str = "trl-internal-testing/example-images"
    repo_type: str = "dataset"


args = tyro.cli(Args)
api = HfApi()

api.upload_folder(
    folder_path=args.folder_path,
    path_in_repo=args.path_in_repo,
    repo_id=args.repo_id,
    repo_type=args.repo_type,
)

File Path: examples/IMDB/evaluation/evaluate_perplexity_all.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed
from trl.core import LengthSampler


tqdm.pandas()

import pdb
import pickle
import datetime
import os
from evaluate import load
from transformers import AutoModel, AutoTokenizer, AutoConfig
from transformers import OpenAIGPTTokenizer, OpenAIGPTModel
import random
random.seed(42)

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
print(date_n_time)
logging_dir = f"/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/eval/{date_n_time}"
os.makedirs(logging_dir)

@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name="gpt2",
            query_dataset="imdb",
            reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            remove_unused_columns=False
        )
    )
    query_dataset: str = field(default="imdb", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="less", metadata={"help": "the dataset to query"})


args = tyro.cli(ScriptArguments)

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead

# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8, data_split = 'train'):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # load imdb with datasets
    # to load partial dataset, just change data split to split=data_split+'[:200]'
    ds = load_dataset(query_dataset, split=data_split)
    ds = ds.rename_columns({"text": "review"})
    ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)

    input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# get test dataset
test_dataset = build_dataset(args.ppo_config, args.query_dataset, data_split='test')
# Take 5k random samples
sample_size = 5000
random_indices = random.sample(range(len(test_dataset)), sample_size)
test_dataset = test_dataset.select(random_indices)

def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])

# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}


zero_shot_model = trl_model_class.from_pretrained(
    'gpt2',
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)

tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"
tokenizer.truncation_side = "left"
tokenizer.pad_token_as_eos_token = True
tokenizer.max_length = 64

# Define all the tester classes 
zero_shot_tester = PPOTrainer(args.ppo_config, zero_shot_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)

# Build the perplexity evaluation pipeline
perplexity = load("perplexity", module_type="measurement")

#date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
#os.makedirs(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/evaluation/{date_n_time}')

model_names_all = ['zs', 'sft', 'rlhf', 'ra-rlhf']

# infer on test dataset
test_scores_all = [[], [], [], []]

for epoch, batch in tqdm(enumerate(zero_shot_tester.dataloader)):
    query_tensors = batch["input_ids"]
    for ii in range(4):
        #pdb.set_trace()

        texts = batch["review"]
        texts = [text[:1000] for text in texts]
        #texts = ["lorem ipsum", "Happy Birthday!", "Bienvenue"]

        if ii == 0:
            # gpt 
            perplexity_scores = perplexity.compute(data=texts, model_id='gpt2')['perplexities']
        elif ii == 1:
            # sft
            perplexity_scores = perplexity.compute(data=texts, model_id='/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_final/2024-01-24_21-17-41/checkpoint-100')['perplexities']
        elif ii == 2:
            # rlhf
            perplexity_scores = perplexity.compute(data=texts, model_id='/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/trl_sft_ppo/2023-10-23_23-30-47')['perplexities']
        elif ii == 3:
            # ra-rlhf
            perplexity_scores = perplexity.compute(data=texts, model_id='/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/imdb_final_1/sft_p/2024-01-24_22-34-51')['perplexities']

        test_scores_all[ii].extend(perplexity_scores)
        print(f'Epoch: {epoch} - Perplexity for {model_names_all[ii]}: {sum(perplexity_scores)/len(perplexity_scores)}')
        

# print and save the final score  
final_test_scores = [sum(test_scores)/len(test_scores) for test_scores in test_scores_all] 
print(f'Total test perelxity-scores for zero-shot, sft, rlhf, ra-rlhf are: {final_test_scores}')
#pickle.dump(final_test_score.item(), open(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_ppo/{date_n_time}/final_test_score', "wb"))



    





File Path: examples/IMDB/evaluation/evaluate_perplexity_single.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed
from trl.core import LengthSampler


tqdm.pandas()

import pdb
import pickle
import datetime
import os
from evaluate import load
from transformers import AutoModel, AutoTokenizer, AutoConfig
from transformers import OpenAIGPTTokenizer, OpenAIGPTModel
import random
random.seed(42)

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
print(date_n_time)
logging_dir = f"/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/eval/{date_n_time}"
os.makedirs(logging_dir)

@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name="gpt2",
            query_dataset="imdb",
            reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            remove_unused_columns=False
        )
    )
    query_dataset: str = field(default="imdb", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="less", metadata={"help": "the dataset to query"})


args = tyro.cli(ScriptArguments)

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead

# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8, data_split = 'train'):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # load imdb with datasets
    # to load partial dataset, just change data split to split=data_split+'[:200]'
    ds = load_dataset(query_dataset, split=data_split)
    ds = ds.rename_columns({"text": "review"})
    ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)

    input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# get test dataset
test_dataset = build_dataset(args.ppo_config, args.query_dataset, data_split='test')
# Take 5k random samples
sample_size = 5000
random_indices = random.sample(range(len(test_dataset)), sample_size)
test_dataset = test_dataset.select(random_indices)

def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])

# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}


zero_shot_model = trl_model_class.from_pretrained(
    'gpt2',
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)

tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"
tokenizer.truncation_side = "left"
tokenizer.pad_token_as_eos_token = True
tokenizer.max_length = 64

# Define all the tester classes 
zero_shot_tester = PPOTrainer(args.ppo_config, zero_shot_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)

# Build the perplexity evaluation pipeline
perplexity = load("perplexity", module_type="measurement")

#date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
#os.makedirs(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/evaluation/{date_n_time}')

model_names_all = ['zs', 'sft', 'rlhf', 'ra-rlhf']

# infer on test dataset
test_scores_all = []

for epoch, batch in tqdm(enumerate(zero_shot_tester.dataloader)):
    query_tensors = batch["input_ids"]

    texts = batch["review"]
    texts = [text[:1000] for text in texts]
    #texts = ["lorem ipsum", "Happy Birthday!", "Bienvenue"]

    # ra-rlhf
    perplexity_scores = perplexity.compute(data=texts, model_id='/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/imdb_final_1/sft_p/2024-01-24_23-28-17')['perplexities']

    test_scores_all.extend(perplexity_scores)
    print(f'Epoch: {epoch} - Perplexity: {sum(perplexity_scores)/len(perplexity_scores)}')
        

# print and save the final score  
final_test_score = sum(test_scores_all)/len(test_scores_all)
print(f'Total test perelxity-score is: {final_test_score}')
#pickle.dump(final_test_score.item(), open(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_ppo/{date_n_time}/final_test_score', "wb"))



    





File Path: examples/IMDB/evaluation/evaluate_sentiment.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed
from trl.core import LengthSampler


tqdm.pandas()

import pdb
import pickle
import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import statistics
import random

@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name="gpt2",
            query_dataset="imdb",
            reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
        )
    )
    query_dataset: str = field(default="imdb", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )


args = tyro.cli(ScriptArguments)


# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8, data_split = 'train'):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # load imdb with datasets
    # to load partial dataset, just change data split to split=data_split+'[:200]'
    ds = load_dataset(query_dataset, split=data_split)
    ds = ds.rename_columns({"text": "review"})
    ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)

    def input_size():
        return 64 #LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)

# build and sample test dataset
test_dataset = build_dataset(args.ppo_config, args.query_dataset, data_split='test')
# Take 5k random samples
sample_size = 5000
random_indices = random.sample(range(len(test_dataset)), sample_size)
test_dataset = test_dataset.select(random_indices)

def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/imdb_final_1/sft_p/2024-01-24_22-34-51',
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.padding_side = "left"
tokenizer.truncation_side = "left"
tokenizer.pad_token_as_eos_token = True
tokenizer.max_length = 64

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)
# Creating a tester object here because dataloader is only available in the PPOTrainer class
ppo_tester = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
task, model_name = args.ppo_config.reward_model.split(":")
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs = {
    "min_length": 48,
    "top_k": 50,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": 48,
}


# infer on test dataset
test_scores = []
test_score_lens = []
for epoch, batch in tqdm(enumerate(ppo_tester.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
    #rewards = [torch.tensor(output[1]["score"]) for output in pipe_outputs]
    rewards = [
        torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"]]))[1]
        for output in pipe_outputs
    ]

    test_scores.append(sum(rewards))
    test_score_lens.append(len(rewards))
    print(sum(rewards)/len(rewards))

# print and save the final score  
#date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
#os.makedirs(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/zero_shot/{date_n_time}')
final_test_score = sum(test_scores)/sum(test_score_lens)     
test_scores_list = [t.item() for t in test_scores]
print(f'Total test sentiment-score and std is: {final_test_score.item(), statistics.stdev(test_scores_list)}')
#pickle.dump(final_test_score.item(), open(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/zero_shot/{date_n_time}/final_test_score', "wb"))




File Path: examples/IMDB/evaluation/evaluate_sentiment_all.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed
from trl.core import LengthSampler


tqdm.pandas()

import pdb
import pickle
import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import statistics
import random

@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name="gpt2",
            query_dataset="imdb",
            reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
        )
    )
    query_dataset: str = field(default="imdb", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )


args = tyro.cli(ScriptArguments)


# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8, data_split = 'train'):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # load imdb with datasets
    # to load partial dataset, just change data split to split=data_split+'[:200]'
    ds = load_dataset(query_dataset, split=data_split)
    ds = ds.rename_columns({"text": "review"})
    ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)

    def input_size():
        return 64 #LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)

# build and sample test dataset
test_dataset = build_dataset(args.ppo_config, args.query_dataset, data_split='test')
# Take 5k random samples
sample_size = 5000
random_indices = random.sample(range(len(test_dataset)), sample_size)
test_dataset = test_dataset.select(random_indices)

def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/imdb_final_1/sft_p/2024-01-24_22-34-51',
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.padding_side = "left"
tokenizer.truncation_side = "left"
tokenizer.pad_token_as_eos_token = True
tokenizer.max_length = 64

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)
# Creating a tester object here because dataloader is only available in the PPOTrainer class
ppo_tester = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
task, model_name = args.ppo_config.reward_model.split(":")
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs = {
    "min_length": 48,
    "top_k": 50,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": 48,
}


# infer on test dataset
test_scores = []
test_score_lens = []
for epoch, batch in tqdm(enumerate(ppo_tester.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
    #rewards = [torch.tensor(output[1]["score"]) for output in pipe_outputs]
    rewards = [
        torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"]]))[1]
        for output in pipe_outputs
    ]

    test_scores.append(sum(rewards))
    test_score_lens.append(len(rewards))
    print(sum(rewards)/len(rewards))

# print and save the final score  
#date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
#os.makedirs(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/zero_shot/{date_n_time}')
final_test_score = sum(test_scores)/sum(test_score_lens)     
test_scores_list = [t.item() for t in test_scores]
print(f'Total test sentiment-score and std is: {final_test_score.item(), statistics.stdev(test_scores_list)}')
#pickle.dump(final_test_score.item(), open(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/zero_shot/{date_n_time}/final_test_score', "wb"))




File Path: examples/IMDB/evaluation/single_model_generic.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed
from trl.core import LengthSampler


tqdm.pandas()

import pdb
import pickle
import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import statistics
import random
from evaluate import load

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
logging_dir = f"/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/eval_generic/{date_n_time}"
os.makedirs(logging_dir)
import pandas as pd

@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name="gpt2",
            query_dataset="imdb",
            reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            #log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            exp_name="ppo",
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
        )
    )
    query_dataset: str = field(default="imdb", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="more", metadata={"help": "the dataset to query"})
    
    generation_kwargs_min_length: Optional[int] = field(default=-1, metadata={"help": "minimum number of tokens while generation"})
    generation_kwargs_top_k: Optional[int] = field(default=0, metadata={"help": "gneration top k"})
    generation_kwargs_top_p: Optional[float] = field(default=1.0, metadata={"help": "gneration top p"})
    generation_kwargs_max_new_tokens: Optional[int] = field(default=32, metadata={"help": "gneration top p"})

    change_tokenizer_args: bool = field(default=False, metadata={"help": "whether to use modify tokenizer settings"})
    tokenizer_kwargs: dict = field(
    default_factory=lambda: {
      "padding_side": "left", 
      "truncation_side": "left",
      "pad_token_as_eos_token": True,
      "max_length": 64
    }
  )


args = tyro.cli(ScriptArguments)

generation_kwargs = {
      "min_length": args.generation_kwargs_min_length, 
      "top_k": args.generation_kwargs_top_k,
      "top_p": args.generation_kwargs_top_p,
      "do_sample": True,
      "max_new_tokens": args.generation_kwargs_max_new_tokens
    }

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8, data_split = 'train'):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # load imdb with datasets
    # to load partial dataset, just change data split to split=data_split+'[:200]'
    ds = load_dataset(query_dataset, split=data_split)
    ds = ds.rename_columns({"text": "review"})
    ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)

    if args.input_query_size == "more":
        def input_size():
            return 64 
    else: 
        input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
#dataset = build_dataset(args.ppo_config, args.query_dataset)

# build and sample test dataset
test_dataset = build_dataset(args.ppo_config, args.query_dataset, data_split='test')
# Take 5k random samples
sample_size = 640 #5000
random_indices = random.sample(range(len(test_dataset)), sample_size)
test_dataset = test_dataset.select(random_indices)

def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}


model = trl_model_class.from_pretrained(
    args.ppo_config.model_name,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)

#'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_positive_1/oct_2_afternoon/checkpoint-120'
# ppo_model_PATH = '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-10-16_19-15-40/model.pt'
# model = torch.load(ppo_model_PATH) 

tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id
if args.change_tokenizer_args == True: 
    tokenizer.padding_side = args.tokenizer_kwargs['padding_side']
    tokenizer.truncation_side = args.tokenizer_kwargs['truncation_side']
    tokenizer.pad_token_as_eos_token = args.tokenizer_kwargs['pad_token_as_eos_token']
    tokenizer.max_length = args.tokenizer_kwargs['max_length']

#pdb.set_trace()
# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
#ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)
# Creating a tester object here because dataloader is only available in the PPOTrainer class
ppo_tester = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_tester.accelerator.device
if ppo_tester.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_tester.accelerator.state.deepspeed_plugin
task, model_name = args.ppo_config.reward_model.split(":")
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs = {
    "min_length": 48,
    "top_k": 50,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": 48,
}

# empty list to store text data
text_test_data = []

# Build the perplexity evaluation pipeline
perplexity = load("perplexity", module_type="metric")

# infer on test dataset
test_scores = []
perplexity_scores_all = []

for epoch, batch in tqdm(enumerate(ppo_tester.dataloader)):
    #pdb.set_trace()
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_tester.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    print(f'model: input:{batch["query"][:3]}, output:{batch["response"][:3]} \n\n')

    #pdb.set_trace()
    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
    
    # if args.reward_function == "trl":
    #     rewards = [torch.tensor(output[1]["score"]) for output in pipe_outputs]
    # else: 
        # pdb.set_trace()
    rewards = [
        torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"]]))[1]
        for output in pipe_outputs
    ]
    
    #pdb.set_trace()
    #perplexity_scores = perplexity.compute(predictions=texts, model_id=args.ppo_config.model_name)['perplexities']
    #perplexity_scores_all.append(sum(perplexity_scores)/len(perplexity_scores))

    for ii in range(3): 
        data_row = {'iter': epoch, 'input':batch["query"][ii], 'output':batch["response"][ii], 'score':rewards[ii].item()}
        text_test_data.append(data_row)
    
    test_scores.extend(rewards)

    #test_scores.append(sum(rewards)/len(rewards))
    print(sum(rewards)/len(rewards))

# print and save the final score  
final_test_score = sum(test_scores)/len(test_scores)     
test_scores_list = [t.item() for t in test_scores]
print(f'Total test sentiment-score and std is: {final_test_score.item(), statistics.stdev(test_scores_list)}')
pickle.dump(final_test_score.item(), open(logging_dir+'final_test_score', "wb"))

df = pd.DataFrame(text_test_data)
df.to_csv(logging_dir+'/textual_data.csv', index=False)
#print('perplexity:', sum(perplexity_scores_all)/len(perplexity_scores_all))


File Path: examples/IMDB/ploting/embed_and_cluster.py
Content:
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import pandas as pd

from sklearn.cluster import KMeans
import pdb

# Load model and tokenizer
model_name = "EleutherAI/gpt-j-6b" #"gpt2"  # Example using GPT-2
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

reviews = pd.read_csv('/mnt/shared-scratch/Anon1/auth1/rlhf/trl/examples/IMDB/ploting/critical_neg_texts.csv')
reviews_list = reviews['review'].tolist()

# get embeddings 
def get_embedding(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs, output_hidden_states=True)
    hidden_states = outputs.hidden_states
    return hidden_states[-1][:, 0, :].squeeze()

embeddings = [get_embedding(review) for review in reviews_list]

# cluster embeddings 
# Assuming embeddings is a list of embeddings generated as shown above
embeddings_array = torch.stack(embeddings).cpu().numpy()

# Perform K-Means clustering
#pdb.set_trace()
n_clusters = 5  # Example number of clusters
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
kmeans.fit(embeddings_array)

# Getting the cluster labels
labels = kmeans.labels_

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Step 1: Dimensionality Reduction
pca = PCA(n_components=2)  # Reduce to 2 dimensions for visualization
reduced_embeddings = pca.fit_transform(embeddings_array)

# Step 2: Plot Clusters
plt.figure(figsize=(10, 8))
for i in range(n_clusters):
    plt.scatter(reduced_embeddings[labels == i, 0], reduced_embeddings[labels == i, 1], label=f'Cluster {i}')
plt.title("Cluster Visualization")
plt.savefig("Cluster_Visualization", bbox_inches="tight")
plt.legend()
plt.show()

# Step 3: Output Reviews from Each Cluster
# Assuming 'your_reviews_list' is the list of reviews corresponding to the embeddings
reviews_df = pd.DataFrame({
    'Review': reviews_list,
    'Cluster': labels
})

for i in range(n_clusters):
    print(f"\nReviews from Cluster {i}:\n")
    sample_reviews = reviews_df[reviews_df['Cluster'] == i].sample(3, random_state=42)  # Sample 3 reviews
    for index, row in sample_reviews.iterrows():
        print(f"- {row['Review']}\n")
File Path: examples/IMDB/ploting/imdb/embed_and_cluster.py
Content:
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import pandas as pd

from sklearn.cluster import KMeans
import pdb

# Load model and tokenizer
model_name = "EleutherAI/gpt-j-6b" #"gpt2"  # Example using GPT-2
tokenizer = AutoTokenizer.from_pretrained(model_name, add_special_tokens=False, unk_token=None, keep_accents=False)
model = AutoModelForCausalLM.from_pretrained(model_name)

reviews_1 = pd.read_csv('/mnt/shared-scratch/Anon1/auth1/rlhf/trl/examples/IMDB/ploting/imdb/critical_neg_texts.csv')
reviews_list = reviews_1['review'].tolist()[:150]

reviews_2 = pd.read_csv('/mnt/shared-scratch/Anon1/auth1/rlhf/trl/examples/IMDB/ploting/imdb/critical_pos_texts.csv')
reviews_list.extend(reviews_2['review'].tolist())

# get embeddings 
def get_embedding(text):
    print('here')
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs, output_hidden_states=True)
    hidden_states = outputs.hidden_states
    return hidden_states[-1][:, 0, :].squeeze()

embeddings = [get_embedding(review) for review in reviews_list]

# cluster embeddings 
# Assuming embeddings is a list of embeddings generated as shown above
embeddings_array = torch.stack(embeddings).cpu().numpy()

# Perform K-Means clustering
#pdb.set_trace()
n_clusters = 8  # Example number of clusters
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
kmeans.fit(embeddings_array)

# Getting the cluster labels
labels = kmeans.labels_

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Step 1: Dimensionality Reduction
pca = PCA(n_components=2)  # Reduce to 2 dimensions for visualization
reduced_embeddings = pca.fit_transform(embeddings_array)

# Step 2: Plot Clusters
fig, axs2 = plt.subplots(figsize=(6,6))
for i in range(n_clusters):
    plt.scatter(reduced_embeddings[labels == i, 0], reduced_embeddings[labels == i, 1], s=50, label=f'Cluster {i}')
axs2.legend()
axs2.set_ylabel(r'x', fontsize=16) # Setting label size
axs2.set_xlabel(r'y', fontsize=16)
axs2.tick_params(axis='both', which='major', labelsize=14) # Adjusting tick size
axs2.set_box_aspect(1)
axs2.grid(True) # Additional aesthetics
plt.show()
plt.savefig("Cluster_Visualization.pdf", bbox_inches="tight")

# Step 3: Output Reviews from Each Cluster
# Assuming 'your_reviews_list' is the list of reviews corresponding to the embeddings
reviews_df = pd.DataFrame({
    'Review': reviews_list,
    'Cluster': labels
})

for i in range(n_clusters):
    print(f"\nReviews from Cluster {i}:\n")
    #sample_reviews = reviews_df[reviews_df['Cluster'] == i].sample(3, random_state=42)  # Sample 3 reviews
    sample_reviews = reviews_df[reviews_df['Cluster'] == i].sample(3, replace=True, random_state=42)
    for index, row in sample_reviews.iterrows():
        print(f"- {row['Review']}\n")
File Path: examples/IMDB/ploting/imdb/imdb_analysis.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed
from trl.core import LengthSampler


tqdm.pandas()
import matplotlib.pyplot as plt
import itertools

import pdb
import pickle
import datetime
import os
from evaluate import load
from transformers import AutoModel, AutoTokenizer, AutoConfig
from transformers import OpenAIGPTTokenizer, OpenAIGPTModel
import random

import pandas as pd
import pdb

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
print(date_n_time)
logging_dir = f"/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/plot_score/{date_n_time}"
os.makedirs(logging_dir)

@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name="lvwerra/gpt2-imdb",
            query_dataset="imdb",
            reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            remove_unused_columns=False
        )
    )
    query_dataset: str = field(default="imdb", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="less", metadata={"help": "the dataset to query"})


args = tyro.cli(ScriptArguments)

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead

# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8, data_split = 'train'):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # load imdb with datasets
    # to load partial dataset, just change data split to split=data_split+'[:200]'
    ds = load_dataset(query_dataset, split=data_split)
    ds = ds.rename_columns({"text": "review"})
    ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)

    input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# get test dataset
test_dataset = build_dataset(args.ppo_config, args.query_dataset, data_split='train')
# Take 5k random samples
# random.seed(10)
# sample_size = 2500
# random_indices = random.sample(range(len(test_dataset)), sample_size)
# test_dataset = test_dataset.select(random_indices)

#pdb.set_trace()

def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])

# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

zero_shot_model = trl_model_class.from_pretrained(
    'gpt2',
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)

sft_model = trl_model_class.from_pretrained(
    'lvwerra/gpt2-imdb',
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)

# For the following to work I should have saved the model using huggingface
# ppo_model = trl_model_class.from_pretrained(
#     '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-09-26_11-27-23/model.pt',
#     trust_remote_code=True,
#     device_map=device_map,
#     peft_config=peft_config,
# )

sft_ppo_model = trl_model_class.from_pretrained(
    '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/trl_sft_ppo/2023-10-23_23-30-47',
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)
"""
ppo_model_PATH = '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-09-26_11-27-23/model.pt'
ppo_model = torch.load(ppo_model_PATH)  

sft_ppo_model_PATH = '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_ppo/2023-09-26_10-36-41/model.pt'
sft_ppo_model = torch.load(sft_ppo_model_PATH)
"""
tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"
tokenizer.truncation_side = "left"
tokenizer.pad_token_as_eos_token = True
tokenizer.max_length = 64

# Define all the tester classes 
zero_shot_tester = PPOTrainer(args.ppo_config, zero_shot_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)
sft_tester = PPOTrainer(args.ppo_config, sft_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)
# ppo_tester = PPOTrainer(args.ppo_config, ppo_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)
sft_ppo_tester = PPOTrainer(args.ppo_config, sft_ppo_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)

#pdb.set_trace()
#ppo_tester._save_pretrained('/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-09-26_11-27-23')
#sft_ppo_tester._save_pretrained('/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_ppo/2023-09-26_10-36-41')

# Build the perplexity evaluation pipeline
perplexity = load("perplexity", module_type="measurement")

# DO I NEED TO TAKE CARE OF PAD TOKEN FOR PERPLEXITY PIPELINE AS WELL??
"""
# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id
"""
device = sft_tester.accelerator.device
if sft_tester.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = sft_tester.accelerator.state.deepspeed_plugin
task, model_name = args.ppo_config.reward_model.split(":")
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id


# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs = {
    "min_length": 48,
    "top_k": 50,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": 48,
}


#date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
#os.makedirs(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/evaluation/{date_n_time}')

model_names_all = ['zero_shot_model', 'sft_model', 'ppo_model', 'sft_ppo_model']
testers_all = [zero_shot_tester, sft_tester, sft_ppo_tester] #ppo_tester,

# infer on test dataset
#test_scores_1, test_scores_2, test_scores_3, test_scores_4 = [], [], [], []
positive_score_all = []
negative_score_all = []
label_all = []
critical_pos_texts = []
critical_neg_texts = []
critical_neg_scores = []
critical_pos_scores = []
total_critical_texts = []
total_critical_scores = []
total_critical_labels = []

for epoch, batch in tqdm(enumerate(zero_shot_tester.dataloader)):
    query_tensors = batch["input_ids"]
    
    texts = batch["review"]
    texts = [text[:200 ] for text in texts]
    #texts = ["lorem ipsum", "Happy Birthday!", "Bienvenue"]

    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
    pos_score = [output[1]["score"] for output in pipe_outputs]
    positive_score_all.append(pos_score)

    neg_score = [output[0]["score"] for output in pipe_outputs]
    negative_score_all.append(neg_score)

    labels = [label.item() for label in batch["label"]]
    label_all.append(labels)

    for ii in range(len(label_all[0])):
        if labels[ii] == 0 and pos_score[ii] > 0:
            critical_neg_texts.append(texts[ii])
            critical_neg_scores.append(pos_score[ii])
            total_critical_texts.append(texts[ii])
            total_critical_scores.append(pos_score[ii])
            total_critical_labels.append(labels[ii])
        if labels[ii] == 1 and pos_score[ii] < 0:
            critical_pos_texts.append(texts[ii])
            critical_pos_scores.append(pos_score[ii])
            total_critical_texts.append(texts[ii])
            total_critical_scores.append(pos_score[ii])
            total_critical_labels.append(labels[ii])

print(f'Total critical reviews: {len(total_critical_labels)}')
# get flattened lists 
pos_scores = list(itertools.chain(*positive_score_all))
neg_scores = list(itertools.chain(*negative_score_all))
labels = list(itertools.chain(*label_all))


# Plot histograms
df = pd.DataFrame(list(zip(neg_scores, pos_scores, labels)), columns = ['neg score', 'pos score', 'label'])
fig2, axs2 = plt.subplots(figsize=(10,5))

df[df['label'] == 0]['pos score'].plot.hist(ax=axs2, bins=30, alpha=0.5, label='Class 0') 
df[df['label'] == 1]['pos score'].plot.hist(ax=axs2, bins=30, alpha=0.5, label='Class 1')
axs2.legend()
axs2.set_ylabel(r'No. of input prompts', fontsize=16) # Setting label size
axs2.set_xlabel(r'Enviroment Score', fontsize=16)
axs2.tick_params(axis='both', which='major', labelsize=14) # Adjusting tick size
axs2.set_box_aspect(0.7)
axs2.grid(True) # Additional aesthetics
#axs2.legend(loc='upper left')
axs2.grid(True)

fig2.savefig('histogram_plot.png')

# save critical reviews as csv 
df1 = pd.DataFrame({'score': critical_neg_scores, 'review': critical_neg_texts})
df2 = pd.DataFrame({'score': critical_pos_scores, 'review': critical_pos_texts})
df3 = pd.DataFrame({'review': total_critical_texts})

df1 = df1.sort_values('score')
df2 = df2.sort_values('score')

df1.to_csv('critical_neg_texts.csv')
df2.to_csv('critical_pos_texts.csv')
df3.to_csv('total_critical_texts.csv')


# histogram plot for scores of critical reviews 
df4 = pd.DataFrame(list(zip(total_critical_scores, total_critical_labels)), columns = ['pos score', 'label'])
fig3, axs3 = plt.subplots()

df4[df4['label'] == 0]['pos score'].plot.hist(ax=axs3, bins=30, alpha=0.5, label='Class 0') 
df4[df4['label'] == 1]['pos score'].plot.hist(ax=axs3, bins=30, alpha=0.5, label='Class 1')
axs2.legend()
axs2.set_ylabel(r'No. of input prompts', fontsize=16) # Setting label size
axs2.set_xlabel(r'Enviroment Score', fontsize=16)
axs2.tick_params(axis='both', which='major', labelsize=14) # Adjusting tick size
axs2.set_box_aspect(0.7)
axs2.grid(True) # Additional aesthetics
#axs2.legend(loc='upper left')
axs2.grid(True)

fig3.savefig('critical_context_scores.png')
File Path: examples/IMDB/ploting/imdb/imdb_analysis_2.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed
from trl.core import LengthSampler


tqdm.pandas()
import matplotlib.pyplot as plt
import itertools

import pdb
import pickle
import datetime
import os
from evaluate import load
from transformers import AutoModel, AutoTokenizer, AutoConfig
from transformers import OpenAIGPTTokenizer, OpenAIGPTModel
import random

import pandas as pd
import pdb

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
print(date_n_time)
logging_dir = f"/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/plot_score/{date_n_time}"
os.makedirs(logging_dir)

@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name="lvwerra/gpt2-imdb",
            query_dataset="imdb",
            reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            remove_unused_columns=False
        )
    )
    query_dataset: str = field(default="imdb", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="less", metadata={"help": "the dataset to query"})


args = tyro.cli(ScriptArguments)

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead

# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8, data_split = 'train'):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # load imdb with datasets
    # to load partial dataset, just change data split to split=data_split+'[:200]'
    ds = load_dataset(query_dataset, split=data_split)
    ds = ds.rename_columns({"text": "review"})
    ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)

    input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# get test dataset
test_dataset = build_dataset(args.ppo_config, args.query_dataset, data_split='test')
# Take 5k random samples
# random.seed(10)
# sample_size = 2500
# random_indices = random.sample(range(len(test_dataset)), sample_size)
# test_dataset = test_dataset.select(random_indices)

#pdb.set_trace()

def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])

# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

zero_shot_model = trl_model_class.from_pretrained(
    'gpt2',
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)

sft_model = trl_model_class.from_pretrained(
    'lvwerra/gpt2-imdb',
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)

# For the following to work I should have saved the model using huggingface
# ppo_model = trl_model_class.from_pretrained(
#     '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-09-26_11-27-23/model.pt',
#     trust_remote_code=True,
#     device_map=device_map,
#     peft_config=peft_config,
# )

sft_ppo_model = trl_model_class.from_pretrained(
    '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/trl_sft_ppo/2023-10-23_23-30-47',
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)
"""
ppo_model_PATH = '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-09-26_11-27-23/model.pt'
ppo_model = torch.load(ppo_model_PATH)  

sft_ppo_model_PATH = '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_ppo/2023-09-26_10-36-41/model.pt'
sft_ppo_model = torch.load(sft_ppo_model_PATH)
"""
tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"
tokenizer.truncation_side = "left"
tokenizer.pad_token_as_eos_token = True
tokenizer.max_length = 64

# Define all the tester classes 
zero_shot_tester = PPOTrainer(args.ppo_config, zero_shot_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)
sft_tester = PPOTrainer(args.ppo_config, sft_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)
# ppo_tester = PPOTrainer(args.ppo_config, ppo_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)
sft_ppo_tester = PPOTrainer(args.ppo_config, sft_ppo_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)

#pdb.set_trace()
#ppo_tester._save_pretrained('/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-09-26_11-27-23')
#sft_ppo_tester._save_pretrained('/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_ppo/2023-09-26_10-36-41')

# Build the perplexity evaluation pipeline
perplexity = load("perplexity", module_type="measurement")

# DO I NEED TO TAKE CARE OF PAD TOKEN FOR PERPLEXITY PIPELINE AS WELL??
"""
# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id
"""
device = sft_tester.accelerator.device
if sft_tester.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = sft_tester.accelerator.state.deepspeed_plugin
task, model_name = args.ppo_config.reward_model.split(":")
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id


# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs = {
    "min_length": 48,
    "top_k": 50,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": 48,
}


#date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
#os.makedirs(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/evaluation/{date_n_time}')

model_names_all = ['zero_shot_model', 'sft_model', 'ppo_model', 'sft_ppo_model']
testers_all = [zero_shot_tester, sft_tester, sft_ppo_tester] #ppo_tester,

# infer on test dataset
#test_scores_1, test_scores_2, test_scores_3, test_scores_4 = [], [], [], []
positive_score_all = []
negative_score_all = []
label_all = []
critical_pos_texts = []
critical_neg_texts = []
critical_neg_scores = []
critical_pos_scores = []
total_critical_texts = []
total_critical_scores = []
total_critical_labels = []

for epoch, batch in tqdm(enumerate(zero_shot_tester.dataloader)):
    query_tensors = batch["input_ids"]
    
    texts = batch["review"]
    texts = [text[:200 ] for text in texts]
    #texts = ["lorem ipsum", "Happy Birthday!", "Bienvenue"]

    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
    pos_score = [output[1]["score"] for output in pipe_outputs]
    positive_score_all.append(pos_score)

    neg_score = [output[0]["score"] for output in pipe_outputs]
    negative_score_all.append(neg_score)

    labels = [label.item() for label in batch["label"]]
    label_all.append(labels)

    for ii in range(len(label_all[0])):
        if labels[ii] == 0 and pos_score[ii] < -2.8:
            critical_neg_texts.append(texts[ii])
            critical_neg_scores.append(pos_score[ii])
            total_critical_texts.append(texts[ii])
            total_critical_scores.append(pos_score[ii])
            total_critical_labels.append(labels[ii])
        if labels[ii] == 1 and pos_score[ii] < -2.8:
            critical_pos_texts.append(texts[ii])
            critical_pos_scores.append(pos_score[ii])
            total_critical_texts.append(texts[ii])
            total_critical_scores.append(pos_score[ii])
            total_critical_labels.append(labels[ii])

print(f'Total critical reviews: {len(total_critical_labels)}')
# get flattened lists 
pos_scores = list(itertools.chain(*positive_score_all))
neg_scores = list(itertools.chain(*negative_score_all))
labels = list(itertools.chain(*label_all))


# Plot histograms
df = pd.DataFrame(list(zip(neg_scores, pos_scores, labels)), columns = ['neg score', 'pos score', 'label'])
fig2, axs2 = plt.subplots(figsize=(10,5))

df[df['label'] == 0]['pos score'].plot.hist(ax=axs2, bins=30, alpha=0.5, label='Negative') 
df[df['label'] == 1]['pos score'].plot.hist(ax=axs2, bins=30, alpha=0.5, label='Positive')
axs2.legend()
axs2.set_ylabel(r'No. of input prompts', fontsize=16) # Setting label size
axs2.set_xlabel(r'Enviroment Score', fontsize=16)
axs2.tick_params(axis='both', which='major', labelsize=14) # Adjusting tick size
axs2.set_box_aspect(0.7)
axs2.grid(True)
fig2.savefig('histogram_plot.pdf', bbox_inches="tight")

# save critical reviews as csv 
df1 = pd.DataFrame({'score': critical_neg_scores, 'review': critical_neg_texts})
df2 = pd.DataFrame({'score': critical_pos_scores, 'review': critical_pos_texts})
df3 = pd.DataFrame({'review': total_critical_texts})

df1 = df1.sort_values('score')
df2 = df2.sort_values('score')

df1.to_csv('critical_neg_texts.csv')
df2.to_csv('critical_pos_texts.csv')
df3.to_csv('total_critical_texts.csv')


# histogram plot for scores of critical reviews 
df4 = pd.DataFrame(list(zip(total_critical_scores, total_critical_labels)), columns = ['pos score', 'label'])
fig3, axs3 = plt.subplots(figsize=(10,5))

df4[df4['label'] == 0]['pos score'].plot.hist(ax=axs3, bins=30, alpha=0.5, label='Negative') 
df4[df4['label'] == 1]['pos score'].plot.hist(ax=axs3, bins=30, alpha=0.5, label='Positive')
axs3.legend()
axs3.set_ylabel(r'No. of input prompts', fontsize=16) # Setting label size
axs3.set_xlabel(r'Enviroment Score', fontsize=16)
axs3.tick_params(axis='both', which='major', labelsize=14) # Adjusting tick size
axs3.set_box_aspect(0.7)
axs3.grid(True) 

fig3.savefig('critical_context_scores.pdf', bbox_inches="tight")
File Path: examples/IMDB/ploting/plot_from_multiple_tb_log.py
Content:
import matplotlib.pyplot as plt
import numpy as np
from tensorboard.backend.event_processing import event_accumulator

import pdb

# Function to extract scalar data from a TensorBoard log file
def extract_scalar_data_from_event_file(event_file_path):
    # Initialize an accumulator
    ea = event_accumulator.EventAccumulator(event_file_path)
    ea.Reload()  # Loads the log data from file

    # Get all scalar tags
    scalar_tags = ea.Tags()['scalars']

    # Dictionary to hold the data
    scalar_data = {}

    # Extract data for each scalar tag
    for tag in scalar_tags:
        events = ea.Scalars(tag)
        times = [e.wall_time for e in events]
        steps = [e.step for e in events]
        values = [e.value for e in events]
        scalar_data[tag] = {'times': times, 'steps': steps, 'values': values}

    return scalar_data

# Function for smoothing the curve (simple moving average for illustration)
def smooth_curve(points, factor=0.9):
    smoothed_points = np.zeros_like(points)
    for i in range(len(points)):
        if i > 0:
            smoothed_points[i] = (smoothed_points[i - 1] * factor) + (points[i] * (1 - factor))
        else:
            smoothed_points[i] = points[i]
    return smoothed_points

# Path to your TensorBoard log file or directory
# Assuming `all_data` is a structured dictionary containing all runs, organized by algorithm and then by seed
# e.g., all_data = {'algorithm1': {'seed1': data1, 'seed2': data2, ...},
#                   'algorithm2': {'seed1': data1, 'seed2': data2, ...}, ...}
#ra-rlhf seed 73: 2024-01-19_21-28-11/trl 
#ra-rlhf seed 42: 2024-01-18_09-45-09/trl
#rlhf seed 42: 2024-01-18_01-23-52/trl
#rlhf seed 73: 2024-01-19_23-48-35/trl

log_file_paths = {'RA-RLHF': ['/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/dummy/2024-01-19_21-28-11/trl', \
                                '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/dummy/2024-01-18_09-45-09/trl'], 
                'RLHF': ['/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/dummy/2024-01-18_01-23-52/trl', \
                                '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/dummy/2024-01-19_23-48-35/trl']
                } 

# Extract data
# Extract data
all_data = {}
for algorithm, paths in log_file_paths.items():
    all_data[algorithm] = {}
    for i, path in enumerate(paths):
        seed_key = f'seed{i+1}'
        # load data
        all_data[algorithm][seed_key] = extract_scalar_data_from_event_file(path)  

#all_data['RLHF']['seed1']['objective/kl'].keys()
#dict_keys(['times', 'steps', 'values'])

# Define a list of tags you want to plot
# desired_tags = ['env/reward_mean', 'env/reward_std', 'objective/entropy', 'objective/kl', 'objective/kl_coef']  # Replace with your actual tags of interest
# desired_tag_names = ['Return', 'reward_std', 'Policy Entropy', 'KL Divergence', 'kl_coeff']

desired_tags = {'env/reward_mean': 'Return', \
                'env/reward_std': 'reward_std', \
                'objective/entropy': 'Policy Entropy', \
                'objective/kl': 'KL Divergence', \
                'objective/kl_coef': 'kl_coeff'}

# Step 1: Calculate mean and standard deviation for each tag for each algorithm
tag_stats_by_algorithm = {}
for algorithm, seeds_data in all_data.items():
    tag_grouped_data = {}
    for seed, data in seeds_data.items():
        print(seed)
        for tag, values in data.items():
            #pdb.set_trace()
            if tag in desired_tags.keys():
                print(tag)
                if tag not in tag_grouped_data:
                    #pdb.set_trace()
                    print('here')
                    tag_grouped_data[tag] = []
                tag_grouped_data[tag].append(values)
    
    #pdb.set_trace()
    tag_stats = {}
    for tag, values_list in tag_grouped_data.items():
        # Assuming all seed runs have the same number of steps and are aligned
        steps = values_list[0]['steps']
        all_values = np.array([values['values'] for values in values_list])
        mean_values = np.mean(all_values, axis=0)
        std_values = np.std(all_values, axis=0)
        tag_stats[tag] = {'steps': steps, 'mean': mean_values, 'std': std_values}
    
    tag_stats_by_algorithm[algorithm] = tag_stats


# # Step 1: Calculate mean and standard deviation for each tag for each algorithm
# tag_stats_by_algorithm = {}
# for algorithm, seeds_data in all_data.items():
#     tag_grouped_data = {}
#     for seed, data in seeds_data.items():
#         for tag, values in data.items():
#             if tag in desired_tags.keys():
#                 if tag not in tag_grouped_data:
#                     tag_grouped_data[desired_tags[tag]] = []
#                 tag_grouped_data[desired_tags[tag]].append(values['values'])  # Store only values for simplicity

#     tag_stats = {}
#     for tag, values_list in tag_grouped_data.items():
#         # Debugging print
#         print(f"{algorithm} - {tag}: Sample values for each seed - {values_list}")

#         # Assuming all seed runs have the same number of steps and are aligned
#         steps = seeds_data[next(iter(seeds_data))][tag]['steps']
#         all_values = np.array(values_list)
#         mean_values = np.mean(all_values, axis=0)
#         std_values = np.std(all_values, axis=0)

#         # Debugging print
#         print(f"{algorithm} - {tag}: Mean values - {mean_values[:5]}")
#         print(f"{algorithm} - {tag}: Std values - {std_values[:5]}")

#         tag_stats[tag] = {'steps': steps, 'mean': mean_values, 'std': std_values}
    
#     tag_stats_by_algorithm[algorithm] = tag_stats


# Step 2: Plot mean and standard deviation for each tag for each algorithm
for tag in tag_grouped_data.keys():
    fig, ax = plt.subplots(figsize=(10, 5))  # Create a figure and an axis object
    
    for algorithm, stats in tag_stats_by_algorithm.items():
        if tag in stats:
            print(tag)
            steps = stats[tag]['steps']
            mean_values = smooth_curve(stats[tag]['mean'])
            std_values = smooth_curve(stats[tag]['std'])
            ax.plot(steps, mean_values, label=f'{algorithm} mean')
            ax.fill_between(steps, mean_values - std_values, mean_values + std_values, alpha=0.3, label=f'{algorithm} std dev')

            ax.set_xlabel('Episode time', fontsize=16)  # Set label size on the axis object
            ax.set_ylabel(f'{tag}', fontsize=16)  # Set label size on the axis object
            ax.tick_params(axis='both', which='major', labelsize=14)  # Adjust tick size on the axis object
            # ax.set_box_aspect(0.7)  # Commented out because this might not be available depending on your matplotlib version
            ax.grid(True)  # Add grid on the axis object
            ax.legend(loc='upper left')  # Add legend on the axis object
            plt.title(f'Mean and Standard Deviation for {tag} across Algorithms')
            plt.show()
            plt.savefig(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/examples/IMDB/ploting/final_results/trial/{desired_tags[tag]}', bbox_inches='tight', pad_inches=0) #as png

File Path: examples/IMDB/ploting/plot_from_tb_log.py
Content:
import matplotlib.pyplot as plt
import numpy as np
from tensorboard.backend.event_processing import event_accumulator

import pdb

# Function to extract scalar data from a TensorBoard log file
def extract_scalar_data_from_event_file(event_file_path):
    # Initialize an accumulator
    ea = event_accumulator.EventAccumulator(event_file_path)
    ea.Reload()  # Loads the log data from file

    # Get all scalar tags
    scalar_tags = ea.Tags()['scalars']

    # Dictionary to hold the data
    scalar_data = {}

    # Extract data for each scalar tag
    for tag in scalar_tags:
        events = ea.Scalars(tag)
        times = [e.wall_time for e in events]
        steps = [e.step for e in events]
        values = [e.value for e in events]
        scalar_data[tag] = {'times': times, 'steps': steps, 'values': values}

    return scalar_data

# Path to your TensorBoard log file or directory
log_file_path = '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/dummy/2024-01-18_01-23-52/trl'  # Replace with the actual path to your log file

# Extract data
data = extract_scalar_data_from_event_file(log_file_path)

# Assuming 'data' is a list of dictionaries loaded from multiple log files
# and each dictionary in 'data' has the same structure as what we've discussed previously.

# Step 1: Group data by tag
tag_grouped_data = {}
for single_run_data in data:
    for tag, values in single_run_data.items():
        if tag not in tag_grouped_data:
            tag_grouped_data[tag] = []
        tag_grouped_data[tag].append(values)

# Step 2: Calculate mean and standard deviation for each tag
tag_stats = {}
for tag, values_list in tag_grouped_data.items():
    # Assuming that all runs have the same number of steps and are aligned
    steps = values_list[0]['steps']
    all_values = np.array([values['values'] for values in values_list])
    mean_values = np.mean(all_values, axis=0)
    std_values = np.std(all_values, axis=0)
    tag_stats[tag] = {'steps': steps, 'mean': mean_values, 'std': std_values}

# Step 3: Plot mean and standard deviation for each tag
for tag, stats in tag_stats.items():
    plt.figure(figsize=(10, 5))
    steps = stats['steps']
    mean_values = stats['mean']
    std_values = stats['std']
    plt.plot(steps, mean_values, label=f'{tag} mean')
    plt.fill_between(steps, mean_values - std_values, mean_values + std_values, alpha=0.2, label=f'{tag} std dev')
    plt.xlabel('Steps')
    plt.ylabel(tag)
    plt.title(f'Mean and Standard Deviation for {tag}')
    plt.legend()
    plt.show()

File Path: examples/IMDB/ploting/plot_score.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed
from trl.core import LengthSampler


tqdm.pandas()
import matplotlib.pyplot as plt
import itertools

import pdb
import pickle
import datetime
import os
from evaluate import load
from transformers import AutoModel, AutoTokenizer, AutoConfig
from transformers import OpenAIGPTTokenizer, OpenAIGPTModel
import random

import pandas as pd
import pdb

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
print(date_n_time)
logging_dir = f"/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/plot_score/{date_n_time}"
os.makedirs(logging_dir)

@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name="lvwerra/gpt2-imdb",
            query_dataset="imdb",
            reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            remove_unused_columns=False
        )
    )
    query_dataset: str = field(default="imdb", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="less", metadata={"help": "the dataset to query"})


args = tyro.cli(ScriptArguments)

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead

# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8, data_split = 'train'):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # load imdb with datasets
    # to load partial dataset, just change data split to split=data_split+'[:200]'
    ds = load_dataset(query_dataset, split=data_split)
    ds = ds.rename_columns({"text": "review"})
    ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)

    input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# get test dataset
test_dataset = build_dataset(args.ppo_config, args.query_dataset, data_split='train')
# Take 5k random samples
# random.seed(10)
# sample_size = 2500
# random_indices = random.sample(range(len(test_dataset)), sample_size)
# test_dataset = test_dataset.select(random_indices)

#pdb.set_trace()

def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])

# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

zero_shot_model = trl_model_class.from_pretrained(
    'gpt2',
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)

sft_model = trl_model_class.from_pretrained(
    'lvwerra/gpt2-imdb',
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)

# For the following to work I should have saved the model using huggingface
# ppo_model = trl_model_class.from_pretrained(
#     '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-09-26_11-27-23/model.pt',
#     trust_remote_code=True,
#     device_map=device_map,
#     peft_config=peft_config,
# )

sft_ppo_model = trl_model_class.from_pretrained(
    '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/trl_sft_ppo/2023-10-23_23-30-47',
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)
"""
ppo_model_PATH = '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-09-26_11-27-23/model.pt'
ppo_model = torch.load(ppo_model_PATH)  

sft_ppo_model_PATH = '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_ppo/2023-09-26_10-36-41/model.pt'
sft_ppo_model = torch.load(sft_ppo_model_PATH)
"""
tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"
tokenizer.truncation_side = "left"
tokenizer.pad_token_as_eos_token = True
tokenizer.max_length = 64

# Define all the tester classes 
zero_shot_tester = PPOTrainer(args.ppo_config, zero_shot_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)
sft_tester = PPOTrainer(args.ppo_config, sft_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)
# ppo_tester = PPOTrainer(args.ppo_config, ppo_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)
sft_ppo_tester = PPOTrainer(args.ppo_config, sft_ppo_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)

#pdb.set_trace()
#ppo_tester._save_pretrained('/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-09-26_11-27-23')
#sft_ppo_tester._save_pretrained('/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_ppo/2023-09-26_10-36-41')

# Build the perplexity evaluation pipeline
perplexity = load("perplexity", module_type="measurement")

# DO I NEED TO TAKE CARE OF PAD TOKEN FOR PERPLEXITY PIPELINE AS WELL??
"""
# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id
"""
device = sft_tester.accelerator.device
if sft_tester.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = sft_tester.accelerator.state.deepspeed_plugin
task, model_name = args.ppo_config.reward_model.split(":")
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id


# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs = {
    "min_length": 48,
    "top_k": 50,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": 48,
}


#date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
#os.makedirs(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/evaluation/{date_n_time}')

model_names_all = ['zero_shot_model', 'sft_model', 'ppo_model', 'sft_ppo_model']
testers_all = [zero_shot_tester, sft_tester, sft_ppo_tester] #ppo_tester,

# infer on test dataset
#test_scores_1, test_scores_2, test_scores_3, test_scores_4 = [], [], [], []
positive_score_all = []
negative_score_all = []
label_all = []
critical_pos_texts = []
critical_neg_texts = []
critical_neg_scores = []
critical_pos_scores = []
total_critical_texts = []
total_critical_scores = []
total_critical_labels = []

for epoch, batch in tqdm(enumerate(zero_shot_tester.dataloader)):
    query_tensors = batch["input_ids"]
    
    texts = batch["review"]
    texts = [text[:1000 ] for text in texts]
    #texts = ["lorem ipsum", "Happy Birthday!", "Bienvenue"]

    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
    pos_score = [output[1]["score"] for output in pipe_outputs]
    positive_score_all.append(pos_score)

    neg_score = [output[0]["score"] for output in pipe_outputs]
    negative_score_all.append(neg_score)

    labels = [label.item() for label in batch["label"]]
    label_all.append(labels)

    for ii in range(len(label_all[0])):
        if labels[ii] == 0 and pos_score[ii] > 0:
            critical_neg_texts.append(texts[ii])
            critical_neg_scores.append(pos_score[ii])
            total_critical_texts.append(texts[ii])
            total_critical_scores.append(pos_score[ii])
            total_critical_labels.append(labels[ii])
        if labels[ii] == 1 and pos_score[ii] < 0:
            critical_pos_texts.append(texts[ii])
            critical_pos_scores.append(pos_score[ii])
            total_critical_texts.append(texts[ii])
            total_critical_scores.append(pos_score[ii])
            total_critical_labels.append(labels[ii])

print(f'Total critical reviews: {len(total_critical_labels)}')
# get flattened lists 
pos_scores = list(itertools.chain(*positive_score_all))
neg_scores = list(itertools.chain(*negative_score_all))
labels = list(itertools.chain(*label_all))

pdb.set_trace()
# plot positive and negative score and save the plot
df = pd.DataFrame(list(zip(neg_scores, pos_scores, labels)), columns = ['neg score', 'pos score', 'label'])
ax = df.plot.scatter(x='neg score', y='pos score', c='label', colormap='viridis')
ax.plot([3,-3], [-3,3], 'ro-')
#fig = plt.scatter(x=df['neg score'], y=df['pos score'], c=df['label'], colormap='viridis')
ax.grid(True)
fig = ax.get_figure()
fig.savefig('scatter_plot.png')

# Plot histograms
fig2, axs2 = plt.subplots(1, 2, figsize=(10,5))

df[df['label'] == 0]['neg score'].plot.hist(ax=axs2[0], bins=10, alpha=0.5, label='Class 0')
df[df['label'] == 1]['neg score'].plot.hist(ax=axs2[0], bins=10, alpha=0.5, label='Class 1')
axs2[0].set_title('Negative Score Distribution')
axs2[0].legend()

df[df['label'] == 0]['pos score'].plot.hist(ax=axs2[1], bins=10, alpha=0.5, label='Class 0') 
df[df['label'] == 1]['pos score'].plot.hist(ax=axs2[1], bins=10, alpha=0.5, label='Class 1')
axs2[1].set_title('Positive Score Distribution')
axs2[1].legend()

fig2.savefig('histograms_plot.png')

# save critical reviews as csv 
df1 = pd.DataFrame({'score': critical_neg_scores, 'review': critical_neg_texts})
df2 = pd.DataFrame({'score': critical_pos_scores, 'review': critical_pos_texts})
df3 = pd.DataFrame({'review': total_critical_texts})

df1.to_csv('critical_neg_texts.csv')
df2.to_csv('critical_pos_texts.csv')
df3.to_csv('total_critical_texts.csv')

# histogram plot for scores of critical reviews 
df4 = pd.DataFrame(list(zip(total_critical_scores, total_critical_labels)), columns = ['pos score', 'label'])
fig3, axs3 = plt.subplots()

df4[df4['label'] == 0]['pos score'].plot.hist(ax=axs3, bins=5, alpha=0.5, label='Class 0') 
df4[df4['label'] == 1]['pos score'].plot.hist(ax=axs3, bins=5, alpha=0.5, label='Class 1')
axs3.set_title('Positive Score Distribution')
axs3.legend()

fig3.savefig('critical_reviews_scores_1000(64+186_tokens).png')
File Path: examples/IMDB/ploting/plot_score_varying_lengths.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed
from trl.core import LengthSampler


tqdm.pandas()
import matplotlib.pyplot as plt
import itertools

import pdb
import pickle
import datetime
import os
from evaluate import load
from transformers import AutoModel, AutoTokenizer, AutoConfig
from transformers import OpenAIGPTTokenizer, OpenAIGPTModel
import random

import pandas as pd
import pdb

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
print(date_n_time)
logging_dir = f"/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/plot_score/{date_n_time}"
os.makedirs(logging_dir)

@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name="lvwerra/gpt2-imdb",
            query_dataset="imdb",
            reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            remove_unused_columns=False
        )
    )
    query_dataset: str = field(default="imdb", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="less", metadata={"help": "the dataset to query"})


args = tyro.cli(ScriptArguments)

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead

# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8, data_split = 'train'):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # load imdb with datasets
    # to load partial dataset, just change data split to split=data_split+'[:200]'
    ds = load_dataset(query_dataset, split=data_split)
    ds = ds.rename_columns({"text": "review"})
    ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)

    input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# get test dataset
test_dataset = build_dataset(args.ppo_config, args.query_dataset, data_split='train')
# Take 5k random samples
# random.seed(10)
# sample_size = 2500
# random_indices = random.sample(range(len(test_dataset)), sample_size)
# test_dataset = test_dataset.select(random_indices)

#pdb.set_trace()

def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])

# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

zero_shot_model = trl_model_class.from_pretrained(
    'gpt2',
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)

sft_model = trl_model_class.from_pretrained(
    'lvwerra/gpt2-imdb',
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)

# For the following to work I should have saved the model using huggingface
# ppo_model = trl_model_class.from_pretrained(
#     '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-09-26_11-27-23/model.pt',
#     trust_remote_code=True,
#     device_map=device_map,
#     peft_config=peft_config,
# )

sft_ppo_model = trl_model_class.from_pretrained(
    '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/trl_sft_ppo/2023-10-23_23-30-47',
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)
"""
ppo_model_PATH = '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-09-26_11-27-23/model.pt'
ppo_model = torch.load(ppo_model_PATH)  

sft_ppo_model_PATH = '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_ppo/2023-09-26_10-36-41/model.pt'
sft_ppo_model = torch.load(sft_ppo_model_PATH)
"""
tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"
tokenizer.truncation_side = "left"
tokenizer.pad_token_as_eos_token = True
tokenizer.max_length = 64

# Define all the tester classes 
zero_shot_tester = PPOTrainer(args.ppo_config, zero_shot_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)
sft_tester = PPOTrainer(args.ppo_config, sft_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)
# ppo_tester = PPOTrainer(args.ppo_config, ppo_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)
sft_ppo_tester = PPOTrainer(args.ppo_config, sft_ppo_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)

#pdb.set_trace()
#ppo_tester._save_pretrained('/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-09-26_11-27-23')
#sft_ppo_tester._save_pretrained('/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_ppo/2023-09-26_10-36-41')

# Build the perplexity evaluation pipeline
perplexity = load("perplexity", module_type="measurement")

# DO I NEED TO TAKE CARE OF PAD TOKEN FOR PERPLEXITY PIPELINE AS WELL??
"""
# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id
"""
device = sft_tester.accelerator.device
if sft_tester.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = sft_tester.accelerator.state.deepspeed_plugin
task, model_name = args.ppo_config.reward_model.split(":")
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id


# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs = {
    "min_length": 48,
    "top_k": 50,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": 48,
}


#date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
#os.makedirs(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/evaluation/{date_n_time}')

model_names_all = ['zero_shot_model', 'sft_model', 'ppo_model', 'sft_ppo_model']
testers_all = [zero_shot_tester, sft_tester, sft_ppo_tester] #ppo_tester,

# infer on test dataset
#test_scores_1, test_scores_2, test_scores_3, test_scores_4 = [], [], [], []
positive_score_all = []
negative_score_all = []
label_all = []
critical_pos_texts = []
critical_neg_texts = []
critical_neg_scores = []
critical_pos_scores = []
total_critical_texts = []
total_critical_scores = []
total_critical_labels = []

for epoch, batch in tqdm(enumerate(zero_shot_tester.dataloader)):
    query_tensors = batch["input_ids"]
    
    texts = batch["review"]
    texts = [text[:1000] for text in texts]
    #texts = ["lorem ipsum", "Happy Birthday!", "Bienvenue"]

    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
    pos_score = [output[1]["score"] for output in pipe_outputs]
    positive_score_all.append(pos_score)

    neg_score = [output[0]["score"] for output in pipe_outputs]
    negative_score_all.append(neg_score)

    labels = [label.item() for label in batch["label"]]
    label_all.append(labels)

    for ii in range(len(label_all[0])):
        if labels[ii] == 0 and pos_score[ii] > 0:
            critical_neg_texts.append(texts[ii])
            critical_neg_scores.append(pos_score[ii])
            total_critical_texts.append(texts[ii])
            total_critical_scores.append(pos_score[ii])
            total_critical_labels.append(labels[ii])
        if labels[ii] == 1 and pos_score[ii] < 0:
            critical_pos_texts.append(texts[ii])
            critical_pos_scores.append(pos_score[ii])
            total_critical_texts.append(texts[ii])
            total_critical_scores.append(pos_score[ii])
            total_critical_labels.append(labels[ii])
    
    #test_scores_all[ii].append(sum(perplexity_scores)/len(perplexity_scores))
    #print(f'Epoch: {epoch} - Perplexity for {model_names_all[ii]}: {sum(perplexity_scores)/len(perplexity_scores)}')
        

# get flattened lists 
pos_scores = list(itertools.chain(*positive_score_all))
neg_scores = list(itertools.chain(*negative_score_all))
labels = list(itertools.chain(*label_all))

# save critical reviews as csv 
df1 = pd.DataFrame({'score': critical_neg_scores, 'review': critical_neg_texts})
df2 = pd.DataFrame({'score': critical_pos_scores, 'review': critical_pos_texts})
df3 = pd.DataFrame({'review': total_critical_texts})

df1.to_csv('critical_neg_texts.csv')
df2.to_csv('critical_pos_texts.csv')
df3.to_csv('total_critical_texts.csv')

total_critical_scores = {}

for jj in [256, 448, 560, 720, 1000]:
    texts = [text[:jj] for text in total_critical_texts]

    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
    pos_score = [output[1]["score"] for output in pipe_outputs]

    total_critical_scores[f'{jj}'] = pos_score

# Plot histograms
df = pd.DataFrame(total_critical_scores, columns = ['256', '448', '560', '720', '1000'])
df['label'] = total_critical_labels
fig2, axs2 = plt.subplots(1, 5, figsize=(10,2))
xmin, xmax, ymin, ymax = -3, 3, 0, 200
plt.rcParams['axes.grid'] = True

print('length of critical reviews', len(total_critical_labels))

df[df['label'] == 0]['256'].plot.hist(ax=axs2[0], bins=10, alpha=0.5, label='Class 0')
df[df['label'] == 1]['256'].plot.hist(ax=axs2[0], bins=10, alpha=0.5, label='Class 1')
axs2[0].set_title('256')
axs2[0].legend()
axs2[0].set_xlim([xmin, xmax])
axs2[0].set_ylim([ymin, ymax])

df[df['label'] == 0]['448'].plot.hist(ax=axs2[1], bins=10, alpha=0.5, label='Class 0')
df[df['label'] == 1]['448'].plot.hist(ax=axs2[1], bins=10, alpha=0.5, label='Class 1')
axs2[1].set_title('448')
axs2[1].set_xlim([xmin, xmax])
axs2[1].set_ylim([ymin, ymax])
axs2[1].get_yaxis().set_ticklabels([])
axs2[1].set(ylabel=None)

df[df['label'] == 0]['560'].plot.hist(ax=axs2[2], bins=10, alpha=0.5, label='Class 0')
df[df['label'] == 1]['560'].plot.hist(ax=axs2[2], bins=10, alpha=0.5, label='Class 1')
axs2[2].set_title('560')
axs2[2].set_xlim([xmin, xmax])
axs2[2].set_ylim([ymin, ymax])
axs2[2].get_yaxis().set_ticklabels([])
axs2[2].set(ylabel=None)

df[df['label'] == 0]['720'].plot.hist(ax=axs2[3], bins=10, alpha=0.5, label='Class 0')
df[df['label'] == 1]['720'].plot.hist(ax=axs2[3], bins=10, alpha=0.5, label='Class 1')
axs2[3].set_title('720')
axs2[3].set_xlim([xmin, xmax])
axs2[3].set_ylim([ymin, ymax])
axs2[3].get_yaxis().set_ticklabels([])
axs2[3].set(ylabel=None)

df[df['label'] == 0]['1000'].plot.hist(ax=axs2[4], bins=10, alpha=0.5, label='Class 0')
df[df['label'] == 1]['1000'].plot.hist(ax=axs2[4], bins=10, alpha=0.5, label='Class 1')
axs2[4].set_title('1000')
axs2[4].set_xlim([xmin, xmax])
axs2[4].set_ylim([ymin, ymax])
axs2[4].get_yaxis().set_ticklabels([])
axs2[4].set(ylabel=None)
handles, labels = axs2[4].get_legend_handles_labels()
#fig2.legend(handles, labels, loc='best')
#fig2.legend(handles, labels, loc = (0.5, 0), ncol=5)
#fig2.legend(labels, loc='lower center', bbox_to_anchor=(2, -0.5), ncol=len(labels), bbox_transform=fig2.transFigure)

fig2.savefig('histograms_all_varying_lengths.png')
File Path: examples/IMDB/ploting/risk_schedule.py
Content:

File Path: examples/IMDB/training/old/ppo.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed
from trl.core import LengthSampler

tqdm.pandas()

import pdb
import pickle
import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
print(date_n_time)
logging_dir = f"/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/{date_n_time}"
os.makedirs(logging_dir)

@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name="gpt2",
            query_dataset="imdb",
            reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5, #1e-6, #
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1, #1280, #The number of gradient accumulation steps
            early_stopping=False,
            target_kl=6.0, #1.0,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            #steps= 128000, #64000,
            #init_kl_coef=0.1,
            #vf_coef=0.5,
            #ppo_epochs=5,
            #gamma=0.99, #discount factor = gamma? 
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            remove_unused_columns=False
        )
    )
    query_dataset: str = field(default="imdb", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )


args = tyro.cli(ScriptArguments)


# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8, data_split = 'train'):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # load imdb with datasets
    # to load partial dataset, just change data split to split=data_split+'[:200]'
    ds = load_dataset(query_dataset, split=data_split)
    ds = ds.rename_columns({"text": "review"})
    ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)

    def input_size():
        return 64 #LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)
test_dataset = build_dataset(args.ppo_config, args.query_dataset, data_split='test')

def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    args.ppo_config.model_name,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.padding_side = "left"
tokenizer.truncation_side = "left"
tokenizer.pad_token_as_eos_token = True
tokenizer.max_length = 64

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)
# Creating a tester object here because dataloader is only available in the PPOTrainer class
ppo_tester = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
task, model_name = args.ppo_config.reward_model.split(":")
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs = {
    "min_length": 48,
    "top_k": 50,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": 48,
}

# stat lists
obj_entropy = []
policy_entropy = []

mean_non_score_reward = []
mean_scores = []
std_scores = []

mean_returns = []
var_returns = []

for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]

    # including the line below to deal with sentiment_pipeline's tokenizer
    texts = [text[:512] if len(text) > 512 else text for text in texts]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
    rewards = [torch.tensor(output[1]["score"]) for output in pipe_outputs]
    
    """
    rewards = [
        torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"]]))[1]
        for output in pipe_outputs
    ]
    """
    # Run PPO step
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
    ppo_trainer.log_stats(stats, batch, rewards)
    
    # print stats
    print(f'input:{batch["query"][:3]}, output:{batch["response"][:3]} \n\n')
    
    obj_entropy.append(stats['objective/entropy'])
    policy_entropy.append(stats['ppo/policy/entropy'])
    mean_non_score_reward.append(stats['ppo/mean_non_score_reward'])
    mean_scores.append(stats['ppo/mean_scores'])
    std_scores.append(stats['ppo/std_scores'])
    mean_returns.append(stats['ppo/returns/mean'])
    var_returns.append(stats['ppo/returns/var'])

    print(f'epoch: {epoch}')
    mean_return = stats['ppo/returns/mean']
    mean_score = stats['ppo/mean_scores']
    print(f'mean_returns: {mean_return}, mean_score: {mean_score}')


# save the final model - maybe this will unnecessarily take memory? What is a good way to go about it? 

#torch.save(model, logging_dir +'/model.pt')
#ppo_trainer.create_model_card(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/{date_n_time}')
#ppo_trainer._save_pretrained(logging_dir)

# save the train stats 
#train_stats_lists = [obj_entropy, policy_entropy, mean_non_score_reward, mean_scores, std_scores, mean_returns, var_returns]
#train_stats_lists_names = ['obj_entropy', 'policy_entropy', 'mean_non_score_reward', 'mean_scores', 'std_scores', 'mean_returns', 'var_returns']
#for ii in range(len(train_stats_lists)):
#    pickle.dump(train_stats_lists[ii], open(logging_dir + f'/{train_stats_lists_names[ii]}', "wb"))


"""
# infer on test dataset
test_scores = []
for epoch, batch in tqdm(enumerate(ppo_tester.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
    rewards = [
        torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"]]))[1]
        for output in pipe_outputs
    ]

    test_scores.append(sum(rewards)/len(rewards))
    print(sum(rewards)/len(rewards))

# print and save the final score  
final_test_score = sum(test_scores)/len(test_scores)     
print(f'Total test sentiment-score is: {final_test_score.item()}')
pickle.dump(final_test_score.item(), open(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/{date_n_time}/final_test_score', "wb"))
"""

    





File Path: examples/IMDB/training/old/sft_lvwerra.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed
from trl.core import LengthSampler


tqdm.pandas()

import pdb
import pickle
import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import random
import statistics
from transformers import GPT2Config
import re

@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name="lvwerra/gpt2-imdb",
            query_dataset="imdb",
            reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
        )
    )
    query_dataset: str = field(default="imdb", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )


args = tyro.cli(ScriptArguments)


# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8, data_split = 'train'):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # load imdb with datasets
    # to load partial dataset, just change data split to split=data_split+'[:200]'
    ds = load_dataset(query_dataset, split=data_split)
    ds = ds.rename_columns({"text": "review"})
    ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)

    def input_size():
        return 64 #LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)

# build and sample test dataset
test_dataset = build_dataset(args.ppo_config, args.query_dataset, data_split='test')
# Take 5k random samples
sample_size = 5000
random_indices = random.sample(range(len(test_dataset)), sample_size)
test_dataset = test_dataset.select(random_indices)

def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

"""
model = trl_model_class.from_pretrained(
    args.ppo_config.model_name,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)
model = trl_model_class.from_pretrained(
    '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_positive/2023-10-03_00-17-00/pytorch_model.bin',
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)
"""

pdb.set_trace()
config = GPT2Config.from_json_file('/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_positive/2023-10-03_00-17-00/config.json')
#config = GPT2Config.from_pretrained('/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_positive/2023-10-03_00-17-00/config.json')
state_dict = torch.load('/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_positive/2023-10-03_00-17-00/pytorch_model.bin')

model = trl_model_class.from_config(config) 
model.load_state_dict(state_dict)
"""# Strip transformer. prefix from state dict 
new_state_dict = {}
for key, value in state_dict.items():
    new_key = key.replace("transformer.", "")
    new_state_dict[new_key] = value

# Remove 'lm_head' keys
cleaned_dict = {k: v for k, v in state_dict.items() if not re.search(r'lm_head.', k)} 

# Load state dict
model.load_state_dict(cleaned_dict)
"""

tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.padding_side = "left"
tokenizer.truncation_side = "left"
tokenizer.pad_token_as_eos_token = True
tokenizer.max_length = 64

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)
# Creating a tester object here because dataloader is only available in the PPOTrainer class
ppo_tester = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
task, model_name = args.ppo_config.reward_model.split(":")
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs = {
    "min_length": 48,
    "top_k": 50,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": 48,
}


# infer on test dataset
test_scores = []
for epoch, batch in tqdm(enumerate(ppo_tester.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_tester.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
    #rewards = [torch.tensor(output[1]["score"]) for output in pipe_outputs]
    rewards = [
        torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"]]))[1]
        for output in pipe_outputs
    ]

    test_scores.append(sum(rewards)/len(rewards))
    print(sum(rewards)/len(rewards))

# print and save the final score  
date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
os.makedirs(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_lvwerra/{date_n_time}')
final_test_score = sum(test_scores)/len(test_scores)     
test_scores_list = [t.item() for t in test_scores]
print(f'Total test sentiment-score and std is: {final_test_score.item(), statistics.stdev(test_scores_list)}')
pickle.dump([final_test_score.item(), statistics.stdev(test_scores_list)], open(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_lvwerra/{date_n_time}/final_test_score', "wb"))




File Path: examples/IMDB/training/old/sft_positive.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoModelForCausalLM, BitsAndBytesConfig, HfArgumentParser, TrainingArguments

from trl import SFTTrainer


tqdm.pandas()

import pdb
from transformers import GenerationConfig
from transformers import AutoTokenizer, pipeline, Seq2SeqTrainingArguments
import datetime
import os

# Define and parse arguments.
@dataclass
class ScriptArguments:
    """
    The name of the Casual LM model we wish to fine-tune with SFTTrainer
    """

    model_name: Optional[str] = field(default="gpt2", metadata={"help": "the model name"})
    dataset_name: Optional[str] = field(
        default="imdb", metadata={"help": "the dataset name"}
    )
    dataset_text_field: Optional[str] = field(default="text", metadata={"help": "the text field of the dataset"})
    log_with: Optional[str] = field(default="none", metadata={"help": "use 'wandb' to log with wandb"})
    
    # learning params
    learning_rate: Optional[float] = field(default=0.00001, metadata={"help": "the learning rate"}) #default=1.41e-5
    batch_size: Optional[int] = field(default=64, metadata={"help": "the batch size"})
    # Using tokenizer max length=64 from RL4LLMs Table 4
    seq_length: Optional[int] = field(default=64, metadata={"help": "Input sequence length"}) #default=512
    gradient_accumulation_steps: Optional[int] = field(
        default=16, metadata={"help": "the number of gradient accumulation steps"}
    )
    num_train_epochs: Optional[int] = field(default=10, metadata={"help": "the number of training epochs"}) #default=3
    max_steps: Optional[int] = field(default=-1, metadata={"help": "the number of training steps"})

    # model download, quantization and peft params
    trust_remote_code: Optional[bool] = field(default=True, metadata={"help": "Enable `trust_remote_code`"})
    use_auth_token: Optional[bool] = field(default=True, metadata={"help": "Use HF auth token to access the model"})
    load_in_8bit: Optional[bool] = field(default=False, metadata={"help": "load the model in 8 bits precision"})
    load_in_4bit: Optional[bool] = field(default=False, metadata={"help": "load the model in 4 bits precision"})
    use_peft: Optional[bool] = field(default=False, metadata={"help": "Wether to use PEFT or not to train adapters"})
    peft_lora_r: Optional[int] = field(default=64, metadata={"help": "the r parameter of the LoRA adapters"})
    peft_lora_alpha: Optional[int] = field(default=16, metadata={"help": "the alpha parameter of the LoRA adapters"})

    # logging
    output_dir: Optional[str] = field(default="/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_positive/", metadata={"help": "the output directory"})
    logging_steps: Optional[int] = field(default=1, metadata={"help": "the number of logging steps"})
    save_steps: Optional[int] = field(
        default=100, metadata={"help": "Number of updates steps before two checkpoint saves"}
    )
    save_total_limit: Optional[int] = field(default=10, metadata={"help": "Limits total number of checkpoints."})

    # post processing
    push_to_hub: Optional[bool] = field(default=False, metadata={"help": "Push the model to HF Hub"})
    hub_model_id: Optional[str] = field(default=None, metadata={"help": "The name of the model on HF Hub"})


parser = HfArgumentParser(ScriptArguments)
script_args = parser.parse_args_into_dataclasses()[0]

# Step 1: Load the model
if script_args.load_in_8bit and script_args.load_in_4bit:
    raise ValueError("You can't load the model in 8 bits and 4 bits at the same time")
elif script_args.load_in_8bit or script_args.load_in_4bit:
    quantization_config = BitsAndBytesConfig(
        load_in_8bit=script_args.load_in_8bit, load_in_4bit=script_args.load_in_4bit
    )
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}
    torch_dtype = torch.bfloat16
else:
    device_map = None
    quantization_config = None
    torch_dtype = None

model = AutoModelForCausalLM.from_pretrained(
    script_args.model_name,
    quantization_config=quantization_config,
    device_map=device_map,
    trust_remote_code=script_args.trust_remote_code,
    torch_dtype=torch_dtype)

# Removing huggingface authorization token param from model loading command above 
#    use_auth_token=script_args.use_auth_token,
# Make sure you have logged in to the Hugging Face Hub using huggingface-cli login 
# or by calling huggingface_hub.login() in Python. This will save an authentication 
# token that Transformers will pick up automatically.

# Step 2: Load the dataset
dataset = load_dataset(script_args.dataset_name, split="train")

# Filter dataset for only positive reviews
dataset = dataset.filter(lambda x: x["label"] == 1, batched=False)


# Step 3: Define generation arguments
generation_kwargs = GenerationConfig(
    min_length = 48,
    top_k = 50,
    top_p = 1.0,
    do_sample = True,
    max_new_tokens = 48,
)
#pad_token_id = tokenizer.eos_token_id,

# Step 3: Define the training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir=script_args.output_dir,
    per_device_train_batch_size=script_args.batch_size,
    gradient_accumulation_steps=script_args.gradient_accumulation_steps,
    learning_rate=script_args.learning_rate,
    logging_steps=script_args.logging_steps,
    num_train_epochs=script_args.num_train_epochs,
    max_steps=script_args.max_steps,
    report_to=script_args.log_with,
    save_steps=script_args.save_steps,
    save_total_limit=script_args.save_total_limit,
    push_to_hub=script_args.push_to_hub,
    hub_model_id=script_args.hub_model_id,
    generation_config=generation_kwargs
)

# Step 4: Define the LoraConfig
if script_args.use_peft:
    peft_config = LoraConfig(
        r=script_args.peft_lora_r,
        lora_alpha=script_args.peft_lora_alpha,
        bias="none",
        task_type="CAUSAL_LM",
    )
else:
    peft_config = None


# trl sft trainer default tokenizer and pad_token
"""
if tokenizer is None:
            tokenizer = AutoTokenizer.from_pretrained(model.config._name_or_path)
            if getattr(tokenizer, "pad_token", None) is None:
                tokenizer.pad_token = tokenizer.eos_token

if max_seq_length is None:
            # to overcome some issues with broken tokenizers
            max_seq_length = min(tokenizer.model_max_length, 1024)

            warnings.warn(
                f"You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to {max_seq_length}"
            )

"""

# Looks like padding_side = "right" by default
# Hence, defining our tokenizer here
tokenizer = AutoTokenizer.from_pretrained(script_args.model_name)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"
tokenizer.truncation_side = "left"
tokenizer.pad_token_as_eos_token = True
tokenizer.max_length = 64

# Step 5: Define the Trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    max_seq_length=script_args.seq_length,
    train_dataset=dataset,
    dataset_text_field=script_args.dataset_text_field,
    peft_config=peft_config,
    tokenizer=tokenizer
)

trainer.train()

# Step 6: Save the model
date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
save_dir_path = f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_positive/{date_n_time}'
os.makedirs(save_dir_path)
trainer.save_model(save_dir_path)
# save the final model - maybe this will unnecessarily take memory? What is a good way to go about it? 
torch.save(model, f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_positive/{date_n_time}/model.pt')
#ppo_trainer.create_model_card(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/{date_n_time}')

File Path: examples/IMDB/training/old/sft_ppo.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed
from trl.core import LengthSampler

tqdm.pandas()

import pdb
import pickle
import datetime
import os

@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name="lvwerra/gpt2-imdb",
            query_dataset="imdb",
            reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
        )
    )
    query_dataset: str = field(default="imdb", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )


args = tyro.cli(ScriptArguments)


# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8, data_split = 'train'):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # load imdb with datasets
    # to load partial dataset, just change data split to split=data_split+'[:200]'
    ds = load_dataset(query_dataset, split=data_split)
    ds = ds.rename_columns({"text": "review"})
    ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)

    input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)
test_dataset = build_dataset(args.ppo_config, args.query_dataset, data_split='test')

def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    args.ppo_config.model_name,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)
# Creating a tester object here because dataloader is only available in the PPOTrainer class
ppo_tester = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
task, model_name = args.ppo_config.reward_model.split(":")
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs = {
    "min_length": -1,
    "top_k": 0.0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": 32,
}

# stat lists
obj_entropy = []
policy_entropy = []

mean_non_score_reward = []
mean_scores = []
std_scores = []

mean_returns = []
var_returns = []

for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
    rewards = [torch.tensor(output[1]["score"]) for output in pipe_outputs]

    # Run PPO step
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
    ppo_trainer.log_stats(stats, batch, rewards)
    
    obj_entropy.append(stats['objective/entropy'])
    policy_entropy.append(stats['ppo/policy/entropy'])
    mean_non_score_reward.append(stats['ppo/mean_non_score_reward'])
    mean_scores.append(stats['ppo/mean_scores'])
    std_scores.append(stats['ppo/std_scores'])
    mean_returns.append(stats['ppo/returns/mean'])
    var_returns.append(stats['ppo/returns/var'])

    print(f'epoch: {epoch}')
    mean_return = stats['ppo/returns/mean']
    mean_score = stats['ppo/mean_scores']
    print(f'mean_returns: {mean_return}, mean_score: {mean_score}')


# save the final model - maybe this will unnecessarily take memory? What is a good way to go about it? 
date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
os.makedirs(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_ppo/{date_n_time}')
torch.save(model, f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_ppo/{date_n_time}/model.pt')
#ppo_trainer.create_model_card(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_ppo/{date_n_time}')

# save the train stats 
train_stats_lists = [obj_entropy, policy_entropy, mean_non_score_reward, mean_scores, std_scores, mean_returns, var_returns]
train_stats_lists_names = ['obj_entropy', 'policy_entropy', 'mean_non_score_reward', 'mean_scores', 'std_scores', 'mean_returns', 'var_returns']
for ii in range(len(train_stats_lists)):
    pickle.dump(train_stats_lists[ii], open(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_ppo/{date_n_time}/{train_stats_lists_names[ii]}', "wb"))


# infer on test dataset
test_scores = []
for epoch, batch in tqdm(enumerate(ppo_tester.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)  ### SHOULD THIS BE ppo_tester instead? 
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
    rewards = [torch.tensor(output[1]["score"]) for output in pipe_outputs]

    test_scores.append(sum(rewards)/len(rewards))

# print and save the final score  
final_test_score = sum(test_scores)/len(test_scores)     
print(f'Total test sentiment-score is: {final_test_score.item()}')
pickle.dump(final_test_score.item(), open(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_ppo/{date_n_time}/final_test_score', "wb"))


    





File Path: examples/IMDB/training/old/zero_shot_vanilla.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed
from transformers import AutoModelForCausalLM, BitsAndBytesConfig, HfArgumentParser, TrainingArguments
from trl.core import LengthSampler


tqdm.pandas()

import pdb
import pickle
import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import statistics
import random
from torch.utils.data import DataLoader
from typing import Callable, List, Optional, Union
import datasets
from datasets import Dataset

# Define and parse arguments.
@dataclass
class ScriptArguments:
    """
    The name of the Casual LM model we wish to fine-tune with SFTTrainer
    """

    model_name: Optional[str] = field(default="gpt2", metadata={"help": "the model name"})
    reward_model: Optional[str] = field(default="lvwerra/distilbert-imdb", metadata={"help": "model to obtain desired task score, e.g, sentiment score"})
    task_type: Optional[str] = field(default="sentiment-analysis", metadata={"help": "task type"})
    dataset_name: Optional[str] = field(
        default="imdb", metadata={"help": "the dataset name"}
    )
    dataset_text_field: Optional[str] = field(default="text", metadata={"help": "the text field of the dataset"})
    log_with: Optional[str] = field(default="none", metadata={"help": "use 'wandb' to log with wandb"})
    seed: Optional[int] = field(default=0, metadata={"help": "seed"})
    
    # learning params
    learning_rate: Optional[float] = field(default=0.00001, metadata={"help": "the learning rate"}) #default=1.41e-5
    batch_size: Optional[int] = field(default=64, metadata={"help": "the batch size"})
    # Using tokenizer max length=64 from RL4LLMs Table 4
    seq_length: Optional[int] = field(default=64, metadata={"help": "Input sequence length"}) #default=512
    gradient_accumulation_steps: Optional[int] = field(
        default=16, metadata={"help": "the number of gradient accumulation steps"}
    )
    num_train_epochs: Optional[int] = field(default=10, metadata={"help": "the number of training epochs"}) #default=3
    max_steps: Optional[int] = field(default=-1, metadata={"help": "the number of training steps"})

    # model download, quantization and peft params
    trust_remote_code: Optional[bool] = field(default=True, metadata={"help": "Enable `trust_remote_code`"})
    use_auth_token: Optional[bool] = field(default=True, metadata={"help": "Use HF auth token to access the model"})
    load_in_8bit: Optional[bool] = field(default=False, metadata={"help": "load the model in 8 bits precision"})
    load_in_4bit: Optional[bool] = field(default=False, metadata={"help": "load the model in 4 bits precision"})
    use_peft: Optional[bool] = field(default=False, metadata={"help": "Wether to use PEFT or not to train adapters"})
    peft_lora_r: Optional[int] = field(default=64, metadata={"help": "the r parameter of the LoRA adapters"})
    peft_lora_alpha: Optional[int] = field(default=16, metadata={"help": "the alpha parameter of the LoRA adapters"})

    # logging
    output_dir: Optional[str] = field(default="/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_positive/", metadata={"help": "the output directory"})
    logging_steps: Optional[int] = field(default=1, metadata={"help": "the number of logging steps"})
    save_steps: Optional[int] = field(
        default=100, metadata={"help": "Number of updates steps before two checkpoint saves"}
    )
    save_total_limit: Optional[int] = field(default=10, metadata={"help": "Limits total number of checkpoints."})

    # post processing
    push_to_hub: Optional[bool] = field(default=False, metadata={"help": "Push the model to HF Hub"})
    hub_model_id: Optional[str] = field(default=None, metadata={"help": "The name of the model on HF Hub"})

parser = HfArgumentParser(ScriptArguments)
args = parser.parse_args_into_dataclasses()[0]

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8, data_split = 'train'):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # load imdb with datasets
    # to load partial dataset, just change data split to split=data_split+'[:200]'
    ds = load_dataset(query_dataset, split=data_split)
    ds = ds.rename_columns({"text": "review"})
    ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)

    def input_size():
        return 64 #LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds

#what is the purpose of data collator? 
def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])

# function to prepare dataloader as one included in ppo trainer class
def prepare_dataloader(dataset: Union[torch.utils.data.Dataset, Dataset], data_collator=None, batch_size=128):
        """
        Prepare the dataloader for training.

        Args:
            dataset (Union[`torch.utils.data.Dataset`, `datasets.Dataset`]):
                PyTorch dataset or Hugging Face dataset. If a Hugging Face dataset is passed, the dataset
                will be preprocessed by removing the columns that are not used by the model.
            data_collator (Optional[function]):
                Data collator function.

        Returns:
            `torch.utils.data.DataLoader`: PyTorch dataloader
        """
        
        dataloader = torch.utils.data.DataLoader(
            dataset,
            batch_size=batch_size,
            collate_fn=data_collator,
            shuffle=True,
            drop_last=True,
        )
        return dataloader


# build and sample test dataset
test_dataset = build_dataset(args, args.dataset_name, data_split='test')

# Take 5k random samples
sample_size = 5000
random_indices = random.sample(range(len(test_dataset)), sample_size)
test_dataset = test_dataset.select(random_indices)

#create dataloader object
dataloader = prepare_dataloader(test_dataset, collator, batch_size=args.batch_size)

# set seed before initializing value head for deterministic eval
set_seed(args.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = AutoModelForCausalLM.from_pretrained(
    args.model_name,
    device_map=device_map,
    trust_remote_code=True,
    torch_dtype=None)

tokenizer = AutoTokenizer.from_pretrained(args.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.padding_side = "left"
tokenizer.truncation_side = "left"
tokenizer.pad_token_as_eos_token = True
tokenizer.max_length = 64

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
task, model_name = args.task_type, args.reward_model
sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs = {
    "min_length": 48,
    "top_k": 50,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": 48,
}

# generation pipeline
generator = pipeline('text-generation', model=model, **generation_kwargs)


# infer on test dataset
test_scores = []
for epoch, batch in tqdm(enumerate(dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2 model
    response_tensors = model(query_tensors, **generation_kwargs)
    response_tensors = ppo_tester.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
    #rewards = [torch.tensor(output[1]["score"]) for output in pipe_outputs]
    rewards = [
        torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"]]))[1]
        for output in pipe_outputs
    ]

    test_scores.append(sum(rewards)/len(rewards))
    print(sum(rewards)/len(rewards))

# print and save the final score  
date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
os.makedirs(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/zero_shot/{date_n_time}')
final_test_score = sum(test_scores)/len(test_scores)     
test_scores_list = [t.item() for t in test_scores]
print(f'Total test sentiment-score and std is: {final_test_score.item(), statistics.stdev(test_scores_list)}')
pickle.dump(final_test_score.item(), open(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/zero_shot/{date_n_time}/final_test_score', "wb"))




File Path: examples/IMDB/training/ppo_auth1.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed
from trl.core import LengthSampler


tqdm.pandas()

import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import pdb
import pandas as pd

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
logging_dir = f"/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/dummy/{date_n_time}"
os.makedirs(logging_dir)

@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name="lvwerra/gpt2-imdb",
            query_dataset="imdb",
            reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            #log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            steps= 128000,
            exp_name="ppo"
        )
    )
    query_dataset: str = field(default="imdb", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="less", metadata={"help": "the dataset to query"})
    
    generation_kwargs_min_length: Optional[int] = field(default=-1, metadata={"help": "minimum number of tokens while generation"})
    generation_kwargs_top_k: Optional[int] = field(default=0, metadata={"help": "gneration top k"})
    generation_kwargs_top_p: Optional[float] = field(default=1.0, metadata={"help": "gneration top p"})
    generation_kwargs_max_new_tokens: Optional[int] = field(default=32, metadata={"help": "gneration top p"})

    change_tokenizer_args: bool = field(default=False, metadata={"help": "whether to use modify tokenizer settings"})
    tokenizer_kwargs: dict = field(
    default_factory=lambda: {
      "padding_side": "left", 
      "truncation_side": "left",
      "pad_token_as_eos_token": True,
      "max_length": 64
    }
  )



args = tyro.cli(ScriptArguments)

generation_kwargs = {
      "min_length": args.generation_kwargs_min_length, 
      "top_k": args.generation_kwargs_top_k,
      "top_p": args.generation_kwargs_top_p,
      "do_sample": True,
      "max_new_tokens": args.generation_kwargs_max_new_tokens
    }

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # load imdb with datasets: for reducing data size, just use +'[:200]'
    ds = load_dataset(query_dataset, split="train")
    ds = ds.rename_columns({"text": "review"})
    ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)


    if args.input_query_size == "more":
        def input_size():
            return 64 
    else: 
        input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    args.ppo_config.model_name,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)

tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

pdb.set_trace()
# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id

#pdb.set_trace()

if args.change_tokenizer_args == True: 
    tokenizer.padding_side = args.tokenizer_kwargs['padding_side']
    tokenizer.truncation_side = args.tokenizer_kwargs['truncation_side']
    tokenizer.pad_token_as_eos_token = args.tokenizer_kwargs['pad_token_as_eos_token']
    tokenizer.max_length = args.tokenizer_kwargs['max_length']

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
task, model_name = args.ppo_config.reward_model.split(":")
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

pdb.set_trace()
# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs['pad_token_id'] = tokenizer.eos_token_id

# empty list to store text data
text_train_data = []

for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)

    if args.reward_function == "trl":
        rewards = [torch.tensor(output[1]["score"]) for output in pipe_outputs]
    else: 
        rewards = [
            torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"]]))[1]
            for output in pipe_outputs
        ]
    
    for ii in range(3): 
        data_row = {'iter': epoch, 'input':batch["query"][ii], 'output':batch["response"][ii], 'score':rewards[ii].item()}
        text_train_data.append(data_row)

    # Run PPO step
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
    ppo_trainer.log_stats(stats, batch, rewards)
    print(epoch)

ppo_trainer._save_pretrained(logging_dir)
df = pd.DataFrame(text_train_data)
df.to_csv(logging_dir+'/textual_data.csv', index=False)
File Path: examples/IMDB/training/ppo_auth1_2.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed
from trl.core import LengthSampler


tqdm.pandas()

import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import pdb
import pandas as pd

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
logging_dir = f"/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/r_diff/{date_n_time}"
os.makedirs(logging_dir)

@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name="lvwerra/gpt2-imdb",
            query_dataset="imdb",
            reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            #log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            steps= 128000,
            exp_name="ppo"
        )
    )
    query_dataset: str = field(default="imdb", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="less", metadata={"help": "the dataset to query"})
    
    generation_kwargs_min_length: Optional[int] = field(default=-1, metadata={"help": "minimum number of tokens while generation"})
    generation_kwargs_top_k: Optional[int] = field(default=0, metadata={"help": "gneration top k"})
    generation_kwargs_top_p: Optional[float] = field(default=1.0, metadata={"help": "gneration top p"})
    generation_kwargs_max_new_tokens: Optional[int] = field(default=32, metadata={"help": "gneration top p"})

    change_tokenizer_args: bool = field(default=False, metadata={"help": "whether to use modify tokenizer settings"})
    tokenizer_kwargs: dict = field(
    default_factory=lambda: {
      "padding_side": "left", 
      "truncation_side": "left",
      "pad_token_as_eos_token": True,
      "max_length": 64
    }
  )



args = tyro.cli(ScriptArguments)

generation_kwargs = {
      "min_length": args.generation_kwargs_min_length, 
      "top_k": args.generation_kwargs_top_k,
      "top_p": args.generation_kwargs_top_p,
      "do_sample": True,
      "max_new_tokens": args.generation_kwargs_max_new_tokens
    }

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # load imdb with datasets: for reducing data size, just use +'[:200]'
    ds = load_dataset(query_dataset, split="train")
    ds = ds.rename_columns({"text": "review"})
    ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)


    if args.input_query_size == "more":
        def input_size():
            return 64 
    else: 
        input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    args.ppo_config.model_name,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id

#pdb.set_trace()

if args.change_tokenizer_args == True: 
    tokenizer.padding_side = args.tokenizer_kwargs['padding_side']
    tokenizer.truncation_side = args.tokenizer_kwargs['truncation_side']
    tokenizer.pad_token_as_eos_token = args.tokenizer_kwargs['pad_token_as_eos_token']
    tokenizer.max_length = args.tokenizer_kwargs['max_length']

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
task, model_name = args.ppo_config.reward_model.split(":")
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs['pad_token_id'] = tokenizer.eos_token_id

# empty list to store text data
text_train_data = []

for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_x_0 = sentiment_pipe(batch["query"], **sent_kwargs)
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)

    if args.reward_function == "trl":
        rewards = [torch.tensor(output[1]["score"]) for output in pipe_outputs]
        rewards_x_0 = [torch.tensor(output[1]["score"]) for output in pipe_x_0]
        rewards = [r1 - r2 for r1, r2 in zip(rewards, rewards_x_0)]
    else: 
        rewards = [
            torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"]]))[1]
            for output in pipe_outputs
        ]
        rewards_x_0 = [
            torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"]]))[1]
            for output in pipe_x_0
        ]
        rewards = [r1 - r2 for r1, r2 in zip(rewards, rewards_x_0)]
    
    for ii in range(3): 
        data_row = {'iter': epoch, 'input':batch["query"][ii], 'output':batch["response"][ii], 'score':rewards[ii].item()}
        text_train_data.append(data_row)

    # Run PPO step
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
    ppo_trainer.log_stats(stats, batch, rewards)
    print(epoch)

ppo_trainer._save_pretrained(logging_dir)
df = pd.DataFrame(text_train_data)
df.to_csv(logging_dir+'/textual_data.csv', index=False)
File Path: examples/IMDB/training/ppo_big_imdb.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
from datasets import load_dataset, Dataset
from torch.optim import Adam
from tqdm import tqdm
from transformers import (
    AutoModelForSequenceClassification,
    AutoModelForCausalLM,
    AutoTokenizer,
    HfArgumentParser,
    RobertaForSequenceClassification,
    RobertaTokenizer,
    DistilBertForSequenceClassification,
    DistilBertTokenizerFast,
)
from peft import LoraConfig
from trl import (
    AutoModelForCausalLMWithValueHead,
    PPOConfig,
    create_reference_model,
    set_seed,
)
from trl.trainer.ppo_trainer_original import PPOTrainer
from trl.core import LengthSampler
import datetime
import os
import pandas as pd

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")

logging_dir = f"/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/big/imdb/{date_n_time}"
os.makedirs(logging_dir, exist_ok=True)

tqdm.pandas()

########################################################################
# This is a fully working simple example to use trl with accelerate.
#
# This example fine-tunes a GPTJ model to generate less toxic contents
# by using allenai/real-toxicity-prompts dataset. We use PPO
#  (proximal policy optimization) to optimize the model.
# in any of the following settings (with the same script):
#   - single CPU or single GPU
#   - multi GPUS (using PyTorch distributed mode)
#   - multi GPUS (using DeepSpeed ZeRO-Offload stages 1 & 2)
#   - fp16 (mixed-precision) or fp32 (normal precision)
#
# To run it in each of these various modes, first initialize the accelerate
# configuration with `accelerate config`
#
########################################################################


# We first define the configuration of the experiment, defining the model, the dataset,
# the training parameters, and the PPO parameters.
# Check the default arguments in the `PPOConfig` class for more details.
# If you want to log with tensorboard, add the kwarg
# `project_kwargs={"logging_dir": PATH_TO_LOGS}` to the PPOConfig.
@dataclass
class ScriptArguments:
    """
    The name of the Casual LM model we wish to fine-tune with PPO
    """

    # NOTE: gpt2 models use Conv1D instead of Linear layers which are not yet supported in 8 bit mode
    # models like gpt-neo* models are more suitable.
    model_name: Optional[str] = field(default="ybelkada/gpt-j-6b-sharded-bf16", metadata={"help": "the model name"})
    log_with: Optional[str] = field(default="tensorboard", metadata={"help": "use 'wandb' to log with wandb"})
    learning_rate: Optional[float] = field(default=(1.47e-5) * 2, metadata={"help": "the learning rate"})
    mini_batch_size: Optional[int] = field(default=8, metadata={"help": "the PPO minibatch size"})
    batch_size: Optional[int] = field(default=8, metadata={"help": "the batch size"})
    gradient_accumulation_steps: Optional[int] = field(
        default=1, metadata={"help": "the number of gradient accumulation steps"}
    )
    seed: Optional[int] = field(default=42, metadata={"help": "the seed for the experiment"})
    exp_name: Optional[str] = field(default="imdb", metadata={"help": "the name of the experiment"})
    query_dataset: Optional[str] = field(default="imdb", metadata={"help": "the name of the dataset"})


model_save_path = os.path.join(logging_dir, "save_pretrained")
prompt_len = 64
gen_len = 48

parser = HfArgumentParser(ScriptArguments)
script_args = parser.parse_args_into_dataclasses()[0]

config = PPOConfig(
    model_name=script_args.model_name,
    learning_rate=script_args.learning_rate,
    log_with=script_args.log_with,
    project_kwargs={"logging_dir": logging_dir},
    # ppo_epochs=100,
    mini_batch_size=script_args.mini_batch_size,
    batch_size=script_args.batch_size,
    gradient_accumulation_steps=script_args.gradient_accumulation_steps,
    seed=script_args.seed,
    exp_name=script_args.exp_name,
)

peft_config = LoraConfig(
    r=16,
    lora_alpha=16,
    bias="none",
    task_type="CAUSAL_LM",
)


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.


def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # load imdb with datasets: for reducing data size, just use +'[:200]'
    ds = load_dataset(query_dataset, split="train")
    ds = ds.rename_columns({"text": "review"})
    ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)

    def input_size():
        return prompt_len

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(config, script_args.query_dataset)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(config.seed)

# Now let's build the model, the reference model, and the tokenizer. We first load the model
# in bfloat16 to save memory using `transformers`.
model = AutoModelForCausalLM.from_pretrained(config.model_name, torch_dtype=torch.bfloat16)
# And then we pass the loaded model to `AutoModelForCausalLMWithValueHead`.
model = AutoModelForCausalLMWithValueHead.from_pretrained(model, peft_config=peft_config)

# We create a reference model by sharing 20 layers
# ref_model = create_reference_model(model, num_shared_layers=20)
ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(
    "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/IMDB/2024-02-18_22-24-38",
    trust_remote_code=True,
)  # sft positive 70_30 gpt2

# We make sure to use `Adam` optimizer on the model parameters that require gradients.
optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=config.learning_rate)

# GPT-2 / GPT-J tokenizer has a pad token, but it is not eos_token by default. We need to set it to eos_token.
# only for this model.
tokenizer = AutoTokenizer.from_pretrained(config.model_name)
tokenizer.pad_token = tokenizer.eos_token

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(
    config,
    model,
    ref_model=ref_model,
    tokenizer=tokenizer,
    dataset=dataset,
    data_collator=collator,
    optimizer=optimizer,
)

# We then build the reward pipeline, we will use the toxicity model to compute the reward.
# We first load the toxicity model and tokenizer.
# toxicity_model_id = "facebook/roberta-hate-speech-dynabench-r4-target"
# toxicity_tokenizer = RobertaTokenizer.from_pretrained(toxicity_model_id)
# # We load the toxicity model in fp16 to save memory.
# toxicity_model = RobertaForSequenceClassification.from_pretrained(toxicity_model_id, torch_dtype=torch.float16).to(
#     ppo_trainer.accelerator.device
# )

toxicity_tokenizer = AutoTokenizer.from_pretrained("lvwerra/distilbert-imdb")
toxicity_model = AutoModelForSequenceClassification.from_pretrained(
    "lvwerra/distilbert-imdb", torch_dtype=torch.float16
).to(ppo_trainer.accelerator.device)


# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs = {
    "min_length": gen_len,
    "top_k": 0.0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
}
# output_min_length = 20
# output_max_length = 30
# output_length_sampler = LengthSampler(output_min_length, output_max_length)

# model_save_path = script_args.model_save_path

for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from the policy model
    response_tensors = []
    for query in query_tensors:
        # gen_len = output_length_sampler()
        # generation_kwargs["max_new_tokens"] = gen_len
        generation_kwargs["max_new_tokens"] = gen_len
        response = ppo_trainer.generate(query, **generation_kwargs)
        response_tensors.append(response.squeeze()[-gen_len:])
    batch["response"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]

    # Compute sentiment score # noqa
    texts = batch["response"]
    toxicity_inputs = toxicity_tokenizer(texts, padding=True, truncation=True, return_tensors="pt").to(
        ppo_trainer.accelerator.device
    )
    logits = toxicity_model(**toxicity_inputs).logits.float()
    toxicity_labels = (logits[:, 1]).tolist()

    rewards = [torch.tensor(output) for output in toxicity_labels]

    # Run PPO step
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
    ppo_trainer.log_stats(stats, batch, rewards)

    # Save model every 100 epochs
    if epoch % 100 == 0:
        if ppo_trainer.accelerator.is_main_process:
            ppo_trainer.save_pretrained(model_save_path)
ppo_trainer.save_pretrained(model_save_path)

File Path: examples/IMDB/training/sft_auth1.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoModelForCausalLM, BitsAndBytesConfig, HfArgumentParser, TrainingArguments

from trl import SFTTrainer


tqdm.pandas()

import pdb
from transformers import GenerationConfig
from transformers import AutoTokenizer, pipeline, Seq2SeqTrainingArguments
import datetime
import os
from trl import set_seed

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
logging_dir = f"/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_final/{date_n_time}"
os.makedirs(logging_dir)

# Define and parse arguments.
@dataclass
class ScriptArguments:
    """
    The name of the Casual LM model we wish to fine-tune with SFTTrainer
    """

    model_name: Optional[str] = field(default="gpt2", metadata={"help": "the model name"})
    dataset_name: Optional[str] = field(
        default="imdb", metadata={"help": "the dataset name"}
    )
    dataset_text_field: Optional[str] = field(default="text", metadata={"help": "the text field of the dataset"})
    log_with: Optional[str] = field(default="tensorboard", metadata={"help": "use 'wandb' to log with wandb"})
    
    # learning params
    learning_rate: Optional[float] = field(default=0.00001, metadata={"help": "the learning rate"}) #default=1.41e-5
    batch_size: Optional[int] = field(default=64, metadata={"help": "the batch size"})
    # Using tokenizer max length=64 from RL4LLMs Table 4
    seq_length: Optional[int] = field(default=64, metadata={"help": "Input sequence length"}) #default=512
    gradient_accumulation_steps: Optional[int] = field(
        default=16, metadata={"help": "the number of gradient accumulation steps"}
    )
    num_train_epochs: Optional[int] = field(default=10, metadata={"help": "the number of training epochs"}) #default=3
    max_steps: Optional[int] = field(default=-1, metadata={"help": "the number of training steps"})

    # model download, quantization and peft params
    trust_remote_code: Optional[bool] = field(default=True, metadata={"help": "Enable `trust_remote_code`"})
    use_auth_token: Optional[bool] = field(default=True, metadata={"help": "Use HF auth token to access the model"})
    load_in_8bit: Optional[bool] = field(default=False, metadata={"help": "load the model in 8 bits precision"})
    load_in_4bit: Optional[bool] = field(default=False, metadata={"help": "load the model in 4 bits precision"})
    use_peft: Optional[bool] = field(default=False, metadata={"help": "Wether to use PEFT or not to train adapters"})
    peft_lora_r: Optional[int] = field(default=64, metadata={"help": "the r parameter of the LoRA adapters"})
    peft_lora_alpha: Optional[int] = field(default=16, metadata={"help": "the alpha parameter of the LoRA adapters"})

    # logging
    output_dir: Optional[str] = field(default=logging_dir, metadata={"help": "the output directory"})
    logging_steps: Optional[int] = field(default=1, metadata={"help": "the number of logging steps"})
    save_steps: Optional[int] = field(
        default=100, metadata={"help": "Number of updates steps before two checkpoint saves"}
    )
    save_total_limit: Optional[int] = field(default=10, metadata={"help": "Limits total number of checkpoints."})

    # post processing
    push_to_hub: Optional[bool] = field(default=False, metadata={"help": "Push the model to HF Hub"})
    hub_model_id: Optional[str] = field(default=None, metadata={"help": "The name of the model on HF Hub"})

    # added by auth1
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="less", metadata={"help": "the dataset to query"})
    generation_kwargs_min_length: Optional[int] = field(default=-1, metadata={"help": "minimum number of tokens while generation"})
    generation_kwargs_top_k: Optional[int] = field(default=0, metadata={"help": "gneration top k"})
    generation_kwargs_top_p: Optional[float] = field(default=1.0, metadata={"help": "gneration top p"})
    generation_kwargs_max_new_tokens: Optional[int] = field(default=32, metadata={"help": "gneration top p"})
    
    change_tokenizer_args: bool = field(default=False, metadata={"help": "whether to use modify tokenizer settings"})
    tokenizer_kwargs: dict = field(
    default_factory=lambda: {
      "padding_side": "left", 
      "truncation_side": "left",
      "pad_token_as_eos_token": True,
      "max_length": 64
    }
  )
    exp_name: str = field(default="sft-positive", metadata={"help": "experiment name"}) 
    seed: Optional[int] = field(default=0, metadata={"help": "seed"})


parser = HfArgumentParser(ScriptArguments)
script_args = parser.parse_args_into_dataclasses()[0]

set_seed(script_args.seed)

generation_kwargs = {
      "min_length": script_args.generation_kwargs_min_length, 
      "top_k": script_args.generation_kwargs_top_k,
      "top_p": script_args.generation_kwargs_top_p,
      "do_sample": True,
      "max_new_tokens": script_args.generation_kwargs_max_new_tokens
    }

# Step 1: Load the model
if script_args.load_in_8bit and script_args.load_in_4bit:
    raise ValueError("You can't load the model in 8 bits and 4 bits at the same time")
elif script_args.load_in_8bit or script_args.load_in_4bit:
    quantization_config = BitsAndBytesConfig(
        load_in_8bit=script_args.load_in_8bit, load_in_4bit=script_args.load_in_4bit
    )
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}
    torch_dtype = torch.bfloat16
else:
    device_map = None
    quantization_config = None
    torch_dtype = None

model = AutoModelForCausalLM.from_pretrained(
    script_args.model_name,
    quantization_config=quantization_config,
    device_map=device_map,
    trust_remote_code=script_args.trust_remote_code,
    torch_dtype=torch_dtype)

# Removing huggingface authorization token param from model loading command above 
#    use_auth_token=script_args.use_auth_token,
# Make sure you have logged in to the Hugging Face Hub using huggingface-cli login 
# or by calling huggingface_hub.login() in Python. This will save an authentication 
# token that Transformers will pick up automatically.

# Step 2: Load the dataset
dataset = load_dataset(script_args.dataset_name, split="train")

# Filter dataset for only positive reviews
dataset = dataset.filter(lambda x: x["label"] == 1, batched=False)


# Step 3: Define generation arguments
# generation_kwargs = GenerationConfig(
#     min_length = 48,
#     top_k = 50,
#     top_p = 1.0,
#     do_sample = True,
#     max_new_tokens = 48,
# )
#pad_token_id = tokenizer.eos_token_id,

# Step 3: Define the training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir=script_args.output_dir,
    per_device_train_batch_size=script_args.batch_size,
    gradient_accumulation_steps=script_args.gradient_accumulation_steps,
    learning_rate=script_args.learning_rate,
    logging_steps=script_args.logging_steps,
    num_train_epochs=script_args.num_train_epochs,
    max_steps=script_args.max_steps,
    report_to=script_args.log_with,
    save_steps=script_args.save_steps,
    save_total_limit=script_args.save_total_limit,
    push_to_hub=script_args.push_to_hub,
    hub_model_id=script_args.hub_model_id,
    generation_config=generation_kwargs
)

# Step 4: Define the LoraConfig
if script_args.use_peft:
    peft_config = LoraConfig(
        r=script_args.peft_lora_r,
        lora_alpha=script_args.peft_lora_alpha,
        bias="none",
        task_type="CAUSAL_LM",
    )
else:
    peft_config = None


# trl sft trainer default tokenizer and pad_token
"""
if tokenizer is None:
            tokenizer = AutoTokenizer.from_pretrained(model.config._name_or_path)
            if getattr(tokenizer, "pad_token", None) is None:
                tokenizer.pad_token = tokenizer.eos_token

if max_seq_length is None:
            # to overcome some issues with broken tokenizers
            max_seq_length = min(tokenizer.model_max_length, 1024)

            warnings.warn(
                f"You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to {max_seq_length}"
            )

"""

# Looks like padding_side = "right" by default
# Hence, defining our tokenizer here
tokenizer = AutoTokenizer.from_pretrained(script_args.model_name)
tokenizer.pad_token = tokenizer.eos_token

if script_args.change_tokenizer_args == True: 
    tokenizer.padding_side = script_args.tokenizer_kwargs['padding_side']
    tokenizer.truncation_side = script_args.tokenizer_kwargs['truncation_side']
    tokenizer.pad_token_as_eos_token = script_args.tokenizer_kwargs['pad_token_as_eos_token']
    tokenizer.max_length = script_args.tokenizer_kwargs['max_length']

# tokenizer.padding_side = "left"
# tokenizer.truncation_side = "left"
# tokenizer.pad_token_as_eos_token = True
# tokenizer.max_length = 64

# Step 5: Define the Trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    max_seq_length=script_args.seq_length,
    train_dataset=dataset,
    dataset_text_field=script_args.dataset_text_field,
    peft_config=peft_config,
    tokenizer=tokenizer
)

trainer.train()

# Step 6: Save the model
trainer.save_model(logging_dir)

File Path: examples/IMDB/training/soft_risk_ppo_auth1.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed
from trl.core import LengthSampler


tqdm.pandas()

import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import pdb
import pandas as pd
import math 

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
logging_dir = f"/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/dummy/{date_n_time}"
os.makedirs(logging_dir)

@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name="lvwerra/gpt2-imdb",
            query_dataset="imdb",
            reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            #log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            steps= 128000,
            exp_name="ppo"
        )
    )
    query_dataset: str = field(default="imdb", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="less", metadata={"help": "the dataset to query"})
    
    generation_kwargs_min_length: Optional[int] = field(default=-1, metadata={"help": "minimum number of tokens while generation"})
    generation_kwargs_top_k: Optional[int] = field(default=0, metadata={"help": "gneration top k"})
    generation_kwargs_top_p: Optional[float] = field(default=1.0, metadata={"help": "gneration top p"})
    generation_kwargs_max_new_tokens: Optional[int] = field(default=32, metadata={"help": "gneration top p"})

    change_tokenizer_args: bool = field(default=False, metadata={"help": "whether to use modify tokenizer settings"})
    tokenizer_kwargs: dict = field(
    default_factory=lambda: {
      "padding_side": "left", 
      "truncation_side": "left",
      "pad_token_as_eos_token": True,
      "max_length": 64
    }
  )



args = tyro.cli(ScriptArguments)

generation_kwargs = {
      "min_length": args.generation_kwargs_min_length, 
      "top_k": args.generation_kwargs_top_k,
      "top_p": args.generation_kwargs_top_p,
      "do_sample": True,
      "max_new_tokens": args.generation_kwargs_max_new_tokens
    }

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # load imdb with datasets: for reducing data size, just use +'[:200]'
    ds = load_dataset(query_dataset, split="train")
    ds = ds.rename_columns({"text": "review"})
    ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)


    if args.input_query_size == "more":
        def input_size():
            return 64 
    else: 
        input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    args.ppo_config.model_name,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id

#pdb.set_trace()

if args.change_tokenizer_args == True: 
    tokenizer.padding_side = args.tokenizer_kwargs['padding_side']
    tokenizer.truncation_side = args.tokenizer_kwargs['truncation_side']
    tokenizer.pad_token_as_eos_token = args.tokenizer_kwargs['pad_token_as_eos_token']
    tokenizer.max_length = args.tokenizer_kwargs['max_length']

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
task, model_name = args.ppo_config.reward_model.split(":")
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs['pad_token_id'] = tokenizer.eos_token_id

# empty list to store text data
text_train_data = []

def get_current_risk_level(alpha, rho, m, M):
    """
    Get current risk_level_alpha*N value

    Args:
        alpha (float):
            Risk level alpha 
        rho (float):
            Risk level scheduler: 0.8 means the soft risk level reaches α after ρ = 80% of the training
        m (int):
            current training step/epoch
        M (int):
            total policy training steps

    Returns:
        alpha_N (int):
            risk_level_alpha*N value
    """
    alpha = 0.35
    rho = 0.98 

    if m <= 240:
        return args.ppo_config.batch_size
    else:
        return math.ceil(args.ppo_config.batch_size * max(alpha, 1 - (1 - alpha) * (m-140) / (rho * M)))


def apply_risk_modification_to_batch(batch, query_tensors, response_tensors, rewards, alpha_N):
    query_tensors = list(map(lambda t: t.cpu(), query_tensors))
    response_tensors = list(map(lambda t: t.cpu(), response_tensors))
    rewards = list(map(lambda t: t.cpu(), rewards))
    #response_lengths = [response_tensors[i].shape[0] for i in response_tensors]

    step_dict = {
        'query_t' : query_tensors,
        'resp_t' : response_tensors,
        'rewards' : rewards
    }

    #batch = {k: [t.cpu() for t in v] if isinstance(v, list) else v.cpu() for k, v in batch.items()}
    batch = {k: [t.cpu() for t in v] if isinstance(v, list) and all(isinstance(t, torch.Tensor) for t in v) else v.cpu() if isinstance(v, torch.Tensor) else v for k, v in batch.items()}
    #pdb.set_trace()

    df_step_dict = pd.DataFrame(step_dict)
    df_batch = pd.DataFrame(batch)

    df_step_dict = df_step_dict.sort_values(by='rewards')#.head(alpha_N)
    df_batch = df_batch.sort_values(by='rewards')#.head(alpha_N).drop('rewards', axis=1)
    batch = df_batch.to_dict(orient='list')  
    # line #996 in ppo_trainer already does response_masks_batch[j] = torch.cat((torch.zeros_like(query_batch[j]), response_masks_batch[j]))[1:]
    #pdb.set_trace()
    #mask = [torch.cat((torch.ones_like(t, dtype =int), torch.zeros([48 - t.shape[0]], dtype =int))) for t in response_tensors]  #torch.zeros([64], dtype =int), 
    mask = [torch.ones([48], dtype =int) for _ in response_tensors]  #torch.zeros([64], dtype =int), 
    for ii in range(args.ppo_config.batch_size - alpha_N):
        mask[-ii-1] = torch.zeros([48], dtype=int)

    sr_query_tensors = list(map(lambda t: t.to(device), df_step_dict['query_t'].tolist()))
    sr_response_tensors = list(map(lambda t: t.to(device), df_step_dict['resp_t'].tolist()))
    sr_rewards = list(map(lambda t: t.to(device), df_step_dict['rewards'].tolist()))
    batch = {k: [t.to(device) for t in v] if isinstance(v, list) and all(isinstance(t, torch.Tensor) for t in v) else v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}

    return batch, sr_query_tensors, sr_response_tensors, sr_rewards, mask


for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)

    if args.reward_function == "trl":
        rewards = [torch.tensor(output[1]["score"]) for output in pipe_outputs]
    else: 
        rewards = [
            torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"]]))[1]
            for output in pipe_outputs
        ]
    
    for ii in range(3): 
        data_row = {'iter': epoch, 'input':batch["query"][ii], 'output':batch["response"][ii], 'score':rewards[ii].item()}
        text_train_data.append(data_row)
    
    batch["rewards"] = rewards
    alpha_N_ceil = get_current_risk_level(PPOConfig.soft_risk_alpha, PPOConfig.risk_level_scheduler, epoch, 194)
    sr_batch, sr_query_t, sr_resp_t, sr_rewards, mask = apply_risk_modification_to_batch(batch, query_tensors, response_tensors, rewards, alpha_N_ceil)

    # Run PPO step
    print(alpha_N_ceil)
    stats = ppo_trainer.step(sr_query_t, sr_resp_t, sr_rewards, mask)
    ppo_trainer.log_stats(stats, sr_batch, sr_rewards)
    print(epoch)

ppo_trainer._save_pretrained(logging_dir)
df = pd.DataFrame(text_train_data)
df.to_csv(logging_dir+'/textual_data.csv', index=False)
File Path: examples/IMDB/training/soft_risk_ppo_auth1_2.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, set_seed ,PPOTrainer
#from trl.trainer.ppo_trainer_2 import PPOTrainer
from trl.core import LengthSampler


tqdm.pandas()

import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import pdb
import pandas as pd
import math 

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
logging_dir = f"/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/imdb_final_1/sft_p_new_schedule/{date_n_time}"
os.makedirs(logging_dir)

# path to the base model 
# npn: /mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_both/pn/checkpoint-200
# p: /mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_final/2024-01-24_21-17-41/checkpoint-100

@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name="/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_final/2024-01-24_21-17-41/checkpoint-100", #"lvwerra/gpt2-imdb",
            query_dataset="imdb",
            reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            #log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            steps= 128000,
            exp_name="ppo"
        )
    )
    query_dataset: str = field(default="imdb", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="less", metadata={"help": "the dataset to query"})
    
    generation_kwargs_min_length: Optional[int] = field(default=-1, metadata={"help": "minimum number of tokens while generation"})
    generation_kwargs_top_k: Optional[int] = field(default=0, metadata={"help": "gneration top k"})
    generation_kwargs_top_p: Optional[float] = field(default=1.0, metadata={"help": "gneration top p"})
    generation_kwargs_max_new_tokens: Optional[int] = field(default=32, metadata={"help": "gneration top p"})

    change_tokenizer_args: bool = field(default=False, metadata={"help": "whether to use modify tokenizer settings"})
    tokenizer_kwargs: dict = field(
    default_factory=lambda: {
      "padding_side": "left", 
      "truncation_side": "left",
      "pad_token_as_eos_token": True,
      "max_length": 64
    }
  )
    
    risk_scheduler: str = field(default="old", metadata={"help": "old risk scheduler that doesn't go down to alpha"})
    risk_n: int = field(default=240, metadata={"help": "240 (> batch size): no RA; 70: RA begins after iter 70; 1: RA throughout"})
    risk_alpha: Optional[float] = field(default=0.2, metadata={"help": "risk alpha value = 20 percent"})
    risk_rho: Optional[float] = field(default=0.8, metadata={"help": "risk alpha reaches a value = 20 percent at 80 percent of total iterations"})

args = tyro.cli(ScriptArguments)

generation_kwargs = {
      "min_length": args.generation_kwargs_min_length, 
      "top_k": args.generation_kwargs_top_k,
      "top_p": args.generation_kwargs_top_p,
      "do_sample": True,
      "max_new_tokens": args.generation_kwargs_max_new_tokens
    }

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # load imdb with datasets: for reducing data size, just use +'[:200]'
    ds = load_dataset(query_dataset, split="train")
    ds = ds.rename_columns({"text": "review"})
    ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)


    if args.input_query_size == "more":
        def input_size():
            return 64 
    else: 
        input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    args.ppo_config.model_name,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id

#pdb.set_trace()

if args.change_tokenizer_args == True: 
    tokenizer.padding_side = args.tokenizer_kwargs['padding_side']
    tokenizer.truncation_side = args.tokenizer_kwargs['truncation_side']
    tokenizer.pad_token_as_eos_token = args.tokenizer_kwargs['pad_token_as_eos_token']
    tokenizer.max_length = args.tokenizer_kwargs['max_length']

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
task, model_name = args.ppo_config.reward_model.split(":")
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs['pad_token_id'] = tokenizer.eos_token_id

# empty list to store text data
text_train_data = []

def get_current_risk_level(alpha, rho, m, M):
    """
    Get current risk_level_alpha*N value

    Args:
        alpha (float):
            Risk level alpha 
        rho (float):
            Risk level scheduler: 0.8 means the soft risk level reaches α after ρ = 80% of the training
        m (int):
            current training step/epoch
        M (int):
            total policy training steps

    Returns:
        alpha_N (int):
            risk_level_alpha*N value
    """
    alpha = args.risk_alpha
    rho = args.risk_rho

    if args.risk_scheduler == "old": 
        if m <= args.risk_n:
            return args.ppo_config.batch_size
        else:
            return math.ceil(args.ppo_config.batch_size * max(alpha, 1 - (1 - alpha) * (m - args.risk_n) / (rho * M)))   
    else: 
        print('here')
        if m <= args.risk_n:
            val =  args.ppo_config.batch_size
        elif m >= math.ceil(rho*M):
            val = math.ceil(alpha*args.ppo_config.batch_size)
        else:
            K = (1 - alpha)/(math.ceil(rho*M)-args.risk_n)
            val = math.ceil(args.ppo_config.batch_size * max(alpha, 1 - K * (m - args.risk_n)))
        return val



def get_current_risk_level_2(alpha, rho, m, M):
    """
    Get current risk_level_alpha*N value

    Args:
        alpha (float):
            Risk level alpha 
        rho (float):
            Risk level scheduler: 0.8 means the soft risk level reaches α after ρ = 80% of the training
        m (int):
            current training step/epoch
        M (int):
            total policy training steps

    Returns:
        alpha_N (int):
            risk_level_alpha*N value
    """
    alpha = 0.2
    rho = 0.8 
    n = 70
    M = 194
    
    if m <= n:
        val =  args.ppo_config.batch_size
    elif m >= math.ceil(rho*M):
        val = alpha*100
    else:
        K = (1 - alpha)/(math.ceil(rho*M)-n)
        val = math.ceil(args.ppo_config.batch_size * max(alpha, 1 - K * (m - n)))
    return val

def apply_risk_modification_to_batch(batch, query_tensors, response_tensors, rewards, alpha_N):
    query_tensors = list(map(lambda t: t.cpu(), query_tensors))
    response_tensors = list(map(lambda t: t.cpu(), response_tensors))
    rewards = list(map(lambda t: t.cpu(), rewards))
    #response_lengths = [response_tensors[i].shape[0] for i in response_tensors]

    step_dict = {
        'query_t' : query_tensors,
        'resp_t' : response_tensors,
        'rewards' : rewards
    }

    #batch = {k: [t.cpu() for t in v] if isinstance(v, list) else v.cpu() for k, v in batch.items()}
    batch = {k: [t.cpu() for t in v] if isinstance(v, list) and all(isinstance(t, torch.Tensor) for t in v) else v.cpu() if isinstance(v, torch.Tensor) else v for k, v in batch.items()}
    #pdb.set_trace()

    df_step_dict = pd.DataFrame(step_dict)
    df_batch = pd.DataFrame(batch)

    df_step_dict = df_step_dict.sort_values(by='rewards')#.head(alpha_N)
    df_batch = df_batch.sort_values(by='rewards')#.head(alpha_N).drop('rewards', axis=1)
    batch = df_batch.to_dict(orient='list')  
    # line #996 in ppo_trainer already does response_masks_batch[j] = torch.cat((torch.zeros_like(query_batch[j]), response_masks_batch[j]))[1:]
    #pdb.set_trace()
    #mask = [torch.cat((torch.ones_like(t, dtype =int), torch.zeros([48 - t.shape[0]], dtype =int))) for t in response_tensors]  #torch.zeros([64], dtype =int), 
    mask = [torch.ones([48], dtype =int) for _ in response_tensors]  #torch.zeros([64], dtype =int), 
    for ii in range(args.ppo_config.batch_size - alpha_N):
        mask[-ii-1] = torch.zeros([48], dtype=int)

    sr_query_tensors = list(map(lambda t: t.to(device), df_step_dict['query_t'].tolist()))
    sr_response_tensors = list(map(lambda t: t.to(device), df_step_dict['resp_t'].tolist()))
    sr_rewards = list(map(lambda t: t.to(device), df_step_dict['rewards'].tolist()))
    batch = {k: [t.to(device) for t in v] if isinstance(v, list) and all(isinstance(t, torch.Tensor) for t in v) else v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}

    return batch, sr_query_tensors, sr_response_tensors, sr_rewards, mask


for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)

    if args.reward_function == "trl":
        rewards = [torch.tensor(output[1]["score"]) for output in pipe_outputs]
    else: 
        rewards = [
            torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"]]))[1]
            for output in pipe_outputs
        ]
    
    for ii in range(3): 
        data_row = {'iter': epoch, 'input':batch["query"][ii], 'output':batch["response"][ii], 'score':rewards[ii].item()}
        text_train_data.append(data_row)
    
    batch["rewards"] = rewards
    alpha_N_ceil = get_current_risk_level(PPOConfig.soft_risk_alpha, PPOConfig.risk_level_scheduler, epoch, 194)
    #sr_batch, sr_query_t, sr_resp_t, sr_rewards, mask = apply_risk_modification_to_batch(batch, query_tensors, response_tensors, rewards, alpha_N_ceil)

    # Run PPO step
    print(alpha_N_ceil)
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards, alpha_N_ceil)
    ppo_trainer.log_stats(stats, batch, rewards)
    print(epoch)

ppo_trainer._save_pretrained(logging_dir)
df = pd.DataFrame(text_train_data)
df.to_csv(logging_dir+'/textual_data.csv', index=False)
File Path: examples/IMDB/training/sr_ppo_big_imdb.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
from datasets import load_dataset, Dataset
from torch.optim import Adam
from tqdm import tqdm
from transformers import (
    AutoModelForSequenceClassification,
    AutoModelForCausalLM,
    AutoTokenizer,
    HfArgumentParser,
    RobertaForSequenceClassification,
    RobertaTokenizer,
    DistilBertForSequenceClassification,
    DistilBertTokenizerFast,
)
from peft import LoraConfig
from trl import (
    AutoModelForCausalLMWithValueHead,
    PPOConfig,
    create_reference_model,
    set_seed,
)
from trl.trainer.ppo_trainer import PPOTrainer
from trl.core import LengthSampler
import datetime
import os
import pandas as pd

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")

logging_dir = f"/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/big/imdb/{date_n_time}"
os.makedirs(logging_dir, exist_ok=True)

tqdm.pandas()

########################################################################
# This is a fully working simple example to use trl with accelerate.
#
# This example fine-tunes a GPTJ model to generate less toxic contents
# by using allenai/real-toxicity-prompts dataset. We use PPO
#  (proximal policy optimization) to optimize the model.
# in any of the following settings (with the same script):
#   - single CPU or single GPU
#   - multi GPUS (using PyTorch distributed mode)
#   - multi GPUS (using DeepSpeed ZeRO-Offload stages 1 & 2)
#   - fp16 (mixed-precision) or fp32 (normal precision)
#
# To run it in each of these various modes, first initialize the accelerate
# configuration with `accelerate config`
#
########################################################################


# We first define the configuration of the experiment, defining the model, the dataset,
# the training parameters, and the PPO parameters.
# Check the default arguments in the `PPOConfig` class for more details.
# If you want to log with tensorboard, add the kwarg
# `project_kwargs={"logging_dir": PATH_TO_LOGS}` to the PPOConfig.
@dataclass
class ScriptArguments:
    """
    The name of the Casual LM model we wish to fine-tune with PPO
    """

    # NOTE: gpt2 models use Conv1D instead of Linear layers which are not yet supported in 8 bit mode
    # models like gpt-neo* models are more suitable.
    model_name: Optional[str] = field(default="ybelkada/gpt-j-6b-sharded-bf16", metadata={"help": "the model name"})
    log_with: Optional[str] = field(default="tensorboard", metadata={"help": "use 'wandb' to log with wandb"})
    learning_rate: Optional[float] = field(default=(1.47e-5) * 2, metadata={"help": "the learning rate"})
    mini_batch_size: Optional[int] = field(default=8, metadata={"help": "the PPO minibatch size"})
    batch_size: Optional[int] = field(default=8, metadata={"help": "the batch size"})
    gradient_accumulation_steps: Optional[int] = field(
        default=1, metadata={"help": "the number of gradient accumulation steps"}
    )
    seed: Optional[int] = field(default=42, metadata={"help": "the seed for the experiment"})
    exp_name: Optional[str] = field(default="imdb", metadata={"help": "the name of the experiment"})
    query_dataset: Optional[str] = field(default="imdb", metadata={"help": "the name of the dataset"})
    risk_n: int = field(
        default=300, metadata={"help": "240 (> batch size): no RA; 70: RA begins after iter 70; 1: RA throughout"}
    )
    risk_alpha: Optional[float] = field(default=0.2, metadata={"help": "risk alpha value = 20 percent"})
    risk_rho: Optional[float] = field(
        default=0.95, metadata={"help": "risk alpha reaches a value = 20 percent at 80 percent of total iterations"}
    )


model_save_path = os.path.join(logging_dir, "save_pretrained")
prompt_len = 64
gen_len = 48

parser = HfArgumentParser(ScriptArguments)
script_args = parser.parse_args_into_dataclasses()[0]

config = PPOConfig(
    model_name=script_args.model_name,
    learning_rate=script_args.learning_rate,
    log_with=script_args.log_with,
    project_kwargs={"logging_dir": logging_dir},
    # ppo_epochs=100,
    mini_batch_size=script_args.mini_batch_size,
    batch_size=script_args.batch_size,
    gradient_accumulation_steps=script_args.gradient_accumulation_steps,
    seed=script_args.seed,
    exp_name=script_args.exp_name,
)

peft_config = LoraConfig(
    r=16,
    lora_alpha=16,
    bias="none",
    task_type="CAUSAL_LM",
)


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.


def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # load imdb with datasets: for reducing data size, just use +'[:200]'
    ds = load_dataset(query_dataset, split="train")
    ds = ds.rename_columns({"text": "review"})
    ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)

    def input_size():
        return prompt_len

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(config, script_args.query_dataset)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(config.seed)

# Now let's build the model, the reference model, and the tokenizer. We first load the model
# in bfloat16 to save memory using `transformers`.
model = AutoModelForCausalLM.from_pretrained(config.model_name, torch_dtype=torch.bfloat16)
# And then we pass the loaded model to `AutoModelForCausalLMWithValueHead`.
model = AutoModelForCausalLMWithValueHead.from_pretrained(model, peft_config=peft_config)

# We create a reference model by sharing 20 layers
# ref_model = create_reference_model(model, num_shared_layers=20)
ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(
    "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/IMDB/2024-02-18_22-24-38",
    trust_remote_code=True,
)  # sft positive 70_30 gpt2

# We make sure to use `Adam` optimizer on the model parameters that require gradients.
optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=config.learning_rate)

# GPT-2 / GPT-J tokenizer has a pad token, but it is not eos_token by default. We need to set it to eos_token.
# only for this model.
tokenizer = AutoTokenizer.from_pretrained(config.model_name)
tokenizer.pad_token = tokenizer.eos_token

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(
    config,
    model,
    ref_model=ref_model,
    tokenizer=tokenizer,
    dataset=dataset,
    data_collator=collator,
    optimizer=optimizer,
)

# We then build the reward pipeline, we will use the toxicity model to compute the reward.
# We first load the toxicity model and tokenizer.
# toxicity_model_id = "facebook/roberta-hate-speech-dynabench-r4-target"
# toxicity_tokenizer = RobertaTokenizer.from_pretrained(toxicity_model_id)
# # We load the toxicity model in fp16 to save memory.
# toxicity_model = RobertaForSequenceClassification.from_pretrained(toxicity_model_id, torch_dtype=torch.float16).to(
#     ppo_trainer.accelerator.device
# )

toxicity_tokenizer = AutoTokenizer.from_pretrained("lvwerra/distilbert-imdb")
toxicity_model = AutoModelForSequenceClassification.from_pretrained(
    "lvwerra/distilbert-imdb", torch_dtype=torch.float16
).to(ppo_trainer.accelerator.device)


# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs = {
    "min_length": gen_len,
    "top_k": 0.0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
}
# output_min_length = 20
# output_max_length = 30
# output_length_sampler = LengthSampler(output_min_length, output_max_length)

# model_save_path = script_args.model_save_path
import math


def get_current_risk_level(alpha, rho, m, M):
    """
    Get current risk_level_alpha*N value

    Args:
        alpha (float):
            Risk level alpha
        rho (float):
            Risk level scheduler: 0.8 means the soft risk level reaches α after ρ = 80% of the training
        m (int):
            current training step/epoch
        M (int):
            total policy training steps

    Returns:
        alpha_N (int):
            risk_level_alpha*N value
    """
    alpha = script_args.risk_alpha
    rho = script_args.risk_rho

    # if script_args.risk_scheduler == "old":
    #     if m <= script_args.risk_n:
    #         return script_args.ppo_config.batch_size
    #     else:
    #         return math.ceil(script_args.ppo_config.batch_size * max(alpha, 1 - (1 - alpha) * (m - script_args.risk_n) / (rho * M)))
    # else:
    # print("here")
    if m <= script_args.risk_n:
        val = script_args.batch_size
    elif m >= math.ceil(rho * M):
        val = math.ceil(alpha * script_args.batch_size)
    else:
        K = (1 - alpha) / (math.ceil(rho * M) - script_args.risk_n)
        val = math.ceil(script_args.batch_size * max(alpha, 1 - K * (m - script_args.risk_n)))
    return val


for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from the policy model
    response_tensors = []
    for query in query_tensors:
        # gen_len = output_length_sampler()
        # generation_kwargs["max_new_tokens"] = gen_len
        generation_kwargs["max_new_tokens"] = gen_len
        response = ppo_trainer.generate(query, **generation_kwargs)
        response_tensors.append(response.squeeze()[-gen_len:])
    batch["response"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]

    # Compute sentiment score # noqa
    texts = batch["response"]
    toxicity_inputs = toxicity_tokenizer(texts, padding=True, truncation=True, return_tensors="pt").to(
        ppo_trainer.accelerator.device
    )
    logits = toxicity_model(**toxicity_inputs).logits.float()
    toxicity_labels = (logits[:, 1]).tolist()

    rewards = [torch.tensor(output) for output in toxicity_labels]

    alpha_N_ceil = get_current_risk_level(PPOConfig.soft_risk_alpha, PPOConfig.risk_level_scheduler, epoch, 3601)

    # Run PPO step
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards, alpha_N_ceil)
    ppo_trainer.log_stats(stats, batch, rewards)

    # Save model every 100 epochs
    if epoch % 100 == 0:
        if ppo_trainer.accelerator.is_main_process:
            ppo_trainer.save_pretrained(model_save_path)
ppo_trainer.save_pretrained(model_save_path)

File Path: examples/IMDB/training/zero_shot.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed
from trl.core import LengthSampler


tqdm.pandas()

import pdb
import pickle
import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import statistics
import random

@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name="gpt2",
            query_dataset="imdb",
            reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
        )
    )
    query_dataset: str = field(default="imdb", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )


args = tyro.cli(ScriptArguments)


# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8, data_split = 'train'):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # load imdb with datasets
    # to load partial dataset, just change data split to split=data_split+'[:200]'
    ds = load_dataset(query_dataset, split=data_split)
    ds = ds.rename_columns({"text": "review"})
    ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)

    def input_size():
        return 64 #LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)

# build and sample test dataset
test_dataset = build_dataset(args.ppo_config, args.query_dataset, data_split='test')
# Take 5k random samples
sample_size = 5000
random_indices = random.sample(range(len(test_dataset)), sample_size)
test_dataset = test_dataset.select(random_indices)

def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_positive_1/oct_2_afternoon/checkpoint-120',
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.padding_side = "left"
tokenizer.truncation_side = "left"
tokenizer.pad_token_as_eos_token = True
tokenizer.max_length = 64

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)
# Creating a tester object here because dataloader is only available in the PPOTrainer class
ppo_tester = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
task, model_name = args.ppo_config.reward_model.split(":")
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs = {
    "min_length": 48,
    "top_k": 50,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": 48,
}


# infer on test dataset
test_scores = []
for epoch, batch in tqdm(enumerate(ppo_tester.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
    #rewards = [torch.tensor(output[1]["score"]) for output in pipe_outputs]
    rewards = [
        torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"]]))[1]
        for output in pipe_outputs
    ]

    test_scores.extend(rewards)

    #test_scores.append(sum(rewards)/len(rewards))
    print(sum(rewards)/len(rewards))

# print and save the final score  
date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
os.makedirs(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/zero_shot/{date_n_time}')
final_test_score = sum(test_scores)/len(test_scores)     
test_scores_list = [t.item() for t in test_scores]
pdb.set_trace()
print(f'Total test sentiment-score and std is: {final_test_score.item(), statistics.stdev(test_scores_list)}')
pickle.dump(final_test_score.item(), open(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/zero_shot/{date_n_time}/final_test_score', "wb"))




File Path: examples/Jigsaw/dataset/eda.py
Content:
from datasets import load_dataset
import matplotlib.pyplot as plt
import pandas as pd
from transformers import AutoTokenizer, pipeline
from tqdm import tqdm

tqdm.pandas()
# Load IMDB dataset
# imdb_train = load_dataset("imdb", split="train")
# imdb_test = load_dataset("imdb", split="test")


# Load Jigsaw toxicity dataset
jigsaw = load_dataset(
    "OxAISH-AL-LLM/wiki_toxic",
    # data_dir="/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/original_dataset",
    # split="train",
)  # Hardcoding path for now TODO: change this


jigsaw = jigsaw.filter(lambda x: len(x["comment_text"]) > 64, batched=False)
jigsaw = jigsaw.rename_column("label", "toxic")
# imdb_train = imdb_train.filter(lambda x: len(x["text"]) > 64, batched=False)
# imdb_test = imdb_test.filter(lambda x: len(x["text"]) > 64, batched=False)


# imdb_df_train = pd.DataFrame(imdb_train)
# imdb_df_test = pd.DataFrame(imdb_test)
jigsaw_df_train = pd.DataFrame(jigsaw["train"])
jigsaw_df_test = pd.DataFrame(jigsaw["test"])


# # Balancing jigsaw dataset for toxic and non-toxic comments

num_toxic_train = jigsaw_df_train["toxic"].sum()
num_toxic_test = jigsaw_df_test["toxic"].sum()

print("Number of toxic comments in train: ", num_toxic_train)
print("Number of toxic comments in test: ", num_toxic_test)
print("Number of non-toxic comments in train: ", len(jigsaw_df_train) - num_toxic_train)
print("Number of non-toxic comments in test: ", len(jigsaw_df_test) - num_toxic_test)
a = input("Press Enter to continue...")

jigsaw_toxic_train = jigsaw_df_train[jigsaw_df_train["toxic"] == True]
jigsaw_non_toxic_train = jigsaw_df_train[jigsaw_df_train["toxic"] == False]

jigsaw_toxic_test = jigsaw_df_test[jigsaw_df_test["toxic"] == True]
jigsaw_non_toxic_test = jigsaw_df_test[jigsaw_df_test["toxic"] == False]

jigsaw_non_toxic_train = jigsaw_non_toxic_train.sample(n=num_toxic_train, random_state=42)
jigsaw_non_toxic_test = jigsaw_non_toxic_test.sample(n=num_toxic_test, random_state=42)

jigsaw_df_train = pd.concat([jigsaw_toxic_train, jigsaw_non_toxic_train], ignore_index=True)
jigsaw_df_test = pd.concat([jigsaw_toxic_test, jigsaw_non_toxic_test], ignore_index=True)

# # Save the dataset
print("Saving the dataset")
print("Jigsaw train size: ", len(jigsaw_df_train))
print("Jigsaw test size: ", len(jigsaw_df_test))
# print("IMDB train size: ", len(imdb_df_train))
# print("IMDB test size: ", len(imdb_df_test))
jigsaw_df_train.to_csv("wiki_toxic_train.csv", index=False)
jigsaw_df_test.to_csv("wiki_toxic_test.csv", index=False)

# imdb_df_train.to_csv("imdb_train.csv", index=False)
# imdb_df_test.to_csv("imdb_test.csv", index=False)


# # Load the dataset
# jigsaw_df_train = pd.read_csv("jigsaw_train.csv")
# jigsaw_df_test = pd.read_csv("jigsaw_test.csv")
# imdb_df_train = pd.read_csv("imdb_train.csv")
# imdb_df_test = pd.read_csv("imdb_test.csv")


# # Reward computation for Jigsaw dataset
# sent_kwargs = {"return_all_scores": False, "function_to_apply": "none"}
# jigsaw_reward_model = pipeline("text-classification", model="unitary/toxic-bert", truncation=True, **sent_kwargs)

# print("Calculating reward for Jigsaw train dataset")
# jigsaw_df_train["reward"] = jigsaw_df_train["comment_text"].progress_apply(
#     lambda x: -jigsaw_reward_model(x, truncation=True)[0]["score"]
# )

# print("Calculating reward for Jigsaw test dataset")
# jigsaw_df_test["reward"] = jigsaw_df_test["comment_text"].progress_apply(
#     lambda x: -jigsaw_reward_model(x, truncation=True)[0]["score"]
# )

# # Save the dataset
# print("Saving the dataset")
# print("Jigsaw train size: ", len(jigsaw_df_train))
# print("Jigsaw test size: ", len(jigsaw_df_test))
# jigsaw_df_train.to_csv("jigsaw_train.csv", index=False)
# jigsaw_df_test.to_csv("jigsaw_test.csv", index=False)


# # Reward computation for IMDB dataset
# sent_kwargs = {"return_all_scores": True, "function_to_apply": "none"}
# imdb_reward_model = pipeline("sentiment-analysis", model="lvwerra/distilbert-imdb", truncation=True, **sent_kwargs)


# print("Calculating reward for IMDB train dataset")
# imdb_df_train["reward"] = imdb_df_train["text"].progress_apply(lambda x: imdb_reward_model(x)[0][1]["score"])
# imdb_df_test["reward"] = imdb_df_test["text"].progress_apply(lambda x: imdb_reward_model(x)[0][1]["score"])

# # Save the dataset
# print("Saving the dataset")
# print("IMDB train size: ", len(imdb_df_train))
# print("IMDB test size: ", len(imdb_df_test))
# imdb_df_train.to_csv("imdb_train.csv", index=False)
# imdb_df_test.to_csv("imdb_test.csv", index=False)


# # Plot the class wise reward distribution for Jigsaw dataset

# toxic_reward = jigsaw_df_train[jigsaw_df_train["toxic"] == True]["reward"]
# non_toxic_reward = jigsaw_df_train[jigsaw_df_train["toxic"] == False]["reward"]

# plt.clf()
# plt.hist(toxic_reward, bins=100, alpha=0.5, label="Toxic")
# plt.hist(non_toxic_reward, bins=100, alpha=0.5, label="Non-Toxic")
# plt.legend(loc="upper right")
# plt.xlabel("Reward")
# plt.ylabel("Frequency")
# plt.title("Reward distribution for Jigsaw dataset (Train)")
# plt.savefig("reward_dist_jigsaw.png")

# toxic_reward = jigsaw_df_test[jigsaw_df_test["toxic"] == True]["reward"]
# non_toxic_reward = jigsaw_df_test[jigsaw_df_test["toxic"] == False]["reward"]

# plt.clf()
# plt.hist(toxic_reward, bins=100, alpha=0.5, label="Toxic")
# plt.hist(non_toxic_reward, bins=100, alpha=0.5, label="Non-Toxic")
# plt.legend(loc="upper right")
# plt.xlabel("Reward")
# plt.ylabel("Frequency")
# plt.title("Reward distribution for Jigsaw dataset (Test)")
# plt.savefig("reward_dist_jigsaw_test.png")


# # Plot the class wise reward distribution for IMDB dataset

# positive_reward = imdb_df_train[imdb_df_train["label"] == 1]["reward"]
# negative_reward = imdb_df_train[imdb_df_train["label"] == 0]["reward"]

# plt.clf()
# plt.hist(positive_reward, bins=100, alpha=0.5, label="Positive")
# plt.hist(negative_reward, bins=100, alpha=0.5, label="Negative")
# plt.legend(loc="upper right")
# plt.xlabel("Reward")
# plt.ylabel("Frequency")
# plt.title("Reward distribution for IMDB dataset (Train)")
# plt.savefig("reward_dist_imdb.png")

# positive_reward = imdb_df_test[imdb_df_test["label"] == 1]["reward"]
# negative_reward = imdb_df_test[imdb_df_test["label"] == 0]["reward"]

# plt.clf()
# plt.hist(positive_reward, bins=100, alpha=0.5, label="Positive")
# plt.hist(negative_reward, bins=100, alpha=0.5, label="Negative")
# plt.legend(loc="upper right")
# plt.xlabel("Reward")
# plt.ylabel("Frequency")
# plt.title("Reward distribution for IMDB dataset (Test)")
# plt.savefig("reward_dist_imdb_test.png")

File Path: examples/Jigsaw/dataset/evaluate_perplexity.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed
from trl.core import LengthSampler
import pandas as pd
from datasets import Dataset

tqdm.pandas()

import pdb
import pickle
import datetime
import os
from evaluate import load
from transformers import AutoModel, AutoTokenizer, AutoConfig
from transformers import OpenAIGPTTokenizer, OpenAIGPTModel
import random
import statistics as st

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
print(date_n_time)
logging_dir = f"/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/ppl_eval/{date_n_time}"
os.makedirs(logging_dir)
prompt_len = 8
response_len = 32

sft_model_path = "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-31_18-06-18"  # sft_pos 70/30
ra_rlhf_models = {
    "seed_2": "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/2024-01-31_22-05-21/save_pretrained",
    "seed_3": "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/2024-01-31_22-52-23/save_pretrained",
    "seed_4": "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/2024-01-31_22-52-23/save_pretrained",
}
rlhf_models = {
    "seed_2": "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/2024-01-31_19-28-10/save_pretrained",
    "seed_3": "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/2024-02-01_14-42-40/save_pretrained",
    "seed_4": "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/2024-01-31_22-52-37/save_pretrained",
}
ra_rlhf_perplexity = {"seed_2": [], "seed_3": [], "seed_4": []}
rlhf_perplexity = {"seed_2": [], "seed_3": [], "seed_4": []}
sft_perplexity = []
zero_shot_perplexity = []


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name=sft_model_path,
            query_dataset="imdb",
            reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            remove_unused_columns=False,
        )
    )
    query_dataset: str = field(default="imdb", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="less", metadata={"help": "the dataset to query"})


args = tyro.cli(ScriptArguments)

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset():
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(sft_model_path)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"

    df = pd.read_csv(
        "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/jigsaw_test.csv",
    )
    # filter out toxic rows
    df = df[df["toxic"] == 0]
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"comment_text": "review"})

    # pdb.set_trace()

    def input_size():
        return prompt_len

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# get test dataset
test_dataset = build_dataset()
# Take 5k random samples
# sample_size = 5000
# random_indices = random.sample(range(len(test_dataset)), sample_size)
# test_dataset = test_dataset.select(random_indices)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

zero_shot_model = trl_model_class.from_pretrained(
    "gpt2",
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)

# sft_model = trl_model_class.from_pretrained(
#     "lvwerra/gpt2-imdb",
#     trust_remote_code=True,
#     device_map=device_map,
#     peft_config=peft_config,
# )

# For the following to work I should have saved the model using huggingface
# ppo_model = trl_model_class.from_pretrained(
#     '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-09-26_11-27-23/model.pt',
#     trust_remote_code=True,
#     device_map=device_map,
#     peft_config=peft_config,
# )

# sft_ppo_model = trl_model_class.from_pretrained(
#     "/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/trl_sft_ppo/2023-10-23_23-30-47",
#     trust_remote_code=True,
#     device_map=device_map,
#     peft_config=peft_config,
# )
"""
ppo_model_PATH = '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-09-26_11-27-23/model.pt'
ppo_model = torch.load(ppo_model_PATH)  

sft_ppo_model_PATH = '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_ppo/2023-09-26_10-36-41/model.pt'
sft_ppo_model = torch.load(sft_ppo_model_PATH)
"""
tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id
# tokenizer.pad_token = tokenizer.eos_token
# tokenizer.padding_side = "left"
# tokenizer.truncation_side = "left"
# tokenizer.pad_token_as_eos_token = True
# tokenizer.max_length = 64

# Define all the tester classes
zero_shot_tester = PPOTrainer(
    args.ppo_config, zero_shot_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator
)
# sft_tester = PPOTrainer(args.ppo_config, sft_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)
# # ppo_tester = PPOTrainer(args.ppo_config, ppo_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)
# sft_ppo_tester = PPOTrainer(
#     args.ppo_config, sft_ppo_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator
# )

# pdb.set_trace()
# ppo_tester._save_pretrained('/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-09-26_11-27-23')
# sft_ppo_tester._save_pretrained('/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_ppo/2023-09-26_10-36-41')

# Build the perplexity evaluation pipeline
perplexity = load("perplexity", module_type="measurement")

# DO I NEED TO TAKE CARE OF PAD TOKEN FOR PERPLEXITY PIPELINE AS WELL??
"""
# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id
"""

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs = {
    "min_length": 32,
    "top_k": 0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": 32,
}


# date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
# os.makedirs(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/evaluation/{date_n_time}')

# model_names_all = ["zero_shot_model", "sft_model", "ppo_model", "sft_ppo_model"]
# testers_all = [zero_shot_tester]  # ppo_tester,

# infer on test dataset
# test_scores_1, test_scores_2, test_scores_3, test_scores_4 = [], [], [], []


for epoch, batch in tqdm(enumerate(zero_shot_tester.dataloader)):
    query_tensors = batch["input_ids"]

    texts = batch["review"]
    texts = [text[:500] for text in texts]
    # texts = ["lorem ipsum", "Happy Birthday!", "Bienvenue"]

    # ra-rlhf
    perplexity_scores_zero_shot = perplexity.compute(data=texts, model_id="gpt2")["perplexities"]
    zero_shot_perplexity.extend(perplexity_scores_zero_shot)
    perplexity_scores_sft = perplexity.compute(data=texts, model_id=sft_model_path)["perplexities"]
    sft_perplexity.extend(perplexity_scores_sft)
    for seed, model_path in ra_rlhf_models.items():
        perplexity_scores = perplexity.compute(data=texts, model_id=model_path)["perplexities"]
        ra_rlhf_perplexity[seed].extend(perplexity_scores)
    for seed, model_path in rlhf_models.items():
        perplexity_scores = perplexity.compute(data=texts, model_id=model_path)["perplexities"]
        rlhf_perplexity[seed].extend(perplexity_scores)
    # if epoch > 5:
    #     break


# print and save the final score
print("Zero Shot Perplexity: ", sum(zero_shot_perplexity) / len(zero_shot_perplexity))
print("SFT Perplexity: ", sum(sft_perplexity) / len(sft_perplexity))
ra_rlhf_ppls_mean = []
rlhf_ppls_mean = []
for seed, scores in ra_rlhf_perplexity.items():
    ra_rlhf_ppls_mean.append(sum(scores) / len(scores))
for seed, scores in rlhf_perplexity.items():
    rlhf_ppls_mean.append(sum(scores) / len(scores))
print(
    "RA-RLHF Perplexity: ",
    ra_rlhf_ppls_mean,
    "Mean: ",
    sum(ra_rlhf_ppls_mean) / len(ra_rlhf_ppls_mean),
    "Std: ",
    st.stdev(ra_rlhf_ppls_mean),
)
print(
    "RLHF Perplexity: ",
    rlhf_ppls_mean,
    "Mean: ",
    sum(rlhf_ppls_mean) / len(rlhf_ppls_mean),
    "Std: ",
    st.stdev(rlhf_ppls_mean),
)

File Path: examples/Jigsaw/dataset/evaluate_toxicity.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset, Dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed
from trl.core import LengthSampler
import pandas as pd

tqdm.pandas()

import pdb
import pickle
import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import statistics
import random

# model_path = "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-25_16-31-51"  # sft_positive
model_path = (
    "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-20_20-32-35"  # sft_both
)

prompt_len = 35
response_len = 32


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name="gpt2",
            query_dataset="imdb",
            reward_model="unitary/toxic-bert",
            learning_rate=1.41e-5,
            log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
        )
    )
    query_dataset: str = field(default="imdb", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )


args = tyro.cli(ScriptArguments)


# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # # Kaggle datasets have to be downloaded manually (using Kaggle API)
    # # datasets.load_dataset("jigsaw_toxicity_pred", data_dir="<path/to/manual/data>")
    # ds = load_dataset(
    #     query_dataset,
    #     data_dir="/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/original_dataset",
    #     split="train",
    # )  # Hardcoding path for now TODO: change this

    # ds = ds.rename_columns({"comment_text": "review"})
    # ds = ds.filter(lambda x: len(x["review"]) > 64, batched=False)

    # df = pd.DataFrame(ds)
    # num_toxic = df["toxic"].sum()

    # toxic_df = df[df["toxic"] == True]
    # non_toxic_df = df[df["toxic"] == False]

    # non_toxic_df = non_toxic_df.sample(n=num_toxic, random_state=config.seed)

    # # Recombine into dataset
    # ds = ds.from_pandas(pd.concat([toxic_df, non_toxic_df], ignore_index=True))
    df = pd.read_csv(
        "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/jigsaw_test.csv",
    )
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"comment_text": "review"})

    # pdb.set_trace()

    def input_size():
        return prompt_len

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)

# build and sample test dataset
test_dataset = build_dataset(args.ppo_config, args.query_dataset)
# Take 5k random samples
# sample_size = 5000
# random_indices = random.sample(range(len(test_dataset)), sample_size)
# test_dataset = test_dataset.select(random_indices)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    model_path,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id
# tokenizer.padding_side = "left"
# tokenizer.truncation_side = "left"
# tokenizer.pad_token_as_eos_token = True
# tokenizer.max_length = 64

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)
# Creating a tester object here because dataloader is only available in the PPOTrainer class
ppo_tester = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
# task, model_name = args.ppo_config.reward_model.split(":")
task = "text-classification"
model_name = args.ppo_config.reward_model
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs = {
    "min_length": response_len,
    "top_k": 0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": response_len,
}


# infer on test dataset
test_scores = []
test_score_lens = []
for epoch, batch in tqdm(enumerate(ppo_tester.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
    rewards = [torch.tensor(-output[0]["score"]) for output in pipe_outputs]
    # rewards = [
    #     torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"]]))[1]
    #     for output in pipe_outputs
    # ]

    test_scores.append(sum(rewards))
    test_score_lens.append(len(rewards))
    print(sum(rewards) / len(rewards))

# print and save the final score
# date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
# os.makedirs(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/zero_shot/{date_n_time}')
final_test_score = sum(test_scores) / sum(test_score_lens)
test_scores_list = [t.item() for t in test_scores]
print(f"Total test sentiment-score and std is: {final_test_score.item(), statistics.stdev(test_scores_list)}")
# pickle.dump(final_test_score.item(), open(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/zero_shot/{date_n_time}/final_test_score', "wb"))

File Path: examples/Jigsaw/dataset/evaluate_toxicity_6B.py
Content:
import argparse
import csv
import pandas as pd
import evaluate
import numpy as np
import torch
from datasets import load_dataset, Dataset
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer

# from trl.import_utils import is_npu_available, is_xpu_available

prompt_len = 8
toxicity = evaluate.load("ybelkada/toxicity", "DaNLP/da-electra-hatespeech-detection", module_type="measurement")


def build_dataset():
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(
        "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-31_18-06-18"
    )
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"

    df = pd.read_csv(
        "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/jigsaw_test.csv",
    )
    # filter out toxic rows
    df = df[df["toxic"] == 1]
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"comment_text": "review"})

    # pdb.set_trace()

    def input_size():
        return prompt_len

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# get test dataset
ds = build_dataset()


# ds = load_dataset("OxAISH-AL-LLM/wiki_toxic", split="test")

parser = argparse.ArgumentParser(description="Evaluate de-toxified models")
parser.add_argument("--model_type", default="all", type=str, help="Relative path to the source model folder")
parser.add_argument(
    "--output_file", default="toxicity_6B.csv", type=str, help="Relative path to the source model folder"
)
parser.add_argument("--batch_size", default=64, type=int, help="Batch size")
parser.add_argument("--num_samples", default=400, type=int, help="Number of samples")
parser.add_argument("--context_length", default=2000, type=int, help="Number of samples")
parser.add_argument("--max_new_tokens", default=32, type=int, help="Max new tokens for generation")
args = parser.parse_args()


if args.model_type == "all":
    MODELS_TO_TEST = [
        "ybelkada/gpt-neo-125m-detox",
        "EleutherAI/gpt-neo-125M",
        "EleutherAI/gpt-neo-2.7B",
        "ybelkada/gpt-neo-2.7B-detox",
        "ybelkada/gpt-j-6b-sharded-bf16",
        "ybelkada/gpt-j-6b-detoxs",
    ]
elif args.model_type == "gpt-neo":
    MODELS_TO_TEST = [
        "ybelkada/gpt-neo-125m-detox",
        "EleutherAI/gpt-neo-125M",
        "EleutherAI/gpt-neo-2.7B",
        "ybelkada/gpt-neo-2.7B-detox",
    ]
elif args.model_type == "gpt-j":
    MODELS_TO_TEST = [
        # "ybelkada/gpt-j-6b-sharded-bf16",
        "ybelkada/gpt-j-6b-detox",
    ]
else:
    MODELS_TO_TEST = [args.model_type]
NUM_SAMPLES = args.num_samples
BATCH_SIZE = args.batch_size
output_file = args.output_file
max_new_tokens = args.max_new_tokens
context_length = args.context_length
# if is_xpu_available():
#     device = torch.xpu.current_device()
# elif is_npu_available():
#     device = torch.npu.current_device()
device = torch.cuda.current_device() if torch.cuda.is_available() else "cpu"

# consider only toxic prompts
# ds = ds.filter(lambda x: x["label"] == 1)

toxicities = {}

# open a csv file
file = open(f"{output_file}", "w", newline="")
writer = csv.writer(file)
# add first rows
writer.writerow(["model_id", "mean_toxicity", "std_toxicity"])


for model_id in tqdm(MODELS_TO_TEST):
    model = AutoModelForCausalLM.from_pretrained(model_id, device_map={"": device}, torch_dtype=torch.bfloat16)
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"
    input_texts = []

    for i, example in enumerate(ds):
        # set seed
        torch.manual_seed(42)

        input_text = example["review"]
        input_texts.append(input_text[:2000])

        if i > NUM_SAMPLES:
            break

        if (i + 1) % BATCH_SIZE == 0:
            inputs = tokenizer(input_texts, return_tensors="pt", padding=True).to(device)
            inputs.input_ids = inputs.input_ids[:context_length]
            inputs.attention_mask = inputs.attention_mask[:context_length]
            outputs = model.generate(**inputs, do_sample=True, max_new_tokens=max_new_tokens, use_cache=True)
            generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)
            generated_texts = [
                generated_text.replace(input_texts[i], "") for i, generated_text in enumerate(generated_texts)
            ]
            toxicity_score = toxicity.compute(predictions=generated_texts)
            input_texts = []

            if model_id not in toxicities:
                toxicities[model_id] = []
            toxicities[model_id].extend(toxicity_score["toxicity"])

    # last batch
    inputs = tokenizer(input_texts, return_tensors="pt", padding=True).to(device)
    outputs = model.generate(**inputs, do_sample=True, max_new_tokens=30)
    generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)
    generated_texts = [generated_text.replace(input_texts[i], "") for i, generated_text in enumerate(generated_texts)]
    toxicity_score = toxicity.compute(predictions=generated_texts)
    toxicities[model_id].extend(toxicity_score["toxicity"])

    # compute mean & std using np
    mean = np.mean(toxicities[model_id])
    std = np.std(toxicities[model_id])

    # save to file
    writer.writerow([model_id, mean, std])

    # print
    print(f"Model: {model_id} - Mean: {mean} - Std: {std}")

    model = None
    # if is_xpu_available():
    #     torch.xpu.empty_cache()
    # elif is_npu_available():
    #     torch.npu.empty_cache()
    # else:
    torch.cuda.empty_cache()

# close file
file.close()

File Path: examples/Jigsaw/dataset/prepare_gpt4_input.py
Content:
import pandas as pd

# Load the generations

jigsaw_generations = pd.read_csv("jigsaw_generations_8_32_alpha_20_seed_2.csv")
gpt4_input = jigsaw_generations[["query", "response", "response_risk"]].copy()
gpt4_input.rename(columns={"query": "Prompt", "response": "Agent A", "response_risk": "Agent B"}, inplace=True)
gpt4_input.to_csv("input_to_gpt4.csv", index=False)

File Path: examples/Jigsaw/dataset/single_perplexity_jigsaw.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed
from trl.core import LengthSampler
import pandas as pd
from datasets import Dataset

tqdm.pandas()

import pdb
import pickle
import datetime
import os
from evaluate import load
from transformers import AutoModel, AutoTokenizer, AutoConfig
from transformers import OpenAIGPTTokenizer, OpenAIGPTModel
import random
import statistics as st

# date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
# print(date_n_time)
# logging_dir = f"/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/ppl_eval/{date_n_time}"
# os.makedirs(logging_dir)
prompt_len = 8
response_len = 32
bs = 4
ra_rlhf_model = "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/big/jigsaw/2024-03-23_22-54-39/save_pretrained"
rlhf_model = "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/big/jigsaw/2024-03-23_22-51-28/save_pretrained"
gpt2_model = "gpt2"
gptj_model = "ybelkada/gpt-j-6b-sharded-bf16"
sft_model = "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-31_18-06-18"

# @dataclass
# class ScriptArguments:
#     ppo_config: PPOConfig = field(
#         default_factory=lambda: PPOConfig(
#             model_name=model_path,
#             query_dataset=None,
#             reward_model=None,
#             learning_rate=1.41e-5,
#             mini_batch_size=128,
#             batch_size=128,
#             gradient_accumulation_steps=1,
#             early_stopping=False,
#             target_kl=6.0,
#             kl_penalty="kl",
#             seed=0,
#             use_score_scaling=False,
#             use_score_norm=False,
#             score_clip=None,
#             log_with="tensorboard",
#             project_kwargs={"logging_dir": logging_dir},
#             remove_unused_columns=False,
#         )
#     )
#     query_dataset: str = field(default="imdb", metadata={"help": "the dataset to query"})
#     use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
#     use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
#     peft_config: Optional[LoraConfig] = field(
#         default_factory=lambda: LoraConfig(
#             r=16,
#             lora_alpha=16,
#             bias="none",
#             task_type="CAUSAL_LM",
#         ),
#     )
#     reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
#     input_query_size: str = field(default="less", metadata={"help": "the dataset to query"})


# args = tyro.cli(ScriptArguments)

# # We then define the arguments to pass to the sentiment analysis pipeline.
# # We set `return_all_scores` to True to get the sentiment score for each token.
# sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

# trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(model_path):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"

    df = pd.read_csv(
        "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/jigsaw_test.csv",
    )
    # filter out toxic rows
    df = df[df["toxic"] == 0]
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"comment_text": "review"})

    # pdb.set_trace()

    def input_size():
        return prompt_len

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# get test dataset
# test_dataset = build_dataset()
# Take 5k random samples
# sample_size = 5000
# random_indices = random.sample(range(len(test_dataset)), sample_size)
# test_dataset = test_dataset.select(random_indices)


# def collator(data):
#     return dict((key, [d[key] for d in data]) for key in data[0])


# # set seed before initializing value head for deterministic eval
# set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
# if not args.use_peft:
#     ref_model = None  # trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
#     device_map = None
#     peft_config = None
# else:
#     peft_config = args.peft_config
#     ref_model = None
#     # Copy the model to each device
#     device_map = {"": Accelerator().local_process_index}

# zero_shot_model = trl_model_class.from_pretrained(
#     "gpt2",
#     trust_remote_code=True,
#     device_map=device_map,
#     peft_config=peft_config,
# )

# sft_model = trl_model_class.from_pretrained(
#     "lvwerra/gpt2-imdb",
#     trust_remote_code=True,
#     device_map=device_map,
#     peft_config=peft_config,
# )

# For the following to work I should have saved the model using huggingface
# ppo_model = trl_model_class.from_pretrained(
#     '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-09-26_11-27-23/model.pt',
#     trust_remote_code=True,
#     device_map=device_map,
#     peft_config=peft_config,
# )

# sft_ppo_model = trl_model_class.from_pretrained(
#     "/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/trl_sft_ppo/2023-10-23_23-30-47",
#     trust_remote_code=True,
#     device_map=device_map,
#     peft_config=peft_config,
# )
"""
ppo_model_PATH = '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-09-26_11-27-23/model.pt'
ppo_model = torch.load(ppo_model_PATH)  

sft_ppo_model_PATH = '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_ppo/2023-09-26_10-36-41/model.pt'
sft_ppo_model = torch.load(sft_ppo_model_PATH)
"""
# tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# # Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
# tokenizer.pad_token_id = tokenizer.eos_token_id
# # tokenizer.pad_token = tokenizer.eos_token
# # tokenizer.padding_side = "left"
# # tokenizer.truncation_side = "left"
# # tokenizer.pad_token_as_eos_token = True
# # tokenizer.max_length = 64
# model = trl_model_class.from_pretrained(
#     model_path,
#     trust_remote_code=True,
#     device_map=device_map,
#     peft_config=peft_config,
# )
# # Define all the tester classes
# zero_shot_tester = PPOTrainer(
#     args.ppo_config, model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator
# )
# # sft_tester = PPOTrainer(args.ppo_config, sft_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)
# # # ppo_tester = PPOTrainer(args.ppo_config, ppo_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)
# # sft_ppo_tester = PPOTrainer(
# #     args.ppo_config, sft_ppo_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator
# # )

# # pdb.set_trace()
# # ppo_tester._save_pretrained('/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-09-26_11-27-23')
# # sft_ppo_tester._save_pretrained('/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_ppo/2023-09-26_10-36-41')

# Build the perplexity evaluation pipeline
perplexity = load("perplexity", module_type="measurement")

# DO I NEED TO TAKE CARE OF PAD TOKEN FOR PERPLEXITY PIPELINE AS WELL??
"""
# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id
"""

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
# generation_kwargs = {
#     "min_length": 32,
#     "top_k": 0,
#     "top_p": 1.0,
#     "do_sample": True,
#     "pad_token_id": tokenizer.eos_token_id,
#     "max_new_tokens": 32,
# }


# date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
# os.makedirs(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/evaluation/{date_n_time}')

# model_names_all = ["zero_shot_model", "sft_model", "ppo_model", "sft_ppo_model"]
# testers_all = [zero_shot_tester]  # ppo_tester,

# infer on test dataset
# test_scores_1, test_scores_2, test_scores_3, test_scores_4 = [], [], [], []

perplexity_scores = {"gpt2": None, "gptj": None, "sft": None, "ra_rlhf": None, "rlhf": None}
# for i in tqdm(range(0, len(test_dataset), bs)):
# batch = test_dataset[i : i + bs]
batch = build_dataset(ra_rlhf_model)  # [i : i + bs]

texts = batch["review"]
texts = [text[:120] for text in texts]
# texts = ["lorem ipsum", "Happy Birthday!", "Bienvenue"]

# ra-rlhf
perplexity_score = perplexity.compute(data=texts, model_id=ra_rlhf_model, batch_size=bs)["perplexities"]
print(f"Epoch: {0}, Perplexity ra_rlhf: {sum(perplexity_score)/len(perplexity_score)}")
perplexity_scores["ra_rlhf"] = perplexity_score


# rlhf
perplexity_score = perplexity.compute(data=texts, model_id=rlhf_model, batch_size=bs)["perplexities"]
print(f"Epoch: {0}, Perplexity rlhf: {sum(perplexity_score)/len(perplexity_score)}")
perplexity_scores["rlhf"] = perplexity_score


# gptj
perplexity_score = perplexity.compute(data=texts, model_id=gptj_model, batch_size=bs)["perplexities"]
print(f"Epoch: {0}, Perplexity gptj: {sum(perplexity_score)/len(perplexity_score)}")
perplexity_scores["gptj"] = perplexity_score


batch = build_dataset(gpt2_model)  # [i : i + bs]

texts = batch["review"]
texts = [text[:120] for text in texts]
# gpt2
perplexity_score = perplexity.compute(data=texts, model_id=gpt2_model, batch_size=bs)["perplexities"]
print(f"Epoch: {0}, Perplexity gpt2: {sum(perplexity_score)/len(perplexity_score)}")
perplexity_scores["gpt2"] = perplexity_score

# sft
perplexity_score = perplexity.compute(data=texts, model_id=sft_model, batch_size=bs)["perplexities"]
print(f"Epoch: {0}, Perplexity sft: {sum(perplexity_score)/len(perplexity_score)}")
perplexity_scores["sft"] = perplexity_score

# save perplexity scores as json in current directory
import json

with open("perplexity_scores.json", "w") as f:
    json.dump(perplexity_scores, f)

File Path: examples/Jigsaw/dataset/table_fill_helper.py
Content:
import pandas as pd
import matplotlib.pyplot as plt

# import plotly.express as px

import pandas as pd

import numpy as np

import pdb

import statistics as st


# Jigsaw
jigsaw_gpt = pd.read_csv("jigsaw_generations_8_32_alpha_40_seed_2.csv")[["R_gpt2", "perplexity_gpt2"]].copy()
jigsaw_1 = pd.read_csv("jigsaw_generations_8_32_alpha_20_seed_2.csv")
jigsaw_1 = pd.concat([jigsaw_1, jigsaw_gpt], axis=1)
# select all sft columns and put into new dataframe
jigsaw_sft = jigsaw_1[["R_sft", "perplexity_sft"]].copy()

jigsaw_2 = pd.read_csv("jigsaw_generations_8_32_alpha_20_seed_94.csv")
jigsaw_2 = pd.concat([jigsaw_2, jigsaw_sft, jigsaw_gpt], axis=1)
jigsaw_3 = pd.read_csv("jigsaw_generations_8_32_alpha_20_seed_56.csv")
jigsaw_3 = pd.concat([jigsaw_3, jigsaw_sft, jigsaw_gpt], axis=1)


print(jigsaw_1.columns)
# a = input()

# pdb.set_trace()


# average env reward

sft_means = [jigsaw_1["R_sft"].mean(), jigsaw_2["R_sft"].mean(), jigsaw_3["R_sft"].mean()]
# print(sft_means)
gpt2_means = [jigsaw_1["R_gpt2"].mean(), jigsaw_2["R_gpt2"].mean(), jigsaw_3["R_gpt2"].mean()]
rlhf_means = [jigsaw_1["R"].mean(), jigsaw_2["R"].mean(), jigsaw_3["R"].mean()]
ra_rlhf_means = [jigsaw_1["R_risk"].mean(), jigsaw_2["R_risk"].mean(), jigsaw_3["R_risk"].mean()]


# print average env reward mean and var

print("gpt2 r mean and var:", round(st.mean(gpt2_means), 3), round(st.stdev(gpt2_means), 3))
print("sft r mean and var:", round(st.mean(sft_means), 3), round(st.stdev(sft_means), 3))
print("rlhf r mean and var:", round(st.mean(rlhf_means), 3), round(st.stdev(rlhf_means), 3))
print("ra rlhf r mean and var:", round(st.mean(ra_rlhf_means), 3), round(st.stdev(ra_rlhf_means), 3))

# perplexities

# gpt2_ppl = [jigsaw_1["perplexity_gpt2"].mean(), jigsaw_2["perplexity_gpt2"].mean(), jigsaw_3["perplexity_gpt2"].mean()]
# sft_ppl = [jigsaw_1["perplexity_sft"].mean(), jigsaw_2["perplexity_sft"].mean(), jigsaw_3["perplexity_sft"].mean()]
# rlhf_ppl = [jigsaw_1["perplexity"].mean(), jigsaw_2["perplexity"].mean(), jigsaw_3["perplexity"].mean()]
# ra_rlhf_ppl = [
#     jigsaw_1["perplexity_risk"].mean(),
#     jigsaw_2["perplexity_risk"].mean(),
#     jigsaw_3["perplexity_risk"].mean(),
# ]

# # print ppl mean and var

# print(f"ra_rlhf_ppl: {ra_rlhf_ppl}", f"rlhf+ppl{rlhf_ppl}")

# print("gpt2 ppl mean and var:", round(st.mean(gpt2_ppl), 3), round(st.stdev(gpt2_ppl), 3))
# print("sft ppl mean and var:", round(st.mean(sft_ppl), 3), round(st.stdev(sft_ppl), 3))
# print("rlhf ppl mean and var:", round(st.mean(rlhf_ppl), 3), round(st.stdev(rlhf_ppl), 3))
# print("ra rlhf ppl mean and var:", round(st.mean(ra_rlhf_ppl), 3), round(st.stdev(ra_rlhf_ppl), 3))


# tail performance

tail_to_save = 5
print(f"\n\ntail_to_save: {tail_to_save}")

tail_1 = jigsaw_1[jigsaw_1["prompt_score"] <= tail_to_save]

tail_2 = jigsaw_2[jigsaw_2["prompt_score"] <= tail_to_save]

tail_3 = jigsaw_3[jigsaw_3["prompt_score"] <= tail_to_save]


gpt2_tl_means = [tail_1["R_gpt2"].mean(), tail_2["R_gpt2"].mean(), tail_3["R_gpt2"].mean()]
sft_tl_means = [tail_1["R_sft"].mean(), tail_2["R_sft"].mean(), tail_3["R_sft"].mean()]
rlhf_tl_means = [tail_1["R"].mean(), tail_2["R"].mean(), tail_3["R"].mean()]
ra_rlhf_tl_means = [tail_1["R_risk"].mean(), tail_2["R_risk"].mean(), tail_3["R_risk"].mean()]

print("gpt2 tail mean and var:", round(st.mean(gpt2_tl_means), 3), round(st.stdev(gpt2_tl_means), 3))
print("sft tail mean and var:", round(st.mean(sft_tl_means), 3), round(st.stdev(sft_tl_means), 3))
print("rlhf tail mean and var:", round(st.mean(rlhf_tl_means), 3), round(st.stdev(rlhf_tl_means), 3))
print("ra rlhf tail mean and var:", round(st.mean(ra_rlhf_tl_means), 3), round(st.stdev(ra_rlhf_tl_means), 3))

tail_to_save = 2.5
print(f"\n\ntail_to_save: {tail_to_save}")

tail_1 = jigsaw_1[jigsaw_1["prompt_score"] <= tail_to_save]

tail_2 = jigsaw_2[jigsaw_2["prompt_score"] <= tail_to_save]

tail_3 = jigsaw_3[jigsaw_3["prompt_score"] <= tail_to_save]


gpt2_tl_means = [tail_1["R_gpt2"].mean(), tail_2["R_gpt2"].mean(), tail_3["R_gpt2"].mean()]
sft_tl_means = [tail_1["R_sft"].mean(), tail_2["R_sft"].mean(), tail_3["R_sft"].mean()]
rlhf_tl_means = [tail_1["R"].mean(), tail_2["R"].mean(), tail_3["R"].mean()]
ra_rlhf_tl_means = [tail_1["R_risk"].mean(), tail_2["R_risk"].mean(), tail_3["R_risk"].mean()]

print("gpt2 tail mean and var:", round(st.mean(gpt2_tl_means), 3), round(st.stdev(gpt2_tl_means), 3))
print("sft tail mean and var:", round(st.mean(sft_tl_means), 3), round(st.stdev(sft_tl_means), 3))
print("rlhf tail mean and var:", round(st.mean(rlhf_tl_means), 3), round(st.stdev(rlhf_tl_means), 3))
print("ra rlhf tail mean and var:", round(st.mean(ra_rlhf_tl_means), 3), round(st.stdev(ra_rlhf_tl_means), 3))

tail_to_save = 0.0
print(f"\n\ntail_to_save: {tail_to_save}")

tail_1 = jigsaw_1[jigsaw_1["prompt_score"] <= tail_to_save]

tail_2 = jigsaw_2[jigsaw_2["prompt_score"] <= tail_to_save]

tail_3 = jigsaw_3[jigsaw_3["prompt_score"] <= tail_to_save]


gpt2_tl_means = [tail_1["R_gpt2"].mean(), tail_2["R_gpt2"].mean(), tail_3["R_gpt2"].mean()]
sft_tl_means = [tail_1["R_sft"].mean(), tail_2["R_sft"].mean(), tail_3["R_sft"].mean()]
rlhf_tl_means = [tail_1["R"].mean(), tail_2["R"].mean(), tail_3["R"].mean()]
ra_rlhf_tl_means = [tail_1["R_risk"].mean(), tail_2["R_risk"].mean(), tail_3["R_risk"].mean()]

print("gpt2 tail mean and var:", round(st.mean(gpt2_tl_means), 3), round(st.stdev(gpt2_tl_means), 3))
print("sft tail mean and var:", round(st.mean(sft_tl_means), 3), round(st.stdev(sft_tl_means), 3))
print("rlhf tail mean and var:", round(st.mean(rlhf_tl_means), 3), round(st.stdev(rlhf_tl_means), 3))
print("ra rlhf tail mean and var:", round(st.mean(ra_rlhf_tl_means), 3), round(st.stdev(ra_rlhf_tl_means), 3))

tail_to_save = -2.5
print(f"\n\ntail_to_save: {tail_to_save}")

tail_1 = jigsaw_1[jigsaw_1["prompt_score"] <= tail_to_save]

tail_2 = jigsaw_2[jigsaw_2["prompt_score"] <= tail_to_save]

tail_3 = jigsaw_3[jigsaw_3["prompt_score"] <= tail_to_save]


gpt2_tl_means = [tail_1["R_gpt2"].mean(), tail_2["R_gpt2"].mean(), tail_3["R_gpt2"].mean()]
sft_tl_means = [tail_1["R_sft"].mean(), tail_2["R_sft"].mean(), tail_3["R_sft"].mean()]
rlhf_tl_means = [tail_1["R"].mean(), tail_2["R"].mean(), tail_3["R"].mean()]
ra_rlhf_tl_means = [tail_1["R_risk"].mean(), tail_2["R_risk"].mean(), tail_3["R_risk"].mean()]

print("gpt2 tail mean and var:", round(st.mean(gpt2_tl_means), 3), round(st.stdev(gpt2_tl_means), 3))
print("sft tail mean and var:", round(st.mean(sft_tl_means), 3), round(st.stdev(sft_tl_means), 3))
print("rlhf tail mean and var:", round(st.mean(rlhf_tl_means), 3), round(st.stdev(rlhf_tl_means), 3))
print("ra rlhf tail mean and var:", round(st.mean(ra_rlhf_tl_means), 3), round(st.stdev(ra_rlhf_tl_means), 3))

tail_to_save = -5
print(f"\n\ntail_to_save: {tail_to_save}")

tail_1 = jigsaw_1[jigsaw_1["prompt_score"] <= tail_to_save]

tail_2 = jigsaw_2[jigsaw_2["prompt_score"] <= tail_to_save]

tail_3 = jigsaw_3[jigsaw_3["prompt_score"] <= tail_to_save]


gpt2_tl_means = [tail_1["R_gpt2"].mean(), tail_2["R_gpt2"].mean(), tail_3["R_gpt2"].mean()]
sft_tl_means = [tail_1["R_sft"].mean(), tail_2["R_sft"].mean(), tail_3["R_sft"].mean()]
rlhf_tl_means = [tail_1["R"].mean(), tail_2["R"].mean(), tail_3["R"].mean()]
ra_rlhf_tl_means = [tail_1["R_risk"].mean(), tail_2["R_risk"].mean(), tail_3["R_risk"].mean()]

print("gpt2 tail mean and var:", round(st.mean(gpt2_tl_means), 3), round(st.stdev(gpt2_tl_means), 3))
print("sft tail mean and var:", round(st.mean(sft_tl_means), 3), round(st.stdev(sft_tl_means), 3))
print("rlhf tail mean and var:", round(st.mean(rlhf_tl_means), 3), round(st.stdev(rlhf_tl_means), 3))
print("ra rlhf tail mean and var:", round(st.mean(ra_rlhf_tl_means), 3), round(st.stdev(ra_rlhf_tl_means), 3))

File Path: examples/Jigsaw/dataset/test_quantile.py
Content:
from transformers import pipeline, AutoTokenizer
import pandas as pd
from tqdm import tqdm
import matplotlib.pyplot as plt
import plotly.express as px

tqdm.pandas()
# load the dataset
# jigsaw_df_train = pd.read_csv("jigsaw_train.csv")
# jigsaw_df_test = pd.read_csv("jigsaw_test.csv")
# imdb_df_train = pd.read_csv("imdb_train.csv")
# imdb_df_test = pd.read_csv("imdb_test.csv")

# imdb_df_test = imdb_df_test[imdb_df_test.index % 10 == 0]
# print("IMDB test size: ", len(imdb_df_test))


# tokenizer = AutoTokenizer.from_pretrained(
#     "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/models_to_test/2024-01-18_01-23-52"
# )

# generation_kwargs = {
#     "min_length": 48,
#     "top_k": 50,
#     "top_p": 1.0,
#     "do_sample": True,
#     "pad_token_id": tokenizer.eos_token_id,
#     "max_new_tokens": 48,
# }

# generator = pipeline(
#     "text-generation",
#     model="/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/models_to_test/2024-01-18_01-23-52",
# )

# generator_risk = pipeline(
#     "text-generation",
#     model="/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/models_to_test/2024-01-18_09-45-09",
# )

# print("Generating completions for RLHF model")
# imdb_df_test["generation"] = imdb_df_test["text"].progress_apply(
#     lambda x: generator(x[:64], **generation_kwargs)[0]["generated_text"]
# )
# print("Generating completions for RLHF risk averse model")
# imdb_df_test["generation_risk"] = imdb_df_test["text"].progress_apply(
#     lambda x: generator_risk(x[:64], **generation_kwargs)[0]["generated_text"]
# )

# imdb_df_test.to_csv("imdb_generations.csv", index=False)

# # Reward computation
# sent_kwargs = {"return_all_scores": True, "function_to_apply": "none"}
# imdb_reward_model = pipeline("sentiment-analysis", model="lvwerra/distilbert-imdb", truncation=True, **sent_kwargs)


# print("Calculating rewards for RLHF model")
# imdb_df_test["generation_reward"] = imdb_df_test["generation"].progress_apply(
#     lambda x: imdb_reward_model(x)[0][1]["score"]
# )

# print("Calculating rewards for RLHF risk averse model")
# imdb_df_test["generation_risk_reward"] = imdb_df_test["generation_risk"].progress_apply(
#     lambda x: imdb_reward_model(x)[0][1]["score"]
# )

# imdb_df_test.to_csv("imdb_generations.csv", index=False)

# Plot reward distribution for RLHF and RLHF risk averse model

# Load final dataset
imdb_df_test = pd.read_csv("jigsaw_generations.csv")
imdb_df_test = imdb_df_test.rename(columns={"R": "RLHF", "R_risk": "RLHF risk averse"})
fig = px.ecdf(
    imdb_df_test,
    x=["RLHF", "RLHF risk averse"],
    labels={
        "value": "Reward",
        "variable": "Model",
    },
)
fig.update_layout(
    title="Reward distribution for RLHF and RLHF risk averse models",
    xaxis_title="Reward",
    yaxis_title="CDF",
)
print("Average reward for RLHF: ", imdb_df_test["RLHF"].mean())
print("Average reward for RLHF risk averse: ", imdb_df_test["RLHF risk averse"].mean())
fig.write_image("jigsaw_R_cdf.png")

imdb_df_test = imdb_df_test.rename(columns={"R_bar": "RLHF returns", "R_bar_risk": "RLHF risk averse returns"})
fig = px.ecdf(
    imdb_df_test,
    x=["RLHF returns", "RLHF risk averse returns"],
    labels={
        "value": "Reward",
        "variable": "Model",
    },
)
fig.update_layout(
    title="Returns distribution for RLHF and RLHF risk averse models",
    xaxis_title="Returns",
    yaxis_title="CDF",
)
print("Average return for RLHF: ", imdb_df_test["RLHF returns"].mean())
print("Average return for RLHF risk averse: ", imdb_df_test["RLHF risk averse returns"].mean())
fig.write_image("jigsaw_R_bar_cdf.png")

File Path: examples/Jigsaw/dataset/test_quantile_batched_imdb.py
Content:
from transformers import pipeline, AutoTokenizer
from datasets import Dataset, load_dataset
from dataclasses import dataclass, field
from typing import Optional
from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead
from peft import LoraConfig
import pandas as pd
from tqdm import tqdm
import matplotlib.pyplot as plt
import plotly.express as px
import pdb
import tyro
import torch

# load the dataset
# jigsaw_df_train = pd.read_csv("jigsaw_train.csv")
# jigsaw_df_test = pd.read_csv("jigsaw_test.csv")

# load reward model
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none"}
imdb_reward_model = pipeline("sentiment-analysis", model="lvwerra/distilbert-imdb", **sent_kwargs)

ref_model_path = "lvwerra/gpt2-imdb"
model_path = "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/IMDB-RLHF-seed42"
risk_model_path = "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/IMDB-RA-RLHF-seed42"

BETA = 0.2
BETA_risk = 0.2
GAMMA = 1.0


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name="gpt2",
            query_dataset=None,
            # reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            init_kl_coef=BETA,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
        )
    )
    ppo_config_risk: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name="gpt2",
            query_dataset=None,
            # reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            init_kl_coef=BETA_risk,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
        )
    )
    query_dataset: str = field(default="imdb", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})


args = tyro.cli(ScriptArguments)

tokenizer = AutoTokenizer.from_pretrained(ref_model_path)
tokenizer.pad_token = tokenizer.eos_token

ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(
    ref_model_path,
    trust_remote_code=True,
    device_map=None,
    peft_config=None,
)


model = AutoModelForCausalLMWithValueHead.from_pretrained(
    model_path,
    trust_remote_code=True,
    device_map=None,
    peft_config=None,
)

risk_model = AutoModelForCausalLMWithValueHead.from_pretrained(
    risk_model_path,
    trust_remote_code=True,
    device_map=None,
    peft_config=None,
)


ppo_trainer = PPOTrainer(args.ppo_config, model=model, ref_model=ref_model, tokenizer=tokenizer)
ppo_trainer_risk = PPOTrainer(args.ppo_config_risk, model=risk_model, ref_model=ref_model, tokenizer=tokenizer)


def build_dataset(config, query_dataset):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # load imdb with datasets: for reducing data size, just use +'[:200]'
    ds = load_dataset(query_dataset, split="test")
    ds = ds.rename_columns({"text": "review"})
    ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)
    ds = ds.shuffle(seed=config.seed).select(
        range(5000)
    )  # Uncomment this line for a smaller subset of imdb test dataset

    def input_size():
        return 64

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side="left")
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"

generation_kwargs = {
    "min_length": 48,
    "top_k": 0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": 48,
}


batch_size = 128
ds = build_dataset(args.ppo_config, args.query_dataset)
df = pd.DataFrame()
for i in tqdm(range(0, len(ds), batch_size)):
    batch = ds[i : i + batch_size]
    query_tensors = batch["input_ids"]
    query_tensors = [q.to(ppo_trainer.current_device) for q in query_tensors]
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    response_tensors_risk = ppo_trainer_risk.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)
    batch["response_risk"] = tokenizer.batch_decode(response_tensors_risk)

    # pdb.set_trace()

    # Compute query sentiment score
    pipe_outputs_prompt = imdb_reward_model(
        batch["query"],
        **sent_kwargs,
        # truncation=True,
    )
    prompt_scores = [torch.tensor(output[1]["score"]) for output in pipe_outputs_prompt]
    batch["prompt_score"] = [s.item() for s in prompt_scores]

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = imdb_reward_model(
        texts,
        **sent_kwargs,
        # truncation=True,
    )
    texts_risk = [q + r for q, r in zip(batch["query"], batch["response_risk"])]
    pipe_outputs_risk = imdb_reward_model(
        texts_risk,
        **sent_kwargs,
        # truncation=True,
    )
    scores = [torch.tensor(output[1]["score"]) for output in pipe_outputs]
    scores_risk = [torch.tensor(output[1]["score"]) for output in pipe_outputs_risk]
    batch["R"] = [s.item() for s in scores]
    batch["R_risk"] = [s.item() for s in scores_risk]

    # Compute R_bar
    scores = torch.tensor(scores, device=ppo_trainer.current_device)
    scores_risk = torch.tensor(scores_risk, device=ppo_trainer_risk.current_device)

    model_inputs = ppo_trainer.prepare_model_inputs(query_tensors, response_tensors)
    model_inputs_risk = ppo_trainer_risk.prepare_model_inputs(query_tensors, response_tensors_risk)

    model_inputs_names = list(model_inputs.keys())

    with torch.no_grad():
        all_logprobs, _, values, masks = ppo_trainer.batched_forward_pass(
            ppo_trainer.model,
            query_tensors,
            response_tensors,
            model_inputs,
            response_masks=None,
            return_logits=False,
        )
        all_logprobs_risk, _, values_risk, masks_risk = ppo_trainer_risk.batched_forward_pass(
            ppo_trainer_risk.model,
            query_tensors,
            response_tensors_risk,
            model_inputs_risk,
            response_masks=None,
            return_logits=False,
        )
        ref_logprobs, _, _, _ = ppo_trainer.batched_forward_pass(
            ppo_trainer.ref_model, query_tensors, response_tensors, model_inputs, return_logits=None
        )
        ref_logprobs_risk, _, _, _ = ppo_trainer_risk.batched_forward_pass(
            ppo_trainer_risk.ref_model, query_tensors, response_tensors_risk, model_inputs_risk, return_logits=None
        )
        rewards, non_score_reward = ppo_trainer.compute_rewards(scores, all_logprobs, ref_logprobs, masks)
        rewards_risk, non_score_reward_risk = ppo_trainer_risk.compute_rewards(
            scores_risk, all_logprobs_risk, ref_logprobs_risk, masks_risk
        )
    if GAMMA < 1.0:
        for t in range(rewards.shape[1]):
            rewards[:, t] = GAMMA**t * rewards[:, t]
            rewards_risk[:, t] = GAMMA**t * rewards_risk[:, t]

    batch["R_bar"] = rewards.sum(axis=1).tolist()
    batch["R_bar_risk"] = rewards_risk.sum(axis=1).tolist()
    # pdb.set_trace()
    try:
        df = pd.concat([df, pd.DataFrame(batch)], ignore_index=True)
    except:
        print("Replacing query tensors with None")
        batch["input_ids"] = [None] * len(batch["review"])
        df = pd.concat([df, pd.DataFrame(batch)], ignore_index=True)


print("Saving results")
df.to_csv("imdb_generations.csv", index=False)

File Path: examples/Jigsaw/dataset/test_quantile_batched_jigsaw.py
Content:
from transformers import pipeline, AutoTokenizer
from datasets import Dataset
from dataclasses import dataclass, field
from typing import Optional
from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead
from peft import LoraConfig
import pandas as pd
from tqdm import tqdm
import matplotlib.pyplot as plt
import plotly.express as px
import pdb
import tyro
import torch
from evaluate import load

# load the dataset
# jigsaw_df_train = pd.read_csv("jigsaw_train.csv")
jigsaw_df_test = pd.read_csv("jigsaw_test.csv")

# load reward model
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none"}
jigsaw_reward_model = pipeline("sentiment-analysis", model="unitary/toxic-bert", **sent_kwargs)

ref_model_path = "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-31_18-06-18"  # sft positive
# ref_model_path = (
# "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-20_20-32-35"  # sft both
# )


model_path = "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/big/jigsaw/2024-03-23_22-51-28/save_pretrained"
risk_model_path = "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/big/jigsaw/2024-03-26_11-21-45/save_pretrained"

BETA = 0.2
BETA_risk = 0.2
GAMMA = 1.0

prompt_len = 8
response_len = 32
alpha_level = 20
seed = 1


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="gpt2",
            query_dataset=None,
            # reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            init_kl_coef=BETA,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
        )
    )
    ppo_config_risk: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="gpt2",
            query_dataset=None,
            # reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            init_kl_coef=BETA_risk,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
        )
    )
    query_dataset: str = field(default=None, metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})


args = tyro.cli(ScriptArguments)

tokenizer = AutoTokenizer.from_pretrained(ref_model_path)
tokenizer.pad_token = tokenizer.eos_token

ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(
    ref_model_path,
    trust_remote_code=True,
    device_map=None,
    peft_config=None,
)


# model = AutoModelForCausalLMWithValueHead.from_pretrained(
#     model_path,
#     trust_remote_code=True,
#     device_map=None,
#     peft_config=None,
# )

risk_model = AutoModelForCausalLMWithValueHead.from_pretrained(
    risk_model_path,
    trust_remote_code=True,
    device_map=None,
    peft_config=None,
)
# gpt2_model = AutoModelForCausalLMWithValueHead.from_pretrained(
#     "gpt2",
#     trust_remote_code=True,
#     device_map=None,
#     peft_config=None,
# )
# gptj_model = AutoModelForCausalLMWithValueHead.from_pretrained(
#     "ybelkada/gpt-j-6b-sharded-bf16",
#     trust_remote_code=True,
#     device_map=None,
#     peft_config=None,
# )
# ppo_trainer_gpt2 = PPOTrainer(args.ppo_config, model=gpt2_model, ref_model=ref_model, tokenizer=tokenizer)
# ppo_trainer_gptj = PPOTrainer(args.ppo_config, model=gptj_model, ref_model=ref_model, tokenizer=tokenizer)
# ppo_trainer_sft = PPOTrainer(args.ppo_config, model=ref_model, ref_model=ref_model, tokenizer=tokenizer)
# ppo_trainer = PPOTrainer(args.ppo_config, model=model, ref_model=ref_model, tokenizer=tokenizer)
ppo_trainer_risk = PPOTrainer(args.ppo_config_risk, model=risk_model, ref_model=ref_model, tokenizer=tokenizer)
# perplexity = load("perplexity", module_type="measurement")


def build_dataset():
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"

    df = pd.read_csv(
        "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/jigsaw_test.csv",
    )
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"comment_text": "review"})

    # pdb.set_trace()

    def input_size():
        return prompt_len

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side="left")
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"

generation_kwargs = {
    "min_length": response_len,
    "top_k": 0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": response_len,
}


batch_size = 128
ds = build_dataset()
df = pd.DataFrame()
for i in tqdm(range(0, len(ds), batch_size)):
    batch = ds[i : i + batch_size]
    query_tensors = batch["input_ids"]
    query_tensors = [q.to(ppo_trainer_risk.current_device) for q in query_tensors]
    # response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    response_tensors_risk = ppo_trainer_risk.generate(query_tensors, return_prompt=False, **generation_kwargs)
    # response_tensors_sft = ppo_trainer_sft.generate(query_tensors, return_prompt=False, **generation_kwargs)
    # response_tensors_gpt2 = ppo_trainer_gpt2.generate(query_tensors, return_prompt=False, **generation_kwargs)
    # response_tensors_gptj = ppo_trainer_gptj.generate(query_tensors, return_prompt=False, **generation_kwargs)
    # batch["response"] = tokenizer.batch_decode(response_tensors)
    batch["response_risk"] = tokenizer.batch_decode(response_tensors_risk)
    # batch["response_sft"] = tokenizer.batch_decode(response_tensors_sft)
    # batch["response_gpt2"] = tokenizer.batch_decode(response_tensors_gpt2)
    # batch["response_gptj"] = tokenizer.batch_decode(response_tensors_gptj)

    # pdb.set_trace()
    # text_data = [text[:500] for text in batch["review"]]
    # Compute query sentiment score
    pipe_outputs_prompt = jigsaw_reward_model(
        batch["query"],
        **sent_kwargs,
        # truncation=True,
    )
    prompt_scores = [torch.tensor(-output[0]["score"]) for output in pipe_outputs_prompt]
    batch["prompt_score"] = [s.item() for s in prompt_scores]

    # Compute sentiment score
    # texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    # pipe_outputs = jigsaw_reward_model(
    #     texts,
    #     **sent_kwargs,
    #     truncation=True,
    # )
    texts_risk = [q + r for q, r in zip(batch["query"], batch["response_risk"])]
    pipe_outputs_risk = jigsaw_reward_model(
        texts_risk,
        **sent_kwargs,
        truncation=True,
    )
    # texts_sft = [q + r for q, r in zip(batch["query"], batch["response_sft"])]
    # pipe_outputs_sft = jigsaw_reward_model(
    #     texts_sft,
    #     **sent_kwargs,
    #     truncation=True,
    # )
    # texts_gpt2 = [q + r for q, r in zip(batch["query"], batch["response_gpt2"])]
    # pipe_outputs_gpt2 = jigsaw_reward_model(
    #     texts_gpt2,
    #     **sent_kwargs,
    #     truncation=True,
    # )
    # texts_gptj = [q + r for q, r in zip(batch["query"], batch["response_gptj"])]
    # pipe_outputs_gptj = jigsaw_reward_model(
    #     texts_gptj,
    #     **sent_kwargs,
    #     truncation=True,
    # )
    # scores = [torch.tensor(-output[0]["score"]) for output in pipe_outputs]
    scores_risk = [torch.tensor(-output[0]["score"]) for output in pipe_outputs_risk]
    # scores_sft = [torch.tensor(-output[0]["score"]) for output in pipe_outputs_sft]
    # scores_gpt2 = [torch.tensor(-output[0]["score"]) for output in pipe_outputs_gpt2]
    # scores_gptj = [torch.tensor(-output[0]["score"]) for output in pipe_outputs_gptj]
    # batch["R"] = [s.item() for s in scores]
    batch["R_risk"] = [s.item() for s in scores_risk]
    # batch["R_sft"] = [s.item() for s in scores_sft]
    # batch["R_gpt2"] = [s.item() for s in scores_gpt2]
    # batch["R_gptj"] = [s.item() for s in scores_gptj]

    # Compute R_bar
    # scores = torch.tensor(scores, device=ppo_trainer.current_device)
    # scores_risk = torch.tensor(scores_risk, device=ppo_trainer_risk.current_device)

    # model_inputs = ppo_trainer.prepare_model_inputs(query_tensors, response_tensors)
    # model_inputs_risk = ppo_trainer_risk.prepare_model_inputs(query_tensors, response_tensors_risk)

    # model_inputs_names = list(model_inputs.keys())

    # with torch.no_grad():
    #     all_logprobs, _, values, masks = ppo_trainer.batched_forward_pass(
    #         ppo_trainer.model,
    #         query_tensors,
    #         response_tensors,
    #         model_inputs,
    #         response_masks=None,
    #         return_logits=False,
    #     )
    #     all_logprobs_risk, _, values_risk, masks_risk = ppo_trainer_risk.batched_forward_pass(
    #         ppo_trainer_risk.model,
    #         query_tensors,
    #         response_tensors_risk,
    #         model_inputs_risk,
    #         response_masks=None,
    #         return_logits=False,
    #     )
    #     ref_logprobs, _, _, _ = ppo_trainer.batched_forward_pass(
    #         ppo_trainer.ref_model, query_tensors, response_tensors, model_inputs, return_logits=None
    #     )
    #     ref_logprobs_risk, _, _, _ = ppo_trainer_risk.batched_forward_pass(
    #         ppo_trainer_risk.ref_model, query_tensors, response_tensors_risk, model_inputs_risk, return_logits=None
    #     )
    #     rewards, non_score_reward = ppo_trainer.compute_rewards(scores, all_logprobs, ref_logprobs, masks)
    #     rewards_risk, non_score_reward_risk = ppo_trainer_risk.compute_rewards(
    #         scores_risk, all_logprobs_risk, ref_logprobs_risk, masks_risk
    #     )
    # if GAMMA < 1.0:
    #     for t in range(rewards.shape[1]):
    #         rewards[:, t] = GAMMA**t * rewards[:, t]
    #         rewards_risk[:, t] = GAMMA**t * rewards_risk[:, t]

    # batch["R_bar"] = rewards.sum(axis=1).tolist()
    # batch["R_bar_risk"] = rewards_risk.sum(axis=1).tolist()

    # Compute perplexity
    # batch["perplexity"] = perplexity.compute(data=text_data, model_id=model_path)["perplexities"]
    # batch["perplexity_risk"] = perplexity.compute(data=text_data, model_id=risk_model_path)["perplexities"]
    # batch["perplexity_sft"] = perplexity.compute(data=text_data, model_id=ref_model_path)["perplexities"]
    # batch["perplexity_gpt2"] = perplexity.compute(data=text_data, model_id="gpt2", batch_size=64)["perplexities"]
    # pdb.set_trace()
    try:
        df = pd.concat([df, pd.DataFrame(batch)], ignore_index=True)
    except:
        batch["input_ids"] = [None] * len(batch["review"])
        df = pd.concat([df, pd.DataFrame(batch)], ignore_index=True)


print("Saving results")
read_df = pd.read_csv("jigsaw_generations_8_32_alpha_20_seed_1_gptj_full.csv")

read_df = read_df.drop(columns=["response_risk", "R_risk", "prompt_score"])
# common_columns = read_df.columns.intersection(df.columns)

# # Decide which DataFrame to drop the common columns from (here, we choose df2 for demonstration)
# df_dropped = df.drop(columns=common_columns)
# df = pd.concat([read_df, df_dropped], axis=1)
df = pd.concat([read_df, df], axis=1)
df.to_csv(
    f"jigsaw_generations_{prompt_len}_{response_len}_alpha_{alpha_level}_seed_{seed}_gptj_full_correct.csv",
    index=False,
)


print("Average RLHF reward:", df["R"].mean())
print("Average RA-RLHF reward:", df["R_risk"].mean())
print("Average SFT reward:", df["R_sft"].mean())
print("Average GPT2 reward:", df["R_gpt2"].mean())
print("Average GPTJ reward:", df["R_gptj"].mean())

df_tail = df[df["prompt_score"] < 5]

print("Average RLHF reward (tail):", df_tail["R"].mean())
print("Average RA-RLHF reward (tail):", df_tail["R_risk"].mean())
print("Average SFT reward (tail):", df_tail["R_sft"].mean())
print("Average GPT2 reward (tail):", df_tail["R_gpt2"].mean())
print("Average GPTJ reward (tail):", df_tail["R_gptj"].mean())

# print("Average RLHF perplexity:", df["perplexity"].mean())
# print("Average RA-RLHF perplexity:", df["perplexity_risk"].mean())
# print("Average SFT perplexity:", df["perplexity_sft"].mean())
# print("Average GPT2 perplexity:", df["perplexity_gpt2"].mean())

File Path: examples/Jigsaw/dataset/test_quantile_batched_jigsaw_seed12.py
Content:
from transformers import pipeline, AutoTokenizer
from datasets import Dataset
from dataclasses import dataclass, field
from typing import Optional
from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead
from peft import LoraConfig
import pandas as pd
from tqdm import tqdm
import matplotlib.pyplot as plt
import plotly.express as px
import pdb
import tyro
import torch
from evaluate import load

# load the dataset
jigsaw_df_train = pd.read_csv("jigsaw_train_70_30.csv")
jigsaw_df_test = pd.read_csv("jigsaw_test.csv")

# load reward model
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none"}
jigsaw_reward_model = pipeline("sentiment-analysis", model="unitary/toxic-bert", **sent_kwargs)

# ref_model_path = (
#     "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-20_20-32-35"  # sft_both
# )
ref_model_path = "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-31_18-06-18"  # sft_positive seed 2
model_path = (
    "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/2024-01-31_20-36-43/save_pretrained"
)
risk_model_path = (
    "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/2024-02-01_00-06-52/save_pretrained"
)

BETA = 0.2
BETA_risk = 0.2
GAMMA = 1.0

prompt_len = 8
response_len = 32
alpha_level = 20
seed = 12


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="gpt2",
            query_dataset=None,
            # reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            init_kl_coef=BETA,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
        )
    )
    ppo_config_risk: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="gpt2",
            query_dataset=None,
            # reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            init_kl_coef=BETA_risk,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
        )
    )
    query_dataset: str = field(default=None, metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})


args = tyro.cli(ScriptArguments)

tokenizer = AutoTokenizer.from_pretrained(ref_model_path)
tokenizer.pad_token = tokenizer.eos_token

ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(
    ref_model_path,
    trust_remote_code=True,
    device_map=None,
    peft_config=None,
)


model = AutoModelForCausalLMWithValueHead.from_pretrained(
    model_path,
    trust_remote_code=True,
    device_map=None,
    peft_config=None,
)

risk_model = AutoModelForCausalLMWithValueHead.from_pretrained(
    risk_model_path,
    trust_remote_code=True,
    device_map=None,
    peft_config=None,
)
# gpt2_model = AutoModelForCausalLMWithValueHead.from_pretrained(
#     "gpt2",
#     trust_remote_code=True,
#     device_map=None,
#     peft_config=None,
# )
# ppo_trainer_gpt2 = PPOTrainer(args.ppo_config, model=gpt2_model, ref_model=ref_model, tokenizer=tokenizer)
# ppo_trainer_sft = PPOTrainer(args.ppo_config, model=ref_model, ref_model=ref_model, tokenizer=tokenizer)
ppo_trainer = PPOTrainer(args.ppo_config, model=model, ref_model=ref_model, tokenizer=tokenizer)
ppo_trainer_risk = PPOTrainer(args.ppo_config_risk, model=risk_model, ref_model=ref_model, tokenizer=tokenizer)
perplexity = load("perplexity", module_type="measurement")


def build_dataset():
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"

    df = pd.read_csv(
        "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/jigsaw_test.csv",
    )
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"comment_text": "review"})

    # pdb.set_trace()

    def input_size():
        return prompt_len

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side="left")
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"

generation_kwargs = {
    "min_length": response_len,
    "top_k": 0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": response_len,
}


batch_size = 128
ds = build_dataset()
df = pd.DataFrame()
for i in tqdm(range(0, len(ds), batch_size)):
    batch = ds[i : i + batch_size]
    query_tensors = batch["input_ids"]
    query_tensors = [q.to(ppo_trainer.current_device) for q in query_tensors]
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    response_tensors_risk = ppo_trainer_risk.generate(query_tensors, return_prompt=False, **generation_kwargs)
    # response_tensors_sft = ppo_trainer_sft.generate(query_tensors, return_prompt=False, **generation_kwargs)
    # response_tensors_gpt2 = ppo_trainer_gpt2.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)
    batch["response_risk"] = tokenizer.batch_decode(response_tensors_risk)
    # batch["response_sft"] = tokenizer.batch_decode(response_tensors_sft)
    # batch["response_gpt2"] = tokenizer.batch_decode(response_tensors_gpt2)

    # pdb.set_trace()
    text_data = [text[:500] for text in batch["review"]]
    # Compute query sentiment score
    pipe_outputs_prompt = jigsaw_reward_model(
        batch["query"],
        **sent_kwargs,
        # truncation=True,
    )
    prompt_scores = [torch.tensor(-output[0]["score"]) for output in pipe_outputs_prompt]
    batch["prompt_score"] = [s.item() for s in prompt_scores]

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = jigsaw_reward_model(
        texts,
        **sent_kwargs,
        truncation=True,
    )
    texts_risk = [q + r for q, r in zip(batch["query"], batch["response_risk"])]
    pipe_outputs_risk = jigsaw_reward_model(
        texts_risk,
        **sent_kwargs,
        truncation=True,
    )
    # texts_sft = [q + r for q, r in zip(batch["query"], batch["response_sft"])]
    # pipe_outputs_sft = jigsaw_reward_model(
    #     texts_sft,
    #     **sent_kwargs,
    #     truncation=True,
    # )
    # texts_gpt2 = [q + r for q, r in zip(batch["query"], batch["response_gpt2"])]
    # pipe_outputs_gpt2 = jigsaw_reward_model(
    #     texts_gpt2,
    #     **sent_kwargs,
    #     truncation=True,
    # )
    scores = [torch.tensor(-output[0]["score"]) for output in pipe_outputs]
    scores_risk = [torch.tensor(-output[0]["score"]) for output in pipe_outputs_risk]
    # scores_sft = [torch.tensor(-output[0]["score"]) for output in pipe_outputs_sft]
    # scores_gpt2 = [torch.tensor(-output[0]["score"]) for output in pipe_outputs_gpt2]
    batch["R"] = [s.item() for s in scores]
    batch["R_risk"] = [s.item() for s in scores_risk]
    # batch["R_sft"] = [s.item() for s in scores_sft]
    # batch["R_gpt2"] = [s.item() for s in scores_gpt2]

    # Compute R_bar
    scores = torch.tensor(scores, device=ppo_trainer.current_device)
    scores_risk = torch.tensor(scores_risk, device=ppo_trainer_risk.current_device)

    model_inputs = ppo_trainer.prepare_model_inputs(query_tensors, response_tensors)
    model_inputs_risk = ppo_trainer_risk.prepare_model_inputs(query_tensors, response_tensors_risk)

    model_inputs_names = list(model_inputs.keys())

    with torch.no_grad():
        all_logprobs, _, values, masks = ppo_trainer.batched_forward_pass(
            ppo_trainer.model,
            query_tensors,
            response_tensors,
            model_inputs,
            response_masks=None,
            return_logits=False,
        )
        all_logprobs_risk, _, values_risk, masks_risk = ppo_trainer_risk.batched_forward_pass(
            ppo_trainer_risk.model,
            query_tensors,
            response_tensors_risk,
            model_inputs_risk,
            response_masks=None,
            return_logits=False,
        )
        ref_logprobs, _, _, _ = ppo_trainer.batched_forward_pass(
            ppo_trainer.ref_model, query_tensors, response_tensors, model_inputs, return_logits=None
        )
        ref_logprobs_risk, _, _, _ = ppo_trainer_risk.batched_forward_pass(
            ppo_trainer_risk.ref_model, query_tensors, response_tensors_risk, model_inputs_risk, return_logits=None
        )
        rewards, non_score_reward = ppo_trainer.compute_rewards(scores, all_logprobs, ref_logprobs, masks)
        rewards_risk, non_score_reward_risk = ppo_trainer_risk.compute_rewards(
            scores_risk, all_logprobs_risk, ref_logprobs_risk, masks_risk
        )
    if GAMMA < 1.0:
        for t in range(rewards.shape[1]):
            rewards[:, t] = GAMMA**t * rewards[:, t]
            rewards_risk[:, t] = GAMMA**t * rewards_risk[:, t]

    batch["R_bar"] = rewards.sum(axis=1).tolist()
    batch["R_bar_risk"] = rewards_risk.sum(axis=1).tolist()

    # Compute perplexity
    batch["perplexity"] = perplexity.compute(data=text_data, model_id=model_path)["perplexities"]
    batch["perplexity_risk"] = perplexity.compute(data=text_data, model_id=risk_model_path)["perplexities"]
    # batch["perplexity_sft"] = perplexity.compute(data=text_data, model_id=ref_model_path)["perplexities"]
    # batch["perplexity_gpt2"] = perplexity.compute(data=text_data, model_id="gpt2", batch_size=64)["perplexities"]
    # pdb.set_trace()
    try:
        df = pd.concat([df, pd.DataFrame(batch)], ignore_index=True)
    except:
        print("Replacing query tensors with None")
        batch["input_ids"] = [None] * len(batch["review"])
        df = pd.concat([df, pd.DataFrame(batch)], ignore_index=True)


print("Saving results")
df.to_csv(f"jigsaw_generations_{prompt_len}_{response_len}_alpha_{alpha_level}_seed_{seed}.csv", index=False)


print("Average RLHF reward:", df["R"].mean())
print("Average RA-RLHF reward:", df["R_risk"].mean())
# print("Average SFT reward:", df["R_sft"].mean())
# print("Average GPT2 reward:", df["R_gpt2"].mean())

print("Average RLHF perplexity:", df["perplexity"].mean())
print("Average RA-RLHF perplexity:", df["perplexity_risk"].mean())
# print("Average SFT perplexity:", df["perplexity_sft"].mean())
# print("Average GPT2 perplexity:", df["perplexity_gpt2"].mean())

File Path: examples/Jigsaw/dataset/test_quantile_batched_jigsaw_seed12_truly.py
Content:
from transformers import pipeline, AutoTokenizer
from datasets import Dataset
from dataclasses import dataclass, field
from typing import Optional
from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead
from peft import LoraConfig
import pandas as pd
from tqdm import tqdm
import matplotlib.pyplot as plt
import plotly.express as px
import pdb
import tyro
import torch
from evaluate import load

# load the dataset
jigsaw_df_train = pd.read_csv("jigsaw_train.csv")
jigsaw_df_test = pd.read_csv("jigsaw_test.csv")

# load reward model
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none"}
jigsaw_reward_model = pipeline("sentiment-analysis", model="unitary/toxic-bert", **sent_kwargs)

# ref_model_path = (
#     "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-20_20-32-35"  # sft_both
# )
ref_model_path = "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-30_00-30-38"  # sft_positive seed 12
model_path = (
    "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/2024-01-30_02-59-24/save_pretrained"
)
risk_model_path = (
    "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/2024-01-30_02-59-35/save_pretrained"
)

BETA = 0.2
BETA_risk = 0.2
GAMMA = 1.0

prompt_len = 8
response_len = 32
alpha_level = 40
seed = 12


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="gpt2",
            query_dataset=None,
            # reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            init_kl_coef=BETA,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
        )
    )
    ppo_config_risk: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="gpt2",
            query_dataset=None,
            # reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            init_kl_coef=BETA_risk,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
        )
    )
    query_dataset: str = field(default=None, metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})


args = tyro.cli(ScriptArguments)

tokenizer = AutoTokenizer.from_pretrained(ref_model_path)
tokenizer.pad_token = tokenizer.eos_token

ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(
    ref_model_path,
    trust_remote_code=True,
    device_map=None,
    peft_config=None,
)


model = AutoModelForCausalLMWithValueHead.from_pretrained(
    model_path,
    trust_remote_code=True,
    device_map=None,
    peft_config=None,
)

risk_model = AutoModelForCausalLMWithValueHead.from_pretrained(
    risk_model_path,
    trust_remote_code=True,
    device_map=None,
    peft_config=None,
)
gpt2_model = AutoModelForCausalLMWithValueHead.from_pretrained(
    "gpt2",
    trust_remote_code=True,
    device_map=None,
    peft_config=None,
)
ppo_trainer_gpt2 = PPOTrainer(args.ppo_config, model=gpt2_model, ref_model=ref_model, tokenizer=tokenizer)
ppo_trainer_sft = PPOTrainer(args.ppo_config, model=ref_model, ref_model=ref_model, tokenizer=tokenizer)
ppo_trainer = PPOTrainer(args.ppo_config, model=model, ref_model=ref_model, tokenizer=tokenizer)
ppo_trainer_risk = PPOTrainer(args.ppo_config_risk, model=risk_model, ref_model=ref_model, tokenizer=tokenizer)
perplexity = load("perplexity", module_type="measurement")


def build_dataset():
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"

    df = pd.read_csv(
        "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/jigsaw_test.csv",
    )
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"comment_text": "review"})

    # pdb.set_trace()

    def input_size():
        return prompt_len

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side="left")
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"

generation_kwargs = {
    "min_length": response_len,
    "top_k": 0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": response_len,
}


batch_size = 128
ds = build_dataset()
df = pd.DataFrame()
for i in tqdm(range(0, len(ds), batch_size)):
    batch = ds[i : i + batch_size]
    query_tensors = batch["input_ids"]
    query_tensors = [q.to(ppo_trainer.current_device) for q in query_tensors]
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    response_tensors_risk = ppo_trainer_risk.generate(query_tensors, return_prompt=False, **generation_kwargs)
    response_tensors_sft = ppo_trainer_sft.generate(query_tensors, return_prompt=False, **generation_kwargs)
    response_tensors_gpt2 = ppo_trainer_gpt2.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)
    batch["response_risk"] = tokenizer.batch_decode(response_tensors_risk)
    batch["response_sft"] = tokenizer.batch_decode(response_tensors_sft)
    batch["response_gpt2"] = tokenizer.batch_decode(response_tensors_gpt2)

    # pdb.set_trace()
    text_data = [text[:500] for text in batch["review"]]
    # Compute query sentiment score
    pipe_outputs_prompt = jigsaw_reward_model(
        batch["query"],
        **sent_kwargs,
        # truncation=True,
    )
    prompt_scores = [torch.tensor(-output[0]["score"]) for output in pipe_outputs_prompt]
    batch["prompt_score"] = [s.item() for s in prompt_scores]

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = jigsaw_reward_model(
        texts,
        **sent_kwargs,
        truncation=True,
    )
    texts_risk = [q + r for q, r in zip(batch["query"], batch["response_risk"])]
    pipe_outputs_risk = jigsaw_reward_model(
        texts_risk,
        **sent_kwargs,
        truncation=True,
    )
    texts_sft = [q + r for q, r in zip(batch["query"], batch["response_sft"])]
    pipe_outputs_sft = jigsaw_reward_model(
        texts_sft,
        **sent_kwargs,
        truncation=True,
    )
    texts_gpt2 = [q + r for q, r in zip(batch["query"], batch["response_gpt2"])]
    pipe_outputs_gpt2 = jigsaw_reward_model(
        texts_gpt2,
        **sent_kwargs,
        truncation=True,
    )
    scores = [torch.tensor(-output[0]["score"]) for output in pipe_outputs]
    scores_risk = [torch.tensor(-output[0]["score"]) for output in pipe_outputs_risk]
    scores_sft = [torch.tensor(-output[0]["score"]) for output in pipe_outputs_sft]
    scores_gpt2 = [torch.tensor(-output[0]["score"]) for output in pipe_outputs_gpt2]
    batch["R"] = [s.item() for s in scores]
    batch["R_risk"] = [s.item() for s in scores_risk]
    batch["R_sft"] = [s.item() for s in scores_sft]
    batch["R_gpt2"] = [s.item() for s in scores_gpt2]

    # Compute R_bar
    scores = torch.tensor(scores, device=ppo_trainer.current_device)
    scores_risk = torch.tensor(scores_risk, device=ppo_trainer_risk.current_device)

    model_inputs = ppo_trainer.prepare_model_inputs(query_tensors, response_tensors)
    model_inputs_risk = ppo_trainer_risk.prepare_model_inputs(query_tensors, response_tensors_risk)

    model_inputs_names = list(model_inputs.keys())

    with torch.no_grad():
        all_logprobs, _, values, masks = ppo_trainer.batched_forward_pass(
            ppo_trainer.model,
            query_tensors,
            response_tensors,
            model_inputs,
            response_masks=None,
            return_logits=False,
        )
        all_logprobs_risk, _, values_risk, masks_risk = ppo_trainer_risk.batched_forward_pass(
            ppo_trainer_risk.model,
            query_tensors,
            response_tensors_risk,
            model_inputs_risk,
            response_masks=None,
            return_logits=False,
        )
        ref_logprobs, _, _, _ = ppo_trainer.batched_forward_pass(
            ppo_trainer.ref_model, query_tensors, response_tensors, model_inputs, return_logits=None
        )
        ref_logprobs_risk, _, _, _ = ppo_trainer_risk.batched_forward_pass(
            ppo_trainer_risk.ref_model, query_tensors, response_tensors_risk, model_inputs_risk, return_logits=None
        )
        rewards, non_score_reward = ppo_trainer.compute_rewards(scores, all_logprobs, ref_logprobs, masks)
        rewards_risk, non_score_reward_risk = ppo_trainer_risk.compute_rewards(
            scores_risk, all_logprobs_risk, ref_logprobs_risk, masks_risk
        )
    if GAMMA < 1.0:
        for t in range(rewards.shape[1]):
            rewards[:, t] = GAMMA**t * rewards[:, t]
            rewards_risk[:, t] = GAMMA**t * rewards_risk[:, t]

    batch["R_bar"] = rewards.sum(axis=1).tolist()
    batch["R_bar_risk"] = rewards_risk.sum(axis=1).tolist()

    # Compute perplexity
    batch["perplexity"] = perplexity.compute(data=text_data, model_id=model_path)["perplexities"]
    batch["perplexity_risk"] = perplexity.compute(data=text_data, model_id=risk_model_path)["perplexities"]
    batch["perplexity_sft"] = perplexity.compute(data=text_data, model_id=ref_model_path)["perplexities"]
    batch["perplexity_gpt2"] = perplexity.compute(data=text_data, model_id="gpt2", batch_size=64)["perplexities"]
    # pdb.set_trace()
    try:
        df = pd.concat([df, pd.DataFrame(batch)], ignore_index=True)
    except:
        print("Replacing query tensors with None")
        batch["input_ids"] = [None] * len(batch["review"])
        df = pd.concat([df, pd.DataFrame(batch)], ignore_index=True)


print("Saving results")
df.to_csv(f"jigsaw_generations_{prompt_len}_{response_len}_alpha_{alpha_level}_seed_{seed}_truly.csv", index=False)


print("Average RLHF reward:", df["R"].mean())
print("Average RA-RLHF reward:", df["R_risk"].mean())
print("Average SFT reward:", df["R_sft"].mean())
print("Average GPT2 reward:", df["R_gpt2"].mean())

print("Average RLHF perplexity:", df["perplexity"].mean())
print("Average RA-RLHF perplexity:", df["perplexity_risk"].mean())
print("Average SFT perplexity:", df["perplexity_sft"].mean())
print("Average GPT2 perplexity:", df["perplexity_gpt2"].mean())

File Path: examples/Jigsaw/dataset/test_quantile_batched_jigsaw_seed2.py
Content:
from transformers import pipeline, AutoTokenizer
from datasets import Dataset
from dataclasses import dataclass, field
from typing import Optional
from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead
from peft import LoraConfig
import pandas as pd
from tqdm import tqdm
import matplotlib.pyplot as plt
import plotly.express as px
import pdb
import tyro
import torch
from evaluate import load

# load the dataset
jigsaw_df_train = pd.read_csv("jigsaw_train_70_30.csv")
jigsaw_df_test = pd.read_csv("jigsaw_test.csv")

# load reward model
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none"}
jigsaw_reward_model = pipeline("sentiment-analysis", model="unitary/toxic-bert", **sent_kwargs)

# ref_model_path = (
#     "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-20_20-32-35"  # sft_both
# )
ref_model_path = "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-31_18-06-18"  # sft_positive seed 2 70_30
model_path = (
    "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/2024-01-31_19-28-10/save_pretrained"
)
risk_model_path = (
    "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/2024-01-31_22-05-21/save_pretrained"
)

BETA = 0.2
BETA_risk = 0.2
GAMMA = 1.0

prompt_len = 8
response_len = 32
alpha_level = 20
seed = 2


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="gpt2",
            query_dataset=None,
            # reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            init_kl_coef=BETA,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
        )
    )
    ppo_config_risk: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="gpt2",
            query_dataset=None,
            # reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            init_kl_coef=BETA_risk,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
        )
    )
    query_dataset: str = field(default=None, metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})


args = tyro.cli(ScriptArguments)

tokenizer = AutoTokenizer.from_pretrained(ref_model_path)
tokenizer.pad_token = tokenizer.eos_token

ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(
    ref_model_path,
    trust_remote_code=True,
    device_map=None,
    peft_config=None,
)


model = AutoModelForCausalLMWithValueHead.from_pretrained(
    model_path,
    trust_remote_code=True,
    device_map=None,
    peft_config=None,
)

risk_model = AutoModelForCausalLMWithValueHead.from_pretrained(
    risk_model_path,
    trust_remote_code=True,
    device_map=None,
    peft_config=None,
)
# gpt2_model = AutoModelForCausalLMWithValueHead.from_pretrained(
#     "gpt2",
#     trust_remote_code=True,
#     device_map=None,
#     peft_config=None,
# )
# ppo_trainer_gpt2 = PPOTrainer(args.ppo_config, model=gpt2_model, ref_model=ref_model, tokenizer=tokenizer)
ppo_trainer_sft = PPOTrainer(args.ppo_config, model=ref_model, ref_model=ref_model, tokenizer=tokenizer)
ppo_trainer = PPOTrainer(args.ppo_config, model=model, ref_model=ref_model, tokenizer=tokenizer)
ppo_trainer_risk = PPOTrainer(args.ppo_config_risk, model=risk_model, ref_model=ref_model, tokenizer=tokenizer)
perplexity = load("perplexity", module_type="measurement")


def build_dataset():
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"

    df = pd.read_csv(
        "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/jigsaw_test.csv",
    )
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"comment_text": "review"})

    # pdb.set_trace()

    def input_size():
        return prompt_len

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side="left")
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"

generation_kwargs = {
    "min_length": response_len,
    "top_k": 0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": response_len,
}


batch_size = 128
ds = build_dataset()
df = pd.DataFrame()
for i in tqdm(range(0, len(ds), batch_size)):
    batch = ds[i : i + batch_size]
    query_tensors = batch["input_ids"]
    query_tensors = [q.to(ppo_trainer.current_device) for q in query_tensors]
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    response_tensors_risk = ppo_trainer_risk.generate(query_tensors, return_prompt=False, **generation_kwargs)
    response_tensors_sft = ppo_trainer_sft.generate(query_tensors, return_prompt=False, **generation_kwargs)
    # response_tensors_gpt2 = ppo_trainer_gpt2.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)
    batch["response_risk"] = tokenizer.batch_decode(response_tensors_risk)
    batch["response_sft"] = tokenizer.batch_decode(response_tensors_sft)
    # batch["response_gpt2"] = tokenizer.batch_decode(response_tensors_gpt2)

    # pdb.set_trace()
    text_data = [text[:500] for text in batch["review"]]
    # Compute query sentiment score
    pipe_outputs_prompt = jigsaw_reward_model(
        batch["query"],
        **sent_kwargs,
        # truncation=True,
    )
    prompt_scores = [torch.tensor(-output[0]["score"]) for output in pipe_outputs_prompt]
    batch["prompt_score"] = [s.item() for s in prompt_scores]

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = jigsaw_reward_model(
        texts,
        **sent_kwargs,
        truncation=True,
    )
    texts_risk = [q + r for q, r in zip(batch["query"], batch["response_risk"])]
    pipe_outputs_risk = jigsaw_reward_model(
        texts_risk,
        **sent_kwargs,
        truncation=True,
    )
    texts_sft = [q + r for q, r in zip(batch["query"], batch["response_sft"])]
    pipe_outputs_sft = jigsaw_reward_model(
        texts_sft,
        **sent_kwargs,
        truncation=True,
    )
    # texts_gpt2 = [q + r for q, r in zip(batch["query"], batch["response_gpt2"])]
    # pipe_outputs_gpt2 = jigsaw_reward_model(
    #     texts_gpt2,
    #     **sent_kwargs,
    #     truncation=True,
    # )
    scores = [torch.tensor(-output[0]["score"]) for output in pipe_outputs]
    scores_risk = [torch.tensor(-output[0]["score"]) for output in pipe_outputs_risk]
    scores_sft = [torch.tensor(-output[0]["score"]) for output in pipe_outputs_sft]
    # scores_gpt2 = [torch.tensor(-output[0]["score"]) for output in pipe_outputs_gpt2]
    batch["R"] = [s.item() for s in scores]
    batch["R_risk"] = [s.item() for s in scores_risk]
    batch["R_sft"] = [s.item() for s in scores_sft]
    # batch["R_gpt2"] = [s.item() for s in scores_gpt2]

    # Compute R_bar
    scores = torch.tensor(scores, device=ppo_trainer.current_device)
    scores_risk = torch.tensor(scores_risk, device=ppo_trainer_risk.current_device)

    model_inputs = ppo_trainer.prepare_model_inputs(query_tensors, response_tensors)
    model_inputs_risk = ppo_trainer_risk.prepare_model_inputs(query_tensors, response_tensors_risk)

    model_inputs_names = list(model_inputs.keys())

    with torch.no_grad():
        all_logprobs, _, values, masks = ppo_trainer.batched_forward_pass(
            ppo_trainer.model,
            query_tensors,
            response_tensors,
            model_inputs,
            response_masks=None,
            return_logits=False,
        )
        all_logprobs_risk, _, values_risk, masks_risk = ppo_trainer_risk.batched_forward_pass(
            ppo_trainer_risk.model,
            query_tensors,
            response_tensors_risk,
            model_inputs_risk,
            response_masks=None,
            return_logits=False,
        )
        ref_logprobs, _, _, _ = ppo_trainer.batched_forward_pass(
            ppo_trainer.ref_model, query_tensors, response_tensors, model_inputs, return_logits=None
        )
        ref_logprobs_risk, _, _, _ = ppo_trainer_risk.batched_forward_pass(
            ppo_trainer_risk.ref_model, query_tensors, response_tensors_risk, model_inputs_risk, return_logits=None
        )
        rewards, non_score_reward = ppo_trainer.compute_rewards(scores, all_logprobs, ref_logprobs, masks)
        rewards_risk, non_score_reward_risk = ppo_trainer_risk.compute_rewards(
            scores_risk, all_logprobs_risk, ref_logprobs_risk, masks_risk
        )
    if GAMMA < 1.0:
        for t in range(rewards.shape[1]):
            rewards[:, t] = GAMMA**t * rewards[:, t]
            rewards_risk[:, t] = GAMMA**t * rewards_risk[:, t]

    batch["R_bar"] = rewards.sum(axis=1).tolist()
    batch["R_bar_risk"] = rewards_risk.sum(axis=1).tolist()

    # Compute perplexity
    batch["perplexity"] = perplexity.compute(data=text_data, model_id=model_path)["perplexities"]
    batch["perplexity_risk"] = perplexity.compute(data=text_data, model_id=risk_model_path)["perplexities"]
    batch["perplexity_sft"] = perplexity.compute(data=text_data, model_id=ref_model_path)["perplexities"]
    # batch["perplexity_gpt2"] = perplexity.compute(data=text_data, model_id="gpt2", batch_size=64)["perplexities"]
    # pdb.set_trace()
    try:
        df = pd.concat([df, pd.DataFrame(batch)], ignore_index=True)
    except:
        print("Replacing query tensors with None")
        batch["input_ids"] = [None] * len(batch["review"])
        df = pd.concat([df, pd.DataFrame(batch)], ignore_index=True)


print("Saving results")
df.to_csv(f"jigsaw_generations_{prompt_len}_{response_len}_alpha_{alpha_level}_seed_{seed}.csv", index=False)


print("Average RLHF reward:", df["R"].mean())
print("Average RA-RLHF reward:", df["R_risk"].mean())
print("Average SFT reward:", df["R_sft"].mean())
# print("Average GPT2 reward:", df["R_gpt2"].mean())

print("Average RLHF perplexity:", df["perplexity"].mean())
print("Average RA-RLHF perplexity:", df["perplexity_risk"].mean())
print("Average SFT perplexity:", df["perplexity_sft"].mean())
# print("Average GPT2 perplexity:", df["perplexity_gpt2"].mean())

File Path: examples/Jigsaw/dataset/test_quantile_batched_jigsaw_seed36.py
Content:
from transformers import pipeline, AutoTokenizer
from datasets import Dataset
from dataclasses import dataclass, field
from typing import Optional
from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead
from peft import LoraConfig
import pandas as pd
from tqdm import tqdm
import matplotlib.pyplot as plt
import plotly.express as px
import pdb
import tyro
import torch
from evaluate import load

# load the dataset
jigsaw_df_train = pd.read_csv("jigsaw_train_70_30.csv")
jigsaw_df_test = pd.read_csv("jigsaw_test.csv")

# load reward model
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none"}
jigsaw_reward_model = pipeline("sentiment-analysis", model="unitary/toxic-bert", **sent_kwargs)

ref_model_path = "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-31_18-06-18"  # sft_positive seed 2
model_path = (
    "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/2024-02-01_15-44-29/save_pretrained"
)
risk_model_path = (
    "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/2024-02-01_15-44-24/save_pretrained"
)

BETA = 0.2
BETA_risk = 0.2
GAMMA = 1.0

prompt_len = 8
response_len = 32
alpha_level = 20
seed = 4


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="gpt2",
            query_dataset=None,
            # reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            init_kl_coef=BETA,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
        )
    )
    ppo_config_risk: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="gpt2",
            query_dataset=None,
            # reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            init_kl_coef=BETA_risk,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
        )
    )
    query_dataset: str = field(default=None, metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})


args = tyro.cli(ScriptArguments)

tokenizer = AutoTokenizer.from_pretrained(ref_model_path)
tokenizer.pad_token = tokenizer.eos_token

ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(
    ref_model_path,
    trust_remote_code=True,
    device_map=None,
    peft_config=None,
)


model = AutoModelForCausalLMWithValueHead.from_pretrained(
    model_path,
    trust_remote_code=True,
    device_map=None,
    peft_config=None,
)

risk_model = AutoModelForCausalLMWithValueHead.from_pretrained(
    risk_model_path,
    trust_remote_code=True,
    device_map=None,
    peft_config=None,
)
# gpt2_model = AutoModelForCausalLMWithValueHead.from_pretrained(
#     "gpt2",
#     trust_remote_code=True,
#     device_map=None,
#     peft_config=None,
# )
# ppo_trainer_gpt2 = PPOTrainer(args.ppo_config, model=gpt2_model, ref_model=ref_model, tokenizer=tokenizer)
# ppo_trainer_sft = PPOTrainer(args.ppo_config, model=ref_model, ref_model=ref_model, tokenizer=tokenizer)
ppo_trainer = PPOTrainer(args.ppo_config, model=model, ref_model=ref_model, tokenizer=tokenizer)
ppo_trainer_risk = PPOTrainer(args.ppo_config_risk, model=risk_model, ref_model=ref_model, tokenizer=tokenizer)
perplexity = load("perplexity", module_type="measurement")


def build_dataset():
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"

    df = pd.read_csv(
        "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/jigsaw_test.csv",
    )
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"comment_text": "review"})

    # pdb.set_trace()

    def input_size():
        return prompt_len

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side="left")
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"

generation_kwargs = {
    "min_length": response_len,
    "top_k": 0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": response_len,
}


batch_size = 128
ds = build_dataset()
df = pd.DataFrame()
for i in tqdm(range(0, len(ds), batch_size)):
    batch = ds[i : i + batch_size]
    query_tensors = batch["input_ids"]
    query_tensors = [q.to(ppo_trainer.current_device) for q in query_tensors]
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    response_tensors_risk = ppo_trainer_risk.generate(query_tensors, return_prompt=False, **generation_kwargs)
    # response_tensors_sft = ppo_trainer_sft.generate(query_tensors, return_prompt=False, **generation_kwargs)
    # response_tensors_gpt2 = ppo_trainer_gpt2.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)
    batch["response_risk"] = tokenizer.batch_decode(response_tensors_risk)
    # batch["response_sft"] = tokenizer.batch_decode(response_tensors_sft)
    # batch["response_gpt2"] = tokenizer.batch_decode(response_tensors_gpt2)

    # pdb.set_trace()
    text_data = [text[:500] for text in batch["review"]]
    # Compute query sentiment score
    pipe_outputs_prompt = jigsaw_reward_model(
        batch["query"],
        **sent_kwargs,
        # truncation=True,
    )
    prompt_scores = [torch.tensor(-output[0]["score"]) for output in pipe_outputs_prompt]
    batch["prompt_score"] = [s.item() for s in prompt_scores]

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = jigsaw_reward_model(
        texts,
        **sent_kwargs,
        truncation=True,
    )
    texts_risk = [q + r for q, r in zip(batch["query"], batch["response_risk"])]
    pipe_outputs_risk = jigsaw_reward_model(
        texts_risk,
        **sent_kwargs,
        truncation=True,
    )
    # texts_sft = [q + r for q, r in zip(batch["query"], batch["response_sft"])]
    # pipe_outputs_sft = jigsaw_reward_model(
    #     texts_sft,
    #     **sent_kwargs,
    #     truncation=True,
    # )
    # texts_gpt2 = [q + r for q, r in zip(batch["query"], batch["response_gpt2"])]
    # pipe_outputs_gpt2 = jigsaw_reward_model(
    #     texts_gpt2,
    #     **sent_kwargs,
    #     truncation=True,
    # )
    scores = [torch.tensor(-output[0]["score"]) for output in pipe_outputs]
    scores_risk = [torch.tensor(-output[0]["score"]) for output in pipe_outputs_risk]
    # scores_sft = [torch.tensor(-output[0]["score"]) for output in pipe_outputs_sft]
    # scores_gpt2 = [torch.tensor(-output[0]["score"]) for output in pipe_outputs_gpt2]
    batch["R"] = [s.item() for s in scores]
    batch["R_risk"] = [s.item() for s in scores_risk]
    # batch["R_sft"] = [s.item() for s in scores_sft]
    # batch["R_gpt2"] = [s.item() for s in scores_gpt2]

    # Compute R_bar
    scores = torch.tensor(scores, device=ppo_trainer.current_device)
    scores_risk = torch.tensor(scores_risk, device=ppo_trainer_risk.current_device)

    model_inputs = ppo_trainer.prepare_model_inputs(query_tensors, response_tensors)
    model_inputs_risk = ppo_trainer_risk.prepare_model_inputs(query_tensors, response_tensors_risk)

    model_inputs_names = list(model_inputs.keys())

    with torch.no_grad():
        all_logprobs, _, values, masks = ppo_trainer.batched_forward_pass(
            ppo_trainer.model,
            query_tensors,
            response_tensors,
            model_inputs,
            response_masks=None,
            return_logits=False,
        )
        all_logprobs_risk, _, values_risk, masks_risk = ppo_trainer_risk.batched_forward_pass(
            ppo_trainer_risk.model,
            query_tensors,
            response_tensors_risk,
            model_inputs_risk,
            response_masks=None,
            return_logits=False,
        )
        ref_logprobs, _, _, _ = ppo_trainer.batched_forward_pass(
            ppo_trainer.ref_model, query_tensors, response_tensors, model_inputs, return_logits=None
        )
        ref_logprobs_risk, _, _, _ = ppo_trainer_risk.batched_forward_pass(
            ppo_trainer_risk.ref_model, query_tensors, response_tensors_risk, model_inputs_risk, return_logits=None
        )
        rewards, non_score_reward = ppo_trainer.compute_rewards(scores, all_logprobs, ref_logprobs, masks)
        rewards_risk, non_score_reward_risk = ppo_trainer_risk.compute_rewards(
            scores_risk, all_logprobs_risk, ref_logprobs_risk, masks_risk
        )
    if GAMMA < 1.0:
        for t in range(rewards.shape[1]):
            rewards[:, t] = GAMMA**t * rewards[:, t]
            rewards_risk[:, t] = GAMMA**t * rewards_risk[:, t]

    batch["R_bar"] = rewards.sum(axis=1).tolist()
    batch["R_bar_risk"] = rewards_risk.sum(axis=1).tolist()

    # Compute perplexity
    batch["perplexity"] = perplexity.compute(data=text_data, model_id=model_path)["perplexities"]
    batch["perplexity_risk"] = perplexity.compute(data=text_data, model_id=risk_model_path)["perplexities"]
    # batch["perplexity_sft"] = perplexity.compute(data=text_data, model_id=ref_model_path)["perplexities"]
    # batch["perplexity_gpt2"] = perplexity.compute(data=text_data, model_id="gpt2", batch_size=64)["perplexities"]
    # pdb.set_trace()
    try:
        df = pd.concat([df, pd.DataFrame(batch)], ignore_index=True)
    except:
        print("Replacing query tensors with None")
        batch["input_ids"] = [None] * len(batch["review"])
        df = pd.concat([df, pd.DataFrame(batch)], ignore_index=True)


print("Saving results")
df.to_csv(f"jigsaw_generations_{prompt_len}_{response_len}_alpha_{alpha_level}_seed_{seed}.csv", index=False)


print("Average RLHF reward:", df["R"].mean())
print("Average RA-RLHF reward:", df["R_risk"].mean())
# print("Average SFT reward:", df["R_sft"].mean())
# print("Average GPT2 reward:", df["R_gpt2"].mean())

print("Average RLHF perplexity:", df["perplexity"].mean())
print("Average RA-RLHF perplexity:", df["perplexity_risk"].mean())
# print("Average SFT perplexity:", df["perplexity_sft"].mean())
# print("Average GPT2 perplexity:", df["perplexity_gpt2"].mean())

File Path: examples/Jigsaw/dataset/test_quantile_batched_jigsaw_seed36_truly.py
Content:
from transformers import pipeline, AutoTokenizer
from datasets import Dataset
from dataclasses import dataclass, field
from typing import Optional
from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead
from peft import LoraConfig
import pandas as pd
from tqdm import tqdm
import matplotlib.pyplot as plt
import plotly.express as px
import pdb
import tyro
import torch
from evaluate import load

# load the dataset
jigsaw_df_train = pd.read_csv("jigsaw_train.csv")
jigsaw_df_test = pd.read_csv("jigsaw_test.csv")

# load reward model
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none"}
jigsaw_reward_model = pipeline("sentiment-analysis", model="unitary/toxic-bert", **sent_kwargs)

ref_model_path = "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-30_00-24-10"  # sft_positive seed 36
model_path = (
    "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/2024-01-30_03-28-17/save_pretrained"
)
risk_model_path = (
    "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/2024-01-30_03-28-31/save_pretrained"
)

BETA = 0.2
BETA_risk = 0.2
GAMMA = 1.0

prompt_len = 8
response_len = 32
alpha_level = 40
seed = 36


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="gpt2",
            query_dataset=None,
            # reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            init_kl_coef=BETA,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
        )
    )
    ppo_config_risk: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="gpt2",
            query_dataset=None,
            # reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            init_kl_coef=BETA_risk,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
        )
    )
    query_dataset: str = field(default=None, metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})


args = tyro.cli(ScriptArguments)

tokenizer = AutoTokenizer.from_pretrained(ref_model_path)
tokenizer.pad_token = tokenizer.eos_token

ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(
    ref_model_path,
    trust_remote_code=True,
    device_map=None,
    peft_config=None,
)


model = AutoModelForCausalLMWithValueHead.from_pretrained(
    model_path,
    trust_remote_code=True,
    device_map=None,
    peft_config=None,
)

risk_model = AutoModelForCausalLMWithValueHead.from_pretrained(
    risk_model_path,
    trust_remote_code=True,
    device_map=None,
    peft_config=None,
)
gpt2_model = AutoModelForCausalLMWithValueHead.from_pretrained(
    "gpt2",
    trust_remote_code=True,
    device_map=None,
    peft_config=None,
)
ppo_trainer_gpt2 = PPOTrainer(args.ppo_config, model=gpt2_model, ref_model=ref_model, tokenizer=tokenizer)
ppo_trainer_sft = PPOTrainer(args.ppo_config, model=ref_model, ref_model=ref_model, tokenizer=tokenizer)
ppo_trainer = PPOTrainer(args.ppo_config, model=model, ref_model=ref_model, tokenizer=tokenizer)
ppo_trainer_risk = PPOTrainer(args.ppo_config_risk, model=risk_model, ref_model=ref_model, tokenizer=tokenizer)
perplexity = load("perplexity", module_type="measurement")


def build_dataset():
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"

    df = pd.read_csv(
        "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/jigsaw_test.csv",
    )
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"comment_text": "review"})

    # pdb.set_trace()

    def input_size():
        return prompt_len

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side="left")
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"

generation_kwargs = {
    "min_length": response_len,
    "top_k": 0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": response_len,
}


batch_size = 128
ds = build_dataset()
df = pd.DataFrame()
for i in tqdm(range(0, len(ds), batch_size)):
    batch = ds[i : i + batch_size]
    query_tensors = batch["input_ids"]
    query_tensors = [q.to(ppo_trainer.current_device) for q in query_tensors]
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    response_tensors_risk = ppo_trainer_risk.generate(query_tensors, return_prompt=False, **generation_kwargs)
    response_tensors_sft = ppo_trainer_sft.generate(query_tensors, return_prompt=False, **generation_kwargs)
    response_tensors_gpt2 = ppo_trainer_gpt2.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)
    batch["response_risk"] = tokenizer.batch_decode(response_tensors_risk)
    batch["response_sft"] = tokenizer.batch_decode(response_tensors_sft)
    batch["response_gpt2"] = tokenizer.batch_decode(response_tensors_gpt2)

    # pdb.set_trace()
    text_data = [text[:500] for text in batch["review"]]
    # Compute query sentiment score
    pipe_outputs_prompt = jigsaw_reward_model(
        batch["query"],
        **sent_kwargs,
        # truncation=True,
    )
    prompt_scores = [torch.tensor(-output[0]["score"]) for output in pipe_outputs_prompt]
    batch["prompt_score"] = [s.item() for s in prompt_scores]

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = jigsaw_reward_model(
        texts,
        **sent_kwargs,
        truncation=True,
    )
    texts_risk = [q + r for q, r in zip(batch["query"], batch["response_risk"])]
    pipe_outputs_risk = jigsaw_reward_model(
        texts_risk,
        **sent_kwargs,
        truncation=True,
    )
    texts_sft = [q + r for q, r in zip(batch["query"], batch["response_sft"])]
    pipe_outputs_sft = jigsaw_reward_model(
        texts_sft,
        **sent_kwargs,
        truncation=True,
    )
    texts_gpt2 = [q + r for q, r in zip(batch["query"], batch["response_gpt2"])]
    pipe_outputs_gpt2 = jigsaw_reward_model(
        texts_gpt2,
        **sent_kwargs,
        truncation=True,
    )
    scores = [torch.tensor(-output[0]["score"]) for output in pipe_outputs]
    scores_risk = [torch.tensor(-output[0]["score"]) for output in pipe_outputs_risk]
    scores_sft = [torch.tensor(-output[0]["score"]) for output in pipe_outputs_sft]
    scores_gpt2 = [torch.tensor(-output[0]["score"]) for output in pipe_outputs_gpt2]
    batch["R"] = [s.item() for s in scores]
    batch["R_risk"] = [s.item() for s in scores_risk]
    batch["R_sft"] = [s.item() for s in scores_sft]
    batch["R_gpt2"] = [s.item() for s in scores_gpt2]

    # Compute R_bar
    scores = torch.tensor(scores, device=ppo_trainer.current_device)
    scores_risk = torch.tensor(scores_risk, device=ppo_trainer_risk.current_device)

    model_inputs = ppo_trainer.prepare_model_inputs(query_tensors, response_tensors)
    model_inputs_risk = ppo_trainer_risk.prepare_model_inputs(query_tensors, response_tensors_risk)

    model_inputs_names = list(model_inputs.keys())

    with torch.no_grad():
        all_logprobs, _, values, masks = ppo_trainer.batched_forward_pass(
            ppo_trainer.model,
            query_tensors,
            response_tensors,
            model_inputs,
            response_masks=None,
            return_logits=False,
        )
        all_logprobs_risk, _, values_risk, masks_risk = ppo_trainer_risk.batched_forward_pass(
            ppo_trainer_risk.model,
            query_tensors,
            response_tensors_risk,
            model_inputs_risk,
            response_masks=None,
            return_logits=False,
        )
        ref_logprobs, _, _, _ = ppo_trainer.batched_forward_pass(
            ppo_trainer.ref_model, query_tensors, response_tensors, model_inputs, return_logits=None
        )
        ref_logprobs_risk, _, _, _ = ppo_trainer_risk.batched_forward_pass(
            ppo_trainer_risk.ref_model, query_tensors, response_tensors_risk, model_inputs_risk, return_logits=None
        )
        rewards, non_score_reward = ppo_trainer.compute_rewards(scores, all_logprobs, ref_logprobs, masks)
        rewards_risk, non_score_reward_risk = ppo_trainer_risk.compute_rewards(
            scores_risk, all_logprobs_risk, ref_logprobs_risk, masks_risk
        )
    if GAMMA < 1.0:
        for t in range(rewards.shape[1]):
            rewards[:, t] = GAMMA**t * rewards[:, t]
            rewards_risk[:, t] = GAMMA**t * rewards_risk[:, t]

    batch["R_bar"] = rewards.sum(axis=1).tolist()
    batch["R_bar_risk"] = rewards_risk.sum(axis=1).tolist()

    # Compute perplexity
    batch["perplexity"] = perplexity.compute(data=text_data, model_id=model_path)["perplexities"]
    batch["perplexity_risk"] = perplexity.compute(data=text_data, model_id=risk_model_path)["perplexities"]
    batch["perplexity_sft"] = perplexity.compute(data=text_data, model_id=ref_model_path)["perplexities"]
    batch["perplexity_gpt2"] = perplexity.compute(data=text_data, model_id="gpt2", batch_size=64)["perplexities"]
    # pdb.set_trace()
    try:
        df = pd.concat([df, pd.DataFrame(batch)], ignore_index=True)
    except:
        print("Replacing query tensors with None")
        batch["input_ids"] = [None] * len(batch["review"])
        df = pd.concat([df, pd.DataFrame(batch)], ignore_index=True)


print("Saving results")
df.to_csv(f"jigsaw_generations_{prompt_len}_{response_len}_alpha_{alpha_level}_seed_{seed}_truly.csv", index=False)


print("Average RLHF reward:", df["R"].mean())
print("Average RA-RLHF reward:", df["R_risk"].mean())
print("Average SFT reward:", df["R_sft"].mean())
print("Average GPT2 reward:", df["R_gpt2"].mean())

print("Average RLHF perplexity:", df["perplexity"].mean())
print("Average RA-RLHF perplexity:", df["perplexity_risk"].mean())
print("Average SFT perplexity:", df["perplexity_sft"].mean())
print("Average GPT2 perplexity:", df["perplexity_gpt2"].mean())

File Path: examples/Jigsaw/plotting/new_quantile_plot_imdb.py
Content:
import matplotlib.pyplot as plt
import pandas as pd


# read imdb_gen_df
imdb_gen_df = pd.read_csv("imdb_generations_all_seed_12.csv")
imdb_gen_df2 = pd.read_csv("imdb_generations_seed_42.csv")

# Replace columns of imdb_gen_df with imdb_gen_df2

imdb_gen_df.drop(columns=imdb_gen_df2.columns, inplace=True)
imdb_gen_df = pd.concat([imdb_gen_df, imdb_gen_df2], axis=1)

# New quantile plot


# print(imdb_gen_df.columns)
# a = input("Press Enter to continue...")

quantile_resolution = 1

quantile_df = imdb_gen_df.copy()
quantile_df = quantile_df.sort_values(by=["prompt_score"])
quantile_df = quantile_df.reset_index(drop=True)

# Stuff to plot
gpt2_quantiles = []
rlhf_quantiles = []
rlhf_risk_quantiles = []
sft_quantiles = []

for quantile_point in range(1, 101, quantile_resolution):
    # selecct first quantile_point% of the data
    relevant_df = quantile_df.iloc[: int((quantile_point / 100) * quantile_df.shape[0])]

    gpt2_quantiles.append(relevant_df["R_gpt2"].mean())
    rlhf_quantiles.append(relevant_df["R_rlhf"].mean())
    rlhf_risk_quantiles.append(relevant_df["R_risk"].mean())
    sft_quantiles.append(relevant_df["R_sft"].mean())

# Plot
fig, axs = plt.subplots(figsize=(10, 5))

axs.plot(range(1, 101, quantile_resolution), gpt2_quantiles, label="GPT2", color="red")
axs.plot(range(1, 101, quantile_resolution), rlhf_quantiles, label="RLHF", color="blue")
axs.plot(range(1, 101, quantile_resolution), rlhf_risk_quantiles, label="RA-RLHF", color="green")
axs.plot(range(1, 101, quantile_resolution), sft_quantiles, label="SFT", color="orange")

axs.set_xlabel("Quantile (%)", fontsize=16)  # Set label size on the axis object
axs.set_ylabel(f"Average Reward", fontsize=16)
axs.tick_params(axis="both", which="major", labelsize=14)
axs.grid(True)  # Add grid on the axis object
axs.legend(loc="upper left")
axs.set_title(f"Average Reward vs Quantile")

plt.tight_layout()
plt.savefig("new_quantile_plot_imdb.png")
plt.savefig("new_quantile_plot_imdb.pdf")
print("Verify plot saved")
print("GPT2: ", gpt2_quantiles[-1])
print("RLHF: ", rlhf_quantiles[-1])
print("RA-RLHF: ", rlhf_risk_quantiles[-1])
print("SFT: ", sft_quantiles[-1])

File Path: examples/Jigsaw/plotting/new_quantile_plot_jigsaw.py
Content:
import matplotlib.pyplot as plt
import pandas as pd


# read jigsaw_gen_df
jigsaw_gen_df = pd.read_csv("../dataset/jigsaw_generations_8_32_alpha_40_seed_36.csv")


# New quantile plot


# print(jigsaw_gen_df.columns)
# a = input("Press Enter to continue...")

quantile_resolution = 1

quantile_df = jigsaw_gen_df.copy()
quantile_df = quantile_df.sort_values(by=["prompt_score"])
quantile_df = quantile_df.reset_index(drop=True)

# Stuff to plot
gpt2_quantiles = []
rlhf_quantiles = []
rlhf_risk_quantiles = []
sft_quantiles = []

for quantile_point in range(1, 101, quantile_resolution):
    # selecct first quantile_point% of the data
    relevant_df = quantile_df.iloc[: int((quantile_point / 100) * quantile_df.shape[0])]

    gpt2_quantiles.append(relevant_df["R_gpt2"].mean())
    rlhf_quantiles.append(relevant_df["R"].mean())
    rlhf_risk_quantiles.append(relevant_df["R_risk"].mean())
    sft_quantiles.append(relevant_df["R_sft"].mean())

# Plot
fig, axs = plt.subplots(figsize=(10, 5))

axs.plot(range(1, 101, quantile_resolution), gpt2_quantiles, label="GPT2", color="red")
axs.plot(range(1, 101, quantile_resolution), rlhf_quantiles, label="RLHF", color="blue")
axs.plot(range(1, 101, quantile_resolution), rlhf_risk_quantiles, label="RA-RLHF", color="green")
axs.plot(range(1, 101, quantile_resolution), sft_quantiles, label="SFT", color="orange")

axs.set_xlabel("Quantile (%)", fontsize=16)  # Set label size on the axis object
axs.set_ylabel(f"Average Reward", fontsize=16)
axs.tick_params(axis="both", which="major", labelsize=14)
axs.grid(True)  # Add grid on the axis object
axs.legend(loc="upper left")
axs.set_title(f"Average Reward vs Quantile")

plt.tight_layout()
plt.savefig("new_quantile_plot_jigsaw_see36.png")
plt.savefig("new_quantile_plot_jigsaw_see36.pdf")
print("Verify plot saved")
print("GPT2: ", gpt2_quantiles[-1])
print("RLHF: ", rlhf_quantiles[-1])
print("RA-RLHF: ", rlhf_risk_quantiles[-1])
print("SFT: ", sft_quantiles[-1])

File Path: examples/Jigsaw/plotting/pick_best_examples.py
Content:
import pandas as pd

df = pd.read_csv("../dataset/jigsaw_generations_8_32_alpha_20_seed_2.csv")
df = df.filter(["query", "prompt_score", "response", "R", "response_risk", "R_risk"])

# Sort by difference between R and R_risk
df["R_diff"] = df["R_risk"] - df["R"]
df = df.sort_values(by=["R_diff"], ascending=False)

# remove columns where prompt score bigger than R risk or R
df = df[df["prompt_score"] < df["R_risk"]]
df = df[df["prompt_score"] < df["R"]]
df = df[df["prompt_score"] < -2.5]

df.to_csv("examples.csv", index=False)

File Path: examples/Jigsaw/plotting/plot_from_multiple_tb_log.py
Content:
import matplotlib.pyplot as plt
import numpy as np
from tensorboard.backend.event_processing import event_accumulator

import pdb


# Function to extract scalar data from a TensorBoard log file
def extract_scalar_data_from_event_file(event_file_path):
    # Initialize an accumulator
    ea = event_accumulator.EventAccumulator(event_file_path)
    ea.Reload()  # Loads the log data from file

    # Get all scalar tags
    scalar_tags = ea.Tags()["scalars"]

    # Dictionary to hold the data
    scalar_data = {}

    # Extract data for each scalar tag
    for tag in scalar_tags:
        events = ea.Scalars(tag)
        times = [e.wall_time for e in events]
        steps = [e.step for e in events]
        values = [e.value for e in events]
        scalar_data[tag] = {"times": times, "steps": steps, "values": values}

    return scalar_data


# Function for smoothing the curve (simple moving average for illustration)
def smooth_curve(points, factor=0.97):
    smoothed_points = np.zeros_like(points)
    for i in range(len(points)):
        if i > 0:
            smoothed_points[i] = (smoothed_points[i - 1] * factor) + (points[i] * (1 - factor))
        else:
            smoothed_points[i] = points[i]
    return smoothed_points


# Path to your TensorBoard log file or directory
# Assuming `all_data` is a structured dictionary containing all runs, organized by algorithm and then by seed
# e.g., all_data = {'algorithm1': {'seed1': data1, 'seed2': data2, ...},
#                   'algorithm2': {'seed1': data1, 'seed2': data2, ...}, ...}

log_file_paths = {
    "RA-RLHF": [
        "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/2024-01-31_22-05-21/trl",
        "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/2024-02-01_14-44-26/trl",
        "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/2024-01-31_22-52-23/trl",
    ],
    "RLHF": [
        "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/2024-01-31_19-28-10/trl",
        "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/2024-02-01_14-42-40/trl",
        "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/2024-01-31_22-52-37/trl",
    ],
}

# Extract data
all_data = {}
for algorithm, paths in log_file_paths.items():
    all_data[algorithm] = {}
    for i, path in enumerate(paths):
        seed_key = f"seed{i+1}"
        # load data
        all_data[algorithm][seed_key] = extract_scalar_data_from_event_file(path)

# all_data['RLHF']['seed1']['objective/kl'].keys()
# dict_keys(['times', 'steps', 'values'])

desired_tags = {
    "env/reward_mean": "Environment Reward",
    "env/reward_std": "Reward std",
    "objective/entropy": "Policy Entropy",
    "objective/kl": "KL Divergence",
    "objective/kl_coef": "Beta",
    "ppo/returns/mean": "Return over Batch",
    "tokens/responses_len_mean": "Response Length",
}

# Step 1: Calculate mean and standard deviation for each tag for each algorithm
tag_stats_by_algorithm = {}
for algorithm, seeds_data in all_data.items():
    tag_grouped_data = {}
    for seed, data in seeds_data.items():
        print(seed)
        for tag, values in data.items():
            # pdb.set_trace()
            if tag in desired_tags.keys():
                print(tag)
                if tag not in tag_grouped_data:
                    # pdb.set_trace()
                    print("here")
                    tag_grouped_data[tag] = []
                tag_grouped_data[tag].append(values)

    # pdb.set_trace()
    tag_stats = {}
    for tag, values_list in tag_grouped_data.items():
        # Assuming all seed runs have the same number of steps and are aligned
        steps = values_list[0]["steps"]
        all_values = np.array([values["values"] for values in values_list])
        mean_values = np.mean(all_values, axis=0)
        std_values = np.std(all_values, axis=0)
        tag_stats[tag] = {"steps": steps, "mean": mean_values, "std": std_values}

    tag_stats_by_algorithm[algorithm] = tag_stats


# Step 2: Plot mean and standard deviation for each tag for each algorithm
for tag in tag_grouped_data.keys():
    fig, ax = plt.subplots(figsize=(5, 5))  # Create a figure and an axis object

    for algorithm, stats in tag_stats_by_algorithm.items():
        if tag in stats:
            print(tag)
            steps = stats[tag]["steps"]
            mean_values = smooth_curve(stats[tag]["mean"])
            std_values = smooth_curve(stats[tag]["std"])

            if algorithm == "RLHF":
                ax.plot(steps, mean_values, label=f"{algorithm}", color="red")
                ax.fill_between(
                    steps, mean_values - std_values, mean_values + std_values, color="red", alpha=0.2
                )  # , label=f'{algorithm} std dev')
            else:
                ax.plot(steps, mean_values, label=f"{algorithm}", color="green")
                ax.fill_between(
                    steps, mean_values - std_values, mean_values + std_values, color="green", alpha=0.2
                )  # , label=f'{algorithm} std dev')
            ax.set_xlabel("Training iteration", fontsize=24)  # Set label size on the axis object
            ax.set_ylabel(f"{desired_tags[tag]}", fontsize=24)  # Set label size on the axis object
            ax.tick_params(axis="both", which="major", labelsize=20)  # Adjust tick size on the axis object
            # ax.set_box_aspect(0.7)  # Commented out because this might not be available depending on your matplotlib version
            ax.grid(True)  # Add grid on the axis object
            ax.legend(loc="upper left", fontsize=16)  # Add legend on the axis object
            # plt.title(f'Mean and Standard Deviation for {tag} across Algorithms')
            plt.savefig(
                f"train_plots_jigsaw/{desired_tags[tag].replace(' ','_')}.pdf", bbox_inches="tight", pad_inches=0
            )  # as png

File Path: examples/Jigsaw/plotting/test_plotting_2.py
Content:
import matplotlib.pyplot as plt

# import plotly.express as px
import pandas as pd
import numpy as np
import pdb

# IMDB
imdb_df_test = pd.read_csv("jigsaw_generations_35_32.csv")

# tail_to_save = -2.8
# tail_df = imdb_df_test[imdb_df_test['prompt_score'] <= tail_to_save]
# tail_df = tail_df.drop(columns=['input_ids'])
# print(tail_df['R'].mean())
# print(tail_df['R_risk'].mean())
# tail_df.to_csv("tail/imdb_tail.csv", index=False)
# pdb.set_trace()

# Rename as needed in plots
# imdb_df_test = imdb_df_test.rename(columns={"R": "RLHF", "R_risk": "RLHF risk averse"})
# imdb_df_test = imdb_df_test.rename(columns={"R_bar": "RLHF returns", "R_bar_risk": "RLHF risk averse returns"})


# # CDF plots
# fig = px.ecdf(
#     imdb_df_test,
#     x=["RLHF", "RLHF risk averse"],
#     labels={
#         "value": "Reward",
#         "variable": "Model",
#     },
# )
# fig.update_layout(
#     title="Reward distribution for RLHF and RLHF risk averse models",
#     xaxis_title="Reward",
#     yaxis_title="CDF",
# )
# print("Average reward for RLHF: ", imdb_df_test["RLHF"].mean())
# print("Average reward for RLHF risk averse: ", imdb_df_test["RLHF risk averse"].mean())
# fig.write_image("imdb_R_cdf.png")

# fig = px.ecdf(
#     imdb_df_test,
#     x=["RLHF returns", "RLHF risk averse returns"],
#     labels={
#         "value": "Reward",
#         "variable": "Model",
#     },
# )
# fig.update_layout(
#     title="Returns distribution for RLHF and RLHF risk averse models",
#     xaxis_title="Returns",
#     yaxis_title="CDF",
# )
# print("Average return for RLHF: ", imdb_df_test["RLHF returns"].mean())
# print("Average return for RLHF risk averse: ", imdb_df_test["RLHF risk averse returns"].mean())
# fig.write_image("imdb_R_bar_cdf.png")

# pdb.set_trace()
# Histogram shift plots for imdb


# Function to plot only the top edges of a histogram
def plot_hist_top_edges(data, ax, bins, color, label):
    hist, bin_edges = np.histogram(data, bins=bins)
    for left, right, height in zip(bin_edges[:-1], bin_edges[1:], hist):
        ax.plot([left, right], [height, height], color=color, label=label)
        label = None  # To avoid duplicate labels in the legend


fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(30, 6))
word_map = {"prompt_score": "Prompt Score", "R": "RLHF", "R_risk": "RA-RLHF"}
for i, col in enumerate(["prompt_score", "R", "R_risk"]):
    imdb_df_test[imdb_df_test["toxic"] == 1][col].plot.hist(ax=axs[i], bins=30, alpha=0.5, label="Toxic")
    imdb_df_test[imdb_df_test["toxic"] == 0][col].plot.hist(ax=axs[i], bins=30, alpha=0.5, label="Non Toxic")

    axs[i].set_title(f"{word_map[col]}", fontsize=30)
    axs[i].set_xlabel("Reward", fontsize=30)  # Set label size on the axis object
    axs[i].set_ylabel("")
    axs[i].tick_params(axis="both", which="major", labelsize=28)
    axs[i].grid(True)  # Add grid on the axis object
    axs[i].legend(loc="upper left", fontsize=24)  # Add legend on the axis objec


# Find the maximum y-value among the first three histograms
max_count = max(axs[0].get_ylim()[1], axs[1].get_ylim()[1], axs[2].get_ylim()[1])

# Set the y-axis limits for the first three subplots to be the same
axs[0].set_ylim(0, max_count)
axs[1].set_ylim(0, max_count)
axs[2].set_ylim(0, max_count)
axs[0].set_ylabel(f"Count", fontsize=30)

# tail prompts
tail_filter_level = -6.0
filtered_df = imdb_df_test[imdb_df_test["prompt_score"] <= tail_filter_level]
print(filtered_df.shape[0])
filtered_df[filtered_df["toxic"] == 0]["prompt_score"].plot.hist(ax=axs[3], bins=30, alpha=0.5, label="Non Toxic")
filtered_df[filtered_df["toxic"] == 1]["prompt_score"].plot.hist(ax=axs[3], bins=30, alpha=0.5, label="Toxic")
axs[3].set_title("Tail Prompts", fontsize=30)
axs[3].set_xlabel("Reward", fontsize=30)  # Set label size on the axis object
axs[3].set_ylabel("")
axs[3].tick_params(axis="both", which="major", labelsize=28)
axs[3].grid(True)  # Add grid on the axis object
axs[3].legend(loc="upper left", fontsize=24)  # Add legend on the axis objec
axs[3].axvline(x=tail_filter_level, color="red", linestyle="-", linewidth=4)

# tail performance
filtered_df["R"].plot.hist(ax=axs[4], bins=30, alpha=0.3, label="RLHF", color="red")
filtered_df["R_risk"].plot.hist(ax=axs[4], bins=30, alpha=0.5, label="RA-RLHF", color="olive")
axs[4].set_title("Tail Performance", fontsize=30)
axs[4].set_xlabel("Reward", fontsize=30)  # Set label size on the axis object
axs[4].set_ylabel("")
axs[4].tick_params(axis="both", which="major", labelsize=28)
axs[4].grid(True)  # Add grid on the axis object
axs[4].legend(loc="upper right", fontsize=24)  # Add legend on the axis objec
axs[4].axvline(x=tail_filter_level, color="red", linestyle="-", linewidth=4)

# Find the maximum y-value among the first three histograms
max_count_2 = max(axs[3].get_ylim()[1], axs[4].get_ylim()[1])

# Set the y-axis limits for the first three subplots to be the same
axs[3].set_ylim(0, max_count_2)
axs[4].set_ylim(0, max_count_2)

fig.tight_layout()
plt.savefig("./test_plots_imdb/imdb_distribution_shifts.png")
plt.savefig("./test_plots_imdb/imdb_distribution_shifts.pdf")
plt.clf()

# Quantile plots of returns for imdb

fig, axs = plt.subplots(figsize=(10, 5))

word_map = {"R_bar": "RLHF returns", "R_bar_risk": "RA-RLHF returns"}

for i, col in enumerate(["R_bar", "R_bar_risk"]):
    x = np.linspace(0, 1, len(imdb_df_test[col])) * 100
    y = imdb_df_test[col].sort_values().values
    axs.plot(x, y, label=word_map[col])

axs.set_xlabel("Quantile (%)", fontsize=16)  # Set label size on the axis object
axs.set_ylabel(f"Returns", fontsize=16)
axs.tick_params(axis="both", which="major", labelsize=14)
axs.grid(True)  # Add grid on the axis object
axs.legend(loc="upper left")
axs.set_title(f"Quantile plot of returns")


fig.tight_layout()
plt.savefig("./test_plots_imdb/imdb_R_bar_quantiles.png")
plt.savefig("./test_plots_imdb/imdb_R_bar_quantiles.pdf")
plt.clf()


# Quantile plots of rewards for imdb

fig, axs = plt.subplots(figsize=(10, 5))

word_map = {"R": "RLHF", "R_risk": "RA-RLHF"}

for i, col in enumerate(["R", "R_risk"]):
    x = np.linspace(0, 1, len(imdb_df_test[col])) * 100
    y = imdb_df_test[col].sort_values().values
    axs.plot(x, y, label=word_map[col])

axs.set_xlabel("Quantile (%)", fontsize=16)  # Set label size on the axis object
axs.set_ylabel(f"Rewards", fontsize=16)
axs.tick_params(axis="both", which="major", labelsize=14)
axs.grid(True)  # Add grid on the axis object
axs.legend(loc="upper left")
axs.set_title(f"Quantile plot of rewards")


fig.tight_layout()
plt.savefig("./test_plots_imdb/imdb_R_quantiles.png")
plt.savefig("./test_plots_imdb/imdb_R_quantiles.pdf")
plt.clf()

# # Box plots of returns for imdb

# fig, axs = plt.subplots(figsize=(10, 5))


# # # Jigsaw
# jigsaw_df_test = pd.read_csv("jigsaw_generations.csv")

# # Rename as needed in plots
# # jigsaw_df_test = jigsaw_df_test.rename(columns={"R": "RLHF", "R_risk": "RLHF risk averse"})
# # jigsaw_df_test = jigsaw_df_test.rename(columns={"R_bar": "RLHF returns", "R_bar_risk": "RLHF risk averse returns"})

# # # CDF plots
# # fig = px.ecdf(
# #     jigsaw_df_test,
# #     x=["RLHF", "RLHF risk averse"],
# #     labels={
# #         "value": "Reward",
# #         "variable": "Model",
# #     },
# # )
# # fig.update_layout(
# #     title="Reward distribution for RLHF and RLHF risk averse models",
# #     xaxis_title="Reward",
# #     yaxis_title="CDF",
# # )
# # print("Average reward for RLHF: ", jigsaw_df_test["RLHF"].mean())
# # print("Average reward for RLHF risk averse: ", jigsaw_df_test["RLHF risk averse"].mean())
# # fig.write_image("jigsaw_R_cdf.png")

# # fig = px.ecdf(
# #     jigsaw_df_test,
# #     x=["RLHF returns", "RLHF risk averse returns"],
# #     labels={
# #         "value": "Reward",
# #         "variable": "Model",
# #     },
# # )
# # fig.update_layout(
# #     title="Returns distribution for RLHF and RLHF risk averse models",
# #     xaxis_title="Returns",
# #     yaxis_title="CDF",
# # )
# # print("Average return for RLHF: ", jigsaw_df_test["RLHF returns"].mean())
# # print("Average return for RLHF risk averse: ", jigsaw_df_test["RLHF risk averse returns"].mean())
# # fig.write_image("jigsaw_R_bar_cdf.png")

# # Plot histogram shifts for jigsaw
# fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(10, 5))
# word_map = {"prompt_score": "Prompt", "R": "RLHF", "R_risk": "RA-RLHF"}
# for i, col in enumerate(["prompt_score", "R", "R_risk"]):
#     jigsaw_df_test[jigsaw_df_test["toxic"] == 0][col].plot.hist(ax=axs[i], bins=30, alpha=0.5, label="Non Toxic")
#     jigsaw_df_test[jigsaw_df_test["toxic"] == 1][col].plot.hist(ax=axs[i], bins=30, alpha=0.5, label="Toxic")

#     axs[i].set_xlabel("Reward", fontsize=16)  # Set label size on the axis object
#     axs[i].set_ylabel(f"Count", fontsize=16)
#     axs[i].tick_params(axis="both", which="major", labelsize=14)
#     axs[i].grid(True)  # Add grid on the axis object
#     axs[i].legend(loc="upper left")  # Add legend on the axis objec
#     axs[i].set_title(f"{word_map[col]}")


# fig.tight_layout()
# plt.savefig("./test_plots_jigsaw/jigsaw_distribution_shifts.png")
# plt.savefig("./test_plots_jigsaw/jigsaw_distribution_shifts.pdf")
# plt.clf()

# # Quantile plots of returns for jigsaw

# fig, axs = plt.subplots(figsize=(10, 5))
# word_map = {"R_bar": "RLHF returns", "R_bar_risk": "RA-RLHF returns"}

# for i, col in enumerate(["R_bar", "R_bar_risk"]):
#     x = np.linspace(0, 1, len(jigsaw_df_test[col])) * 100
#     y = jigsaw_df_test[col].sort_values().values
#     axs.plot(x, y, label=word_map[col])

# axs.set_xlabel("Quantile (%)", fontsize=16)  # Set label size on the axis object
# axs.set_ylabel(f"Returns", fontsize=16)
# axs.tick_params(axis="both", which="major", labelsize=14)
# axs.grid(True)  # Add grid on the axis object
# axs.legend(loc="upper left")
# axs.set_title(f"Quantile plot of returns")


# fig.tight_layout()
# plt.savefig("./test_plots_jigsaw/jigsaw_R_bar_quantiles.png")
# plt.savefig("./test_plots_jigsaw/jigsaw_R_bar_quantiles.pdf")
# plt.clf()

# # Quantile plots of rewards for jigsaw

# fig, axs = plt.subplots(figsize=(10, 5))
# word_map = {"R": "RLHF", "R_risk": "RA-RLHF"}
# for i, col in enumerate(["R", "R_risk"]):
#     x = np.linspace(0, 1, len(jigsaw_df_test[col])) * 100
#     y = jigsaw_df_test[col].sort_values().values
#     axs.plot(x, y, label=word_map[col])

# axs.set_xlabel("Quantile (%)", fontsize=16)  # Set label size on the axis object
# axs.set_ylabel(f"Rewards", fontsize=16)
# axs.tick_params(axis="both", which="major", labelsize=14)
# axs.grid(True)  # Add grid on the axis object
# axs.legend(loc="upper left")
# axs.set_title(f"Quantile plot of rewards")


# fig.tight_layout()
# plt.savefig("./test_plots_jigsaw/jigsaw_R_quantiles.png")
# plt.savefig("./test_plots_jigsaw/jigsaw_R_quantiles.pdf")
# plt.clf()

File Path: examples/Jigsaw/plotting/test_plotting_new.py
Content:
import matplotlib.pyplot as plt
import plotly.express as px
import pandas as pd
import numpy as np

# # IMDB
# jigsaw_df_test = pd.read_csv("imdb_generations.csv")

# # Rename as needed in plots
# # jigsaw_df_test = jigsaw_df_test.rename(columns={"R": "RLHF", "R_risk": "RLHF risk averse"})
# # jigsaw_df_test = jigsaw_df_test.rename(columns={"R_bar": "RLHF returns", "R_bar_risk": "RLHF risk averse returns"})


# # # CDF plots
# # fig = px.ecdf(
# #     jigsaw_df_test,
# #     x=["RLHF", "RLHF risk averse"],
# #     labels={
# #         "value": "Reward",
# #         "variable": "Model",
# #     },
# # )
# # fig.update_layout(
# #     title="Reward distribution for RLHF and RLHF risk averse models",
# #     xaxis_title="Reward",
# #     yaxis_title="CDF",
# # )
# # print("Average reward for RLHF: ", jigsaw_df_test["RLHF"].mean())
# # print("Average reward for RLHF risk averse: ", jigsaw_df_test["RLHF risk averse"].mean())
# # fig.write_image("imdb_R_cdf.png")

# # fig = px.ecdf(
# #     jigsaw_df_test,
# #     x=["RLHF returns", "RLHF risk averse returns"],
# #     labels={
# #         "value": "Reward",
# #         "variable": "Model",
# #     },
# # )
# # fig.update_layout(
# #     title="Returns distribution for RLHF and RLHF risk averse models",
# #     xaxis_title="Returns",
# #     yaxis_title="CDF",
# # )
# # print("Average return for RLHF: ", jigsaw_df_test["RLHF returns"].mean())
# # print("Average return for RLHF risk averse: ", jigsaw_df_test["RLHF risk averse returns"].mean())
# # fig.write_image("imdb_R_bar_cdf.png")


# # Histogram shift plots for imdb
# fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(10, 5))
# word_map = {"prompt_score": "Prompt", "R": "RLHF", "R_risk": "RA-RLHF"}
# for i, col in enumerate(["prompt_score", "R", "R_risk"]):
#     jigsaw_df_test[jigsaw_df_test["label"] == 0][col].plot.hist(ax=axs[i], bins=30, alpha=0.5, label="Negative")
#     jigsaw_df_test[jigsaw_df_test["label"] == 1][col].plot.hist(ax=axs[i], bins=30, alpha=0.5, label="Positive")

#     axs[i].set_title(f"{word_map[col]}")
#     axs[i].set_xlabel("Reward", fontsize=16)  # Set label size on the axis object
#     axs[i].set_ylabel(f"Count", fontsize=16)
#     axs[i].tick_params(axis="both", which="major", labelsize=14)
#     axs[i].grid(True)  # Add grid on the axis object
#     axs[i].legend(loc="upper left")  # Add legend on the axis objec

# fig.tight_layout()
# plt.savefig("./test_plots_imdb/imdb_distribution_shifts.png")
# plt.savefig("./test_plots_imdb/imdb_distribution_shifts.pdf")
# plt.clf()

# # Quantile plots of returns for imdb

# fig, axs = plt.subplots(figsize=(10, 5))

# word_map = {"R_bar": "RLHF returns", "R_bar_risk": "RA-RLHF returns"}

# for i, col in enumerate(["R_bar", "R_bar_risk"]):
#     x = np.linspace(0, 1, len(jigsaw_df_test[col])) * 100
#     y = jigsaw_df_test[col].sort_values().values
#     axs.plot(x, y, label=word_map[col])

# axs.set_xlabel("Quantile (%)", fontsize=16)  # Set label size on the axis object
# axs.set_ylabel(f"Returns", fontsize=16)
# axs.tick_params(axis="both", which="major", labelsize=14)
# axs.grid(True)  # Add grid on the axis object
# axs.legend(loc="upper left")
# axs.set_title(f"Quantile plot of returns")


# fig.tight_layout()
# plt.savefig("./test_plots_imdb/imdb_R_bar_quantiles.png")
# plt.savefig("./test_plots_imdb/imdb_R_bar_quantiles.pdf")
# plt.clf()


# # Quantile plots of rewards for imdb

# fig, axs = plt.subplots(figsize=(10, 5))

# word_map = {"R": "RLHF", "R_risk": "RA-RLHF"}

# for i, col in enumerate(["R", "R_risk"]):
#     x = np.linspace(0, 1, len(jigsaw_df_test[col])) * 100
#     y = jigsaw_df_test[col].sort_values().values
#     axs.plot(x, y, label=word_map[col])

# axs.set_xlabel("Quantile (%)", fontsize=16)  # Set label size on the axis object
# axs.set_ylabel(f"Rewards", fontsize=16)
# axs.tick_params(axis="both", which="major", labelsize=14)
# axs.grid(True)  # Add grid on the axis object
# axs.legend(loc="upper left")
# axs.set_title(f"Quantile plot of rewards")


# fig.tight_layout()
# plt.savefig("./test_plots_imdb/imdb_R_quantiles.png")
# plt.savefig("./test_plots_imdb/imdb_R_quantiles.pdf")
# plt.clf()

# # Box plots of returns for imdb

# fig, axs = plt.subplots(figsize=(10, 5))


# # Jigsaw
jigsaw_df_test = pd.read_csv(
    "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw_2/dataset/jigsaw_generations_8_32_alpha_20_seed_1_gptj_full_correct.csv"
)
# jigsaw_df_test_2 = pd.read_csv("../dataset/jigsaw_generations_8_32_alpha_40_seed_2.csv")

# jigsaw_df_test_2.drop(columns=jigsaw_df_test.columns, inplace=True)
# jigsaw_df_test = pd.concat([jigsaw_df_test, jigsaw_df_test_2], axis=1)


# Rename as needed in plots
# jigsaw_df_test = jigsaw_df_test.rename(columns={"R": "RLHF", "R_risk": "RLHF risk averse"})
# jigsaw_df_test = jigsaw_df_test.rename(columns={"R_bar": "RLHF returns", "R_bar_risk": "RLHF risk averse returns"})

# # CDF plots
# fig = px.ecdf(
#     jigsaw_df_test,
#     x=["RLHF", "RLHF risk averse"],
#     labels={
#         "value": "Reward",
#         "variable": "Model",
#     },
# )
# fig.update_layout(
#     title="Reward distribution for RLHF and RLHF risk averse models",
#     xaxis_title="Reward",
#     yaxis_title="CDF",
# )
# print("Average reward for RLHF: ", jigsaw_df_test["RLHF"].mean())
# print("Average reward for RLHF risk averse: ", jigsaw_df_test["RLHF risk averse"].mean())
# fig.write_image("jigsaw_R_cdf.png")

# fig = px.ecdf(
#     jigsaw_df_test,
#     x=["RLHF returns", "RLHF risk averse returns"],
#     labels={
#         "value": "Reward",
#         "variable": "Model",
#     },
# )
# fig.update_layout(
#     title="Returns distribution for RLHF and RLHF risk averse models",
#     xaxis_title="Returns",
#     yaxis_title="CDF",
# )
# print("Average return for RLHF: ", jigsaw_df_test["RLHF returns"].mean())
# print("Average return for RLHF risk averse: ", jigsaw_df_test["RLHF risk averse returns"].mean())
# fig.write_image("jigsaw_R_bar_cdf.png")

# tail_to_save = -3

# tail_df = jigsaw_df_test[jigsaw_df_test["prompt_score"] <= tail_to_save]

# tail_df = tail_df.drop(columns=["input_ids"])

# print(tail_df["R"].mean())

# print(tail_df["R_risk"].mean())

zoom_border_color = "limegreen"


# Plot histogram shifts for jigsaw
def plot_hist_top_edges(data, ax, bins, color, label):
    hist, bin_edges = np.histogram(data, bins=bins)

    for left, right, height in zip(bin_edges[:-1], bin_edges[1:], hist):
        ax.plot([left, right], [height, height], color=color, label=label)

        label = None  # To avoid duplicate labels in the legend


fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(30, 6))

word_map = {"prompt_score": "Prompt Score", "R": "RLHF", "R_risk": "RA-RLHF", "R_sft": "SFT"}

for i, col in enumerate(["prompt_score", "R_sft", "R", "R_risk"]):
    jigsaw_df_test[jigsaw_df_test["toxic"] == 0][col].plot.hist(ax=axs[i], bins=30, alpha=0.5, label="Non Toxic")
    jigsaw_df_test[jigsaw_df_test["toxic"] == 1][col].plot.hist(ax=axs[i], bins=30, alpha=0.5, label="Toxic")

    axs[i].set_title(f"{word_map[col]}", fontsize=30)

    axs[i].set_xlabel("Reward", fontsize=30)  # Set label size on the axis object

    axs[i].set_ylabel("")

    axs[i].tick_params(axis="both", which="major", labelsize=28)

    axs[i].grid(True)  # Add grid on the axis object

    axs[i].legend(loc="upper left", fontsize=24)  # Add legend on the axis objec

    # zoomed in bit
    left_lim = -7
    right_lim = 5
    axs_zoom = axs[i].inset_axes([0.15, 0.20, 0.65, 0.40])  # left, bottom, width, height

    axs_zoom.hist(
        jigsaw_df_test[jigsaw_df_test["toxic"] == 0][col],
        bins=30,
        range=[left_lim, right_lim],
        alpha=0.5,
        label="Non Toxic",
    )
    axs_zoom.hist(
        jigsaw_df_test[jigsaw_df_test["toxic"] == 1][col],
        bins=30,
        range=[left_lim, right_lim],
        alpha=0.5,
        label="Toxic",
    )
    axs_zoom.set_ylim(0, 150)
    axs_zoom.set_xlim(left_lim, right_lim)
    axs_zoom.xaxis.set_major_locator(plt.MaxNLocator(3))
    axs_zoom.yaxis.set_major_locator(plt.MaxNLocator(3))
    axs_zoom.tick_params(axis="both", which="major", labelsize=20)
    # set border color to orange
    axs_zoom.spines["bottom"].set_color(zoom_border_color)
    axs_zoom.spines["top"].set_color(zoom_border_color)
    axs_zoom.spines["left"].set_color(zoom_border_color)
    axs_zoom.spines["right"].set_color(zoom_border_color)
    # set border width to zoom_thickness
    zoom_thickness = 3.0
    axs_zoom.spines["bottom"].set_linewidth(zoom_thickness)
    axs_zoom.spines["top"].set_linewidth(zoom_thickness)
    axs_zoom.spines["left"].set_linewidth(zoom_thickness)
    axs_zoom.spines["right"].set_linewidth(zoom_thickness)
    # rectangle coords in zoomed in plot
    rect_height = 300
    rectangle = plt.Rectangle(
        (left_lim, 0),
        right_lim - left_lim,
        rect_height,
        facecolor="none",
        edgecolor=zoom_border_color,
        lw=zoom_thickness,
    )
    axs[i].add_patch(rectangle)


# Find the maximum y-value among the first three histograms

max_count = max(axs[0].get_ylim()[1], axs[1].get_ylim()[1], axs[2].get_ylim()[1])


# Set the y-axis limits for the first three subplots to be the same
axs[0].set_ylim(0, max_count)
axs[1].set_ylim(0, max_count)
axs[2].set_ylim(0, max_count)
axs[3].set_ylim(0, max_count)
axs[0].set_ylabel(f"Count", fontsize=30)

# make quantile plot
# imdb_gen_df = imdb_df_test2
# imdb_gen_df2 = imdb_df_test
# imdb_gen_df.drop(columns=imdb_gen_df2.columns, inplace=True)
# imdb_gen_df = pd.concat([imdb_gen_df, imdb_gen_df2], axis=1)

quantile_resolution = 1

quantile_df = jigsaw_df_test.copy()
quantile_df = quantile_df.sort_values(by=["prompt_score"])
quantile_df = quantile_df.reset_index(drop=True)

# Stuff to plot
gpt2_quantiles = []
gptj_quantiles = []
rlhf_quantiles = []
rlhf_risk_quantiles = []
sft_quantiles = []

for quantile_point in range(1, 101, quantile_resolution):
    # select first quantile_point% of the data
    relevant_df = quantile_df.iloc[: int((quantile_point / 100) * quantile_df.shape[0])]

    gpt2_quantiles.append(relevant_df["R_gpt2"].mean())
    gptj_quantiles.append(relevant_df["R_gptj"].mean())
    rlhf_quantiles.append(relevant_df["R"].mean())
    rlhf_risk_quantiles.append(relevant_df["R_risk"].mean())
    sft_quantiles.append(relevant_df["R_sft"].mean())

axs[4].plot(range(1, 101, quantile_resolution), gpt2_quantiles, label="GPT2", color="black")
axs[4].plot(range(1, 101, quantile_resolution), gptj_quantiles, label="GPTJ", color="pink")
axs[4].plot(range(1, 101, quantile_resolution), sft_quantiles, label="SFT", color="blue")
axs[4].plot(range(1, 101, quantile_resolution), rlhf_quantiles, label="RLHF", color="red")
axs[4].plot(
    range(1, 101, quantile_resolution),
    rlhf_risk_quantiles,
    label="RA-RLHF",
    color="green",
)


axs[4].set_xlabel("Quantile (%)", fontsize=30)  # Set label size on the axis object
axs[4].set_ylabel(f"Average Reward", fontsize=30)
# axs[4].set_yscale("log")
axs[4].tick_params(axis="both", which="major", labelsize=28)
axs[4].grid(True)  # Add grid on the axis object
axs[4].legend(loc="lower right", fontsize=16)
axs[4].set_title(f"Reward vs Quantile", fontsize=30)


# Zoomed in bit for the quantile plot
axs_zoom = axs[4].inset_axes([0.65, 0.42, 0.30, 0.30])
axs_zoom.plot(range(1, 101, quantile_resolution), gpt2_quantiles, label="GPT2", color="black")
axs_zoom.plot(range(1, 101, quantile_resolution), gptj_quantiles, label="GPTJ", color="pink")
axs_zoom.plot(range(1, 101, quantile_resolution), sft_quantiles, label="SFT", color="blue")
axs_zoom.plot(range(1, 101, quantile_resolution), rlhf_quantiles, label="RLHF", color="red")
axs_zoom.plot(range(1, 101, quantile_resolution), rlhf_risk_quantiles, label="RA-RLHF", color="green")
# increase tick font size
axs_zoom.tick_params(axis="both", which="major", labelsize=18)
# set border color to orange
axs_zoom.spines["bottom"].set_color(zoom_border_color)
axs_zoom.spines["top"].set_color(zoom_border_color)
axs_zoom.spines["left"].set_color(zoom_border_color)
axs_zoom.spines["right"].set_color(zoom_border_color)
# set border width to zoom_thickness
zoom_thickness = 3.0
axs_zoom.spines["bottom"].set_linewidth(zoom_thickness)
axs_zoom.spines["top"].set_linewidth(zoom_thickness)
axs_zoom.spines["left"].set_linewidth(zoom_thickness)
axs_zoom.spines["right"].set_linewidth(zoom_thickness)


# rectangle coords in zoomed in plot
x1, x2, y1, y2 = 0.3, 6.5, -6.15, -5.0
axs_zoom.set_xlim(x1, x2)
axs_zoom.set_ylim(y1, y2)
rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, facecolor="none", edgecolor=zoom_border_color, lw=zoom_thickness)
axs[4].add_patch(rect)

fig.tight_layout()
plt.savefig("./test_plots_jigsaw_2/jigsaw_distribution_shifts.png")
plt.savefig("./test_plots_jigsaw_2/jigsaw_distribution_shifts.pdf")
plt.clf()


print("\n\n")
print("Average reward for RLHF: ", jigsaw_df_test["R"].mean())
print("Average reward for RA-RLHF: ", jigsaw_df_test["R_risk"].mean())
print("Average reward for SFT: ", jigsaw_df_test["R_sft"].mean())
print("Average reward for GPT2:", jigsaw_df_test["R_gpt2"].mean())
print("Average reward for GPTJ:", jigsaw_df_test["R_gptj"].mean())
# print("Average Perplexity for RLHF: ", jigsaw_df_test["perplexity"].mean())
# print("Average Perplexity for RA-RLHF: ", jigsaw_df_test["perplexity_risk"].mean())
# print("Average Perplexity for SFT: ", jigsaw_df_test["perplexity_sft"].mean())
# print("Average Perplexity for GPT2: ", jigsaw_df_test["perplexity_gpt2"].mean())
print("\n\n")
filtered_df = jigsaw_df_test[jigsaw_df_test["prompt_score"] < 5]
print("Tail Average reward for RLHF: ", filtered_df["R"].mean())
print("Tail Average reward for RA-RLHF: ", filtered_df["R_risk"].mean())
print("Tail Average reward for SFT: ", filtered_df["R_sft"].mean())
print("Tail Average reward for GPT2: ", filtered_df["R_gpt2"].mean())
print("Tail Average reward for GPTJ: ", filtered_df["R_gptj"].mean())
# print("Tail Average Perplexity for RLHF: ", filtered_df["perplexity"].mean())
# print("Tail Average Perplexity for RA-RLHF: ", filtered_df["perplexity_risk"].mean())
# print("Tail Average Perplexity for SFT: ", filtered_df["perplexity_sft"].mean())
# print("Tail Average Perplexity for GPT2: ", filtered_df["perplexity_gpt2"].mean())


# Quantile plots of returns for jigsaw

# rlhf_returns_quantiles = []

# rlhf_risk_returns_quantiles = []

# for quantile_point in range(1, 101, quantile_resolution):
#     # select first quantile_point% of the data
#     relevant_df = quantile_df.iloc[: int((quantile_point / 100) * quantile_df.shape[0])]

#     rlhf_returns_quantiles.append(relevant_df["R_bar"].mean())
#     rlhf_risk_returns_quantiles.append(relevant_df["R_bar_risk"].mean())


# fig, axs = plt.subplots(figsize=(10, 5))
# word_map = {"R_bar": "RLHF returns", "R_bar_risk": "RA-RLHF returns"}

# axs.plot(range(1, 101, quantile_resolution), rlhf_returns_quantiles, label="RLHF", color="red")
# axs.plot(range(1, 101, quantile_resolution), rlhf_risk_returns_quantiles, label="RA-RLHF", color="green")

# axs.set_xlabel("Quantile (%)", fontsize=30)  # Set label size on the axis object
# axs.set_ylabel(f"Returns", fontsize=30)
# axs.tick_params(axis="both", which="major", labelsize=14)
# axs.grid(True)  # Add grid on the axis object
# axs.legend(loc="upper left", fontsize=16)
# axs.set_title(f"Quantile plot of returns", fontsize=30)


# fig.tight_layout()
# plt.savefig("./test_plots_jigsaw_2/jigsaw_R_bar_quantiles.png")
# plt.savefig("./test_plots_jigsaw_2/jigsaw_R_bar_quantiles.pdf")
# plt.clf()

File Path: examples/Jigsaw/plotting/test_plotting_old.py
Content:
import matplotlib.pyplot as plt
import plotly.express as px
import pandas as pd
import numpy as np

# # IMDB
# jigsaw_df_test = pd.read_csv("imdb_generations.csv")

# # Rename as needed in plots
# # jigsaw_df_test = jigsaw_df_test.rename(columns={"R": "RLHF", "R_risk": "RLHF risk averse"})
# # jigsaw_df_test = jigsaw_df_test.rename(columns={"R_bar": "RLHF returns", "R_bar_risk": "RLHF risk averse returns"})


# # # CDF plots
# # fig = px.ecdf(
# #     jigsaw_df_test,
# #     x=["RLHF", "RLHF risk averse"],
# #     labels={
# #         "value": "Reward",
# #         "variable": "Model",
# #     },
# # )
# # fig.update_layout(
# #     title="Reward distribution for RLHF and RLHF risk averse models",
# #     xaxis_title="Reward",
# #     yaxis_title="CDF",
# # )
# # print("Average reward for RLHF: ", jigsaw_df_test["RLHF"].mean())
# # print("Average reward for RLHF risk averse: ", jigsaw_df_test["RLHF risk averse"].mean())
# # fig.write_image("imdb_R_cdf.png")

# # fig = px.ecdf(
# #     jigsaw_df_test,
# #     x=["RLHF returns", "RLHF risk averse returns"],
# #     labels={
# #         "value": "Reward",
# #         "variable": "Model",
# #     },
# # )
# # fig.update_layout(
# #     title="Returns distribution for RLHF and RLHF risk averse models",
# #     xaxis_title="Returns",
# #     yaxis_title="CDF",
# # )
# # print("Average return for RLHF: ", jigsaw_df_test["RLHF returns"].mean())
# # print("Average return for RLHF risk averse: ", jigsaw_df_test["RLHF risk averse returns"].mean())
# # fig.write_image("imdb_R_bar_cdf.png")


# # Histogram shift plots for imdb
# fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(10, 5))
# word_map = {"prompt_score": "Prompt", "R": "RLHF", "R_risk": "RA-RLHF"}
# for i, col in enumerate(["prompt_score", "R", "R_risk"]):
#     jigsaw_df_test[jigsaw_df_test["label"] == 0][col].plot.hist(ax=axs[i], bins=30, alpha=0.5, label="Negative")
#     jigsaw_df_test[jigsaw_df_test["label"] == 1][col].plot.hist(ax=axs[i], bins=30, alpha=0.5, label="Positive")

#     axs[i].set_title(f"{word_map[col]}")
#     axs[i].set_xlabel("Reward", fontsize=16)  # Set label size on the axis object
#     axs[i].set_ylabel(f"Count", fontsize=16)
#     axs[i].tick_params(axis="both", which="major", labelsize=14)
#     axs[i].grid(True)  # Add grid on the axis object
#     axs[i].legend(loc="upper left")  # Add legend on the axis objec

# fig.tight_layout()
# plt.savefig("./test_plots_imdb/imdb_distribution_shifts.png")
# plt.savefig("./test_plots_imdb/imdb_distribution_shifts.pdf")
# plt.clf()

# # Quantile plots of returns for imdb

# fig, axs = plt.subplots(figsize=(10, 5))

# word_map = {"R_bar": "RLHF returns", "R_bar_risk": "RA-RLHF returns"}

# for i, col in enumerate(["R_bar", "R_bar_risk"]):
#     x = np.linspace(0, 1, len(jigsaw_df_test[col])) * 100
#     y = jigsaw_df_test[col].sort_values().values
#     axs.plot(x, y, label=word_map[col])

# axs.set_xlabel("Quantile (%)", fontsize=16)  # Set label size on the axis object
# axs.set_ylabel(f"Returns", fontsize=16)
# axs.tick_params(axis="both", which="major", labelsize=14)
# axs.grid(True)  # Add grid on the axis object
# axs.legend(loc="upper left")
# axs.set_title(f"Quantile plot of returns")


# fig.tight_layout()
# plt.savefig("./test_plots_imdb/imdb_R_bar_quantiles.png")
# plt.savefig("./test_plots_imdb/imdb_R_bar_quantiles.pdf")
# plt.clf()


# # Quantile plots of rewards for imdb

# fig, axs = plt.subplots(figsize=(10, 5))

# word_map = {"R": "RLHF", "R_risk": "RA-RLHF"}

# for i, col in enumerate(["R", "R_risk"]):
#     x = np.linspace(0, 1, len(jigsaw_df_test[col])) * 100
#     y = jigsaw_df_test[col].sort_values().values
#     axs.plot(x, y, label=word_map[col])

# axs.set_xlabel("Quantile (%)", fontsize=16)  # Set label size on the axis object
# axs.set_ylabel(f"Rewards", fontsize=16)
# axs.tick_params(axis="both", which="major", labelsize=14)
# axs.grid(True)  # Add grid on the axis object
# axs.legend(loc="upper left")
# axs.set_title(f"Quantile plot of rewards")


# fig.tight_layout()
# plt.savefig("./test_plots_imdb/imdb_R_quantiles.png")
# plt.savefig("./test_plots_imdb/imdb_R_quantiles.pdf")
# plt.clf()

# # Box plots of returns for imdb

# fig, axs = plt.subplots(figsize=(10, 5))


# # Jigsaw
jigsaw_df_test = pd.read_csv("../dataset/jigsaw_generations_8_32_alpha_40_seed_2.csv")

# Rename as needed in plots
# jigsaw_df_test = jigsaw_df_test.rename(columns={"R": "RLHF", "R_risk": "RLHF risk averse"})
# jigsaw_df_test = jigsaw_df_test.rename(columns={"R_bar": "RLHF returns", "R_bar_risk": "RLHF risk averse returns"})

# # CDF plots
# fig = px.ecdf(
#     jigsaw_df_test,
#     x=["RLHF", "RLHF risk averse"],
#     labels={
#         "value": "Reward",
#         "variable": "Model",
#     },
# )
# fig.update_layout(
#     title="Reward distribution for RLHF and RLHF risk averse models",
#     xaxis_title="Reward",
#     yaxis_title="CDF",
# )
# print("Average reward for RLHF: ", jigsaw_df_test["RLHF"].mean())
# print("Average reward for RLHF risk averse: ", jigsaw_df_test["RLHF risk averse"].mean())
# fig.write_image("jigsaw_R_cdf.png")

# fig = px.ecdf(
#     jigsaw_df_test,
#     x=["RLHF returns", "RLHF risk averse returns"],
#     labels={
#         "value": "Reward",
#         "variable": "Model",
#     },
# )
# fig.update_layout(
#     title="Returns distribution for RLHF and RLHF risk averse models",
#     xaxis_title="Returns",
#     yaxis_title="CDF",
# )
# print("Average return for RLHF: ", jigsaw_df_test["RLHF returns"].mean())
# print("Average return for RLHF risk averse: ", jigsaw_df_test["RLHF risk averse returns"].mean())
# fig.write_image("jigsaw_R_bar_cdf.png")

tail_to_save = -3

tail_df = jigsaw_df_test[jigsaw_df_test["prompt_score"] <= tail_to_save]

tail_df = tail_df.drop(columns=["input_ids"])

print(tail_df["R"].mean())

print(tail_df["R_risk"].mean())


# Plot histogram shifts for jigsaw
def plot_hist_top_edges(data, ax, bins, color, label):
    hist, bin_edges = np.histogram(data, bins=bins)

    for left, right, height in zip(bin_edges[:-1], bin_edges[1:], hist):
        ax.plot([left, right], [height, height], color=color, label=label)

        label = None  # To avoid duplicate labels in the legend


fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(30, 6))

word_map = {"prompt_score": "Prompt Score", "R": "RLHF", "R_risk": "RA-RLHF"}

for i, col in enumerate(["prompt_score", "R", "R_risk"]):
    jigsaw_df_test[jigsaw_df_test["toxic"] == 1][col].plot.hist(ax=axs[i], bins=30, alpha=0.5, label="Toxic")
    jigsaw_df_test[jigsaw_df_test["toxic"] == 0][col].plot.hist(ax=axs[i], bins=30, alpha=0.5, label="Non Toxic")

    axs[i].set_title(f"{word_map[col]}", fontsize=30)

    axs[i].set_xlabel("Reward", fontsize=30)  # Set label size on the axis object

    axs[i].set_ylabel("")

    axs[i].tick_params(axis="both", which="major", labelsize=28)

    axs[i].grid(True)  # Add grid on the axis object

    axs[i].legend(loc="upper left", fontsize=24)  # Add legend on the axis objec


# Find the maximum y-value among the first three histograms

max_count = max(axs[0].get_ylim()[1], axs[1].get_ylim()[1], axs[2].get_ylim()[1])


# Set the y-axis limits for the first three subplots to be the same

axs[0].set_ylim(0, max_count)

axs[1].set_ylim(0, max_count)

axs[2].set_ylim(0, max_count)

axs[0].set_ylabel(f"Count", fontsize=30)


# tail prompts

tail_filter_level = tail_to_save

filtered_df = jigsaw_df_test[jigsaw_df_test["prompt_score"] <= tail_filter_level]

print(filtered_df.shape[0])

filtered_df[filtered_df["toxic"] == 1]["prompt_score"].plot.hist(ax=axs[3], bins=30, alpha=0.5, label="Toxic")
filtered_df[filtered_df["toxic"] == 0]["prompt_score"].plot.hist(ax=axs[3], bins=30, alpha=0.5, label="Non Toxic")


axs[3].set_title("Tail Prompts", fontsize=30)

axs[3].set_xlabel("Reward", fontsize=30)  # Set label size on the axis object

axs[3].set_ylabel("")

axs[3].tick_params(axis="both", which="major", labelsize=28)

axs[3].grid(True)  # Add grid on the axis object

axs[3].legend(loc="upper left", fontsize=24)  # Add legend on the axis objec

axs[3].axvline(x=tail_filter_level, color="red", linestyle="-", linewidth=4)


# tail performance

filtered_df["R"].plot.hist(ax=axs[4], bins=30, alpha=0.3, label="RLHF", color="red")

filtered_df["R_risk"].plot.hist(ax=axs[4], bins=30, alpha=0.5, label="RA-RLHF", color="olive")

axs[4].set_title("Tail Performance", fontsize=30)

axs[4].set_xlabel("Reward", fontsize=30)  # Set label size on the axis object

axs[4].set_ylabel("")

axs[4].tick_params(axis="both", which="major", labelsize=28)

axs[4].grid(True)  # Add grid on the axis object

axs[4].legend(loc="upper right", fontsize=24)  # Add legend on the axis objec

axs[4].axvline(x=tail_filter_level, color="red", linestyle="-", linewidth=4)


# Find the maximum y-value among the first three histograms

max_count_2 = max(axs[3].get_ylim()[1], axs[4].get_ylim()[1])


# Set the y-axis limits for the first three subplots to be the same

axs[3].set_ylim(0, max_count_2)

axs[4].set_ylim(0, max_count_2)

# axs[3].set_xlim(-7.5, -5.9)
# axs[4].set_xlim(-8, 0)


fig.tight_layout()

plt.savefig("./test_plots_jigsaw/jigsaw_distribution_shifts.png")

plt.savefig("./test_plots_jigsaw/jigsaw_distribution_shifts.pdf")

plt.clf()


print("\n\n")
print("Average reward for RLHF: ", jigsaw_df_test["R"].mean())
print("Average reward for RA-RLHF: ", jigsaw_df_test["R_risk"].mean())
print("Average reward for SFT: ", jigsaw_df_test["R_sft"].mean())
print("Average reward for GPT2:", jigsaw_df_test["R_gpt2"].mean())
print("Average Perplexity for RLHF: ", jigsaw_df_test["perplexity"].mean())
print("Average Perplexity for RA-RLHF: ", jigsaw_df_test["perplexity_risk"].mean())
print("Average Perplexity for SFT: ", jigsaw_df_test["perplexity_sft"].mean())
print("Average Perplexity for GPT2: ", jigsaw_df_test["perplexity_gpt2"].mean())
print("\n\n")
print("Tail Average reward for RLHF: ", filtered_df["R"].mean())
print("Tail Average reward for RA-RLHF: ", filtered_df["R_risk"].mean())
print("Tail Average reward for SFT: ", filtered_df["R_sft"].mean())
print("Tail Average reward for GPT2: ", filtered_df["R_gpt2"].mean())
print("Tail Average Perplexity for RLHF: ", filtered_df["perplexity"].mean())
print("Tail Average Perplexity for RA-RLHF: ", filtered_df["perplexity_risk"].mean())
print("Tail Average Perplexity for SFT: ", filtered_df["perplexity_sft"].mean())
print("Tail Average Perplexity for GPT2: ", filtered_df["perplexity_gpt2"].mean())


# Quantile plots of returns for jigsaw

fig, axs = plt.subplots(figsize=(10, 5))
word_map = {"R_bar": "RLHF returns", "R_bar_risk": "RA-RLHF returns"}

for i, col in enumerate(["R_bar", "R_bar_risk"]):
    x = np.linspace(0, 1, len(jigsaw_df_test[col])) * 100
    y = jigsaw_df_test[col].sort_values().values
    axs.plot(x, y, label=word_map[col])

axs.set_xlabel("Quantile (%)", fontsize=16)  # Set label size on the axis object
axs.set_ylabel(f"Returns", fontsize=16)
axs.tick_params(axis="both", which="major", labelsize=14)
axs.grid(True)  # Add grid on the axis object
axs.legend(loc="upper left")
axs.set_title(f"Quantile plot of returns")


fig.tight_layout()
plt.savefig("./test_plots_jigsaw/jigsaw_R_bar_quantiles.png")
plt.savefig("./test_plots_jigsaw/jigsaw_R_bar_quantiles.pdf")
plt.clf()

# Quantile plots of rewards for jigsaw

fig, axs = plt.subplots(figsize=(10, 5))
word_map = {"R": "RLHF", "R_risk": "RA-RLHF"}
for i, col in enumerate(["R", "R_risk"]):
    x = np.linspace(0, 1, len(jigsaw_df_test[col])) * 100
    y = jigsaw_df_test[col].sort_values().values
    axs.plot(x, y, label=word_map[col])

axs.set_xlabel("Quantile (%)", fontsize=16)  # Set label size on the axis object
axs.set_ylabel(f"Rewards", fontsize=16)
axs.tick_params(axis="both", which="major", labelsize=14)
axs.grid(True)  # Add grid on the axis object
axs.legend(loc="upper left")
axs.set_title(f"Quantile plot of rewards")


fig.tight_layout()
plt.savefig("./test_plots_jigsaw/jigsaw_R_quantiles.png")
plt.savefig("./test_plots_jigsaw/jigsaw_R_quantiles.pdf")
plt.clf()

File Path: examples/Jigsaw/training/appendix/imdb_analysis.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, set_seed
from trl.trainer.ppo_trainer_original import PPOTrainer
from trl.core import LengthSampler


tqdm.pandas()
import matplotlib.pyplot as plt
import itertools

import pdb
import pickle
import datetime
import os
from evaluate import load
from transformers import AutoModel, AutoTokenizer, AutoConfig
from transformers import OpenAIGPTTokenizer, OpenAIGPTModel
import random

import pandas as pd
import pdb

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
print(date_n_time)
logging_dir = f"/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy_old/{date_n_time}"
os.makedirs(logging_dir)


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name="lvwerra/gpt2-imdb",
            query_dataset="imdb",
            reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            remove_unused_columns=False,
        )
    )
    query_dataset: str = field(default="imdb", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="less", metadata={"help": "the dataset to query"})


args = tyro.cli(ScriptArguments)

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8, data_split="train"):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # load imdb with datasets
    # to load partial dataset, just change data split to split=data_split+'[:200]'
    ds = load_dataset(query_dataset, split=data_split)
    ds = ds.rename_columns({"text": "review"})
    ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)

    input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# get test dataset
test_dataset = build_dataset(args.ppo_config, args.query_dataset, data_split="train")
# Take 5k random samples
# random.seed(10)
# sample_size = 2500
# random_indices = random.sample(range(len(test_dataset)), sample_size)
# test_dataset = test_dataset.select(random_indices)

# pdb.set_trace()


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

zero_shot_model = trl_model_class.from_pretrained(
    "gpt2",
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)

sft_model = trl_model_class.from_pretrained(
    "lvwerra/gpt2-imdb",
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)

# For the following to work I should have saved the model using huggingface
# ppo_model = trl_model_class.from_pretrained(
#     '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-09-26_11-27-23/model.pt',
#     trust_remote_code=True,
#     device_map=device_map,
#     peft_config=peft_config,
# )

# sft_ppo_model = trl_model_class.from_pretrained(
#     "/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/trl_sft_ppo/2023-10-23_23-30-47",
#     trust_remote_code=True,
#     device_map=device_map,
#     peft_config=peft_config,
# )
"""
ppo_model_PATH = '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-09-26_11-27-23/model.pt'
ppo_model = torch.load(ppo_model_PATH)  

sft_ppo_model_PATH = '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_ppo/2023-09-26_10-36-41/model.pt'
sft_ppo_model = torch.load(sft_ppo_model_PATH)
"""
tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"
tokenizer.truncation_side = "left"
tokenizer.pad_token_as_eos_token = True
tokenizer.max_length = 64

# Define all the tester classes
zero_shot_tester = PPOTrainer(
    args.ppo_config, zero_shot_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator
)
sft_tester = PPOTrainer(args.ppo_config, sft_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)
# ppo_tester = PPOTrainer(args.ppo_config, ppo_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)
# sft_ppo_tester = PPOTrainer(
#     args.ppo_config, sft_ppo_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator
# )

# pdb.set_trace()
# ppo_tester._save_pretrained('/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-09-26_11-27-23')
# sft_ppo_tester._save_pretrained('/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_ppo/2023-09-26_10-36-41')

# Build the perplexity evaluation pipeline
perplexity = load("perplexity", module_type="measurement")

# DO I NEED TO TAKE CARE OF PAD TOKEN FOR PERPLEXITY PIPELINE AS WELL??
"""
# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id
"""
device = sft_tester.accelerator.device
if sft_tester.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = sft_tester.accelerator.state.deepspeed_plugin
task, model_name = args.ppo_config.reward_model.split(":")
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id


# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs = {
    "min_length": 48,
    "top_k": 0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": 48,
}


# date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
# os.makedirs(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/evaluation/{date_n_time}')

model_names_all = ["zero_shot_model", "sft_model", "ppo_model", "sft_ppo_model"]
testers_all = [
    zero_shot_tester,
    sft_tester,
]  # sft_ppo_tester]  # ppo_tester,

# infer on test dataset
# test_scores_1, test_scores_2, test_scores_3, test_scores_4 = [], [], [], []
positive_score_all = []
negative_score_all = []
label_all = []
critical_pos_texts = []
critical_neg_texts = []
critical_neg_scores = []
critical_pos_scores = []
total_critical_texts = []
total_critical_scores = []
total_critical_labels = []

for epoch, batch in tqdm(enumerate(zero_shot_tester.dataloader)):
    query_tensors = batch["input_ids"]

    texts = batch["review"]
    texts = [text[:200] for text in texts]
    # texts = ["lorem ipsum", "Happy Birthday!", "Bienvenue"]

    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
    pos_score = [output[1]["score"] for output in pipe_outputs]
    positive_score_all.append(pos_score)

    neg_score = [output[0]["score"] for output in pipe_outputs]
    negative_score_all.append(neg_score)

    labels = [label.item() for label in batch["label"]]
    label_all.append(labels)

    for ii in range(len(label_all[0])):
        if labels[ii] == 0 and pos_score[ii] < -2.8:
            critical_neg_texts.append(texts[ii])
            critical_neg_scores.append(pos_score[ii])
            total_critical_texts.append(texts[ii])
            total_critical_scores.append(pos_score[ii])
            total_critical_labels.append(labels[ii])
        if labels[ii] == 1 and pos_score[ii] < -2.8:
            critical_pos_texts.append(texts[ii])
            critical_pos_scores.append(pos_score[ii])
            total_critical_texts.append(texts[ii])
            total_critical_scores.append(pos_score[ii])
            total_critical_labels.append(labels[ii])

print(f"Total critical reviews: {len(total_critical_labels)}")
# get flattened lists
pos_scores = list(itertools.chain(*positive_score_all))
neg_scores = list(itertools.chain(*negative_score_all))
labels = list(itertools.chain(*label_all))


# Plot histograms
df = pd.DataFrame(list(zip(neg_scores, pos_scores, labels)), columns=["neg score", "pos score", "label"])
fig2, axs2 = plt.subplots(figsize=(10, 5))

df[df["label"] == 1]["pos score"].plot.hist(ax=axs2, bins=30, alpha=0.5, label="Positive")
df[df["label"] == 0]["pos score"].plot.hist(ax=axs2, bins=30, alpha=0.5, label="Negative")
axs2.legend()
axs2.set_ylabel(r"No. of input prompts", fontsize=16)  # Setting label size
axs2.set_xlabel(r"Enviroment Score", fontsize=16)
axs2.tick_params(axis="both", which="major", labelsize=14)  # Adjusting tick size
axs2.set_box_aspect(0.7)
axs2.grid(True)
fig2.savefig("histogram_plot.pdf", bbox_inches="tight")

# save critical reviews as csv
df1 = pd.DataFrame({"score": critical_neg_scores, "review": critical_neg_texts})
df2 = pd.DataFrame({"score": critical_pos_scores, "review": critical_pos_texts})
df3 = pd.DataFrame({"review": total_critical_texts})

df1 = df1.sort_values("score")
df2 = df2.sort_values("score")

df1.to_csv("critical_neg_texts.csv")
df2.to_csv("critical_pos_texts.csv")
df3.to_csv("total_critical_texts.csv")


# histogram plot for scores of critical reviews
df4 = pd.DataFrame(list(zip(total_critical_scores, total_critical_labels)), columns=["pos score", "label"])
fig3, axs3 = plt.subplots(figsize=(10, 5))

df4[df4["label"] == 1]["pos score"].plot.hist(ax=axs3, bins=30, alpha=0.5, label="Positive")
df4[df4["label"] == 0]["pos score"].plot.hist(ax=axs3, bins=30, alpha=0.5, label="Negative")
axs3.legend()
axs3.set_ylabel(r"No. of input prompts", fontsize=16)  # Setting label size
axs3.set_xlabel(r"Enviroment Score", fontsize=16)
axs3.tick_params(axis="both", which="major", labelsize=14)  # Adjusting tick size
axs3.set_box_aspect(0.7)
axs3.grid(True)

fig3.savefig("critical_context_scores.pdf", bbox_inches="tight")

File Path: examples/Jigsaw/training/appendix/jigsaw_analysis.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset, Dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, set_seed
from trl.trainer.ppo_trainer_original import PPOTrainer
from trl.core import LengthSampler


tqdm.pandas()
import matplotlib.pyplot as plt
import itertools

import pdb
import pickle
import datetime
import os
from evaluate import load
from transformers import AutoModel, AutoTokenizer, AutoConfig
from transformers import OpenAIGPTTokenizer, OpenAIGPTModel
import random

import pandas as pd
import pdb

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")

logging_dir = f"/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy_old/{date_n_time}"
os.makedirs(logging_dir)


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name="lvwerra/gpt2-imdb",
            query_dataset="jigsaw_toxicity_pred",
            reward_model="unitary/toxic-bert",
            # reward_model="citizenlab/twitter-xlm-roberta-base-sentiment-finetunned",
            learning_rate=1.41e-5,
            # log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            steps=128000,
            exp_name="ppo",
            remove_unused_columns=False,
        )
    )
    query_dataset: str = field(default="jigsaw_toxicity_pred", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="more", metadata={"help": "the dataset to query"})

    generation_kwargs_min_length: Optional[int] = field(
        default=-1, metadata={"help": "minimum number of tokens while generation"}
    )
    generation_kwargs_top_k: Optional[int] = field(default=0, metadata={"help": "gneration top k"})
    generation_kwargs_top_p: Optional[float] = field(default=1.0, metadata={"help": "gneration top p"})
    generation_kwargs_max_new_tokens: Optional[int] = field(default=32, metadata={"help": "gneration top p"})

    change_tokenizer_args: bool = field(default=False, metadata={"help": "whether to use modify tokenizer settings"})
    tokenizer_kwargs: dict = field(
        default_factory=lambda: {
            "padding_side": "left",
            "truncation_side": "left",
            "pad_token_as_eos_token": True,
            "max_length": 64,
        }
    )


args = tyro.cli(ScriptArguments)

generation_kwargs = {
    "min_length": args.generation_kwargs_min_length,
    "top_k": args.generation_kwargs_top_k,
    "top_p": args.generation_kwargs_top_p,
    "do_sample": True,
    "max_new_tokens": args.generation_kwargs_max_new_tokens,
}

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # Kaggle datasets have to be downloaded manually (using Kaggle API)
    # datasets.load_dataset("jigsaw_toxicity_pred", data_dir="<path/to/manual/data>")
    # ds = load_dataset(
    #     query_dataset,
    #     data_dir="/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/original_dataset",
    #     split="train",
    # )  # Hardcoding path for now TODO: change this

    # ds = ds.rename_columns({"comment_text": "review"})
    # ds = ds.filter(lambda x: len(x["review"]) > 64, batched=False)

    # df = pd.DataFrame(ds)
    # num_toxic = df["toxic"].sum()

    # toxic_df = df[df["toxic"] == True]
    # non_toxic_df = df[df["toxic"] == False]

    # non_toxic_df = non_toxic_df.sample(n=num_toxic, random_state=config.seed)

    # # Recombine into dataset
    # ds = ds.from_pandas(pd.concat([toxic_df, non_toxic_df], ignore_index=True))
    df = pd.read_csv(
        "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw_2/dataset/jigsaw_train_70_30.csv",
    )
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"comment_text": "review"})

    # pdb.set_trace()

    if args.input_query_size == "more":

        def input_size():
            return 20

    else:
        input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    args.ppo_config.model_name,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id

# pdb.set_trace()

if args.change_tokenizer_args == True:
    tokenizer.padding_side = args.tokenizer_kwargs["padding_side"]
    tokenizer.truncation_side = args.tokenizer_kwargs["truncation_side"]
    tokenizer.pad_token_as_eos_token = args.tokenizer_kwargs["pad_token_as_eos_token"]
    tokenizer.max_length = args.tokenizer_kwargs["max_length"]

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
# task, model_name = args.ppo_config.reward_model.split(":")
task = "text-classification"
model_name = args.ppo_config.reward_model
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs["pad_token_id"] = tokenizer.eos_token_id

# empty list to store text data
text_train_data = []

# infer on test dataset
# test_scores_1, test_scores_2, test_scores_3, test_scores_4 = [], [], [], []
positive_score_all = []
negative_score_all = []
label_all = []
critical_pos_texts = []
critical_neg_texts = []
critical_neg_scores = []
critical_pos_scores = []
total_critical_texts = []
total_critical_scores = []
total_critical_labels = []

for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    texts = batch["query"]
    texts = [text[:200] for text in texts]

    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
    pos_score = [-output[0]["score"] for output in pipe_outputs]
    positive_score_all.append(pos_score)
    # rewards = [torch.tensor(-output[0]["score"]) for output in pipe_outputs]

    neg_score = [-output[1]["score"] for output in pipe_outputs]
    negative_score_all.append(neg_score)

    labels = [label.item() for label in batch["toxic"]]
    label_all.append(labels)

    for ii in range(len(label_all[0])):
        if labels[ii] == 0 and pos_score[ii] < -5.0:
            critical_neg_texts.append(texts[ii])
            critical_neg_scores.append(pos_score[ii])
            total_critical_texts.append(texts[ii])
            total_critical_scores.append(pos_score[ii])
            total_critical_labels.append(labels[ii])
        if labels[ii] == 1 and pos_score[ii] < -5.0:
            critical_pos_texts.append(texts[ii])
            critical_pos_scores.append(pos_score[ii])
            total_critical_texts.append(texts[ii])
            total_critical_scores.append(pos_score[ii])
            total_critical_labels.append(labels[ii])

print(f"Total critical reviews: {len(total_critical_labels)}")
# get flattened lists
pos_scores = list(itertools.chain(*positive_score_all))
neg_scores = list(itertools.chain(*negative_score_all))
labels = list(itertools.chain(*label_all))


# Plot histograms
df = pd.DataFrame(list(zip(neg_scores, pos_scores, labels)), columns=["neg score", "pos score", "label"])
fig2, axs2 = plt.subplots(figsize=(10, 5))

df[df["label"] == 0]["pos score"].plot.hist(ax=axs2, bins=30, alpha=0.5, label="Non-toxic")
df[df["label"] == 1]["pos score"].plot.hist(ax=axs2, bins=30, alpha=0.5, label="Toxic")
axs2.legend()
axs2.set_ylabel(r"No. of input prompts", fontsize=16)  # Setting label size
axs2.set_xlabel(r"Enviroment Score", fontsize=16)
axs2.tick_params(axis="both", which="major", labelsize=14)  # Adjusting tick size
axs2.set_box_aspect(0.7)
axs2.grid(True)
fig2.savefig("histogram_plot.pdf", bbox_inches="tight")

# save critical reviews as csv
df1 = pd.DataFrame({"score": critical_neg_scores, "review": critical_neg_texts})
df2 = pd.DataFrame({"score": critical_pos_scores, "review": critical_pos_texts})
df3 = pd.DataFrame({"review": total_critical_texts})

df1 = df1.sort_values("score")
df2 = df2.sort_values("score")

df1.to_csv("critical_neg_texts.csv")
df2.to_csv("critical_pos_texts.csv")
df3.to_csv("total_critical_texts.csv")


# histogram plot for scores of critical reviews
df4 = pd.DataFrame(list(zip(total_critical_scores, total_critical_labels)), columns=["pos score", "label"])
fig3, axs3 = plt.subplots(figsize=(10, 5))

df4[df4["label"] == 0]["pos score"].plot.hist(ax=axs3, bins=30, alpha=0.5, label="Non-toxic")
df4[df4["label"] == 1]["pos score"].plot.hist(ax=axs3, bins=30, alpha=0.5, label="Toxic")
axs3.legend()
axs3.set_ylabel(r"No. of input prompts", fontsize=16)  # Setting label size
axs3.set_xlabel(r"Enviroment Score", fontsize=16)
axs3.tick_params(axis="both", which="major", labelsize=14)  # Adjusting tick size
axs3.set_box_aspect(0.7)
axs3.grid(True)

fig3.savefig("critical_context_scores.pdf", bbox_inches="tight")

File Path: examples/Jigsaw/training/ppo_auth2.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset, Dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, set_seed
from trl.core import LengthSampler
from trl.trainer.ppo_trainer_original import PPOTrainer

tqdm.pandas()

import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import pdb
import pandas as pd

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")

logging_dir = f"/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/{date_n_time}"
os.makedirs(logging_dir)


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-20_20-32-35",  # sft_both
            model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-25_16-31-51",  # sft_positive
            query_dataset="jigsaw_toxicity_pred",
            reward_model="unitary/toxic-bert",
            # reward_model="citizenlab/twitter-xlm-roberta-base-sentiment-finetunned",
            learning_rate=1.41e-5,
            # log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            # adap_kl_ctrl=False,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            steps=128000,
            exp_name="ppo",
        )
    )
    query_dataset: str = field(default="jigsaw_toxicity_pred", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="more", metadata={"help": "the dataset to query"})

    generation_kwargs_min_length: Optional[int] = field(
        default=32, metadata={"help": "minimum number of tokens while generation"}
    )
    generation_kwargs_top_k: Optional[int] = field(default=0, metadata={"help": "gneration top k"})
    generation_kwargs_top_p: Optional[float] = field(default=1.0, metadata={"help": "gneration top p"})
    generation_kwargs_max_new_tokens: Optional[int] = field(default=32, metadata={"help": "gneration top p"})

    change_tokenizer_args: bool = field(default=False, metadata={"help": "whether to use modify tokenizer settings"})
    tokenizer_kwargs: dict = field(
        default_factory=lambda: {
            "padding_side": "left",
            "truncation_side": "left",
            "pad_token_as_eos_token": True,
            "max_length": 64,
        }
    )
    prompt_len: Optional[int] = field(default=32, metadata={"help": "prompt length"})


args = tyro.cli(ScriptArguments)


generation_kwargs = {
    "min_length": args.generation_kwargs_min_length,
    "top_k": args.generation_kwargs_top_k,
    "top_p": args.generation_kwargs_top_p,
    "do_sample": True,
    "max_new_tokens": args.generation_kwargs_max_new_tokens,
}

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # # Kaggle datasets have to be downloaded manually (using Kaggle API)
    # # datasets.load_dataset("jigsaw_toxicity_pred", data_dir="<path/to/manual/data>")
    # ds = load_dataset(
    #     query_dataset,
    #     data_dir="/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/original_dataset",
    #     split="train",
    # )  # Hardcoding path for now TODO: change this

    # ds = ds.rename_columns({"comment_text": "review"})
    # ds = ds.filter(lambda x: len(x["review"]) > 64, batched=False)

    # df = pd.DataFrame(ds)
    # num_toxic = df["toxic"].sum()

    # toxic_df = df[df["toxic"] == True]
    # non_toxic_df = df[df["toxic"] == False]

    # non_toxic_df = non_toxic_df.sample(n=num_toxic, random_state=config.seed)

    # # Recombine into dataset
    # ds = ds.from_pandas(pd.concat([toxic_df, non_toxic_df], ignore_index=True))
    df = pd.read_csv(
        "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/jigsaw_train.csv",
    )
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"comment_text": "review"})

    # pdb.set_trace()

    if args.input_query_size == "more":

        def input_size():
            return args.prompt_len

    else:
        input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    args.ppo_config.model_name,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id

# pdb.set_trace()

if args.change_tokenizer_args == True:
    tokenizer.padding_side = args.tokenizer_kwargs["padding_side"]
    tokenizer.truncation_side = args.tokenizer_kwargs["truncation_side"]
    tokenizer.pad_token_as_eos_token = args.tokenizer_kwargs["pad_token_as_eos_token"]
    tokenizer.max_length = args.tokenizer_kwargs["max_length"]

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
# task, model_name = args.ppo_config.reward_model.split(":")
task = "text-classification"
model_name = args.ppo_config.reward_model
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs["pad_token_id"] = tokenizer.eos_token_id

# empty list to store text data
text_train_data = []

for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(
        texts,
        **sent_kwargs,
        # truncation=True,
    )

    if args.reward_function == "trl":
        rewards = [torch.tensor(-output[0]["score"]) for output in pipe_outputs]
    else:  # depreciated for jigsaw task
        raise NotImplementedError
        rewards = [
            -torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"], output[2]["score"]]))[2]
            for output in pipe_outputs
        ]
    # pdb.set_trace()
    for ii in range(3):
        data_row = {
            "iter": epoch,
            "input": batch["query"][ii],
            "output": batch["response"][ii],
            "score": rewards[ii].item(),
        }
        text_train_data.append(data_row)

    # pdb.set_trace()

    # Run PPO step
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
    ppo_trainer.log_stats(stats, batch, rewards)
    print(epoch)

ppo_trainer._save_pretrained(logging_dir + "/save_pretrained")
df = pd.DataFrame(text_train_data)
df.to_csv(logging_dir + "/textual_data.csv", index=False)

File Path: examples/Jigsaw/training/ppo_auth2_seed12.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset, Dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, set_seed
from trl.core import LengthSampler
from trl.trainer.ppo_trainer_original import PPOTrainer

tqdm.pandas()

import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import pdb
import pandas as pd

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")

logging_dir = f"/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/{date_n_time}"
os.makedirs(logging_dir)


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-20_20-32-35",  # sft_both
            model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-30_00-30-38",  # sft_positive seed 12
            query_dataset="jigsaw_toxicity_pred",
            reward_model="unitary/toxic-bert",
            # reward_model="citizenlab/twitter-xlm-roberta-base-sentiment-finetunned",
            learning_rate=1.41e-5,
            # log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            # adap_kl_ctrl=False,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            steps=192000,
            exp_name="ppo",
        )
    )
    query_dataset: str = field(default="jigsaw_toxicity_pred", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="more", metadata={"help": "the dataset to query"})

    generation_kwargs_min_length: Optional[int] = field(
        default=32, metadata={"help": "minimum number of tokens while generation"}
    )
    generation_kwargs_top_k: Optional[int] = field(default=0, metadata={"help": "gneration top k"})
    generation_kwargs_top_p: Optional[float] = field(default=1.0, metadata={"help": "gneration top p"})
    generation_kwargs_max_new_tokens: Optional[int] = field(default=32, metadata={"help": "gneration top p"})

    change_tokenizer_args: bool = field(default=False, metadata={"help": "whether to use modify tokenizer settings"})
    tokenizer_kwargs: dict = field(
        default_factory=lambda: {
            "padding_side": "left",
            "truncation_side": "left",
            "pad_token_as_eos_token": True,
            "max_length": 64,
        }
    )
    prompt_len: Optional[int] = field(default=32, metadata={"help": "prompt length"})


args = tyro.cli(ScriptArguments)


generation_kwargs = {
    "min_length": args.generation_kwargs_min_length,
    "top_k": args.generation_kwargs_top_k,
    "top_p": args.generation_kwargs_top_p,
    "do_sample": True,
    "max_new_tokens": args.generation_kwargs_max_new_tokens,
}

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # # Kaggle datasets have to be downloaded manually (using Kaggle API)
    # # datasets.load_dataset("jigsaw_toxicity_pred", data_dir="<path/to/manual/data>")
    # ds = load_dataset(
    #     query_dataset,
    #     data_dir="/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/original_dataset",
    #     split="train",
    # )  # Hardcoding path for now TODO: change this

    # ds = ds.rename_columns({"comment_text": "review"})
    # ds = ds.filter(lambda x: len(x["review"]) > 64, batched=False)

    # df = pd.DataFrame(ds)
    # num_toxic = df["toxic"].sum()

    # toxic_df = df[df["toxic"] == True]
    # non_toxic_df = df[df["toxic"] == False]

    # non_toxic_df = non_toxic_df.sample(n=num_toxic, random_state=config.seed)

    # # Recombine into dataset
    # ds = ds.from_pandas(pd.concat([toxic_df, non_toxic_df], ignore_index=True))
    df = pd.read_csv(
        "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/jigsaw_train.csv",
    )
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"comment_text": "review"})

    # pdb.set_trace()

    if args.input_query_size == "more":

        def input_size():
            return args.prompt_len

    else:
        input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    args.ppo_config.model_name,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id

# pdb.set_trace()

if args.change_tokenizer_args == True:
    tokenizer.padding_side = args.tokenizer_kwargs["padding_side"]
    tokenizer.truncation_side = args.tokenizer_kwargs["truncation_side"]
    tokenizer.pad_token_as_eos_token = args.tokenizer_kwargs["pad_token_as_eos_token"]
    tokenizer.max_length = args.tokenizer_kwargs["max_length"]

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
# task, model_name = args.ppo_config.reward_model.split(":")
task = "text-classification"
model_name = args.ppo_config.reward_model
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs["pad_token_id"] = tokenizer.eos_token_id

# empty list to store text data
text_train_data = []

for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(
        texts,
        **sent_kwargs,
        # truncation=True,
    )

    if args.reward_function == "trl":
        rewards = [torch.tensor(-output[0]["score"]) for output in pipe_outputs]
    else:  # depreciated for jigsaw task
        raise NotImplementedError
        rewards = [
            -torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"], output[2]["score"]]))[2]
            for output in pipe_outputs
        ]
    # pdb.set_trace()
    for ii in range(3):
        data_row = {
            "iter": epoch,
            "input": batch["query"][ii],
            "output": batch["response"][ii],
            "score": rewards[ii].item(),
        }
        text_train_data.append(data_row)

    # pdb.set_trace()

    # Run PPO step
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
    ppo_trainer.log_stats(stats, batch, rewards)
    print(epoch)

ppo_trainer._save_pretrained(logging_dir + "/save_pretrained")
df = pd.DataFrame(text_train_data)
df.to_csv(logging_dir + "/textual_data.csv", index=False)

File Path: examples/Jigsaw/training/ppo_auth2_seed2.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset, Dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, set_seed
from trl.core import LengthSampler
from trl.trainer.ppo_trainer_original import PPOTrainer

tqdm.pandas()

import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import pdb
import pandas as pd

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")

logging_dir = f"/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/{date_n_time}"
os.makedirs(logging_dir)


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-20_20-32-35",  # sft_both
            model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-31_18-06-18",  # sft_positive 70/30 seed 2
            query_dataset="jigsaw_toxicity_pred",
            reward_model="unitary/toxic-bert",
            # reward_model="citizenlab/twitter-xlm-roberta-base-sentiment-finetunned",
            learning_rate=1.41e-5,
            # log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            # adap_kl_ctrl=False,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            steps=192000,
            exp_name="ppo",
        )
    )
    query_dataset: str = field(default="jigsaw_toxicity_pred", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="more", metadata={"help": "the dataset to query"})

    generation_kwargs_min_length: Optional[int] = field(
        default=32, metadata={"help": "minimum number of tokens while generation"}
    )
    generation_kwargs_top_k: Optional[int] = field(default=0, metadata={"help": "gneration top k"})
    generation_kwargs_top_p: Optional[float] = field(default=1.0, metadata={"help": "gneration top p"})
    generation_kwargs_max_new_tokens: Optional[int] = field(default=32, metadata={"help": "gneration top p"})

    change_tokenizer_args: bool = field(default=False, metadata={"help": "whether to use modify tokenizer settings"})
    tokenizer_kwargs: dict = field(
        default_factory=lambda: {
            "padding_side": "left",
            "truncation_side": "left",
            "pad_token_as_eos_token": True,
            "max_length": 64,
        }
    )
    prompt_len: Optional[int] = field(default=32, metadata={"help": "prompt length"})


args = tyro.cli(ScriptArguments)


generation_kwargs = {
    "min_length": args.generation_kwargs_min_length,
    "top_k": args.generation_kwargs_top_k,
    "top_p": args.generation_kwargs_top_p,
    "do_sample": True,
    "max_new_tokens": args.generation_kwargs_max_new_tokens,
}

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # # Kaggle datasets have to be downloaded manually (using Kaggle API)
    # # datasets.load_dataset("jigsaw_toxicity_pred", data_dir="<path/to/manual/data>")
    # ds = load_dataset(
    #     query_dataset,
    #     data_dir="/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/original_dataset",
    #     split="train",
    # )  # Hardcoding path for now TODO: change this

    # ds = ds.rename_columns({"comment_text": "review"})
    # ds = ds.filter(lambda x: len(x["review"]) > 64, batched=False)

    # df = pd.DataFrame(ds)
    # num_toxic = df["toxic"].sum()

    # toxic_df = df[df["toxic"] == True]
    # non_toxic_df = df[df["toxic"] == False]

    # non_toxic_df = non_toxic_df.sample(n=num_toxic, random_state=config.seed)

    # # Recombine into dataset
    # ds = ds.from_pandas(pd.concat([toxic_df, non_toxic_df], ignore_index=True))
    df = pd.read_csv(
        "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/jigsaw_train_70_30.csv",
    )
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"comment_text": "review"})

    # pdb.set_trace()

    if args.input_query_size == "more":

        def input_size():
            return args.prompt_len

    else:
        input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    args.ppo_config.model_name,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id

# pdb.set_trace()

if args.change_tokenizer_args == True:
    tokenizer.padding_side = args.tokenizer_kwargs["padding_side"]
    tokenizer.truncation_side = args.tokenizer_kwargs["truncation_side"]
    tokenizer.pad_token_as_eos_token = args.tokenizer_kwargs["pad_token_as_eos_token"]
    tokenizer.max_length = args.tokenizer_kwargs["max_length"]

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
# task, model_name = args.ppo_config.reward_model.split(":")
task = "text-classification"
model_name = args.ppo_config.reward_model
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs["pad_token_id"] = tokenizer.eos_token_id

# empty list to store text data
text_train_data = []

for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(
        texts,
        **sent_kwargs,
        # truncation=True,
    )

    if args.reward_function == "trl":
        rewards = [torch.tensor(-output[0]["score"]) for output in pipe_outputs]
    else:  # depreciated for jigsaw task
        raise NotImplementedError
        rewards = [
            -torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"], output[2]["score"]]))[2]
            for output in pipe_outputs
        ]
    # pdb.set_trace()
    for ii in range(3):
        data_row = {
            "iter": epoch,
            "input": batch["query"][ii],
            "output": batch["response"][ii],
            "score": rewards[ii].item(),
        }
        text_train_data.append(data_row)

    # pdb.set_trace()

    # Run PPO step
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
    ppo_trainer.log_stats(stats, batch, rewards)
    print(epoch)

ppo_trainer._save_pretrained(logging_dir + "/save_pretrained")
df = pd.DataFrame(text_train_data)
df.to_csv(logging_dir + "/textual_data.csv", index=False)

File Path: examples/Jigsaw/training/ppo_auth2_seed36.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset, Dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, set_seed
from trl.core import LengthSampler
from trl.trainer.ppo_trainer_original import PPOTrainer

tqdm.pandas()

import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import pdb
import pandas as pd

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")

logging_dir = f"/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/{date_n_time}"
os.makedirs(logging_dir)


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-20_20-32-35",  # sft_both
            model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-30_00-24-10",  # sft_positive seed 36
            query_dataset="jigsaw_toxicity_pred",
            reward_model="unitary/toxic-bert",
            # reward_model="citizenlab/twitter-xlm-roberta-base-sentiment-finetunned",
            learning_rate=1.41e-5,
            # log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            # adap_kl_ctrl=False,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            steps=192000,
            exp_name="ppo",
        )
    )
    query_dataset: str = field(default="jigsaw_toxicity_pred", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="more", metadata={"help": "the dataset to query"})

    generation_kwargs_min_length: Optional[int] = field(
        default=32, metadata={"help": "minimum number of tokens while generation"}
    )
    generation_kwargs_top_k: Optional[int] = field(default=0, metadata={"help": "gneration top k"})
    generation_kwargs_top_p: Optional[float] = field(default=1.0, metadata={"help": "gneration top p"})
    generation_kwargs_max_new_tokens: Optional[int] = field(default=32, metadata={"help": "gneration top p"})

    change_tokenizer_args: bool = field(default=False, metadata={"help": "whether to use modify tokenizer settings"})
    tokenizer_kwargs: dict = field(
        default_factory=lambda: {
            "padding_side": "left",
            "truncation_side": "left",
            "pad_token_as_eos_token": True,
            "max_length": 64,
        }
    )
    prompt_len: Optional[int] = field(default=32, metadata={"help": "prompt length"})


args = tyro.cli(ScriptArguments)


generation_kwargs = {
    "min_length": args.generation_kwargs_min_length,
    "top_k": args.generation_kwargs_top_k,
    "top_p": args.generation_kwargs_top_p,
    "do_sample": True,
    "max_new_tokens": args.generation_kwargs_max_new_tokens,
}

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # # Kaggle datasets have to be downloaded manually (using Kaggle API)
    # # datasets.load_dataset("jigsaw_toxicity_pred", data_dir="<path/to/manual/data>")
    # ds = load_dataset(
    #     query_dataset,
    #     data_dir="/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/original_dataset",
    #     split="train",
    # )  # Hardcoding path for now TODO: change this

    # ds = ds.rename_columns({"comment_text": "review"})
    # ds = ds.filter(lambda x: len(x["review"]) > 64, batched=False)

    # df = pd.DataFrame(ds)
    # num_toxic = df["toxic"].sum()

    # toxic_df = df[df["toxic"] == True]
    # non_toxic_df = df[df["toxic"] == False]

    # non_toxic_df = non_toxic_df.sample(n=num_toxic, random_state=config.seed)

    # # Recombine into dataset
    # ds = ds.from_pandas(pd.concat([toxic_df, non_toxic_df], ignore_index=True))
    df = pd.read_csv(
        "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/jigsaw_train.csv",
    )
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"comment_text": "review"})

    # pdb.set_trace()

    if args.input_query_size == "more":

        def input_size():
            return args.prompt_len

    else:
        input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    args.ppo_config.model_name,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id

# pdb.set_trace()

if args.change_tokenizer_args == True:
    tokenizer.padding_side = args.tokenizer_kwargs["padding_side"]
    tokenizer.truncation_side = args.tokenizer_kwargs["truncation_side"]
    tokenizer.pad_token_as_eos_token = args.tokenizer_kwargs["pad_token_as_eos_token"]
    tokenizer.max_length = args.tokenizer_kwargs["max_length"]

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
# task, model_name = args.ppo_config.reward_model.split(":")
task = "text-classification"
model_name = args.ppo_config.reward_model
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs["pad_token_id"] = tokenizer.eos_token_id

# empty list to store text data
text_train_data = []

for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(
        texts,
        **sent_kwargs,
        # truncation=True,
    )

    if args.reward_function == "trl":
        rewards = [torch.tensor(-output[0]["score"]) for output in pipe_outputs]
    else:  # depreciated for jigsaw task
        raise NotImplementedError
        rewards = [
            -torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"], output[2]["score"]]))[2]
            for output in pipe_outputs
        ]
    # pdb.set_trace()
    for ii in range(3):
        data_row = {
            "iter": epoch,
            "input": batch["query"][ii],
            "output": batch["response"][ii],
            "score": rewards[ii].item(),
        }
        text_train_data.append(data_row)

    # pdb.set_trace()

    # Run PPO step
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
    ppo_trainer.log_stats(stats, batch, rewards)
    print(epoch)

ppo_trainer._save_pretrained(logging_dir + "/save_pretrained")
df = pd.DataFrame(text_train_data)
df.to_csv(logging_dir + "/textual_data.csv", index=False)

File Path: examples/Jigsaw/training/sft_auth2.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional
import pandas as pd
import torch
from accelerate import Accelerator
from datasets import load_dataset, Dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoModelForCausalLM, BitsAndBytesConfig, HfArgumentParser, TrainingArguments

from trl import SFTTrainer


tqdm.pandas()

import pdb
from transformers import GenerationConfig
from transformers import AutoTokenizer, pipeline, Seq2SeqTrainingArguments
import datetime
import os
from trl import set_seed

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
logging_dir = f"/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/{date_n_time}"
os.makedirs(logging_dir)


# Define and parse arguments.
@dataclass
class ScriptArguments:
    """
    The name of the Casual LM model we wish to fine-tune with SFTTrainer
    """

    model_name: Optional[str] = field(default="gpt2", metadata={"help": "the model name"})
    dataset_name: Optional[str] = field(default="jigsaw_toxicity_pred", metadata={"help": "the dataset name"})
    dataset_text_field: Optional[str] = field(
        default="comment_text", metadata={"help": "the text field of the dataset"}
    )
    log_with: Optional[str] = field(default="tensorboard", metadata={"help": "use 'wandb' to log with wandb"})

    # learning params
    learning_rate: Optional[float] = field(default=0.00001, metadata={"help": "the learning rate"})  # default=1.41e-5
    batch_size: Optional[int] = field(default=64, metadata={"help": "the batch size"})
    # Using tokenizer max length=64 from RL4LLMs Table 4
    seq_length: Optional[int] = field(default=64, metadata={"help": "Input sequence length"})  # default=512
    gradient_accumulation_steps: Optional[int] = field(
        default=16, metadata={"help": "the number of gradient accumulation steps"}
    )
    num_train_epochs: Optional[int] = field(
        default=10, metadata={"help": "the number of training epochs"}
    )  # default=3
    max_steps: Optional[int] = field(default=-1, metadata={"help": "the number of training steps"})

    # model download, quantization and peft params
    trust_remote_code: Optional[bool] = field(default=True, metadata={"help": "Enable `trust_remote_code`"})
    use_auth_token: Optional[bool] = field(default=True, metadata={"help": "Use HF auth token to access the model"})
    load_in_8bit: Optional[bool] = field(default=False, metadata={"help": "load the model in 8 bits precision"})
    load_in_4bit: Optional[bool] = field(default=False, metadata={"help": "load the model in 4 bits precision"})
    use_peft: Optional[bool] = field(default=False, metadata={"help": "Wether to use PEFT or not to train adapters"})
    peft_lora_r: Optional[int] = field(default=64, metadata={"help": "the r parameter of the LoRA adapters"})
    peft_lora_alpha: Optional[int] = field(default=16, metadata={"help": "the alpha parameter of the LoRA adapters"})

    # logging
    output_dir: Optional[str] = field(default=logging_dir, metadata={"help": "the output directory"})
    logging_steps: Optional[int] = field(default=1, metadata={"help": "the number of logging steps"})
    save_steps: Optional[int] = field(
        default=100, metadata={"help": "Number of updates steps before two checkpoint saves"}
    )
    save_total_limit: Optional[int] = field(default=10, metadata={"help": "Limits total number of checkpoints."})

    # post processing
    push_to_hub: Optional[bool] = field(default=False, metadata={"help": "Push the model to HF Hub"})
    hub_model_id: Optional[str] = field(default=None, metadata={"help": "The name of the model on HF Hub"})

    # added by auth1
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="less", metadata={"help": "the dataset to query"})
    generation_kwargs_min_length: Optional[int] = field(
        default=-1, metadata={"help": "minimum number of tokens while generation"}
    )
    generation_kwargs_top_k: Optional[int] = field(default=0, metadata={"help": "gneration top k"})
    generation_kwargs_top_p: Optional[float] = field(default=1.0, metadata={"help": "gneration top p"})
    generation_kwargs_max_new_tokens: Optional[int] = field(default=32, metadata={"help": "gneration top p"})

    change_tokenizer_args: bool = field(default=False, metadata={"help": "whether to use modify tokenizer settings"})
    tokenizer_kwargs: dict = field(
        default_factory=lambda: {
            "padding_side": "left",
            "truncation_side": "left",
            "pad_token_as_eos_token": True,
            "max_length": 64,
        }
    )
    exp_name: str = field(default="sft-positive", metadata={"help": "experiment name"})
    seed: Optional[int] = field(default=0, metadata={"help": "seed"})


parser = HfArgumentParser(ScriptArguments)
script_args = parser.parse_args_into_dataclasses()[0]

set_seed(script_args.seed)

generation_kwargs = {
    "min_length": script_args.generation_kwargs_min_length,
    "top_k": script_args.generation_kwargs_top_k,
    "top_p": script_args.generation_kwargs_top_p,
    "do_sample": True,
    "max_new_tokens": script_args.generation_kwargs_max_new_tokens,
}

# Step 1: Load the model
if script_args.load_in_8bit and script_args.load_in_4bit:
    raise ValueError("You can't load the model in 8 bits and 4 bits at the same time")
elif script_args.load_in_8bit or script_args.load_in_4bit:
    quantization_config = BitsAndBytesConfig(
        load_in_8bit=script_args.load_in_8bit, load_in_4bit=script_args.load_in_4bit
    )
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}
    torch_dtype = torch.bfloat16
else:
    device_map = None
    quantization_config = None
    torch_dtype = None

model = AutoModelForCausalLM.from_pretrained(
    script_args.model_name,
    quantization_config=quantization_config,
    device_map=device_map,
    trust_remote_code=script_args.trust_remote_code,
    torch_dtype=torch_dtype,
)

# Removing huggingface authorization token param from model loading command above
#    use_auth_token=script_args.use_auth_token,
# Make sure you have logged in to the Hugging Face Hub using huggingface-cli login
# or by calling huggingface_hub.login() in Python. This will save an authentication
# token that Transformers will pick up automatically.

# Step 2: Load the dataset
df = pd.read_csv(
    "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/jigsaw_train_70_30.csv",
)
df = df[df["toxic"] == 0]  # SFT only on positive(non-toxic) dataset
dataset = Dataset.from_pandas(df)


# Balance the dataset

# Filter dataset for only positive reviews
# dataset = dataset.filter(lambda x: x["label"] == 1, batched=False)


# Step 3: Define generation arguments
# generation_kwargs = GenerationConfig(
#     min_length = 48,
#     top_k = 50,
#     top_p = 1.0,
#     do_sample = True,
#     max_new_tokens = 48,
# )
# pad_token_id = tokenizer.eos_token_id,

# Step 3: Define the training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir=script_args.output_dir,
    per_device_train_batch_size=script_args.batch_size,
    gradient_accumulation_steps=script_args.gradient_accumulation_steps,
    learning_rate=script_args.learning_rate,
    logging_steps=script_args.logging_steps,
    num_train_epochs=script_args.num_train_epochs,
    max_steps=script_args.max_steps,
    report_to=script_args.log_with,
    save_steps=script_args.save_steps,
    save_total_limit=script_args.save_total_limit,
    push_to_hub=script_args.push_to_hub,
    hub_model_id=script_args.hub_model_id,
    generation_config=generation_kwargs,
)

# Step 4: Define the LoraConfig
if script_args.use_peft:
    peft_config = LoraConfig(
        r=script_args.peft_lora_r,
        lora_alpha=script_args.peft_lora_alpha,
        bias="none",
        task_type="CAUSAL_LM",
    )
else:
    peft_config = None


# trl sft trainer default tokenizer and pad_token
"""
if tokenizer is None:
            tokenizer = AutoTokenizer.from_pretrained(model.config._name_or_path)
            if getattr(tokenizer, "pad_token", None) is None:
                tokenizer.pad_token = tokenizer.eos_token

if max_seq_length is None:
            # to overcome some issues with broken tokenizers
            max_seq_length = min(tokenizer.model_max_length, 1024)

            warnings.warn(
                f"You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to {max_seq_length}"
            )

"""

# Looks like padding_side = "right" by default
# Hence, defining our tokenizer here
tokenizer = AutoTokenizer.from_pretrained(script_args.model_name)
tokenizer.pad_token = tokenizer.eos_token

if script_args.change_tokenizer_args == True:
    tokenizer.padding_side = script_args.tokenizer_kwargs["padding_side"]
    tokenizer.truncation_side = script_args.tokenizer_kwargs["truncation_side"]
    tokenizer.pad_token_as_eos_token = script_args.tokenizer_kwargs["pad_token_as_eos_token"]
    tokenizer.max_length = script_args.tokenizer_kwargs["max_length"]

# tokenizer.padding_side = "left"
# tokenizer.truncation_side = "left"
# tokenizer.pad_token_as_eos_token = True
# tokenizer.max_length = 64

# Step 5: Define the Trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    max_seq_length=script_args.seq_length,
    train_dataset=dataset,
    dataset_text_field=script_args.dataset_text_field,
    peft_config=peft_config,
    tokenizer=tokenizer,
)

trainer.train()

# Step 6: Save the model
trainer.save_model(logging_dir)

File Path: examples/Jigsaw/training/soft_risk_ppo_auth2_2.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset, Dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import (
    AutoModelForCausalLMWithValueHead,
    AutoModelForSeq2SeqLMWithValueHead,
    PPOConfig,
    set_seed,
)  # ,PPOTrainer
from trl.trainer.ppo_trainer import PPOTrainer
from trl.core import LengthSampler


tqdm.pandas()

import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import pdb
import pandas as pd
import math

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
logging_dir = f"/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/{date_n_time}"
os.makedirs(logging_dir)


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="gpt2",
            # model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-20_20-32-35",  # sft_both
            model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-31_18-06-18",  # sft_positive seed2 70_30
            query_dataset="jigsaw_toxicity_pred",
            reward_model="unitary/toxic-bert",
            learning_rate=1.41e-5,
            # log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            # adap_kl_ctrl=False,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            steps=192000,
            exp_name="ppo",
        )
    )
    query_dataset: str = field(default="jigsaw_toxicity_pred", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="less", metadata={"help": "the dataset to query"})

    generation_kwargs_min_length: Optional[int] = field(
        default=-1, metadata={"help": "minimum number of tokens while generation"}
    )
    generation_kwargs_top_k: Optional[int] = field(default=0, metadata={"help": "gneration top k"})
    generation_kwargs_top_p: Optional[float] = field(default=1.0, metadata={"help": "gneration top p"})
    generation_kwargs_max_new_tokens: Optional[int] = field(default=32, metadata={"help": "gneration top p"})

    change_tokenizer_args: bool = field(default=False, metadata={"help": "whether to use modify tokenizer settings"})
    tokenizer_kwargs: dict = field(
        default_factory=lambda: {
            "padding_side": "left",
            "truncation_side": "left",
            "pad_token_as_eos_token": True,
            "max_length": 64,
        }
    )
    risk_scheduler: str = field(default="old", metadata={"help": "old risk scheduler that doesn't go down to alpha"})
    risk_n: int = field(
        default=240, metadata={"help": "240 (> batch size): no RA; 70: RA begins after iter 70; 1: RA throughout"}
    )
    risk_alpha: Optional[float] = field(default=0.2, metadata={"help": "risk alpha value = 20 percent"})
    risk_rho: Optional[float] = field(
        default=0.8, metadata={"help": "risk alpha reaches a value = 20 percent at 80 percent of total iterations"}
    )
    prompt_len: Optional[int] = field(default=32, metadata={"help": "prompt length"})


args = tyro.cli(ScriptArguments)


generation_kwargs = {
    "min_length": args.generation_kwargs_min_length,
    "top_k": args.generation_kwargs_top_k,
    "top_p": args.generation_kwargs_top_p,
    "do_sample": True,
    "max_new_tokens": args.generation_kwargs_max_new_tokens,
}

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # Kaggle datasets have to be downloaded manually (using Kaggle API)
    # datasets.load_dataset("jigsaw_toxicity_pred", data_dir="<path/to/manual/data>")
    # ds = load_dataset(
    #     query_dataset,
    #     data_dir="/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/original_dataset",
    #     split="train",
    # )  # Hardcoding path for now TODO: change this

    # ds = ds.rename_columns({"comment_text": "review"})
    # ds = ds.filter(lambda x: len(x["review"]) > 64, batched=False)

    # df = pd.DataFrame(ds)
    # num_toxic = df["toxic"].sum()

    # toxic_df = df[df["toxic"] == True]
    # non_toxic_df = df[df["toxic"] == False]

    # non_toxic_df = non_toxic_df.sample(n=num_toxic, random_state=config.seed)

    # # Recombine into dataset
    # ds = ds.from_pandas(pd.concat([toxic_df, non_toxic_df], ignore_index=True))
    df = pd.read_csv(
        "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/jigsaw_train_70_30.csv",
    )
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"comment_text": "review"})

    # pdb.set_trace()

    if args.input_query_size == "more":

        def input_size():
            return args.prompt_len

    else:
        input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    args.ppo_config.model_name,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id

# pdb.set_trace()

if args.change_tokenizer_args == True:
    tokenizer.padding_side = args.tokenizer_kwargs["padding_side"]
    tokenizer.truncation_side = args.tokenizer_kwargs["truncation_side"]
    tokenizer.pad_token_as_eos_token = args.tokenizer_kwargs["pad_token_as_eos_token"]
    tokenizer.max_length = args.tokenizer_kwargs["max_length"]

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
task = "text-classification"
model_name = args.ppo_config.reward_model
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs["pad_token_id"] = tokenizer.eos_token_id

# empty list to store text data
text_train_data = []


def get_current_risk_level(alpha, rho, m, M):
    """
    Get current risk_level_alpha*N value

    Args:
        alpha (float):
            Risk level alpha
        rho (float):
            Risk level scheduler: 0.8 means the soft risk level reaches α after ρ = 80% of the training
        m (int):
            current training step/epoch
        M (int):
            total policy training steps

    Returns:
        alpha_N (int):
            risk_level_alpha*N value
    """
    alpha = args.risk_alpha
    rho = args.risk_rho

    if args.risk_scheduler == "old":
        if m <= args.risk_n:
            return args.ppo_config.batch_size
        else:
            return math.ceil(args.ppo_config.batch_size * max(alpha, 1 - (1 - alpha) * (m - args.risk_n) / (rho * M)))
    else:
        print("here")
        if m <= args.risk_n:
            val = args.ppo_config.batch_size
        elif m >= math.ceil(rho * M):
            val = math.ceil(alpha * args.ppo_config.batch_size)
        else:
            K = (1 - alpha) / (math.ceil(rho * M) - args.risk_n)
            val = math.ceil(args.ppo_config.batch_size * max(alpha, 1 - K * (m - args.risk_n)))
        return val


def get_current_risk_level_2(alpha, rho, m, M):
    """
    Get current risk_level_alpha*N value

    Args:
        alpha (float):
            Risk level alpha
        rho (float):
            Risk level scheduler: 0.8 means the soft risk level reaches α after ρ = 80% of the training
        m (int):
            current training step/epoch
        M (int):
            total policy training steps

    Returns:
        alpha_N (int):
            risk_level_alpha*N value
    """
    alpha = 0.2
    rho = 0.8
    n = 70
    M = 194

    if m <= n:
        val = args.ppo_config.batch_size
    elif m >= math.ceil(rho * M):
        val = alpha * 100
    else:
        K = (1 - alpha) / (math.ceil(rho * M) - n)
        val = math.ceil(args.ppo_config.batch_size * max(alpha, 1 - K * (m - n)))
    return val


def apply_risk_modification_to_batch(batch, query_tensors, response_tensors, rewards, alpha_N):
    query_tensors = list(map(lambda t: t.cpu(), query_tensors))
    response_tensors = list(map(lambda t: t.cpu(), response_tensors))
    rewards = list(map(lambda t: t.cpu(), rewards))
    # response_lengths = [response_tensors[i].shape[0] for i in response_tensors]

    step_dict = {"query_t": query_tensors, "resp_t": response_tensors, "rewards": rewards}

    # batch = {k: [t.cpu() for t in v] if isinstance(v, list) else v.cpu() for k, v in batch.items()}
    batch = {
        k: [t.cpu() for t in v]
        if isinstance(v, list) and all(isinstance(t, torch.Tensor) for t in v)
        else v.cpu()
        if isinstance(v, torch.Tensor)
        else v
        for k, v in batch.items()
    }
    # pdb.set_trace()

    df_step_dict = pd.DataFrame(step_dict)
    df_batch = pd.DataFrame(batch)

    df_step_dict = df_step_dict.sort_values(by="rewards")  # .head(alpha_N)
    df_batch = df_batch.sort_values(by="rewards")  # .head(alpha_N).drop('rewards', axis=1)
    batch = df_batch.to_dict(orient="list")
    # line #996 in ppo_trainer already does response_masks_batch[j] = torch.cat((torch.zeros_like(query_batch[j]), response_masks_batch[j]))[1:]
    # pdb.set_trace()
    # mask = [torch.cat((torch.ones_like(t, dtype =int), torch.zeros([48 - t.shape[0]], dtype =int))) for t in response_tensors]  #torch.zeros([64], dtype =int),
    mask = [torch.ones([32], dtype=int) for _ in response_tensors]  # torch.zeros([64], dtype =int),
    for ii in range(args.ppo_config.batch_size - alpha_N):
        mask[-ii - 1] = torch.zeros([32], dtype=int)

    sr_query_tensors = list(map(lambda t: t.to(device), df_step_dict["query_t"].tolist()))
    sr_response_tensors = list(map(lambda t: t.to(device), df_step_dict["resp_t"].tolist()))
    sr_rewards = list(map(lambda t: t.to(device), df_step_dict["rewards"].tolist()))
    batch = {
        k: [t.to(device) for t in v]
        if isinstance(v, list) and all(isinstance(t, torch.Tensor) for t in v)
        else v.to(device)
        if isinstance(v, torch.Tensor)
        else v
        for k, v in batch.items()
    }

    return batch, sr_query_tensors, sr_response_tensors, sr_rewards, mask


for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)

    if args.reward_function == "trl":
        rewards = [torch.tensor(-output[0]["score"]) for output in pipe_outputs]
    else:  # depreciated
        raise NotImplementedError
        rewards = [
            torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"]]))[1]
            for output in pipe_outputs
        ]

    for ii in range(3):
        data_row = {
            "iter": epoch,
            "input": batch["query"][ii],
            "output": batch["response"][ii],
            "score": rewards[ii].item(),
        }
        text_train_data.append(data_row)

    batch["rewards"] = rewards
    alpha_N_ceil = get_current_risk_level(PPOConfig.soft_risk_alpha, PPOConfig.risk_level_scheduler, epoch, 288)
    # sr_batch, sr_query_t, sr_resp_t, sr_rewards, mask = apply_risk_modification_to_batch(batch, query_tensors, response_tensors, rewards, alpha_N_ceil)

    # Run PPO step
    print(alpha_N_ceil)
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards, alpha_N_ceil)
    ppo_trainer.log_stats(stats, batch, rewards)
    print(epoch)

ppo_trainer._save_pretrained(logging_dir + "/save_pretrained")
df = pd.DataFrame(text_train_data)
df.to_csv(logging_dir + "/textual_data.csv", index=False)

File Path: examples/Jigsaw/training/soft_risk_ppo_auth2_seed12.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset, Dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import (
    AutoModelForCausalLMWithValueHead,
    AutoModelForSeq2SeqLMWithValueHead,
    PPOConfig,
    set_seed,
)  # ,PPOTrainer
from trl.trainer.ppo_trainer import PPOTrainer
from trl.core import LengthSampler


tqdm.pandas()

import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import pdb
import pandas as pd
import math

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
logging_dir = f"/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/{date_n_time}"
os.makedirs(logging_dir)


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="gpt2",
            # model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-20_20-32-35",  # sft_both
            model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-30_00-30-38",  # sft_positive seed 12
            query_dataset="jigsaw_toxicity_pred",
            reward_model="unitary/toxic-bert",
            learning_rate=1.41e-5,
            # log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            # adap_kl_ctrl=False,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            steps=192000,
            exp_name="ppo",
        )
    )
    query_dataset: str = field(default="jigsaw_toxicity_pred", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="less", metadata={"help": "the dataset to query"})

    generation_kwargs_min_length: Optional[int] = field(
        default=-1, metadata={"help": "minimum number of tokens while generation"}
    )
    generation_kwargs_top_k: Optional[int] = field(default=0, metadata={"help": "gneration top k"})
    generation_kwargs_top_p: Optional[float] = field(default=1.0, metadata={"help": "gneration top p"})
    generation_kwargs_max_new_tokens: Optional[int] = field(default=32, metadata={"help": "gneration top p"})

    change_tokenizer_args: bool = field(default=False, metadata={"help": "whether to use modify tokenizer settings"})
    tokenizer_kwargs: dict = field(
        default_factory=lambda: {
            "padding_side": "left",
            "truncation_side": "left",
            "pad_token_as_eos_token": True,
            "max_length": 64,
        }
    )
    risk_scheduler: str = field(default="old", metadata={"help": "old risk scheduler that doesn't go down to alpha"})
    risk_n: int = field(
        default=240, metadata={"help": "240 (> batch size): no RA; 70: RA begins after iter 70; 1: RA throughout"}
    )
    risk_alpha: Optional[float] = field(default=0.2, metadata={"help": "risk alpha value = 20 percent"})
    risk_rho: Optional[float] = field(
        default=0.8, metadata={"help": "risk alpha reaches a value = 20 percent at 80 percent of total iterations"}
    )
    prompt_len: Optional[int] = field(default=32, metadata={"help": "prompt length"})


args = tyro.cli(ScriptArguments)


generation_kwargs = {
    "min_length": args.generation_kwargs_min_length,
    "top_k": args.generation_kwargs_top_k,
    "top_p": args.generation_kwargs_top_p,
    "do_sample": True,
    "max_new_tokens": args.generation_kwargs_max_new_tokens,
}

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # Kaggle datasets have to be downloaded manually (using Kaggle API)
    # datasets.load_dataset("jigsaw_toxicity_pred", data_dir="<path/to/manual/data>")
    # ds = load_dataset(
    #     query_dataset,
    #     data_dir="/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/original_dataset",
    #     split="train",
    # )  # Hardcoding path for now TODO: change this

    # ds = ds.rename_columns({"comment_text": "review"})
    # ds = ds.filter(lambda x: len(x["review"]) > 64, batched=False)

    # df = pd.DataFrame(ds)
    # num_toxic = df["toxic"].sum()

    # toxic_df = df[df["toxic"] == True]
    # non_toxic_df = df[df["toxic"] == False]

    # non_toxic_df = non_toxic_df.sample(n=num_toxic, random_state=config.seed)

    # # Recombine into dataset
    # ds = ds.from_pandas(pd.concat([toxic_df, non_toxic_df], ignore_index=True))
    df = pd.read_csv(
        "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/jigsaw_train.csv",
    )
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"comment_text": "review"})

    # pdb.set_trace()

    if args.input_query_size == "more":

        def input_size():
            return args.prompt_len

    else:
        input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    args.ppo_config.model_name,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id

# pdb.set_trace()

if args.change_tokenizer_args == True:
    tokenizer.padding_side = args.tokenizer_kwargs["padding_side"]
    tokenizer.truncation_side = args.tokenizer_kwargs["truncation_side"]
    tokenizer.pad_token_as_eos_token = args.tokenizer_kwargs["pad_token_as_eos_token"]
    tokenizer.max_length = args.tokenizer_kwargs["max_length"]

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
task = "text-classification"
model_name = args.ppo_config.reward_model
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs["pad_token_id"] = tokenizer.eos_token_id

# empty list to store text data
text_train_data = []


def get_current_risk_level(alpha, rho, m, M):
    """
    Get current risk_level_alpha*N value

    Args:
        alpha (float):
            Risk level alpha
        rho (float):
            Risk level scheduler: 0.8 means the soft risk level reaches α after ρ = 80% of the training
        m (int):
            current training step/epoch
        M (int):
            total policy training steps

    Returns:
        alpha_N (int):
            risk_level_alpha*N value
    """
    alpha = args.risk_alpha
    rho = args.risk_rho

    if args.risk_scheduler == "old":
        if m <= args.risk_n:
            return args.ppo_config.batch_size
        else:
            return math.ceil(args.ppo_config.batch_size * max(alpha, 1 - (1 - alpha) * (m - args.risk_n) / (rho * M)))
    else:
        print("here")
        if m <= args.risk_n:
            val = args.ppo_config.batch_size
        elif m >= math.ceil(rho * M):
            val = math.ceil(alpha * args.ppo_config.batch_size)
        else:
            K = (1 - alpha) / (math.ceil(rho * M) - args.risk_n)
            val = math.ceil(args.ppo_config.batch_size * max(alpha, 1 - K * (m - args.risk_n)))
        return val


def get_current_risk_level_2(alpha, rho, m, M):
    """
    Get current risk_level_alpha*N value

    Args:
        alpha (float):
            Risk level alpha
        rho (float):
            Risk level scheduler: 0.8 means the soft risk level reaches α after ρ = 80% of the training
        m (int):
            current training step/epoch
        M (int):
            total policy training steps

    Returns:
        alpha_N (int):
            risk_level_alpha*N value
    """
    alpha = 0.2
    rho = 0.8
    n = 70
    M = 194

    if m <= n:
        val = args.ppo_config.batch_size
    elif m >= math.ceil(rho * M):
        val = alpha * 100
    else:
        K = (1 - alpha) / (math.ceil(rho * M) - n)
        val = math.ceil(args.ppo_config.batch_size * max(alpha, 1 - K * (m - n)))
    return val


def apply_risk_modification_to_batch(batch, query_tensors, response_tensors, rewards, alpha_N):
    query_tensors = list(map(lambda t: t.cpu(), query_tensors))
    response_tensors = list(map(lambda t: t.cpu(), response_tensors))
    rewards = list(map(lambda t: t.cpu(), rewards))
    # response_lengths = [response_tensors[i].shape[0] for i in response_tensors]

    step_dict = {"query_t": query_tensors, "resp_t": response_tensors, "rewards": rewards}

    # batch = {k: [t.cpu() for t in v] if isinstance(v, list) else v.cpu() for k, v in batch.items()}
    batch = {
        k: [t.cpu() for t in v]
        if isinstance(v, list) and all(isinstance(t, torch.Tensor) for t in v)
        else v.cpu()
        if isinstance(v, torch.Tensor)
        else v
        for k, v in batch.items()
    }
    # pdb.set_trace()

    df_step_dict = pd.DataFrame(step_dict)
    df_batch = pd.DataFrame(batch)

    df_step_dict = df_step_dict.sort_values(by="rewards")  # .head(alpha_N)
    df_batch = df_batch.sort_values(by="rewards")  # .head(alpha_N).drop('rewards', axis=1)
    batch = df_batch.to_dict(orient="list")
    # line #996 in ppo_trainer already does response_masks_batch[j] = torch.cat((torch.zeros_like(query_batch[j]), response_masks_batch[j]))[1:]
    # pdb.set_trace()
    # mask = [torch.cat((torch.ones_like(t, dtype =int), torch.zeros([48 - t.shape[0]], dtype =int))) for t in response_tensors]  #torch.zeros([64], dtype =int),
    mask = [torch.ones([32], dtype=int) for _ in response_tensors]  # torch.zeros([64], dtype =int),
    for ii in range(args.ppo_config.batch_size - alpha_N):
        mask[-ii - 1] = torch.zeros([32], dtype=int)

    sr_query_tensors = list(map(lambda t: t.to(device), df_step_dict["query_t"].tolist()))
    sr_response_tensors = list(map(lambda t: t.to(device), df_step_dict["resp_t"].tolist()))
    sr_rewards = list(map(lambda t: t.to(device), df_step_dict["rewards"].tolist()))
    batch = {
        k: [t.to(device) for t in v]
        if isinstance(v, list) and all(isinstance(t, torch.Tensor) for t in v)
        else v.to(device)
        if isinstance(v, torch.Tensor)
        else v
        for k, v in batch.items()
    }

    return batch, sr_query_tensors, sr_response_tensors, sr_rewards, mask


for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)

    if args.reward_function == "trl":
        rewards = [torch.tensor(-output[0]["score"]) for output in pipe_outputs]
    else:  # depreciated
        raise NotImplementedError
        rewards = [
            torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"]]))[1]
            for output in pipe_outputs
        ]

    for ii in range(3):
        data_row = {
            "iter": epoch,
            "input": batch["query"][ii],
            "output": batch["response"][ii],
            "score": rewards[ii].item(),
        }
        text_train_data.append(data_row)

    batch["rewards"] = rewards
    alpha_N_ceil = get_current_risk_level(PPOConfig.soft_risk_alpha, PPOConfig.risk_level_scheduler, epoch, 264)
    # sr_batch, sr_query_t, sr_resp_t, sr_rewards, mask = apply_risk_modification_to_batch(batch, query_tensors, response_tensors, rewards, alpha_N_ceil)

    # Run PPO step
    print(alpha_N_ceil)
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards, alpha_N_ceil)
    ppo_trainer.log_stats(stats, batch, rewards)
    print(epoch)

ppo_trainer._save_pretrained(logging_dir + "/save_pretrained")
df = pd.DataFrame(text_train_data)
df.to_csv(logging_dir + "/textual_data.csv", index=False)

File Path: examples/Jigsaw/training/soft_risk_ppo_auth2_seed2.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset, Dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import (
    AutoModelForCausalLMWithValueHead,
    AutoModelForSeq2SeqLMWithValueHead,
    PPOConfig,
    set_seed,
)  # ,PPOTrainer
from trl.trainer.ppo_trainer import PPOTrainer
from trl.core import LengthSampler


tqdm.pandas()

import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import pdb
import pandas as pd
import math

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
logging_dir = f"/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/{date_n_time}"
os.makedirs(logging_dir)


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="gpt2",
            # model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-20_20-32-35",  # sft_both
            model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-31_18-06-18",  # sft_positive seed 2
            query_dataset="jigsaw_toxicity_pred",
            reward_model="unitary/toxic-bert",
            learning_rate=1.41e-5,
            # log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            # adap_kl_ctrl=False,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            steps=192000,
            exp_name="ppo",
        )
    )
    query_dataset: str = field(default="jigsaw_toxicity_pred", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="less", metadata={"help": "the dataset to query"})

    generation_kwargs_min_length: Optional[int] = field(
        default=-1, metadata={"help": "minimum number of tokens while generation"}
    )
    generation_kwargs_top_k: Optional[int] = field(default=0, metadata={"help": "gneration top k"})
    generation_kwargs_top_p: Optional[float] = field(default=1.0, metadata={"help": "gneration top p"})
    generation_kwargs_max_new_tokens: Optional[int] = field(default=32, metadata={"help": "gneration top p"})

    change_tokenizer_args: bool = field(default=False, metadata={"help": "whether to use modify tokenizer settings"})
    tokenizer_kwargs: dict = field(
        default_factory=lambda: {
            "padding_side": "left",
            "truncation_side": "left",
            "pad_token_as_eos_token": True,
            "max_length": 64,
        }
    )
    risk_scheduler: str = field(default="old", metadata={"help": "old risk scheduler that doesn't go down to alpha"})
    risk_n: int = field(
        default=240, metadata={"help": "240 (> batch size): no RA; 70: RA begins after iter 70; 1: RA throughout"}
    )
    risk_alpha: Optional[float] = field(default=0.2, metadata={"help": "risk alpha value = 20 percent"})
    risk_rho: Optional[float] = field(
        default=0.8, metadata={"help": "risk alpha reaches a value = 20 percent at 80 percent of total iterations"}
    )
    prompt_len: Optional[int] = field(default=32, metadata={"help": "prompt length"})


args = tyro.cli(ScriptArguments)


generation_kwargs = {
    "min_length": args.generation_kwargs_min_length,
    "top_k": args.generation_kwargs_top_k,
    "top_p": args.generation_kwargs_top_p,
    "do_sample": True,
    "max_new_tokens": args.generation_kwargs_max_new_tokens,
}

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # Kaggle datasets have to be downloaded manually (using Kaggle API)
    # datasets.load_dataset("jigsaw_toxicity_pred", data_dir="<path/to/manual/data>")
    # ds = load_dataset(
    #     query_dataset,
    #     data_dir="/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/original_dataset",
    #     split="train",
    # )  # Hardcoding path for now TODO: change this

    # ds = ds.rename_columns({"comment_text": "review"})
    # ds = ds.filter(lambda x: len(x["review"]) > 64, batched=False)

    # df = pd.DataFrame(ds)
    # num_toxic = df["toxic"].sum()

    # toxic_df = df[df["toxic"] == True]
    # non_toxic_df = df[df["toxic"] == False]

    # non_toxic_df = non_toxic_df.sample(n=num_toxic, random_state=config.seed)

    # # Recombine into dataset
    # ds = ds.from_pandas(pd.concat([toxic_df, non_toxic_df], ignore_index=True))
    df = pd.read_csv(
        "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/jigsaw_train_70_30.csv",
    )
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"comment_text": "review"})

    # pdb.set_trace()

    if args.input_query_size == "more":

        def input_size():
            return args.prompt_len

    else:
        input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    args.ppo_config.model_name,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id

# pdb.set_trace()

if args.change_tokenizer_args == True:
    tokenizer.padding_side = args.tokenizer_kwargs["padding_side"]
    tokenizer.truncation_side = args.tokenizer_kwargs["truncation_side"]
    tokenizer.pad_token_as_eos_token = args.tokenizer_kwargs["pad_token_as_eos_token"]
    tokenizer.max_length = args.tokenizer_kwargs["max_length"]

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
task = "text-classification"
model_name = args.ppo_config.reward_model
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs["pad_token_id"] = tokenizer.eos_token_id

# empty list to store text data
text_train_data = []


def get_current_risk_level(alpha, rho, m, M):
    """
    Get current risk_level_alpha*N value

    Args:
        alpha (float):
            Risk level alpha
        rho (float):
            Risk level scheduler: 0.8 means the soft risk level reaches α after ρ = 80% of the training
        m (int):
            current training step/epoch
        M (int):
            total policy training steps

    Returns:
        alpha_N (int):
            risk_level_alpha*N value
    """
    alpha = args.risk_alpha
    rho = args.risk_rho

    if args.risk_scheduler == "old":
        if m <= args.risk_n:
            return args.ppo_config.batch_size
        else:
            return math.ceil(args.ppo_config.batch_size * max(alpha, 1 - (1 - alpha) * (m - args.risk_n) / (rho * M)))
    else:
        print("here")
        if m <= args.risk_n:
            val = args.ppo_config.batch_size
        elif m >= math.ceil(rho * M):
            val = math.ceil(alpha * args.ppo_config.batch_size)
        else:
            K = (1 - alpha) / (math.ceil(rho * M) - args.risk_n)
            val = math.ceil(args.ppo_config.batch_size * max(alpha, 1 - K * (m - args.risk_n)))
        return val


def get_current_risk_level_2(alpha, rho, m, M):
    """
    Get current risk_level_alpha*N value

    Args:
        alpha (float):
            Risk level alpha
        rho (float):
            Risk level scheduler: 0.8 means the soft risk level reaches α after ρ = 80% of the training
        m (int):
            current training step/epoch
        M (int):
            total policy training steps

    Returns:
        alpha_N (int):
            risk_level_alpha*N value
    """
    alpha = 0.2
    rho = 0.8
    n = 70
    M = 194

    if m <= n:
        val = args.ppo_config.batch_size
    elif m >= math.ceil(rho * M):
        val = alpha * 100
    else:
        K = (1 - alpha) / (math.ceil(rho * M) - n)
        val = math.ceil(args.ppo_config.batch_size * max(alpha, 1 - K * (m - n)))
    return val


def apply_risk_modification_to_batch(batch, query_tensors, response_tensors, rewards, alpha_N):
    query_tensors = list(map(lambda t: t.cpu(), query_tensors))
    response_tensors = list(map(lambda t: t.cpu(), response_tensors))
    rewards = list(map(lambda t: t.cpu(), rewards))
    # response_lengths = [response_tensors[i].shape[0] for i in response_tensors]

    step_dict = {"query_t": query_tensors, "resp_t": response_tensors, "rewards": rewards}

    # batch = {k: [t.cpu() for t in v] if isinstance(v, list) else v.cpu() for k, v in batch.items()}
    batch = {
        k: [t.cpu() for t in v]
        if isinstance(v, list) and all(isinstance(t, torch.Tensor) for t in v)
        else v.cpu()
        if isinstance(v, torch.Tensor)
        else v
        for k, v in batch.items()
    }
    # pdb.set_trace()

    df_step_dict = pd.DataFrame(step_dict)
    df_batch = pd.DataFrame(batch)

    df_step_dict = df_step_dict.sort_values(by="rewards")  # .head(alpha_N)
    df_batch = df_batch.sort_values(by="rewards")  # .head(alpha_N).drop('rewards', axis=1)
    batch = df_batch.to_dict(orient="list")
    # line #996 in ppo_trainer already does response_masks_batch[j] = torch.cat((torch.zeros_like(query_batch[j]), response_masks_batch[j]))[1:]
    # pdb.set_trace()
    # mask = [torch.cat((torch.ones_like(t, dtype =int), torch.zeros([48 - t.shape[0]], dtype =int))) for t in response_tensors]  #torch.zeros([64], dtype =int),
    mask = [torch.ones([32], dtype=int) for _ in response_tensors]  # torch.zeros([64], dtype =int),
    for ii in range(args.ppo_config.batch_size - alpha_N):
        mask[-ii - 1] = torch.zeros([32], dtype=int)

    sr_query_tensors = list(map(lambda t: t.to(device), df_step_dict["query_t"].tolist()))
    sr_response_tensors = list(map(lambda t: t.to(device), df_step_dict["resp_t"].tolist()))
    sr_rewards = list(map(lambda t: t.to(device), df_step_dict["rewards"].tolist()))
    batch = {
        k: [t.to(device) for t in v]
        if isinstance(v, list) and all(isinstance(t, torch.Tensor) for t in v)
        else v.to(device)
        if isinstance(v, torch.Tensor)
        else v
        for k, v in batch.items()
    }

    return batch, sr_query_tensors, sr_response_tensors, sr_rewards, mask


for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs, truncation=True)

    if args.reward_function == "trl":
        rewards = [torch.tensor(-output[0]["score"]) for output in pipe_outputs]
    else:  # depreciated
        raise NotImplementedError
        rewards = [
            torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"]]))[1]
            for output in pipe_outputs
        ]

    for ii in range(3):
        data_row = {
            "iter": epoch,
            "input": batch["query"][ii],
            "output": batch["response"][ii],
            "score": rewards[ii].item(),
        }
        text_train_data.append(data_row)

    batch["rewards"] = rewards
    alpha_N_ceil = get_current_risk_level(PPOConfig.soft_risk_alpha, PPOConfig.risk_level_scheduler, epoch, 264)
    # sr_batch, sr_query_t, sr_resp_t, sr_rewards, mask = apply_risk_modification_to_batch(batch, query_tensors, response_tensors, rewards, alpha_N_ceil)

    # Run PPO step
    print(alpha_N_ceil)
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards, alpha_N_ceil)
    ppo_trainer.log_stats(stats, batch, rewards)
    print(epoch)

ppo_trainer._save_pretrained(logging_dir + "/save_pretrained")
df = pd.DataFrame(text_train_data)
df.to_csv(logging_dir + "/textual_data.csv", index=False)

File Path: examples/Jigsaw/training/soft_risk_ppo_auth2_seed36.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset, Dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import (
    AutoModelForCausalLMWithValueHead,
    AutoModelForSeq2SeqLMWithValueHead,
    PPOConfig,
    set_seed,
)  # ,PPOTrainer
from trl.trainer.ppo_trainer import PPOTrainer
from trl.core import LengthSampler


tqdm.pandas()

import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import pdb
import pandas as pd
import math

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
logging_dir = f"/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/{date_n_time}"
os.makedirs(logging_dir)


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="gpt2",
            # model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-20_20-32-35",  # sft_both
            model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-30_00-24-10",  # sft_positive seed 36
            query_dataset="jigsaw_toxicity_pred",
            reward_model="unitary/toxic-bert",
            learning_rate=1.41e-5,
            # log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            # adap_kl_ctrl=False,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            steps=192000,
            exp_name="ppo",
        )
    )
    query_dataset: str = field(default="jigsaw_toxicity_pred", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="less", metadata={"help": "the dataset to query"})

    generation_kwargs_min_length: Optional[int] = field(
        default=-1, metadata={"help": "minimum number of tokens while generation"}
    )
    generation_kwargs_top_k: Optional[int] = field(default=0, metadata={"help": "gneration top k"})
    generation_kwargs_top_p: Optional[float] = field(default=1.0, metadata={"help": "gneration top p"})
    generation_kwargs_max_new_tokens: Optional[int] = field(default=32, metadata={"help": "gneration top p"})

    change_tokenizer_args: bool = field(default=False, metadata={"help": "whether to use modify tokenizer settings"})
    tokenizer_kwargs: dict = field(
        default_factory=lambda: {
            "padding_side": "left",
            "truncation_side": "left",
            "pad_token_as_eos_token": True,
            "max_length": 64,
        }
    )
    risk_scheduler: str = field(default="old", metadata={"help": "old risk scheduler that doesn't go down to alpha"})
    risk_n: int = field(
        default=240, metadata={"help": "240 (> batch size): no RA; 70: RA begins after iter 70; 1: RA throughout"}
    )
    risk_alpha: Optional[float] = field(default=0.2, metadata={"help": "risk alpha value = 20 percent"})
    risk_rho: Optional[float] = field(
        default=0.8, metadata={"help": "risk alpha reaches a value = 20 percent at 80 percent of total iterations"}
    )
    prompt_len: Optional[int] = field(default=32, metadata={"help": "prompt length"})


args = tyro.cli(ScriptArguments)


generation_kwargs = {
    "min_length": args.generation_kwargs_min_length,
    "top_k": args.generation_kwargs_top_k,
    "top_p": args.generation_kwargs_top_p,
    "do_sample": True,
    "max_new_tokens": args.generation_kwargs_max_new_tokens,
}

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # Kaggle datasets have to be downloaded manually (using Kaggle API)
    # datasets.load_dataset("jigsaw_toxicity_pred", data_dir="<path/to/manual/data>")
    # ds = load_dataset(
    #     query_dataset,
    #     data_dir="/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/original_dataset",
    #     split="train",
    # )  # Hardcoding path for now TODO: change this

    # ds = ds.rename_columns({"comment_text": "review"})
    # ds = ds.filter(lambda x: len(x["review"]) > 64, batched=False)

    # df = pd.DataFrame(ds)
    # num_toxic = df["toxic"].sum()

    # toxic_df = df[df["toxic"] == True]
    # non_toxic_df = df[df["toxic"] == False]

    # non_toxic_df = non_toxic_df.sample(n=num_toxic, random_state=config.seed)

    # # Recombine into dataset
    # ds = ds.from_pandas(pd.concat([toxic_df, non_toxic_df], ignore_index=True))
    df = pd.read_csv(
        "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/jigsaw_train.csv",
    )
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"comment_text": "review"})

    # pdb.set_trace()

    if args.input_query_size == "more":

        def input_size():
            return args.prompt_len

    else:
        input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    args.ppo_config.model_name,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id

# pdb.set_trace()

if args.change_tokenizer_args == True:
    tokenizer.padding_side = args.tokenizer_kwargs["padding_side"]
    tokenizer.truncation_side = args.tokenizer_kwargs["truncation_side"]
    tokenizer.pad_token_as_eos_token = args.tokenizer_kwargs["pad_token_as_eos_token"]
    tokenizer.max_length = args.tokenizer_kwargs["max_length"]

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
task = "text-classification"
model_name = args.ppo_config.reward_model
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs["pad_token_id"] = tokenizer.eos_token_id

# empty list to store text data
text_train_data = []


def get_current_risk_level(alpha, rho, m, M):
    """
    Get current risk_level_alpha*N value

    Args:
        alpha (float):
            Risk level alpha
        rho (float):
            Risk level scheduler: 0.8 means the soft risk level reaches α after ρ = 80% of the training
        m (int):
            current training step/epoch
        M (int):
            total policy training steps

    Returns:
        alpha_N (int):
            risk_level_alpha*N value
    """
    alpha = args.risk_alpha
    rho = args.risk_rho

    if args.risk_scheduler == "old":
        if m <= args.risk_n:
            return args.ppo_config.batch_size
        else:
            return math.ceil(args.ppo_config.batch_size * max(alpha, 1 - (1 - alpha) * (m - args.risk_n) / (rho * M)))
    else:
        print("here")
        if m <= args.risk_n:
            val = args.ppo_config.batch_size
        elif m >= math.ceil(rho * M):
            val = math.ceil(alpha * args.ppo_config.batch_size)
        else:
            K = (1 - alpha) / (math.ceil(rho * M) - args.risk_n)
            val = math.ceil(args.ppo_config.batch_size * max(alpha, 1 - K * (m - args.risk_n)))
        return val


def get_current_risk_level_2(alpha, rho, m, M):
    """
    Get current risk_level_alpha*N value

    Args:
        alpha (float):
            Risk level alpha
        rho (float):
            Risk level scheduler: 0.8 means the soft risk level reaches α after ρ = 80% of the training
        m (int):
            current training step/epoch
        M (int):
            total policy training steps

    Returns:
        alpha_N (int):
            risk_level_alpha*N value
    """
    alpha = 0.2
    rho = 0.8
    n = 70
    M = 194

    if m <= n:
        val = args.ppo_config.batch_size
    elif m >= math.ceil(rho * M):
        val = alpha * 100
    else:
        K = (1 - alpha) / (math.ceil(rho * M) - n)
        val = math.ceil(args.ppo_config.batch_size * max(alpha, 1 - K * (m - n)))
    return val


def apply_risk_modification_to_batch(batch, query_tensors, response_tensors, rewards, alpha_N):
    query_tensors = list(map(lambda t: t.cpu(), query_tensors))
    response_tensors = list(map(lambda t: t.cpu(), response_tensors))
    rewards = list(map(lambda t: t.cpu(), rewards))
    # response_lengths = [response_tensors[i].shape[0] for i in response_tensors]

    step_dict = {"query_t": query_tensors, "resp_t": response_tensors, "rewards": rewards}

    # batch = {k: [t.cpu() for t in v] if isinstance(v, list) else v.cpu() for k, v in batch.items()}
    batch = {
        k: [t.cpu() for t in v]
        if isinstance(v, list) and all(isinstance(t, torch.Tensor) for t in v)
        else v.cpu()
        if isinstance(v, torch.Tensor)
        else v
        for k, v in batch.items()
    }
    # pdb.set_trace()

    df_step_dict = pd.DataFrame(step_dict)
    df_batch = pd.DataFrame(batch)

    df_step_dict = df_step_dict.sort_values(by="rewards")  # .head(alpha_N)
    df_batch = df_batch.sort_values(by="rewards")  # .head(alpha_N).drop('rewards', axis=1)
    batch = df_batch.to_dict(orient="list")
    # line #996 in ppo_trainer already does response_masks_batch[j] = torch.cat((torch.zeros_like(query_batch[j]), response_masks_batch[j]))[1:]
    # pdb.set_trace()
    # mask = [torch.cat((torch.ones_like(t, dtype =int), torch.zeros([48 - t.shape[0]], dtype =int))) for t in response_tensors]  #torch.zeros([64], dtype =int),
    mask = [torch.ones([32], dtype=int) for _ in response_tensors]  # torch.zeros([64], dtype =int),
    for ii in range(args.ppo_config.batch_size - alpha_N):
        mask[-ii - 1] = torch.zeros([32], dtype=int)

    sr_query_tensors = list(map(lambda t: t.to(device), df_step_dict["query_t"].tolist()))
    sr_response_tensors = list(map(lambda t: t.to(device), df_step_dict["resp_t"].tolist()))
    sr_rewards = list(map(lambda t: t.to(device), df_step_dict["rewards"].tolist()))
    batch = {
        k: [t.to(device) for t in v]
        if isinstance(v, list) and all(isinstance(t, torch.Tensor) for t in v)
        else v.to(device)
        if isinstance(v, torch.Tensor)
        else v
        for k, v in batch.items()
    }

    return batch, sr_query_tensors, sr_response_tensors, sr_rewards, mask


for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)

    if args.reward_function == "trl":
        rewards = [torch.tensor(-output[0]["score"]) for output in pipe_outputs]
    else:  # depreciated
        raise NotImplementedError
        rewards = [
            torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"]]))[1]
            for output in pipe_outputs
        ]

    for ii in range(3):
        data_row = {
            "iter": epoch,
            "input": batch["query"][ii],
            "output": batch["response"][ii],
            "score": rewards[ii].item(),
        }
        text_train_data.append(data_row)

    batch["rewards"] = rewards
    alpha_N_ceil = get_current_risk_level(PPOConfig.soft_risk_alpha, PPOConfig.risk_level_scheduler, epoch, 264)
    # sr_batch, sr_query_t, sr_resp_t, sr_rewards, mask = apply_risk_modification_to_batch(batch, query_tensors, response_tensors, rewards, alpha_N_ceil)

    # Run PPO step
    print(alpha_N_ceil)
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards, alpha_N_ceil)
    ppo_trainer.log_stats(stats, batch, rewards)
    print(epoch)

ppo_trainer._save_pretrained(logging_dir + "/save_pretrained")
df = pd.DataFrame(text_train_data)
df.to_csv(logging_dir + "/textual_data.csv", index=False)

File Path: examples/Jigsaw/training/sr_ppo_big_jigsaw.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
from datasets import load_dataset, Dataset
from torch.optim import Adam
from tqdm import tqdm
from transformers import (
    AutoModelForSequenceClassification,
    AutoModelForCausalLM,
    AutoTokenizer,
    HfArgumentParser,
    RobertaForSequenceClassification,
    RobertaTokenizer,
    DistilBertForSequenceClassification,
    DistilBertTokenizerFast,
)
from peft import LoraConfig
from trl import (
    AutoModelForCausalLMWithValueHead,
    PPOConfig,
    create_reference_model,
    set_seed,
)
from trl.trainer.ppo_trainer import PPOTrainer
from trl.core import LengthSampler
import datetime
import os
import pandas as pd

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")

logging_dir = f"/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/big/jigsaw/{date_n_time}"
os.makedirs(logging_dir, exist_ok=True)

tqdm.pandas()

########################################################################
# This is a fully working simple example to use trl with accelerate.
#
# This example fine-tunes a GPTJ model to generate less toxic contents
# by using allenai/real-toxicity-prompts dataset. We use PPO
#  (proximal policy optimization) to optimize the model.
# in any of the following settings (with the same script):
#   - single CPU or single GPU
#   - multi GPUS (using PyTorch distributed mode)
#   - multi GPUS (using DeepSpeed ZeRO-Offload stages 1 & 2)
#   - fp16 (mixed-precision) or fp32 (normal precision)
#
# To run it in each of these various modes, first initialize the accelerate
# configuration with `accelerate config`
#
########################################################################


# We first define the configuration of the experiment, defining the model, the dataset,
# the training parameters, and the PPO parameters.
# Check the default arguments in the `PPOConfig` class for more details.
# If you want to log with tensorboard, add the kwarg
# `project_kwargs={"logging_dir": PATH_TO_LOGS}` to the PPOConfig.
@dataclass
class ScriptArguments:
    """
    The name of the Casual LM model we wish to fine-tune with PPO
    """

    # NOTE: gpt2 models use Conv1D instead of Linear layers which are not yet supported in 8 bit mode
    # models like gpt-neo* models are more suitable.
    model_name: Optional[str] = field(default="ybelkada/gpt-j-6b-sharded-bf16", metadata={"help": "the model name"})
    log_with: Optional[str] = field(default="tensorboard", metadata={"help": "use 'wandb' to log with wandb"})
    learning_rate: Optional[float] = field(default=(1.47e-5) * 2, metadata={"help": "the learning rate"})
    mini_batch_size: Optional[int] = field(default=8, metadata={"help": "the PPO minibatch size"})
    batch_size: Optional[int] = field(default=8, metadata={"help": "the batch size"})
    gradient_accumulation_steps: Optional[int] = field(
        default=1, metadata={"help": "the number of gradient accumulation steps"}
    )
    seed: Optional[int] = field(default=42, metadata={"help": "the seed for the experiment"})
    exp_name: Optional[str] = field(default="jigsaw", metadata={"help": "the name of the experiment"})
    query_dataset: Optional[str] = field(default="jigsaw", metadata={"help": "the name of the dataset"})
    risk_n: int = field(
        default=500, metadata={"help": "240 (> batch size): no RA; 70: RA begins after iter 70; 1: RA throughout"}
    )
    risk_alpha: Optional[float] = field(default=0.2, metadata={"help": "risk alpha value = 20 percent"})
    risk_rho: Optional[float] = field(
        default=0.95, metadata={"help": "risk alpha reaches a value = 20 percent at 80 percent of total iterations"}
    )


model_save_path = os.path.join(logging_dir, "save_pretrained")
prompt_len = 8
gen_len = 32

parser = HfArgumentParser(ScriptArguments)
script_args = parser.parse_args_into_dataclasses()[0]

config = PPOConfig(
    model_name=script_args.model_name,
    learning_rate=script_args.learning_rate,
    log_with=script_args.log_with,
    project_kwargs={"logging_dir": logging_dir},
    # ppo_epochs=100,
    mini_batch_size=script_args.mini_batch_size,
    batch_size=script_args.batch_size,
    gradient_accumulation_steps=script_args.gradient_accumulation_steps,
    seed=script_args.seed,
    exp_name=script_args.exp_name,
)

peft_config = LoraConfig(
    r=16,
    lora_alpha=16,
    bias="none",
    task_type="CAUSAL_LM",
)


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.


def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    df = pd.read_csv(
        "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/jigsaw_train_70_30.csv",
    )
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"comment_text": "review"})

    # pdb.set_trace()

    def input_size():
        return prompt_len

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(config, script_args.query_dataset)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(config.seed)

# Now let's build the model, the reference model, and the tokenizer. We first load the model
# in bfloat16 to save memory using `transformers`.
model = AutoModelForCausalLM.from_pretrained(config.model_name, torch_dtype=torch.bfloat16)
# And then we pass the loaded model to `AutoModelForCausalLMWithValueHead`.
model = AutoModelForCausalLMWithValueHead.from_pretrained(model, peft_config=peft_config)

# We create a reference model by sharing 20 layers
# ref_model = create_reference_model(model, num_shared_layers=20)
ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(
    "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-31_18-06-18",
    trust_remote_code=True,
)  # sft positive 70_30 gpt2

# We make sure to use `Adam` optimizer on the model parameters that require gradients.
optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=config.learning_rate)

# GPT-2 / GPT-J tokenizer has a pad token, but it is not eos_token by default. We need to set it to eos_token.
# only for this model.
tokenizer = AutoTokenizer.from_pretrained(config.model_name)
tokenizer.pad_token = tokenizer.eos_token

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(
    config,
    model,
    ref_model=ref_model,
    tokenizer=tokenizer,
    dataset=dataset,
    data_collator=collator,
    optimizer=optimizer,
)

# We then build the reward pipeline, we will use the toxicity model to compute the reward.
# We first load the toxicity model and tokenizer.
# toxicity_model_id = "facebook/roberta-hate-speech-dynabench-r4-target"
# toxicity_tokenizer = RobertaTokenizer.from_pretrained(toxicity_model_id)
# # We load the toxicity model in fp16 to save memory.
# toxicity_model = RobertaForSequenceClassification.from_pretrained(toxicity_model_id, torch_dtype=torch.float16).to(
#     ppo_trainer.accelerator.device
# )

toxicity_tokenizer = AutoTokenizer.from_pretrained("unitary/toxic-bert")
toxicity_model = AutoModelForSequenceClassification.from_pretrained(
    "unitary/toxic-bert", torch_dtype=torch.float16
).to(ppo_trainer.accelerator.device)


# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs = {
    "min_length": gen_len,
    "top_k": 0.0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
}
# output_min_length = 20
# output_max_length = 30
# output_length_sampler = LengthSampler(output_min_length, output_max_length)

# model_save_path = script_args.model_save_path
import math


def get_current_risk_level(alpha, rho, m, M):
    """
    Get current risk_level_alpha*N value

    Args:
        alpha (float):
            Risk level alpha
        rho (float):
            Risk level scheduler: 0.8 means the soft risk level reaches α after ρ = 80% of the training
        m (int):
            current training step/epoch
        M (int):
            total policy training steps

    Returns:
        alpha_N (int):
            risk_level_alpha*N value
    """
    alpha = script_args.risk_alpha
    rho = script_args.risk_rho

    # if script_args.risk_scheduler == "old":
    #     if m <= script_args.risk_n:
    #         return script_args.ppo_config.batch_size
    #     else:
    #         return math.ceil(script_args.ppo_config.batch_size * max(alpha, 1 - (1 - alpha) * (m - script_args.risk_n) / (rho * M)))
    # else:
    # print("here")
    if m <= script_args.risk_n:
        val = script_args.batch_size
    elif m >= math.ceil(rho * M):
        val = math.ceil(alpha * script_args.batch_size)
    else:
        K = (1 - alpha) / (math.ceil(rho * M) - script_args.risk_n)
        val = math.ceil(script_args.batch_size * max(alpha, 1 - K * (m - script_args.risk_n)))
    return val


for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from the policy model
    response_tensors = []
    for query in query_tensors:
        # gen_len = output_length_sampler()
        # generation_kwargs["max_new_tokens"] = gen_len
        generation_kwargs["max_new_tokens"] = gen_len
        response = ppo_trainer.generate(query, **generation_kwargs)
        response_tensors.append(response.squeeze()[-gen_len:])
    batch["response"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]

    # Compute sentiment score # noqa
    texts = batch["response"]
    toxicity_inputs = toxicity_tokenizer(texts, padding=True, truncation=True, return_tensors="pt").to(
        ppo_trainer.accelerator.device
    )
    logits = toxicity_model(**toxicity_inputs).logits.float()
    toxicity_labels = (-logits[:, 0]).tolist()

    rewards = [torch.tensor(output) for output in toxicity_labels]

    alpha_N_ceil = get_current_risk_level(PPOConfig.soft_risk_alpha, PPOConfig.risk_level_scheduler, epoch, 4621)

    # Run PPO step
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards, alpha_N_ceil)
    ppo_trainer.log_stats(stats, batch, rewards)

    # Save model every 100 epochs
    if epoch % 100 == 0:
        if ppo_trainer.accelerator.is_main_process:
            ppo_trainer.save_pretrained(model_save_path)
ppo_trainer.save_pretrained(model_save_path)

File Path: examples/OLID/dataset/eda.py
Content:
from datasets import load_dataset
import matplotlib.pyplot as plt
import pandas as pd
from transformers import AutoTokenizer, pipeline
from tqdm import tqdm
import pdb

tqdm.pandas()


# Load OLID dataset
olid = pd.read_csv(
    "/mnt/research/Anon2/Students/auth2/repos/trl/examples/OLID/dataset/OLIDv1.0/olid-training-v1.0.tsv",
    sep="\t",
)
olid_offense = olid[olid["subtask_a"] == "OFF"]
olid_not_offense = olid[olid["subtask_a"] == "NOT"]

print("Offensive: ", len(olid_offense))
print("Not Offensive: ", len(olid_not_offense))

# Load OLID test dataset
olid_test = pd.read_csv(
    "/mnt/research/Anon2/Students/auth2/repos/trl/examples/OLID/dataset/OLIDv1.0/testset-levela.tsv",
    sep="\t",
)
olid_test_labels = pd.read_csv(
    "/mnt/research/Anon2/Students/auth2/repos/trl/examples/OLID/dataset/OLIDv1.0/labels-levela.csv",
    header=None,
)
olid_test_labels.columns = ["id", "subtask_a"]
olid_test = pd.concat([olid_test, olid_test_labels["subtask_a"]], axis=1)

print("Offensive: ", len(olid_test[olid_test["subtask_a"] == "OFF"]))
print("Not Offensive: ", len(olid_test[olid_test["subtask_a"] == "NOT"]))

# save train and test datasets
olid_test.to_csv(
    "/mnt/research/Anon2/Students/auth2/repos/trl/examples/OLID/dataset/OLIDv1.0/olid-test.csv",
    index=False,
)
olid.to_csv(
    "/mnt/research/Anon2/Students/auth2/repos/trl/examples/OLID/dataset/OLIDv1.0/olid-train.csv",
    index=False,
)

File Path: examples/OLID/dataset/evaluate_perplexity.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed
from trl.core import LengthSampler
import pandas as pd
from datasets import Dataset

tqdm.pandas()

import pdb
import pickle
import datetime
import os
from evaluate import load
from transformers import AutoModel, AutoTokenizer, AutoConfig
from transformers import OpenAIGPTTokenizer, OpenAIGPTModel
import random
import statistics as st

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
print(date_n_time)
logging_dir = f"/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/ppl_eval/{date_n_time}"
os.makedirs(logging_dir)
prompt_len = 8
response_len = 32

sft_model_path = "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-31_18-06-18"  # sft_pos 70/30
ra_rlhf_models = {
    "seed_2": "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/2024-01-31_22-05-21/save_pretrained",
    "seed_3": "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/2024-01-31_22-52-23/save_pretrained",
    "seed_4": "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/2024-01-31_22-52-23/save_pretrained",
}
rlhf_models = {
    "seed_2": "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/2024-01-31_19-28-10/save_pretrained",
    "seed_3": "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/2024-02-01_14-42-40/save_pretrained",
    "seed_4": "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/2024-01-31_22-52-37/save_pretrained",
}
ra_rlhf_perplexity = {"seed_2": [], "seed_3": [], "seed_4": []}
rlhf_perplexity = {"seed_2": [], "seed_3": [], "seed_4": []}
sft_perplexity = []
zero_shot_perplexity = []


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name=sft_model_path,
            query_dataset="imdb",
            reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            remove_unused_columns=False,
        )
    )
    query_dataset: str = field(default="imdb", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="less", metadata={"help": "the dataset to query"})


args = tyro.cli(ScriptArguments)

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset():
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(sft_model_path)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"

    df = pd.read_csv(
        "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/jigsaw_test.csv",
    )
    # filter out toxic rows
    df = df[df["toxic"] == 0]
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"comment_text": "review"})

    # pdb.set_trace()

    def input_size():
        return prompt_len

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# get test dataset
test_dataset = build_dataset()
# Take 5k random samples
# sample_size = 5000
# random_indices = random.sample(range(len(test_dataset)), sample_size)
# test_dataset = test_dataset.select(random_indices)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

zero_shot_model = trl_model_class.from_pretrained(
    "gpt2",
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)

# sft_model = trl_model_class.from_pretrained(
#     "lvwerra/gpt2-imdb",
#     trust_remote_code=True,
#     device_map=device_map,
#     peft_config=peft_config,
# )

# For the following to work I should have saved the model using huggingface
# ppo_model = trl_model_class.from_pretrained(
#     '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-09-26_11-27-23/model.pt',
#     trust_remote_code=True,
#     device_map=device_map,
#     peft_config=peft_config,
# )

# sft_ppo_model = trl_model_class.from_pretrained(
#     "/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/trl_sft_ppo/2023-10-23_23-30-47",
#     trust_remote_code=True,
#     device_map=device_map,
#     peft_config=peft_config,
# )
"""
ppo_model_PATH = '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-09-26_11-27-23/model.pt'
ppo_model = torch.load(ppo_model_PATH)  

sft_ppo_model_PATH = '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_ppo/2023-09-26_10-36-41/model.pt'
sft_ppo_model = torch.load(sft_ppo_model_PATH)
"""
tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id
# tokenizer.pad_token = tokenizer.eos_token
# tokenizer.padding_side = "left"
# tokenizer.truncation_side = "left"
# tokenizer.pad_token_as_eos_token = True
# tokenizer.max_length = 64

# Define all the tester classes
zero_shot_tester = PPOTrainer(
    args.ppo_config, zero_shot_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator
)
# sft_tester = PPOTrainer(args.ppo_config, sft_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)
# # ppo_tester = PPOTrainer(args.ppo_config, ppo_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)
# sft_ppo_tester = PPOTrainer(
#     args.ppo_config, sft_ppo_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator
# )

# pdb.set_trace()
# ppo_tester._save_pretrained('/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-09-26_11-27-23')
# sft_ppo_tester._save_pretrained('/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_ppo/2023-09-26_10-36-41')

# Build the perplexity evaluation pipeline
perplexity = load("perplexity", module_type="measurement")

# DO I NEED TO TAKE CARE OF PAD TOKEN FOR PERPLEXITY PIPELINE AS WELL??
"""
# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id
"""

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs = {
    "min_length": 32,
    "top_k": 0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": 32,
}


# date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
# os.makedirs(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/evaluation/{date_n_time}')

# model_names_all = ["zero_shot_model", "sft_model", "ppo_model", "sft_ppo_model"]
# testers_all = [zero_shot_tester]  # ppo_tester,

# infer on test dataset
# test_scores_1, test_scores_2, test_scores_3, test_scores_4 = [], [], [], []


for epoch, batch in tqdm(enumerate(zero_shot_tester.dataloader)):
    query_tensors = batch["input_ids"]

    texts = batch["review"]
    texts = [text[:500] for text in texts]
    # texts = ["lorem ipsum", "Happy Birthday!", "Bienvenue"]

    # ra-rlhf
    perplexity_scores_zero_shot = perplexity.compute(data=texts, model_id="gpt2")["perplexities"]
    zero_shot_perplexity.extend(perplexity_scores_zero_shot)
    perplexity_scores_sft = perplexity.compute(data=texts, model_id=sft_model_path)["perplexities"]
    sft_perplexity.extend(perplexity_scores_sft)
    for seed, model_path in ra_rlhf_models.items():
        perplexity_scores = perplexity.compute(data=texts, model_id=model_path)["perplexities"]
        ra_rlhf_perplexity[seed].extend(perplexity_scores)
    for seed, model_path in rlhf_models.items():
        perplexity_scores = perplexity.compute(data=texts, model_id=model_path)["perplexities"]
        rlhf_perplexity[seed].extend(perplexity_scores)
    # if epoch > 5:
    #     break


# print and save the final score
print("Zero Shot Perplexity: ", sum(zero_shot_perplexity) / len(zero_shot_perplexity))
print("SFT Perplexity: ", sum(sft_perplexity) / len(sft_perplexity))
ra_rlhf_ppls_mean = []
rlhf_ppls_mean = []
for seed, scores in ra_rlhf_perplexity.items():
    ra_rlhf_ppls_mean.append(sum(scores) / len(scores))
for seed, scores in rlhf_perplexity.items():
    rlhf_ppls_mean.append(sum(scores) / len(scores))
print(
    "RA-RLHF Perplexity: ",
    ra_rlhf_ppls_mean,
    "Mean: ",
    sum(ra_rlhf_ppls_mean) / len(ra_rlhf_ppls_mean),
    "Std: ",
    st.stdev(ra_rlhf_ppls_mean),
)
print(
    "RLHF Perplexity: ",
    rlhf_ppls_mean,
    "Mean: ",
    sum(rlhf_ppls_mean) / len(rlhf_ppls_mean),
    "Std: ",
    st.stdev(rlhf_ppls_mean),
)

File Path: examples/OLID/dataset/evaluate_toxicity.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset, Dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed
from trl.core import LengthSampler
import pandas as pd

tqdm.pandas()

import pdb
import pickle
import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import statistics
import random

# model_path = "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-25_16-31-51"  # sft_positive
model_path = (
    "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-20_20-32-35"  # sft_both
)

prompt_len = 35
response_len = 32


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name="gpt2",
            query_dataset="imdb",
            reward_model="unitary/toxic-bert",
            learning_rate=1.41e-5,
            log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
        )
    )
    query_dataset: str = field(default="imdb", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )


args = tyro.cli(ScriptArguments)


# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # # Kaggle datasets have to be downloaded manually (using Kaggle API)
    # # datasets.load_dataset("jigsaw_toxicity_pred", data_dir="<path/to/manual/data>")
    # ds = load_dataset(
    #     query_dataset,
    #     data_dir="/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/original_dataset",
    #     split="train",
    # )  # Hardcoding path for now TODO: change this

    # ds = ds.rename_columns({"comment_text": "review"})
    # ds = ds.filter(lambda x: len(x["review"]) > 64, batched=False)

    # df = pd.DataFrame(ds)
    # num_toxic = df["toxic"].sum()

    # toxic_df = df[df["toxic"] == True]
    # non_toxic_df = df[df["toxic"] == False]

    # non_toxic_df = non_toxic_df.sample(n=num_toxic, random_state=config.seed)

    # # Recombine into dataset
    # ds = ds.from_pandas(pd.concat([toxic_df, non_toxic_df], ignore_index=True))
    df = pd.read_csv(
        "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/jigsaw_test.csv",
    )
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"comment_text": "review"})

    # pdb.set_trace()

    def input_size():
        return prompt_len

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)

# build and sample test dataset
test_dataset = build_dataset(args.ppo_config, args.query_dataset)
# Take 5k random samples
# sample_size = 5000
# random_indices = random.sample(range(len(test_dataset)), sample_size)
# test_dataset = test_dataset.select(random_indices)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    model_path,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id
# tokenizer.padding_side = "left"
# tokenizer.truncation_side = "left"
# tokenizer.pad_token_as_eos_token = True
# tokenizer.max_length = 64

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)
# Creating a tester object here because dataloader is only available in the PPOTrainer class
ppo_tester = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
# task, model_name = args.ppo_config.reward_model.split(":")
task = "text-classification"
model_name = args.ppo_config.reward_model
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs = {
    "min_length": response_len,
    "top_k": 0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": response_len,
}


# infer on test dataset
test_scores = []
test_score_lens = []
for epoch, batch in tqdm(enumerate(ppo_tester.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
    rewards = [torch.tensor(-output[0]["score"]) for output in pipe_outputs]
    # rewards = [
    #     torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"]]))[1]
    #     for output in pipe_outputs
    # ]

    test_scores.append(sum(rewards))
    test_score_lens.append(len(rewards))
    print(sum(rewards) / len(rewards))

# print and save the final score
# date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
# os.makedirs(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/zero_shot/{date_n_time}')
final_test_score = sum(test_scores) / sum(test_score_lens)
test_scores_list = [t.item() for t in test_scores]
print(f"Total test sentiment-score and std is: {final_test_score.item(), statistics.stdev(test_scores_list)}")
# pickle.dump(final_test_score.item(), open(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/zero_shot/{date_n_time}/final_test_score', "wb"))

File Path: examples/OLID/dataset/evaluate_toxicity_6B.py
Content:
import argparse
import csv
import pandas as pd
import evaluate
import numpy as np
import torch
from datasets import load_dataset, Dataset
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer

# from trl.import_utils import is_npu_available, is_xpu_available

prompt_len = 8
toxicity = evaluate.load("ybelkada/toxicity", "DaNLP/da-electra-hatespeech-detection", module_type="measurement")


def build_dataset():
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(
        "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-31_18-06-18"
    )
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"

    df = pd.read_csv(
        "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/jigsaw_test.csv",
    )
    # filter out toxic rows
    df = df[df["toxic"] == 1]
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"comment_text": "review"})

    # pdb.set_trace()

    def input_size():
        return prompt_len

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# get test dataset
ds = build_dataset()


# ds = load_dataset("OxAISH-AL-LLM/wiki_toxic", split="test")

parser = argparse.ArgumentParser(description="Evaluate de-toxified models")
parser.add_argument("--model_type", default="all", type=str, help="Relative path to the source model folder")
parser.add_argument(
    "--output_file", default="toxicity_6B.csv", type=str, help="Relative path to the source model folder"
)
parser.add_argument("--batch_size", default=64, type=int, help="Batch size")
parser.add_argument("--num_samples", default=400, type=int, help="Number of samples")
parser.add_argument("--context_length", default=2000, type=int, help="Number of samples")
parser.add_argument("--max_new_tokens", default=32, type=int, help="Max new tokens for generation")
args = parser.parse_args()


if args.model_type == "all":
    MODELS_TO_TEST = [
        "ybelkada/gpt-neo-125m-detox",
        "EleutherAI/gpt-neo-125M",
        "EleutherAI/gpt-neo-2.7B",
        "ybelkada/gpt-neo-2.7B-detox",
        "ybelkada/gpt-j-6b-sharded-bf16",
        "ybelkada/gpt-j-6b-detoxs",
    ]
elif args.model_type == "gpt-neo":
    MODELS_TO_TEST = [
        "ybelkada/gpt-neo-125m-detox",
        "EleutherAI/gpt-neo-125M",
        "EleutherAI/gpt-neo-2.7B",
        "ybelkada/gpt-neo-2.7B-detox",
    ]
elif args.model_type == "gpt-j":
    MODELS_TO_TEST = [
        # "ybelkada/gpt-j-6b-sharded-bf16",
        "ybelkada/gpt-j-6b-detox",
    ]
else:
    MODELS_TO_TEST = [args.model_type]
NUM_SAMPLES = args.num_samples
BATCH_SIZE = args.batch_size
output_file = args.output_file
max_new_tokens = args.max_new_tokens
context_length = args.context_length
# if is_xpu_available():
#     device = torch.xpu.current_device()
# elif is_npu_available():
#     device = torch.npu.current_device()
device = torch.cuda.current_device() if torch.cuda.is_available() else "cpu"

# consider only toxic prompts
# ds = ds.filter(lambda x: x["label"] == 1)

toxicities = {}

# open a csv file
file = open(f"{output_file}", "w", newline="")
writer = csv.writer(file)
# add first rows
writer.writerow(["model_id", "mean_toxicity", "std_toxicity"])


for model_id in tqdm(MODELS_TO_TEST):
    model = AutoModelForCausalLM.from_pretrained(model_id, device_map={"": device}, torch_dtype=torch.bfloat16)
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"
    input_texts = []

    for i, example in enumerate(ds):
        # set seed
        torch.manual_seed(42)

        input_text = example["review"]
        input_texts.append(input_text[:2000])

        if i > NUM_SAMPLES:
            break

        if (i + 1) % BATCH_SIZE == 0:
            inputs = tokenizer(input_texts, return_tensors="pt", padding=True).to(device)
            inputs.input_ids = inputs.input_ids[:context_length]
            inputs.attention_mask = inputs.attention_mask[:context_length]
            outputs = model.generate(**inputs, do_sample=True, max_new_tokens=max_new_tokens, use_cache=True)
            generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)
            generated_texts = [
                generated_text.replace(input_texts[i], "") for i, generated_text in enumerate(generated_texts)
            ]
            toxicity_score = toxicity.compute(predictions=generated_texts)
            input_texts = []

            if model_id not in toxicities:
                toxicities[model_id] = []
            toxicities[model_id].extend(toxicity_score["toxicity"])

    # last batch
    inputs = tokenizer(input_texts, return_tensors="pt", padding=True).to(device)
    outputs = model.generate(**inputs, do_sample=True, max_new_tokens=30)
    generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)
    generated_texts = [generated_text.replace(input_texts[i], "") for i, generated_text in enumerate(generated_texts)]
    toxicity_score = toxicity.compute(predictions=generated_texts)
    toxicities[model_id].extend(toxicity_score["toxicity"])

    # compute mean & std using np
    mean = np.mean(toxicities[model_id])
    std = np.std(toxicities[model_id])

    # save to file
    writer.writerow([model_id, mean, std])

    # print
    print(f"Model: {model_id} - Mean: {mean} - Std: {std}")

    model = None
    # if is_xpu_available():
    #     torch.xpu.empty_cache()
    # elif is_npu_available():
    #     torch.npu.empty_cache()
    # else:
    torch.cuda.empty_cache()

# close file
file.close()

File Path: examples/OLID/dataset/prepare_gpt4_input.py
Content:
import pandas as pd

# Load the generations

jigsaw_generations = pd.read_csv("jigsaw_generations_8_32_alpha_20_seed_2.csv")
gpt4_input = jigsaw_generations[["query", "response", "response_risk"]].copy()
gpt4_input.rename(columns={"query": "Prompt", "response": "Agent A", "response_risk": "Agent B"}, inplace=True)
gpt4_input.to_csv("input_to_gpt4.csv", index=False)

File Path: examples/OLID/dataset/single_perplexity_imdb.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed
from trl.core import LengthSampler
import pandas as pd
from datasets import Dataset

tqdm.pandas()

import pdb
import pickle
import datetime
import os
from evaluate import load
from transformers import AutoModel, AutoTokenizer, AutoConfig
from transformers import OpenAIGPTTokenizer, OpenAIGPTModel
import random
import statistics as st

# date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
# print(date_n_time)
# logging_dir = f"/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/ppl_eval/{date_n_time}"
# os.makedirs(logging_dir)
prompt_len = 64
response_len = 48

rlhf_model = "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/big/2024-03-01_12-06-14/save_pretrained"  # sft_pos 70/30
ra_rlhf_model = (
    "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/big/2024-03-03_18-51-26/save_pretrained"
)
gpt2_model = "gpt2"
gptj_model = "ybelkada/gpt-j-6b-sharded-bf16"
sft_model = "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/IMDB/2024-02-18_22-24-38"
bs = 8


# @dataclass
# class ScriptArguments:
#     ppo_config: PPOConfig = field(
#         default_factory=lambda: PPOConfig(
#             model_name=model_path,
#             query_dataset=None,
#             reward_model=None,
#             learning_rate=1.41e-5,
#             mini_batch_size=bs,
#             batch_size=bs,
#             gradient_accumulation_steps=1,
#             early_stopping=False,
#             target_kl=6.0,
#             kl_penalty="kl",
#             seed=0,
#             use_score_scaling=False,
#             use_score_norm=False,
#             score_clip=None,
#             log_with="tensorboard",
#             project_kwargs={"logging_dir": logging_dir},
#             remove_unused_columns=False,
#         )
#     )
#     query_dataset: str = field(default="imdb", metadata={"help": "the dataset to query"})
#     use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
#     use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
#     peft_config: Optional[LoraConfig] = field(
#         default_factory=lambda: LoraConfig(
#             r=16,
#             lora_alpha=16,
#             bias="none",
#             task_type="CAUSAL_LM",
#         ),
#     )
#     reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
#     input_query_size: str = field(default="less", metadata={"help": "the dataset to query"})


# args = tyro.cli(ScriptArguments)

# # We then define the arguments to pass to the sentiment analysis pipeline.
# # We set `return_all_scores` to True to get the sentiment score for each token.
# sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

# trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(model_path):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"

    ds = load_dataset("imdb", split="test")
    ds = ds.rename_columns({"text": "review"})
    ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)
    ds = ds.filter(lambda x: x["label"] == 1, batched=False)  # Positive reviews only
    ds = ds.shuffle(seed=42).select(range(5000))

    # pdb.set_trace()

    def input_size():
        return prompt_len

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# Take 5k random samples
# sample_size = 5000
# random_indices = random.sample(range(len(test_dataset)), sample_size)
# test_dataset = test_dataset.select(random_indices)


# def collator(data):
#     return dict((key, [d[key] for d in data]) for key in data[0])


# # set seed before initializing value head for deterministic eval
# set_seed(args.ppo_config.seed)

# # Now let's build the model, the reference model, and the tokenizer.
# if not args.use_peft:
#     ref_model = None  # trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
#     device_map = None
#     peft_config = None
# else:
#     peft_config = args.peft_config
#     ref_model = None
#     # Copy the model to each device
#     device_map = {"": Accelerator().local_process_index}

# zero_shot_model = trl_model_class.from_pretrained(
#     "gpt2",
#     trust_remote_code=True,
#     device_map=device_map,
#     peft_config=peft_config,
# )

# sft_model = trl_model_class.from_pretrained(
#     "lvwerra/gpt2-imdb",
#     trust_remote_code=True,
#     device_map=device_map,
#     peft_config=peft_config,
# )

# For the following to work I should have saved the model using huggingface
# ppo_model = trl_model_class.from_pretrained(
#     '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-09-26_11-27-23/model.pt',
#     trust_remote_code=True,
#     device_map=device_map,
#     peft_config=peft_config,
# )

# sft_ppo_model = trl_model_class.from_pretrained(
#     "/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/trl_sft_ppo/2023-10-23_23-30-47",
#     trust_remote_code=True,
#     device_map=device_map,
#     peft_config=peft_config,
# )
"""
ppo_model_PATH = '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-09-26_11-27-23/model.pt'
ppo_model = torch.load(ppo_model_PATH)  

sft_ppo_model_PATH = '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_ppo/2023-09-26_10-36-41/model.pt'
sft_ppo_model = torch.load(sft_ppo_model_PATH)
"""
# tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
# tokenizer.pad_token_id = tokenizer.eos_token_id
# tokenizer.pad_token = tokenizer.eos_token
# tokenizer.padding_side = "left"
# tokenizer.truncation_side = "left"
# tokenizer.pad_token_as_eos_token = True
# tokenizer.max_length = 64
# model = trl_model_class.from_pretrained(
#     model_path,
#     trust_remote_code=True,
#     device_map=device_map,
#     peft_config=peft_config,
# )
# Define all the tester classes
# zero_shot_tester = PPOTrainer(args.ppo_config, model, None, tokenizer, dataset=test_dataset, data_collator=collator)
# sft_tester = PPOTrainer(args.ppo_config, sft_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)
# # ppo_tester = PPOTrainer(args.ppo_config, ppo_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)
# sft_ppo_tester = PPOTrainer(
#     args.ppo_config, sft_ppo_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator
# )

# pdb.set_trace()
# ppo_tester._save_pretrained('/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-09-26_11-27-23')
# sft_ppo_tester._save_pretrained('/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_ppo/2023-09-26_10-36-41')

# Build the perplexity evaluation pipeline
perplexity = load("perplexity", module_type="measurement")

# DO I NEED TO TAKE CARE OF PAD TOKEN FOR PERPLEXITY PIPELINE AS WELL??
"""
# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id
"""

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
# generation_kwargs = {
#     "min_length": response_len,
#     "top_k": 0,
#     "top_p": 1.0,
#     "do_sample": True,
#     "pad_token_id": tokenizer.eos_token_id,
#     "max_new_tokens": response_len,
# }


# date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
# os.makedirs(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/evaluation/{date_n_time}')

# model_names_all = ["zero_shot_model", "sft_model", "ppo_model", "sft_ppo_model"]
# testers_all = [zero_shot_tester]  # ppo_tester,

# infer on test dataset
# test_scores_1, test_scores_2, test_scores_3, test_scores_4 = [], [], [], []

perplexity_scores = {"gpt2": None, "gptj": None, "sft": None, "ra_rlhf": None, "rlhf": None}
# for i in tqdm(range(0, len(test_dataset), bs)):
# batch = test_dataset[i : i + bs]
batch = build_dataset(ra_rlhf_model)  # [i : i + bs]

texts = batch["review"]
texts = [text[:1000] for text in texts]
# texts = ["lorem ipsum", "Happy Birthday!", "Bienvenue"]

# ra-rlhf
perplexity_score = perplexity.compute(data=texts, model_id=ra_rlhf_model, batch_size=bs)["perplexities"]
print(f"Epoch: {0}, Perplexity ra_rlhf: {sum(perplexity_score)/len(perplexity_score)}")
perplexity_scores["ra_rlhf"] = perplexity_score


# rlhf
perplexity_score = perplexity.compute(data=texts, model_id=rlhf_model, batch_size=bs)["perplexities"]
print(f"Epoch: {0}, Perplexity rlhf: {sum(perplexity_score)/len(perplexity_score)}")
perplexity_scores["rlhf"] = perplexity_score


# gptj
perplexity_score = perplexity.compute(data=texts, model_id=gptj_model, batch_size=bs)["perplexities"]
print(f"Epoch: {0}, Perplexity gptj: {sum(perplexity_score)/len(perplexity_score)}")
perplexity_scores["gptj"] = perplexity_score


batch = build_dataset(gpt2_model)  # [i : i + bs]

texts = batch["review"]
texts = [text[:1000] for text in texts]
# gpt2
perplexity_score = perplexity.compute(data=texts, model_id=gpt2_model, batch_size=bs)["perplexities"]
print(f"Epoch: {0}, Perplexity gpt2: {sum(perplexity_score)/len(perplexity_score)}")
perplexity_scores["gpt2"] = perplexity_score

# sft
perplexity_score = perplexity.compute(data=texts, model_id=sft_model, batch_size=bs)["perplexities"]
print(f"Epoch: {0}, Perplexity sft: {sum(perplexity_score)/len(perplexity_score)}")
perplexity_scores["sft"] = perplexity_score

# save perplexity scores as json in current directory
import json

with open("perplexity_scores.json", "w") as f:
    json.dump(perplexity_scores, f)

File Path: examples/OLID/dataset/table_fill_helper.py
Content:
import pandas as pd
import matplotlib.pyplot as plt

# import plotly.express as px

import pandas as pd

import numpy as np

import pdb

import statistics as st


# Jigsaw
jigsaw_gpt = pd.read_csv("jigsaw_generations_8_32_alpha_40_seed_2.csv")[["R_gpt2", "perplexity_gpt2"]].copy()
jigsaw_1 = pd.read_csv("jigsaw_generations_8_32_alpha_20_seed_2.csv")
jigsaw_1 = pd.concat([jigsaw_1, jigsaw_gpt], axis=1)
# select all sft columns and put into new dataframe
jigsaw_sft = jigsaw_1[["R_sft", "perplexity_sft"]].copy()

jigsaw_2 = pd.read_csv("jigsaw_generations_8_32_alpha_20_seed_94.csv")
jigsaw_2 = pd.concat([jigsaw_2, jigsaw_sft, jigsaw_gpt], axis=1)
jigsaw_3 = pd.read_csv("jigsaw_generations_8_32_alpha_20_seed_56.csv")
jigsaw_3 = pd.concat([jigsaw_3, jigsaw_sft, jigsaw_gpt], axis=1)


print(jigsaw_1.columns)
# a = input()

# pdb.set_trace()


# average env reward

sft_means = [jigsaw_1["R_sft"].mean(), jigsaw_2["R_sft"].mean(), jigsaw_3["R_sft"].mean()]
# print(sft_means)
gpt2_means = [jigsaw_1["R_gpt2"].mean(), jigsaw_2["R_gpt2"].mean(), jigsaw_3["R_gpt2"].mean()]
rlhf_means = [jigsaw_1["R"].mean(), jigsaw_2["R"].mean(), jigsaw_3["R"].mean()]
ra_rlhf_means = [jigsaw_1["R_risk"].mean(), jigsaw_2["R_risk"].mean(), jigsaw_3["R_risk"].mean()]


# print average env reward mean and var

print("gpt2 r mean and var:", round(st.mean(gpt2_means), 3), round(st.stdev(gpt2_means), 3))
print("sft r mean and var:", round(st.mean(sft_means), 3), round(st.stdev(sft_means), 3))
print("rlhf r mean and var:", round(st.mean(rlhf_means), 3), round(st.stdev(rlhf_means), 3))
print("ra rlhf r mean and var:", round(st.mean(ra_rlhf_means), 3), round(st.stdev(ra_rlhf_means), 3))

# perplexities

# gpt2_ppl = [jigsaw_1["perplexity_gpt2"].mean(), jigsaw_2["perplexity_gpt2"].mean(), jigsaw_3["perplexity_gpt2"].mean()]
# sft_ppl = [jigsaw_1["perplexity_sft"].mean(), jigsaw_2["perplexity_sft"].mean(), jigsaw_3["perplexity_sft"].mean()]
# rlhf_ppl = [jigsaw_1["perplexity"].mean(), jigsaw_2["perplexity"].mean(), jigsaw_3["perplexity"].mean()]
# ra_rlhf_ppl = [
#     jigsaw_1["perplexity_risk"].mean(),
#     jigsaw_2["perplexity_risk"].mean(),
#     jigsaw_3["perplexity_risk"].mean(),
# ]

# # print ppl mean and var

# print(f"ra_rlhf_ppl: {ra_rlhf_ppl}", f"rlhf+ppl{rlhf_ppl}")

# print("gpt2 ppl mean and var:", round(st.mean(gpt2_ppl), 3), round(st.stdev(gpt2_ppl), 3))
# print("sft ppl mean and var:", round(st.mean(sft_ppl), 3), round(st.stdev(sft_ppl), 3))
# print("rlhf ppl mean and var:", round(st.mean(rlhf_ppl), 3), round(st.stdev(rlhf_ppl), 3))
# print("ra rlhf ppl mean and var:", round(st.mean(ra_rlhf_ppl), 3), round(st.stdev(ra_rlhf_ppl), 3))


# tail performance

tail_to_save = 5
print(f"\n\ntail_to_save: {tail_to_save}")

tail_1 = jigsaw_1[jigsaw_1["prompt_score"] <= tail_to_save]

tail_2 = jigsaw_2[jigsaw_2["prompt_score"] <= tail_to_save]

tail_3 = jigsaw_3[jigsaw_3["prompt_score"] <= tail_to_save]


gpt2_tl_means = [tail_1["R_gpt2"].mean(), tail_2["R_gpt2"].mean(), tail_3["R_gpt2"].mean()]
sft_tl_means = [tail_1["R_sft"].mean(), tail_2["R_sft"].mean(), tail_3["R_sft"].mean()]
rlhf_tl_means = [tail_1["R"].mean(), tail_2["R"].mean(), tail_3["R"].mean()]
ra_rlhf_tl_means = [tail_1["R_risk"].mean(), tail_2["R_risk"].mean(), tail_3["R_risk"].mean()]

print("gpt2 tail mean and var:", round(st.mean(gpt2_tl_means), 3), round(st.stdev(gpt2_tl_means), 3))
print("sft tail mean and var:", round(st.mean(sft_tl_means), 3), round(st.stdev(sft_tl_means), 3))
print("rlhf tail mean and var:", round(st.mean(rlhf_tl_means), 3), round(st.stdev(rlhf_tl_means), 3))
print("ra rlhf tail mean and var:", round(st.mean(ra_rlhf_tl_means), 3), round(st.stdev(ra_rlhf_tl_means), 3))

tail_to_save = 2.5
print(f"\n\ntail_to_save: {tail_to_save}")

tail_1 = jigsaw_1[jigsaw_1["prompt_score"] <= tail_to_save]

tail_2 = jigsaw_2[jigsaw_2["prompt_score"] <= tail_to_save]

tail_3 = jigsaw_3[jigsaw_3["prompt_score"] <= tail_to_save]


gpt2_tl_means = [tail_1["R_gpt2"].mean(), tail_2["R_gpt2"].mean(), tail_3["R_gpt2"].mean()]
sft_tl_means = [tail_1["R_sft"].mean(), tail_2["R_sft"].mean(), tail_3["R_sft"].mean()]
rlhf_tl_means = [tail_1["R"].mean(), tail_2["R"].mean(), tail_3["R"].mean()]
ra_rlhf_tl_means = [tail_1["R_risk"].mean(), tail_2["R_risk"].mean(), tail_3["R_risk"].mean()]

print("gpt2 tail mean and var:", round(st.mean(gpt2_tl_means), 3), round(st.stdev(gpt2_tl_means), 3))
print("sft tail mean and var:", round(st.mean(sft_tl_means), 3), round(st.stdev(sft_tl_means), 3))
print("rlhf tail mean and var:", round(st.mean(rlhf_tl_means), 3), round(st.stdev(rlhf_tl_means), 3))
print("ra rlhf tail mean and var:", round(st.mean(ra_rlhf_tl_means), 3), round(st.stdev(ra_rlhf_tl_means), 3))

tail_to_save = 0.0
print(f"\n\ntail_to_save: {tail_to_save}")

tail_1 = jigsaw_1[jigsaw_1["prompt_score"] <= tail_to_save]

tail_2 = jigsaw_2[jigsaw_2["prompt_score"] <= tail_to_save]

tail_3 = jigsaw_3[jigsaw_3["prompt_score"] <= tail_to_save]


gpt2_tl_means = [tail_1["R_gpt2"].mean(), tail_2["R_gpt2"].mean(), tail_3["R_gpt2"].mean()]
sft_tl_means = [tail_1["R_sft"].mean(), tail_2["R_sft"].mean(), tail_3["R_sft"].mean()]
rlhf_tl_means = [tail_1["R"].mean(), tail_2["R"].mean(), tail_3["R"].mean()]
ra_rlhf_tl_means = [tail_1["R_risk"].mean(), tail_2["R_risk"].mean(), tail_3["R_risk"].mean()]

print("gpt2 tail mean and var:", round(st.mean(gpt2_tl_means), 3), round(st.stdev(gpt2_tl_means), 3))
print("sft tail mean and var:", round(st.mean(sft_tl_means), 3), round(st.stdev(sft_tl_means), 3))
print("rlhf tail mean and var:", round(st.mean(rlhf_tl_means), 3), round(st.stdev(rlhf_tl_means), 3))
print("ra rlhf tail mean and var:", round(st.mean(ra_rlhf_tl_means), 3), round(st.stdev(ra_rlhf_tl_means), 3))

tail_to_save = -2.5
print(f"\n\ntail_to_save: {tail_to_save}")

tail_1 = jigsaw_1[jigsaw_1["prompt_score"] <= tail_to_save]

tail_2 = jigsaw_2[jigsaw_2["prompt_score"] <= tail_to_save]

tail_3 = jigsaw_3[jigsaw_3["prompt_score"] <= tail_to_save]


gpt2_tl_means = [tail_1["R_gpt2"].mean(), tail_2["R_gpt2"].mean(), tail_3["R_gpt2"].mean()]
sft_tl_means = [tail_1["R_sft"].mean(), tail_2["R_sft"].mean(), tail_3["R_sft"].mean()]
rlhf_tl_means = [tail_1["R"].mean(), tail_2["R"].mean(), tail_3["R"].mean()]
ra_rlhf_tl_means = [tail_1["R_risk"].mean(), tail_2["R_risk"].mean(), tail_3["R_risk"].mean()]

print("gpt2 tail mean and var:", round(st.mean(gpt2_tl_means), 3), round(st.stdev(gpt2_tl_means), 3))
print("sft tail mean and var:", round(st.mean(sft_tl_means), 3), round(st.stdev(sft_tl_means), 3))
print("rlhf tail mean and var:", round(st.mean(rlhf_tl_means), 3), round(st.stdev(rlhf_tl_means), 3))
print("ra rlhf tail mean and var:", round(st.mean(ra_rlhf_tl_means), 3), round(st.stdev(ra_rlhf_tl_means), 3))

tail_to_save = -5
print(f"\n\ntail_to_save: {tail_to_save}")

tail_1 = jigsaw_1[jigsaw_1["prompt_score"] <= tail_to_save]

tail_2 = jigsaw_2[jigsaw_2["prompt_score"] <= tail_to_save]

tail_3 = jigsaw_3[jigsaw_3["prompt_score"] <= tail_to_save]


gpt2_tl_means = [tail_1["R_gpt2"].mean(), tail_2["R_gpt2"].mean(), tail_3["R_gpt2"].mean()]
sft_tl_means = [tail_1["R_sft"].mean(), tail_2["R_sft"].mean(), tail_3["R_sft"].mean()]
rlhf_tl_means = [tail_1["R"].mean(), tail_2["R"].mean(), tail_3["R"].mean()]
ra_rlhf_tl_means = [tail_1["R_risk"].mean(), tail_2["R_risk"].mean(), tail_3["R_risk"].mean()]

print("gpt2 tail mean and var:", round(st.mean(gpt2_tl_means), 3), round(st.stdev(gpt2_tl_means), 3))
print("sft tail mean and var:", round(st.mean(sft_tl_means), 3), round(st.stdev(sft_tl_means), 3))
print("rlhf tail mean and var:", round(st.mean(rlhf_tl_means), 3), round(st.stdev(rlhf_tl_means), 3))
print("ra rlhf tail mean and var:", round(st.mean(ra_rlhf_tl_means), 3), round(st.stdev(ra_rlhf_tl_means), 3))

File Path: examples/OLID/dataset/test_quantile.py
Content:
from transformers import pipeline, AutoTokenizer
import pandas as pd
from tqdm import tqdm
import matplotlib.pyplot as plt
import plotly.express as px

tqdm.pandas()
# load the dataset
# jigsaw_df_train = pd.read_csv("jigsaw_train.csv")
# jigsaw_df_test = pd.read_csv("jigsaw_test.csv")
# imdb_df_train = pd.read_csv("imdb_train.csv")
# imdb_df_test = pd.read_csv("imdb_test.csv")

# imdb_df_test = imdb_df_test[imdb_df_test.index % 10 == 0]
# print("IMDB test size: ", len(imdb_df_test))


# tokenizer = AutoTokenizer.from_pretrained(
#     "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/models_to_test/2024-01-18_01-23-52"
# )

# generation_kwargs = {
#     "min_length": 48,
#     "top_k": 50,
#     "top_p": 1.0,
#     "do_sample": True,
#     "pad_token_id": tokenizer.eos_token_id,
#     "max_new_tokens": 48,
# }

# generator = pipeline(
#     "text-generation",
#     model="/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/models_to_test/2024-01-18_01-23-52",
# )

# generator_risk = pipeline(
#     "text-generation",
#     model="/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/models_to_test/2024-01-18_09-45-09",
# )

# print("Generating completions for RLHF model")
# imdb_df_test["generation"] = imdb_df_test["text"].progress_apply(
#     lambda x: generator(x[:64], **generation_kwargs)[0]["generated_text"]
# )
# print("Generating completions for RLHF risk averse model")
# imdb_df_test["generation_risk"] = imdb_df_test["text"].progress_apply(
#     lambda x: generator_risk(x[:64], **generation_kwargs)[0]["generated_text"]
# )

# imdb_df_test.to_csv("imdb_generations.csv", index=False)

# # Reward computation
# sent_kwargs = {"return_all_scores": True, "function_to_apply": "none"}
# imdb_reward_model = pipeline("sentiment-analysis", model="lvwerra/distilbert-imdb", truncation=True, **sent_kwargs)


# print("Calculating rewards for RLHF model")
# imdb_df_test["generation_reward"] = imdb_df_test["generation"].progress_apply(
#     lambda x: imdb_reward_model(x)[0][1]["score"]
# )

# print("Calculating rewards for RLHF risk averse model")
# imdb_df_test["generation_risk_reward"] = imdb_df_test["generation_risk"].progress_apply(
#     lambda x: imdb_reward_model(x)[0][1]["score"]
# )

# imdb_df_test.to_csv("imdb_generations.csv", index=False)

# Plot reward distribution for RLHF and RLHF risk averse model

# Load final dataset
imdb_df_test = pd.read_csv("jigsaw_generations.csv")
imdb_df_test = imdb_df_test.rename(columns={"R": "RLHF", "R_risk": "RLHF risk averse"})
fig = px.ecdf(
    imdb_df_test,
    x=["RLHF", "RLHF risk averse"],
    labels={
        "value": "Reward",
        "variable": "Model",
    },
)
fig.update_layout(
    title="Reward distribution for RLHF and RLHF risk averse models",
    xaxis_title="Reward",
    yaxis_title="CDF",
)
print("Average reward for RLHF: ", imdb_df_test["RLHF"].mean())
print("Average reward for RLHF risk averse: ", imdb_df_test["RLHF risk averse"].mean())
fig.write_image("jigsaw_R_cdf.png")

imdb_df_test = imdb_df_test.rename(columns={"R_bar": "RLHF returns", "R_bar_risk": "RLHF risk averse returns"})
fig = px.ecdf(
    imdb_df_test,
    x=["RLHF returns", "RLHF risk averse returns"],
    labels={
        "value": "Reward",
        "variable": "Model",
    },
)
fig.update_layout(
    title="Returns distribution for RLHF and RLHF risk averse models",
    xaxis_title="Returns",
    yaxis_title="CDF",
)
print("Average return for RLHF: ", imdb_df_test["RLHF returns"].mean())
print("Average return for RLHF risk averse: ", imdb_df_test["RLHF risk averse returns"].mean())
fig.write_image("jigsaw_R_bar_cdf.png")

File Path: examples/OLID/dataset/test_quantile_batched_imdb.py
Content:
from transformers import pipeline, AutoTokenizer
from datasets import Dataset, load_dataset
from dataclasses import dataclass, field
from typing import Optional
from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead
from peft import LoraConfig
import pandas as pd
from tqdm import tqdm
import matplotlib.pyplot as plt
import plotly.express as px
import pdb
import tyro
import torch
from evaluate import load

# load the dataset
# jigsaw_df_train = pd.read_csv("jigsaw_train.csv")
# jigsaw_df_test = pd.read_csv("jigsaw_test.csv")

# load reward model
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none"}
imdb_reward_model = pipeline("sentiment-analysis", model="lvwerra/distilbert-imdb", **sent_kwargs)

ref_model_path = (
    "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/IMDB/2024-02-18_22-24-38"
)
# model_path = (
#     "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/big/2024-03-01_12-06-14/save_pretrained"
# )
risk_model_path = (
    "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/big/2024-03-03_18-51-26/save_pretrained"
)

BETA = 0.2
BETA_risk = 0.2
GAMMA = 1.0

prompt_len = 64
response_len = 48
alpha_level = 40
seed = 4


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name="gpt2",
            query_dataset=None,
            # reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            init_kl_coef=BETA,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
        )
    )
    ppo_config_risk: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name="gpt2",
            query_dataset=None,
            # reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            init_kl_coef=BETA_risk,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
        )
    )
    query_dataset: str = field(default="imdb", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})


args = tyro.cli(ScriptArguments)

tokenizer = AutoTokenizer.from_pretrained(ref_model_path)
tokenizer.pad_token = tokenizer.eos_token

ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(
    ref_model_path,
    trust_remote_code=True,
    device_map=None,
    peft_config=None,
)


# model = AutoModelForCausalLMWithValueHead.from_pretrained(
#     model_path,
#     trust_remote_code=True,
#     device_map=None,
#     peft_config=None,
# )

# risk_model = AutoModelForCausalLMWithValueHead.from_pretrained(
#     risk_model_path,
#     trust_remote_code=True,
#     device_map=None,
#     peft_config=None,
# )
# gpt2_model = AutoModelForCausalLMWithValueHead.from_pretrained(
#     "gpt2",
#     trust_remote_code=True,
#     device_map=None,
#     peft_config=None,
# )
gptj_model = AutoModelForCausalLMWithValueHead.from_pretrained(
    "ybelkada/gpt-j-6b-sharded-bf16",
    trust_remote_code=True,
    device_map=None,
    peft_config=None,
)
# ppo_trainer_gpt2 = PPOTrainer(args.ppo_config, model=gpt2_model, ref_model=ref_model, tokenizer=tokenizer)
# ppo_trainer_sft = PPOTrainer(args.ppo_config, model=ref_model, ref_model=ref_model, tokenizer=tokenizer)

# ppo_trainer = PPOTrainer(args.ppo_config, model=model, ref_model=ref_model, tokenizer=tokenizer)
# ppo_trainer_risk = PPOTrainer(args.ppo_config_risk, model=risk_model, ref_model=ref_model, tokenizer=tokenizer)
ppo_trainer_gptj = PPOTrainer(args.ppo_config, model=gptj_model, ref_model=ref_model, tokenizer=tokenizer)
# perplexity = load("perplexity", module_type="measurement")


def build_dataset(config, query_dataset):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # load imdb with datasets: for reducing data size, just use +'[:200]'
    ds = load_dataset(query_dataset, split="test")
    ds = ds.rename_columns({"text": "review"})
    ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)
    ds = ds.shuffle(seed=config.seed).select(
        range(5000)
    )  # Uncomment this line for a smaller subset of imdb test dataset

    def input_size():
        return 64

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


tokenizer = AutoTokenizer.from_pretrained(risk_model_path, padding_side="left")
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"

generation_kwargs = {
    "min_length": 48,
    "top_k": 0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": 48,
}


batch_size = 8
ds = build_dataset(args.ppo_config, args.query_dataset)
df = pd.DataFrame()
with torch.no_grad():
    for i in tqdm(range(0, len(ds), batch_size)):
        batch = ds[i : i + batch_size]
        query_tensors = batch["input_ids"]
        query_tensors = [q.to(ppo_trainer_gptj.current_device) for q in query_tensors]
        # response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
        # response_tensors_risk = ppo_trainer_risk.generate(query_tensors, return_prompt=False, **generation_kwargs)
        # response_tensors_sft = ppo_trainer_sft.generate(query_tensors, return_prompt=False, **generation_kwargs)
        # response_tensors_gpt2 = ppo_trainer_gpt2.generate(query_tensors, return_prompt=False, **generation_kwargs)
        response_tensors_gptj = ppo_trainer_gptj.generate(query_tensors, return_prompt=False, **generation_kwargs)
        # batch["response"] = tokenizer.batch_decode(response_tensors)
        # batch["response_risk"] = tokenizer.batch_decode(response_tensors_risk)
        # batch["response_sft"] = tokenizer.batch_decode(response_tensors_sft)
        # batch["response_gpt2"] = tokenizer.batch_decode(response_tensors_gpt2)
        batch["response_gptj"] = tokenizer.batch_decode(response_tensors_gptj)

        # text_data = [text[:500] for text in batch["review"]]

        # pdb.set_trace()

        # Compute query sentiment score
        pipe_outputs_prompt = imdb_reward_model(
            batch["query"],
            **sent_kwargs,
            # truncation=True,
        )
        prompt_scores = [torch.tensor(output[1]["score"]) for output in pipe_outputs_prompt]
        batch["prompt_score"] = [s.item() for s in prompt_scores]

        # Compute sentiment score
        # texts = [q + r for q, r in zip(batch["query"], batch["response"])]
        # pipe_outputs = imdb_reward_model(
        #     texts,
        #     **sent_kwargs,
        #     truncation=True,
        # )
        # texts_risk = [q + r for q, r in zip(batch["query"], batch["response_risk"])]
        # pipe_outputs_risk = imdb_reward_model(
        #     texts_risk,
        #     **sent_kwargs,
        #     truncation=True,
        # )
        # texts_sft = [q + r for q, r in zip(batch["query"], batch["response_sft"])]
        # pipe_outputs_sft = imdb_reward_model(
        #     texts_sft,
        #     **sent_kwargs,
        #     truncation=True,
        # )
        # texts_gpt2 = [q + r for q, r in zip(batch["query"], batch["response_gpt2"])]
        # pipe_outputs_gpt2 = imdb_reward_model(
        #     texts_gpt2,
        #     **sent_kwargs,
        #     truncation=True,
        # )
        texts_gptj = [q + r for q, r in zip(batch["query"], batch["response_gptj"])]
        pipe_outputs_gptj = imdb_reward_model(
            texts_gptj,
            **sent_kwargs,
            truncation=True,
        )
        # scores = [torch.tensor(output[1]["score"]) for output in pipe_outputs]
        # scores_risk = [torch.tensor(output[1]["score"]) for output in pipe_outputs_risk]
        # scores_sft = [torch.tensor(output[1]["score"]) for output in pipe_outputs_sft]
        # scores_gpt2 = [torch.tensor(output[1]["score"]) for output in pipe_outputs_gpt2]
        scores_gptj = [torch.tensor(output[1]["score"]) for output in pipe_outputs_gptj]
        # batch["R"] = [s.item() for s in scores]
        # batch["R_risk"] = [s.item() for s in scores_risk]
        # batch["R_sft"] = [s.item() for s in scores_sft]
        # batch["R_gpt2"] = [s.item() for s in scores_gpt2]
        batch["R_gptj"] = [s.item() for s in scores_gptj]

        # Compute R_bar
        # scores = torch.tensor(scores, device=ppo_trainer_risk.current_device)
        # scores_risk = torch.tensor(scores_risk, device=ppo_trainer_risk.current_device)

        # # model_inputs = ppo_trainer.prepare_model_inputs(query_tensors, response_tensors)
        # model_inputs_risk = ppo_trainer_risk.prepare_model_inputs(query_tensors, response_tensors_risk)

        # model_inputs_names = list(model_inputs_risk.keys())

        # all_logprobs, _, values, masks = ppo_trainer.batched_forward_pass(
        #     ppo_trainer.model,
        #     query_tensors,
        #     response_tensors,
        #     model_inputs,
        #     response_masks=None,
        #     return_logits=False,
        # )
        # all_logprobs_risk, _, values_risk, masks_risk = ppo_trainer_risk.batched_forward_pass(
        #     ppo_trainer_risk.model,
        #     query_tensors,
        #     response_tensors_risk,
        #     model_inputs_risk,
        #     response_masks=None,
        #     return_logits=False,
        # )
        # ref_logprobs, _, _, _ = ppo_trainer.batched_forward_pass(
        #     ppo_trainer.ref_model, query_tensors, response_tensors, model_inputs, return_logits=None
        # )
        # ref_logprobs_risk, _, _, _ = ppo_trainer_risk.batched_forward_pass(
        #     ppo_trainer_risk.ref_model, query_tensors, response_tensors_risk, model_inputs_risk, return_logits=None
        # )
        # rewards, non_score_reward = ppo_trainer.compute_rewards(scores, all_logprobs, ref_logprobs, masks)
        # rewards_risk, non_score_reward_risk = ppo_trainer_risk.compute_rewards(
        #     scores_risk, all_logprobs_risk, ref_logprobs_risk, masks_risk
        # )
        # if GAMMA < 1.0:
        #     for t in range(rewards_risk.shape[1]):
        #         # rewards[:, t] = GAMMA**t * rewards[:, t]
        #         rewards_risk[:, t] = GAMMA**t * rewards_risk[:, t]

        # batch["R_bar"] = rewards.sum(axis=1).tolist()
        # batch["R_bar_risk"] = rewards_risk.sum(axis=1).tolist()

        # Compute perplexity
        # batch["perplexity"] = perplexity.compute(data=text_data, model_id=model_path)["perplexities"]
        # batch["perplexity_risk"] = perplexity.compute(data=text_data, model_id=risk_model_path)["perplexities"]
        # batch["perplexity_sft"] = perplexity.compute(data=text_data, model_id=ref_model_path)["perplexities"]
        # batch["perplexity_gpt2"] = perplexity.compute(data=text_data, model_id="gpt2", batch_size=64)["perplexities"]
        # pdb.set_trace()
        try:
            df = pd.concat([df, pd.DataFrame(batch)], ignore_index=True)
        except:
            print("Replacing query tensors with None")
            batch["input_ids"] = [None] * len(batch["review"])
            df = pd.concat([df, pd.DataFrame(batch)], ignore_index=True)


print("Saving results")
read_df = pd.read_csv("imdb_generations_64_48_alpha_40_seed_4_full.csv")
common_columns = read_df.columns.intersection(df.columns)

# Decide which DataFrame to drop the common columns from (here, we choose df2 for demonstration)
df_dropped = df.drop(columns=common_columns)
df = pd.concat([read_df, df_dropped], axis=1)
df.to_csv(f"imdb_generations_{prompt_len}_{response_len}_alpha_{alpha_level}_seed_{seed}_gptj.csv", index=False)


print("Average RLHF reward:", df["R"].mean())
print("Average RA-RLHF reward:", df["R_risk"].mean())
print("Average SFT reward:", df["R_sft"].mean())
print("Average GPT2 reward:", df["R_gpt2"].mean())
print("Average GPTJ reward:", df["R_gptj"].mean())

tail = 5
tail_df = df[df["prompt_score"] < tail]
print("Average RLHF reward in tail:", tail_df["R"].mean())
print("Average RA-RLHF reward in tail:", tail_df["R_risk"].mean())
print("Average SFT reward in tail:", tail_df["R_sft"].mean())
print("Average GPT2 reward in tail:", tail_df["R_gpt2"].mean())
print("Average GPTJ reward in tail:", tail_df["R_gptj"].mean())


# print("Average RLHF perplexity:", df["perplexity"].mean())
# print("Average RA-RLHF perplexity:", df["perplexity_risk"].mean())
# print("Average SFT perplexity:", df["perplexity_sft"].mean())
# print("Average GPT2 perplexity:", df["perplexity_gpt2"].mean())

File Path: examples/OLID/dataset/test_quantile_batched_jigsaw.py
Content:
from transformers import pipeline, AutoTokenizer
from datasets import Dataset
from dataclasses import dataclass, field
from typing import Optional
from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead
from peft import LoraConfig
import pandas as pd
from tqdm import tqdm
import matplotlib.pyplot as plt
import plotly.express as px
import pdb
import tyro
import torch
from evaluate import load

# load the dataset
jigsaw_df_train = pd.read_csv("jigsaw_train.csv")
jigsaw_df_test = pd.read_csv("jigsaw_test.csv")

# load reward model
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none"}
jigsaw_reward_model = pipeline("sentiment-analysis", model="unitary/toxic-bert", **sent_kwargs)

ref_model_path = "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-25_16-31-51"  # sft positive
# ref_model_path = (
# "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-20_20-32-35"  # sft both
# )


model_path = (
    "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/2024-01-28_04-47-51/save_pretrained"
)
risk_model_path = (
    "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/2024-01-28_04-31-03/save_pretrained"
)

BETA = 0.2
BETA_risk = 0.2
GAMMA = 1.0

prompt_len = 35
response_len = 32
alpha_level = 40
seed = 1


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="gpt2",
            query_dataset=None,
            # reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            init_kl_coef=BETA,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
        )
    )
    ppo_config_risk: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="gpt2",
            query_dataset=None,
            # reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            init_kl_coef=BETA_risk,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
        )
    )
    query_dataset: str = field(default=None, metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})


args = tyro.cli(ScriptArguments)

tokenizer = AutoTokenizer.from_pretrained(ref_model_path)
tokenizer.pad_token = tokenizer.eos_token

ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(
    ref_model_path,
    trust_remote_code=True,
    device_map=None,
    peft_config=None,
)


model = AutoModelForCausalLMWithValueHead.from_pretrained(
    model_path,
    trust_remote_code=True,
    device_map=None,
    peft_config=None,
)

risk_model = AutoModelForCausalLMWithValueHead.from_pretrained(
    risk_model_path,
    trust_remote_code=True,
    device_map=None,
    peft_config=None,
)
gpt2_model = AutoModelForCausalLMWithValueHead.from_pretrained(
    "gpt2",
    trust_remote_code=True,
    device_map=None,
    peft_config=None,
)
ppo_trainer_gpt2 = PPOTrainer(args.ppo_config, model=gpt2_model, ref_model=ref_model, tokenizer=tokenizer)
ppo_trainer_sft = PPOTrainer(args.ppo_config, model=ref_model, ref_model=ref_model, tokenizer=tokenizer)
ppo_trainer = PPOTrainer(args.ppo_config, model=model, ref_model=ref_model, tokenizer=tokenizer)
ppo_trainer_risk = PPOTrainer(args.ppo_config_risk, model=risk_model, ref_model=ref_model, tokenizer=tokenizer)
perplexity = load("perplexity", module_type="measurement")


def build_dataset():
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"

    df = pd.read_csv(
        "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/jigsaw_test.csv",
    )
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"comment_text": "review"})

    # pdb.set_trace()

    def input_size():
        return prompt_len

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side="left")
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"

generation_kwargs = {
    "min_length": response_len,
    "top_k": 0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": response_len,
}


batch_size = 128
ds = build_dataset()
df = pd.DataFrame()
for i in tqdm(range(0, len(ds), batch_size)):
    batch = ds[i : i + batch_size]
    query_tensors = batch["input_ids"]
    query_tensors = [q.to(ppo_trainer.current_device) for q in query_tensors]
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    response_tensors_risk = ppo_trainer_risk.generate(query_tensors, return_prompt=False, **generation_kwargs)
    response_tensors_sft = ppo_trainer_sft.generate(query_tensors, return_prompt=False, **generation_kwargs)
    response_tensors_gpt2 = ppo_trainer_gpt2.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)
    batch["response_risk"] = tokenizer.batch_decode(response_tensors_risk)
    batch["response_sft"] = tokenizer.batch_decode(response_tensors_sft)
    batch["response_gpt2"] = tokenizer.batch_decode(response_tensors_gpt2)

    # pdb.set_trace()
    text_data = [text[:500] for text in batch["review"]]
    # Compute query sentiment score
    pipe_outputs_prompt = jigsaw_reward_model(
        batch["query"],
        **sent_kwargs,
        # truncation=True,
    )
    prompt_scores = [torch.tensor(-output[0]["score"]) for output in pipe_outputs_prompt]
    batch["prompt_score"] = [s.item() for s in prompt_scores]

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = jigsaw_reward_model(
        texts,
        **sent_kwargs,
        truncation=True,
    )
    texts_risk = [q + r for q, r in zip(batch["query"], batch["response_risk"])]
    pipe_outputs_risk = jigsaw_reward_model(
        texts_risk,
        **sent_kwargs,
        truncation=True,
    )
    texts_sft = [q + r for q, r in zip(batch["query"], batch["response_sft"])]
    pipe_outputs_sft = jigsaw_reward_model(
        texts_sft,
        **sent_kwargs,
        truncation=True,
    )
    texts_gpt2 = [q + r for q, r in zip(batch["query"], batch["response_gpt2"])]
    pipe_outputs_gpt2 = jigsaw_reward_model(
        texts_gpt2,
        **sent_kwargs,
        truncation=True,
    )
    scores = [torch.tensor(-output[0]["score"]) for output in pipe_outputs]
    scores_risk = [torch.tensor(-output[0]["score"]) for output in pipe_outputs_risk]
    scores_sft = [torch.tensor(-output[0]["score"]) for output in pipe_outputs_sft]
    scores_gpt2 = [torch.tensor(-output[0]["score"]) for output in pipe_outputs_gpt2]
    batch["R"] = [s.item() for s in scores]
    batch["R_risk"] = [s.item() for s in scores_risk]
    batch["R_sft"] = [s.item() for s in scores_sft]
    batch["R_gpt2"] = [s.item() for s in scores_gpt2]

    # Compute R_bar
    scores = torch.tensor(scores, device=ppo_trainer.current_device)
    scores_risk = torch.tensor(scores_risk, device=ppo_trainer_risk.current_device)

    model_inputs = ppo_trainer.prepare_model_inputs(query_tensors, response_tensors)
    model_inputs_risk = ppo_trainer_risk.prepare_model_inputs(query_tensors, response_tensors_risk)

    model_inputs_names = list(model_inputs.keys())

    with torch.no_grad():
        all_logprobs, _, values, masks = ppo_trainer.batched_forward_pass(
            ppo_trainer.model,
            query_tensors,
            response_tensors,
            model_inputs,
            response_masks=None,
            return_logits=False,
        )
        all_logprobs_risk, _, values_risk, masks_risk = ppo_trainer_risk.batched_forward_pass(
            ppo_trainer_risk.model,
            query_tensors,
            response_tensors_risk,
            model_inputs_risk,
            response_masks=None,
            return_logits=False,
        )
        ref_logprobs, _, _, _ = ppo_trainer.batched_forward_pass(
            ppo_trainer.ref_model, query_tensors, response_tensors, model_inputs, return_logits=None
        )
        ref_logprobs_risk, _, _, _ = ppo_trainer_risk.batched_forward_pass(
            ppo_trainer_risk.ref_model, query_tensors, response_tensors_risk, model_inputs_risk, return_logits=None
        )
        rewards, non_score_reward = ppo_trainer.compute_rewards(scores, all_logprobs, ref_logprobs, masks)
        rewards_risk, non_score_reward_risk = ppo_trainer_risk.compute_rewards(
            scores_risk, all_logprobs_risk, ref_logprobs_risk, masks_risk
        )
    if GAMMA < 1.0:
        for t in range(rewards.shape[1]):
            rewards[:, t] = GAMMA**t * rewards[:, t]
            rewards_risk[:, t] = GAMMA**t * rewards_risk[:, t]

    batch["R_bar"] = rewards.sum(axis=1).tolist()
    batch["R_bar_risk"] = rewards_risk.sum(axis=1).tolist()

    # Compute perplexity
    batch["perplexity"] = perplexity.compute(data=text_data, model_id=model_path)["perplexities"]
    batch["perplexity_risk"] = perplexity.compute(data=text_data, model_id=risk_model_path)["perplexities"]
    batch["perplexity_sft"] = perplexity.compute(data=text_data, model_id=ref_model_path)["perplexities"]
    batch["perplexity_gpt2"] = perplexity.compute(data=text_data, model_id="gpt2", batch_size=64)["perplexities"]
    # pdb.set_trace()
    try:
        df = pd.concat([df, pd.DataFrame(batch)], ignore_index=True)
    except:
        batch["input_ids"] = [None] * len(batch["review"])
        df = pd.concat([df, pd.DataFrame(batch)], ignore_index=True)


print("Saving results")
df.to_csv(f"jigsaw_generations_{prompt_len}_{response_len}_alpha_{alpha_level}_seed_{seed}.csv", index=False)


print("Average RLHF reward:", df["R"].mean())
print("Average RA-RLHF reward:", df["R_risk"].mean())
print("Average SFT reward:", df["R_sft"].mean())
print("Average GPT2 reward:", df["R_gpt2"].mean())

print("Average RLHF perplexity:", df["perplexity"].mean())
print("Average RA-RLHF perplexity:", df["perplexity_risk"].mean())
print("Average SFT perplexity:", df["perplexity_sft"].mean())
print("Average GPT2 perplexity:", df["perplexity_gpt2"].mean())

File Path: examples/OLID/dataset/test_quantile_batched_jigsaw_seed12.py
Content:
from transformers import pipeline, AutoTokenizer
from datasets import Dataset
from dataclasses import dataclass, field
from typing import Optional
from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead
from peft import LoraConfig
import pandas as pd
from tqdm import tqdm
import matplotlib.pyplot as plt
import plotly.express as px
import pdb
import tyro
import torch
from evaluate import load

# load the dataset
jigsaw_df_train = pd.read_csv("jigsaw_train_70_30.csv")
jigsaw_df_test = pd.read_csv("jigsaw_test.csv")

# load reward model
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none"}
jigsaw_reward_model = pipeline("sentiment-analysis", model="unitary/toxic-bert", **sent_kwargs)

# ref_model_path = (
#     "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-20_20-32-35"  # sft_both
# )
ref_model_path = "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-31_18-06-18"  # sft_positive seed 2
model_path = (
    "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/2024-01-31_20-36-43/save_pretrained"
)
risk_model_path = (
    "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/2024-02-01_00-06-52/save_pretrained"
)

BETA = 0.2
BETA_risk = 0.2
GAMMA = 1.0

prompt_len = 8
response_len = 32
alpha_level = 20
seed = 12


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="gpt2",
            query_dataset=None,
            # reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            init_kl_coef=BETA,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
        )
    )
    ppo_config_risk: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="gpt2",
            query_dataset=None,
            # reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            init_kl_coef=BETA_risk,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
        )
    )
    query_dataset: str = field(default=None, metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})


args = tyro.cli(ScriptArguments)

tokenizer = AutoTokenizer.from_pretrained(ref_model_path)
tokenizer.pad_token = tokenizer.eos_token

ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(
    ref_model_path,
    trust_remote_code=True,
    device_map=None,
    peft_config=None,
)


model = AutoModelForCausalLMWithValueHead.from_pretrained(
    model_path,
    trust_remote_code=True,
    device_map=None,
    peft_config=None,
)

risk_model = AutoModelForCausalLMWithValueHead.from_pretrained(
    risk_model_path,
    trust_remote_code=True,
    device_map=None,
    peft_config=None,
)
# gpt2_model = AutoModelForCausalLMWithValueHead.from_pretrained(
#     "gpt2",
#     trust_remote_code=True,
#     device_map=None,
#     peft_config=None,
# )
# ppo_trainer_gpt2 = PPOTrainer(args.ppo_config, model=gpt2_model, ref_model=ref_model, tokenizer=tokenizer)
# ppo_trainer_sft = PPOTrainer(args.ppo_config, model=ref_model, ref_model=ref_model, tokenizer=tokenizer)
ppo_trainer = PPOTrainer(args.ppo_config, model=model, ref_model=ref_model, tokenizer=tokenizer)
ppo_trainer_risk = PPOTrainer(args.ppo_config_risk, model=risk_model, ref_model=ref_model, tokenizer=tokenizer)
perplexity = load("perplexity", module_type="measurement")


def build_dataset():
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"

    df = pd.read_csv(
        "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/jigsaw_test.csv",
    )
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"comment_text": "review"})

    # pdb.set_trace()

    def input_size():
        return prompt_len

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side="left")
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"

generation_kwargs = {
    "min_length": response_len,
    "top_k": 0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": response_len,
}


batch_size = 128
ds = build_dataset()
df = pd.DataFrame()
for i in tqdm(range(0, len(ds), batch_size)):
    batch = ds[i : i + batch_size]
    query_tensors = batch["input_ids"]
    query_tensors = [q.to(ppo_trainer.current_device) for q in query_tensors]
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    response_tensors_risk = ppo_trainer_risk.generate(query_tensors, return_prompt=False, **generation_kwargs)
    # response_tensors_sft = ppo_trainer_sft.generate(query_tensors, return_prompt=False, **generation_kwargs)
    # response_tensors_gpt2 = ppo_trainer_gpt2.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)
    batch["response_risk"] = tokenizer.batch_decode(response_tensors_risk)
    # batch["response_sft"] = tokenizer.batch_decode(response_tensors_sft)
    # batch["response_gpt2"] = tokenizer.batch_decode(response_tensors_gpt2)

    # pdb.set_trace()
    text_data = [text[:500] for text in batch["review"]]
    # Compute query sentiment score
    pipe_outputs_prompt = jigsaw_reward_model(
        batch["query"],
        **sent_kwargs,
        # truncation=True,
    )
    prompt_scores = [torch.tensor(-output[0]["score"]) for output in pipe_outputs_prompt]
    batch["prompt_score"] = [s.item() for s in prompt_scores]

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = jigsaw_reward_model(
        texts,
        **sent_kwargs,
        truncation=True,
    )
    texts_risk = [q + r for q, r in zip(batch["query"], batch["response_risk"])]
    pipe_outputs_risk = jigsaw_reward_model(
        texts_risk,
        **sent_kwargs,
        truncation=True,
    )
    # texts_sft = [q + r for q, r in zip(batch["query"], batch["response_sft"])]
    # pipe_outputs_sft = jigsaw_reward_model(
    #     texts_sft,
    #     **sent_kwargs,
    #     truncation=True,
    # )
    # texts_gpt2 = [q + r for q, r in zip(batch["query"], batch["response_gpt2"])]
    # pipe_outputs_gpt2 = jigsaw_reward_model(
    #     texts_gpt2,
    #     **sent_kwargs,
    #     truncation=True,
    # )
    scores = [torch.tensor(-output[0]["score"]) for output in pipe_outputs]
    scores_risk = [torch.tensor(-output[0]["score"]) for output in pipe_outputs_risk]
    # scores_sft = [torch.tensor(-output[0]["score"]) for output in pipe_outputs_sft]
    # scores_gpt2 = [torch.tensor(-output[0]["score"]) for output in pipe_outputs_gpt2]
    batch["R"] = [s.item() for s in scores]
    batch["R_risk"] = [s.item() for s in scores_risk]
    # batch["R_sft"] = [s.item() for s in scores_sft]
    # batch["R_gpt2"] = [s.item() for s in scores_gpt2]

    # Compute R_bar
    scores = torch.tensor(scores, device=ppo_trainer.current_device)
    scores_risk = torch.tensor(scores_risk, device=ppo_trainer_risk.current_device)

    model_inputs = ppo_trainer.prepare_model_inputs(query_tensors, response_tensors)
    model_inputs_risk = ppo_trainer_risk.prepare_model_inputs(query_tensors, response_tensors_risk)

    model_inputs_names = list(model_inputs.keys())

    with torch.no_grad():
        all_logprobs, _, values, masks = ppo_trainer.batched_forward_pass(
            ppo_trainer.model,
            query_tensors,
            response_tensors,
            model_inputs,
            response_masks=None,
            return_logits=False,
        )
        all_logprobs_risk, _, values_risk, masks_risk = ppo_trainer_risk.batched_forward_pass(
            ppo_trainer_risk.model,
            query_tensors,
            response_tensors_risk,
            model_inputs_risk,
            response_masks=None,
            return_logits=False,
        )
        ref_logprobs, _, _, _ = ppo_trainer.batched_forward_pass(
            ppo_trainer.ref_model, query_tensors, response_tensors, model_inputs, return_logits=None
        )
        ref_logprobs_risk, _, _, _ = ppo_trainer_risk.batched_forward_pass(
            ppo_trainer_risk.ref_model, query_tensors, response_tensors_risk, model_inputs_risk, return_logits=None
        )
        rewards, non_score_reward = ppo_trainer.compute_rewards(scores, all_logprobs, ref_logprobs, masks)
        rewards_risk, non_score_reward_risk = ppo_trainer_risk.compute_rewards(
            scores_risk, all_logprobs_risk, ref_logprobs_risk, masks_risk
        )
    if GAMMA < 1.0:
        for t in range(rewards.shape[1]):
            rewards[:, t] = GAMMA**t * rewards[:, t]
            rewards_risk[:, t] = GAMMA**t * rewards_risk[:, t]

    batch["R_bar"] = rewards.sum(axis=1).tolist()
    batch["R_bar_risk"] = rewards_risk.sum(axis=1).tolist()

    # Compute perplexity
    batch["perplexity"] = perplexity.compute(data=text_data, model_id=model_path)["perplexities"]
    batch["perplexity_risk"] = perplexity.compute(data=text_data, model_id=risk_model_path)["perplexities"]
    # batch["perplexity_sft"] = perplexity.compute(data=text_data, model_id=ref_model_path)["perplexities"]
    # batch["perplexity_gpt2"] = perplexity.compute(data=text_data, model_id="gpt2", batch_size=64)["perplexities"]
    # pdb.set_trace()
    try:
        df = pd.concat([df, pd.DataFrame(batch)], ignore_index=True)
    except:
        print("Replacing query tensors with None")
        batch["input_ids"] = [None] * len(batch["review"])
        df = pd.concat([df, pd.DataFrame(batch)], ignore_index=True)


print("Saving results")
df.to_csv(f"jigsaw_generations_{prompt_len}_{response_len}_alpha_{alpha_level}_seed_{seed}.csv", index=False)


print("Average RLHF reward:", df["R"].mean())
print("Average RA-RLHF reward:", df["R_risk"].mean())
# print("Average SFT reward:", df["R_sft"].mean())
# print("Average GPT2 reward:", df["R_gpt2"].mean())

print("Average RLHF perplexity:", df["perplexity"].mean())
print("Average RA-RLHF perplexity:", df["perplexity_risk"].mean())
# print("Average SFT perplexity:", df["perplexity_sft"].mean())
# print("Average GPT2 perplexity:", df["perplexity_gpt2"].mean())

File Path: examples/OLID/dataset/test_quantile_batched_jigsaw_seed2.py
Content:
from transformers import pipeline, AutoTokenizer
from datasets import Dataset
from dataclasses import dataclass, field
from typing import Optional
from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead
from peft import LoraConfig
import pandas as pd
from tqdm import tqdm
import matplotlib.pyplot as plt
import plotly.express as px
import pdb
import tyro
import torch
from evaluate import load

# load the dataset
# olid_df_train = pd.read_csv("olid_train_70_30.csv")
olid_df_test = pd.read_csv(
    "/mnt/research/Anon2/Students/auth2/repos/trl/examples/OLID/dataset/OLIDv1.0/olid-test.csv"
)

# load reward model
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none"}
olid_reward_model = pipeline("sentiment-analysis", model="pigeon-phobia/bertweet-base_finetuned_olid_a", **sent_kwargs)

# ref_model_path = (
#     "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-20_20-32-35"  # sft_both
# )
ref_model_path = "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/OLID/2024-02-18_15-25-28"  # OLID SFT pos
model_path = "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/OLID/2024-02-19_01-27-30/save_pretrained"
risk_model_path = "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/OLID/2024-02-19_01-24-53/save_pretrained"

BETA = 0.2
BETA_risk = 0.2
GAMMA = 1.0

prompt_len = 8
response_len = 22
alpha_level = 20
seed = 56


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="gpt2",
            query_dataset=None,
            # reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            init_kl_coef=BETA,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
        )
    )
    ppo_config_risk: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="gpt2",
            query_dataset=None,
            # reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            init_kl_coef=BETA_risk,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
        )
    )
    query_dataset: str = field(default=None, metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})


args = tyro.cli(ScriptArguments)

tokenizer = AutoTokenizer.from_pretrained(ref_model_path)
tokenizer.pad_token = tokenizer.eos_token

ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(
    ref_model_path,
    trust_remote_code=True,
    device_map=None,
    peft_config=None,
)


model = AutoModelForCausalLMWithValueHead.from_pretrained(
    model_path,
    trust_remote_code=True,
    device_map=None,
    peft_config=None,
)

risk_model = AutoModelForCausalLMWithValueHead.from_pretrained(
    risk_model_path,
    trust_remote_code=True,
    device_map=None,
    peft_config=None,
)
gpt2_model = AutoModelForCausalLMWithValueHead.from_pretrained(
    "gpt2",
    trust_remote_code=True,
    device_map=None,
    peft_config=None,
)
ppo_trainer_gpt2 = PPOTrainer(args.ppo_config, model=gpt2_model, ref_model=ref_model, tokenizer=tokenizer)
ppo_trainer_sft = PPOTrainer(args.ppo_config, model=ref_model, ref_model=ref_model, tokenizer=tokenizer)
ppo_trainer = PPOTrainer(args.ppo_config, model=model, ref_model=ref_model, tokenizer=tokenizer)
ppo_trainer_risk = PPOTrainer(args.ppo_config_risk, model=risk_model, ref_model=ref_model, tokenizer=tokenizer)
perplexity = load("perplexity", module_type="measurement")


def build_dataset():
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"

    df = olid_df_test
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"tweet": "review"})

    # pdb.set_trace()

    def input_size():
        return prompt_len

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side="left")
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"

generation_kwargs = {
    "min_length": response_len,
    "top_k": 0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": response_len,
}


batch_size = 128
ds = build_dataset()
df = pd.DataFrame()
for i in tqdm(range(0, len(ds), batch_size)):
    batch = ds[i : i + batch_size]
    query_tensors = batch["input_ids"]
    query_tensors = [q.to(ppo_trainer.current_device) for q in query_tensors]
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    response_tensors_risk = ppo_trainer_risk.generate(query_tensors, return_prompt=False, **generation_kwargs)
    response_tensors_sft = ppo_trainer_sft.generate(query_tensors, return_prompt=False, **generation_kwargs)
    response_tensors_gpt2 = ppo_trainer_gpt2.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)
    batch["response_risk"] = tokenizer.batch_decode(response_tensors_risk)
    batch["response_sft"] = tokenizer.batch_decode(response_tensors_sft)
    batch["response_gpt2"] = tokenizer.batch_decode(response_tensors_gpt2)

    # pdb.set_trace()
    text_data = [text[:500] for text in batch["review"]]
    # Compute query sentiment score
    pipe_outputs_prompt = olid_reward_model(
        batch["query"],
        **sent_kwargs,
        # truncation=True,
    )
    prompt_scores = [torch.tensor(output[0]["score"]) for output in pipe_outputs_prompt]
    batch["prompt_score"] = [s.item() for s in prompt_scores]

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = olid_reward_model(
        texts,
        **sent_kwargs,
        truncation=True,
    )
    texts_risk = [q + r for q, r in zip(batch["query"], batch["response_risk"])]
    pipe_outputs_risk = olid_reward_model(
        texts_risk,
        **sent_kwargs,
        truncation=True,
    )
    texts_sft = [q + r for q, r in zip(batch["query"], batch["response_sft"])]
    pipe_outputs_sft = olid_reward_model(
        texts_sft,
        **sent_kwargs,
        truncation=True,
    )
    texts_gpt2 = [q + r for q, r in zip(batch["query"], batch["response_gpt2"])]
    pipe_outputs_gpt2 = olid_reward_model(
        texts_gpt2,
        **sent_kwargs,
        truncation=True,
    )
    scores = [torch.tensor(output[0]["score"]) for output in pipe_outputs]
    scores_risk = [torch.tensor(output[0]["score"]) for output in pipe_outputs_risk]
    scores_sft = [torch.tensor(output[0]["score"]) for output in pipe_outputs_sft]
    scores_gpt2 = [torch.tensor(output[0]["score"]) for output in pipe_outputs_gpt2]
    batch["R"] = [s.item() for s in scores]
    batch["R_risk"] = [s.item() for s in scores_risk]
    batch["R_sft"] = [s.item() for s in scores_sft]
    batch["R_gpt2"] = [s.item() for s in scores_gpt2]

    # Compute R_bar
    scores = torch.tensor(scores, device=ppo_trainer.current_device)
    scores_risk = torch.tensor(scores_risk, device=ppo_trainer_risk.current_device)

    model_inputs = ppo_trainer.prepare_model_inputs(query_tensors, response_tensors)
    model_inputs_risk = ppo_trainer_risk.prepare_model_inputs(query_tensors, response_tensors_risk)

    model_inputs_names = list(model_inputs.keys())

    with torch.no_grad():
        all_logprobs, _, values, masks = ppo_trainer.batched_forward_pass(
            ppo_trainer.model,
            query_tensors,
            response_tensors,
            model_inputs,
            response_masks=None,
            return_logits=False,
        )
        all_logprobs_risk, _, values_risk, masks_risk = ppo_trainer_risk.batched_forward_pass(
            ppo_trainer_risk.model,
            query_tensors,
            response_tensors_risk,
            model_inputs_risk,
            response_masks=None,
            return_logits=False,
        )
        ref_logprobs, _, _, _ = ppo_trainer.batched_forward_pass(
            ppo_trainer.ref_model, query_tensors, response_tensors, model_inputs, return_logits=None
        )
        ref_logprobs_risk, _, _, _ = ppo_trainer_risk.batched_forward_pass(
            ppo_trainer_risk.ref_model, query_tensors, response_tensors_risk, model_inputs_risk, return_logits=None
        )
        rewards, non_score_reward = ppo_trainer.compute_rewards(scores, all_logprobs, ref_logprobs, masks)
        rewards_risk, non_score_reward_risk = ppo_trainer_risk.compute_rewards(
            scores_risk, all_logprobs_risk, ref_logprobs_risk, masks_risk
        )
    if GAMMA < 1.0:
        for t in range(rewards.shape[1]):
            rewards[:, t] = GAMMA**t * rewards[:, t]
            rewards_risk[:, t] = GAMMA**t * rewards_risk[:, t]

    batch["R_bar"] = rewards.sum(axis=1).tolist()
    batch["R_bar_risk"] = rewards_risk.sum(axis=1).tolist()

    # Compute perplexity
    batch["perplexity"] = perplexity.compute(data=text_data, model_id=model_path)["perplexities"]
    batch["perplexity_risk"] = perplexity.compute(data=text_data, model_id=risk_model_path)["perplexities"]
    batch["perplexity_sft"] = perplexity.compute(data=text_data, model_id=ref_model_path)["perplexities"]
    batch["perplexity_gpt2"] = perplexity.compute(data=text_data, model_id="gpt2", batch_size=64)["perplexities"]
    # pdb.set_trace()
    try:
        df = pd.concat([df, pd.DataFrame(batch)], ignore_index=True)
    except:
        print("Replacing query tensors with None")
        batch["input_ids"] = [None] * len(batch["review"])
        df = pd.concat([df, pd.DataFrame(batch)], ignore_index=True)


print("Saving results")
df.to_csv(f"olid_generations_{prompt_len}_{response_len}_alpha_{alpha_level}_seed_{seed}.csv", index=False)


print("Average RLHF reward:", df["R"].mean())
print("Average RA-RLHF reward:", df["R_risk"].mean())
print("Average SFT reward:", df["R_sft"].mean())
print("Average GPT2 reward:", df["R_gpt2"].mean())

print("Average RLHF perplexity:", df["perplexity"].mean())
print("Average RA-RLHF perplexity:", df["perplexity_risk"].mean())
print("Average SFT perplexity:", df["perplexity_sft"].mean())
print("Average GPT2 perplexity:", df["perplexity_gpt2"].mean())

File Path: examples/OLID/dataset/test_quantile_batched_jigsaw_seed36.py
Content:
from transformers import pipeline, AutoTokenizer
from datasets import Dataset
from dataclasses import dataclass, field
from typing import Optional
from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead
from peft import LoraConfig
import pandas as pd
from tqdm import tqdm
import matplotlib.pyplot as plt
import plotly.express as px
import pdb
import tyro
import torch
from evaluate import load

# load the dataset
jigsaw_df_train = pd.read_csv("jigsaw_train_70_30.csv")
jigsaw_df_test = pd.read_csv("jigsaw_test.csv")

# load reward model
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none"}
jigsaw_reward_model = pipeline("sentiment-analysis", model="unitary/toxic-bert", **sent_kwargs)

ref_model_path = "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-31_18-06-18"  # sft_positive seed 2
model_path = (
    "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/2024-02-01_15-44-29/save_pretrained"
)
risk_model_path = (
    "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/2024-02-01_15-44-24/save_pretrained"
)

BETA = 0.2
BETA_risk = 0.2
GAMMA = 1.0

prompt_len = 8
response_len = 32
alpha_level = 20
seed = 4


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="gpt2",
            query_dataset=None,
            # reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            init_kl_coef=BETA,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
        )
    )
    ppo_config_risk: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="gpt2",
            query_dataset=None,
            # reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            init_kl_coef=BETA_risk,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
        )
    )
    query_dataset: str = field(default=None, metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})


args = tyro.cli(ScriptArguments)

tokenizer = AutoTokenizer.from_pretrained(ref_model_path)
tokenizer.pad_token = tokenizer.eos_token

ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(
    ref_model_path,
    trust_remote_code=True,
    device_map=None,
    peft_config=None,
)


model = AutoModelForCausalLMWithValueHead.from_pretrained(
    model_path,
    trust_remote_code=True,
    device_map=None,
    peft_config=None,
)

risk_model = AutoModelForCausalLMWithValueHead.from_pretrained(
    risk_model_path,
    trust_remote_code=True,
    device_map=None,
    peft_config=None,
)
# gpt2_model = AutoModelForCausalLMWithValueHead.from_pretrained(
#     "gpt2",
#     trust_remote_code=True,
#     device_map=None,
#     peft_config=None,
# )
# ppo_trainer_gpt2 = PPOTrainer(args.ppo_config, model=gpt2_model, ref_model=ref_model, tokenizer=tokenizer)
# ppo_trainer_sft = PPOTrainer(args.ppo_config, model=ref_model, ref_model=ref_model, tokenizer=tokenizer)
ppo_trainer = PPOTrainer(args.ppo_config, model=model, ref_model=ref_model, tokenizer=tokenizer)
ppo_trainer_risk = PPOTrainer(args.ppo_config_risk, model=risk_model, ref_model=ref_model, tokenizer=tokenizer)
perplexity = load("perplexity", module_type="measurement")


def build_dataset():
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"

    df = pd.read_csv(
        "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/jigsaw_test.csv",
    )
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"comment_text": "review"})

    # pdb.set_trace()

    def input_size():
        return prompt_len

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side="left")
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"

generation_kwargs = {
    "min_length": response_len,
    "top_k": 0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": response_len,
}


batch_size = 128
ds = build_dataset()
df = pd.DataFrame()
for i in tqdm(range(0, len(ds), batch_size)):
    batch = ds[i : i + batch_size]
    query_tensors = batch["input_ids"]
    query_tensors = [q.to(ppo_trainer.current_device) for q in query_tensors]
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    response_tensors_risk = ppo_trainer_risk.generate(query_tensors, return_prompt=False, **generation_kwargs)
    # response_tensors_sft = ppo_trainer_sft.generate(query_tensors, return_prompt=False, **generation_kwargs)
    # response_tensors_gpt2 = ppo_trainer_gpt2.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)
    batch["response_risk"] = tokenizer.batch_decode(response_tensors_risk)
    # batch["response_sft"] = tokenizer.batch_decode(response_tensors_sft)
    # batch["response_gpt2"] = tokenizer.batch_decode(response_tensors_gpt2)

    # pdb.set_trace()
    text_data = [text[:500] for text in batch["review"]]
    # Compute query sentiment score
    pipe_outputs_prompt = jigsaw_reward_model(
        batch["query"],
        **sent_kwargs,
        # truncation=True,
    )
    prompt_scores = [torch.tensor(-output[0]["score"]) for output in pipe_outputs_prompt]
    batch["prompt_score"] = [s.item() for s in prompt_scores]

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = jigsaw_reward_model(
        texts,
        **sent_kwargs,
        truncation=True,
    )
    texts_risk = [q + r for q, r in zip(batch["query"], batch["response_risk"])]
    pipe_outputs_risk = jigsaw_reward_model(
        texts_risk,
        **sent_kwargs,
        truncation=True,
    )
    # texts_sft = [q + r for q, r in zip(batch["query"], batch["response_sft"])]
    # pipe_outputs_sft = jigsaw_reward_model(
    #     texts_sft,
    #     **sent_kwargs,
    #     truncation=True,
    # )
    # texts_gpt2 = [q + r for q, r in zip(batch["query"], batch["response_gpt2"])]
    # pipe_outputs_gpt2 = jigsaw_reward_model(
    #     texts_gpt2,
    #     **sent_kwargs,
    #     truncation=True,
    # )
    scores = [torch.tensor(-output[0]["score"]) for output in pipe_outputs]
    scores_risk = [torch.tensor(-output[0]["score"]) for output in pipe_outputs_risk]
    # scores_sft = [torch.tensor(-output[0]["score"]) for output in pipe_outputs_sft]
    # scores_gpt2 = [torch.tensor(-output[0]["score"]) for output in pipe_outputs_gpt2]
    batch["R"] = [s.item() for s in scores]
    batch["R_risk"] = [s.item() for s in scores_risk]
    # batch["R_sft"] = [s.item() for s in scores_sft]
    # batch["R_gpt2"] = [s.item() for s in scores_gpt2]

    # Compute R_bar
    scores = torch.tensor(scores, device=ppo_trainer.current_device)
    scores_risk = torch.tensor(scores_risk, device=ppo_trainer_risk.current_device)

    model_inputs = ppo_trainer.prepare_model_inputs(query_tensors, response_tensors)
    model_inputs_risk = ppo_trainer_risk.prepare_model_inputs(query_tensors, response_tensors_risk)

    model_inputs_names = list(model_inputs.keys())

    with torch.no_grad():
        all_logprobs, _, values, masks = ppo_trainer.batched_forward_pass(
            ppo_trainer.model,
            query_tensors,
            response_tensors,
            model_inputs,
            response_masks=None,
            return_logits=False,
        )
        all_logprobs_risk, _, values_risk, masks_risk = ppo_trainer_risk.batched_forward_pass(
            ppo_trainer_risk.model,
            query_tensors,
            response_tensors_risk,
            model_inputs_risk,
            response_masks=None,
            return_logits=False,
        )
        ref_logprobs, _, _, _ = ppo_trainer.batched_forward_pass(
            ppo_trainer.ref_model, query_tensors, response_tensors, model_inputs, return_logits=None
        )
        ref_logprobs_risk, _, _, _ = ppo_trainer_risk.batched_forward_pass(
            ppo_trainer_risk.ref_model, query_tensors, response_tensors_risk, model_inputs_risk, return_logits=None
        )
        rewards, non_score_reward = ppo_trainer.compute_rewards(scores, all_logprobs, ref_logprobs, masks)
        rewards_risk, non_score_reward_risk = ppo_trainer_risk.compute_rewards(
            scores_risk, all_logprobs_risk, ref_logprobs_risk, masks_risk
        )
    if GAMMA < 1.0:
        for t in range(rewards.shape[1]):
            rewards[:, t] = GAMMA**t * rewards[:, t]
            rewards_risk[:, t] = GAMMA**t * rewards_risk[:, t]

    batch["R_bar"] = rewards.sum(axis=1).tolist()
    batch["R_bar_risk"] = rewards_risk.sum(axis=1).tolist()

    # Compute perplexity
    batch["perplexity"] = perplexity.compute(data=text_data, model_id=model_path)["perplexities"]
    batch["perplexity_risk"] = perplexity.compute(data=text_data, model_id=risk_model_path)["perplexities"]
    # batch["perplexity_sft"] = perplexity.compute(data=text_data, model_id=ref_model_path)["perplexities"]
    # batch["perplexity_gpt2"] = perplexity.compute(data=text_data, model_id="gpt2", batch_size=64)["perplexities"]
    # pdb.set_trace()
    try:
        df = pd.concat([df, pd.DataFrame(batch)], ignore_index=True)
    except:
        print("Replacing query tensors with None")
        batch["input_ids"] = [None] * len(batch["review"])
        df = pd.concat([df, pd.DataFrame(batch)], ignore_index=True)


print("Saving results")
df.to_csv(f"jigsaw_generations_{prompt_len}_{response_len}_alpha_{alpha_level}_seed_{seed}.csv", index=False)


print("Average RLHF reward:", df["R"].mean())
print("Average RA-RLHF reward:", df["R_risk"].mean())
# print("Average SFT reward:", df["R_sft"].mean())
# print("Average GPT2 reward:", df["R_gpt2"].mean())

print("Average RLHF perplexity:", df["perplexity"].mean())
print("Average RA-RLHF perplexity:", df["perplexity_risk"].mean())
# print("Average SFT perplexity:", df["perplexity_sft"].mean())
# print("Average GPT2 perplexity:", df["perplexity_gpt2"].mean())

File Path: examples/OLID/plotting/new_quantile_plot_imdb.py
Content:
import matplotlib.pyplot as plt
import pandas as pd


# read imdb_gen_df
imdb_gen_df = pd.read_csv(
    "/mnt/research/Anon2/Students/auth2/repos/trl/examples/OLID/dataset/imdb_generations_64_48_alpha_40_seed_4_full.csv"
)
# imdb_gen_df2 = pd.read_csv("imdb_generations_seed_42.csv")

# Replace columns of imdb_gen_df with imdb_gen_df2

# imdb_gen_df.drop(columns=imdb_gen_df2.columns, inplace=True)
# imdb_gen_df = pd.concat([imdb_gen_df, imdb_gen_df2], axis=1)

# New quantile plot


# print(imdb_gen_df.columns)
# a = input("Press Enter to continue...")

quantile_resolution = 1

quantile_df = imdb_gen_df.copy()
quantile_df = quantile_df.sort_values(by=["prompt_score"])
quantile_df = quantile_df.reset_index(drop=True)

# Stuff to plot
gpt2_quantiles = []
rlhf_quantiles = []
rlhf_risk_quantiles = []
sft_quantiles = []

for quantile_point in range(1, 101, quantile_resolution):
    # selecct first quantile_point% of the data
    relevant_df = quantile_df.iloc[: int((quantile_point / 100) * quantile_df.shape[0])]

    gpt2_quantiles.append(relevant_df["R_gpt2"].mean())
    rlhf_quantiles.append(relevant_df["R"].mean())
    rlhf_risk_quantiles.append(relevant_df["R_risk"].mean())
    sft_quantiles.append(relevant_df["R_sft"].mean())

# Plot
fig, axs = plt.subplots(figsize=(10, 5))

axs.plot(range(1, 101, quantile_resolution), gpt2_quantiles, label="GPT2", color="red")
axs.plot(range(1, 101, quantile_resolution), rlhf_quantiles, label="RLHF", color="blue")
axs.plot(range(1, 101, quantile_resolution), rlhf_risk_quantiles, label="RA-RLHF", color="green")
axs.plot(range(1, 101, quantile_resolution), sft_quantiles, label="SFT", color="orange")

axs.set_xlabel("Quantile (%)", fontsize=16)  # Set label size on the axis object
axs.set_ylabel(f"Average Reward", fontsize=16)
axs.tick_params(axis="both", which="major", labelsize=14)
axs.grid(True)  # Add grid on the axis object
axs.legend(loc="upper left")
axs.set_title(f"Average Reward vs Quantile")

plt.tight_layout()
plt.savefig("new_quantile_plot_imdb.png")
plt.savefig("new_quantile_plot_imdb.pdf")
print("Verify plot saved")
print("GPT2: ", gpt2_quantiles[-1])
print("RLHF: ", rlhf_quantiles[-1])
print("RA-RLHF: ", rlhf_risk_quantiles[-1])
print("SFT: ", sft_quantiles[-1])

File Path: examples/OLID/plotting/new_quantile_plot_jigsaw.py
Content:
import matplotlib.pyplot as plt
import pandas as pd


# read jigsaw_gen_df
jigsaw_gen_df = pd.read_csv("../dataset/jigsaw_generations_8_32_alpha_40_seed_36.csv")


# New quantile plot


# print(jigsaw_gen_df.columns)
# a = input("Press Enter to continue...")

quantile_resolution = 1

quantile_df = jigsaw_gen_df.copy()
quantile_df = quantile_df.sort_values(by=["prompt_score"])
quantile_df = quantile_df.reset_index(drop=True)

# Stuff to plot
gpt2_quantiles = []
rlhf_quantiles = []
rlhf_risk_quantiles = []
sft_quantiles = []

for quantile_point in range(1, 101, quantile_resolution):
    # selecct first quantile_point% of the data
    relevant_df = quantile_df.iloc[: int((quantile_point / 100) * quantile_df.shape[0])]

    gpt2_quantiles.append(relevant_df["R_gpt2"].mean())
    rlhf_quantiles.append(relevant_df["R"].mean())
    rlhf_risk_quantiles.append(relevant_df["R_risk"].mean())
    sft_quantiles.append(relevant_df["R_sft"].mean())

# Plot
fig, axs = plt.subplots(figsize=(10, 5))

axs.plot(range(1, 101, quantile_resolution), gpt2_quantiles, label="GPT2", color="red")
axs.plot(range(1, 101, quantile_resolution), rlhf_quantiles, label="RLHF", color="blue")
axs.plot(range(1, 101, quantile_resolution), rlhf_risk_quantiles, label="RA-RLHF", color="green")
axs.plot(range(1, 101, quantile_resolution), sft_quantiles, label="SFT", color="orange")

axs.set_xlabel("Quantile (%)", fontsize=16)  # Set label size on the axis object
axs.set_ylabel(f"Average Reward", fontsize=16)
axs.tick_params(axis="both", which="major", labelsize=14)
axs.grid(True)  # Add grid on the axis object
axs.legend(loc="upper left")
axs.set_title(f"Average Reward vs Quantile")

plt.tight_layout()
plt.savefig("new_quantile_plot_jigsaw_see36.png")
plt.savefig("new_quantile_plot_jigsaw_see36.pdf")
print("Verify plot saved")
print("GPT2: ", gpt2_quantiles[-1])
print("RLHF: ", rlhf_quantiles[-1])
print("RA-RLHF: ", rlhf_risk_quantiles[-1])
print("SFT: ", sft_quantiles[-1])

File Path: examples/OLID/plotting/pick_best_examples.py
Content:
import pandas as pd

df = pd.read_csv("../dataset/jigsaw_generations_8_32_alpha_20_seed_2.csv")
df = df.filter(["query", "prompt_score", "response", "R", "response_risk", "R_risk"])

# Sort by difference between R and R_risk
df["R_diff"] = df["R_risk"] - df["R"]
df = df.sort_values(by=["R_diff"], ascending=False)

# remove columns where prompt score bigger than R risk or R
df = df[df["prompt_score"] < df["R_risk"]]
df = df[df["prompt_score"] < df["R"]]
df = df[df["prompt_score"] < -2.5]

df.to_csv("examples.csv", index=False)

File Path: examples/OLID/plotting/plot_from_multiple_tb_log.py
Content:
import matplotlib.pyplot as plt
import numpy as np
from tensorboard.backend.event_processing import event_accumulator

import pdb


# Function to extract scalar data from a TensorBoard log file
def extract_scalar_data_from_event_file(event_file_path):
    # Initialize an accumulator
    ea = event_accumulator.EventAccumulator(event_file_path)
    ea.Reload()  # Loads the log data from file

    # Get all scalar tags
    scalar_tags = ea.Tags()["scalars"]

    # Dictionary to hold the data
    scalar_data = {}

    # Extract data for each scalar tag
    for tag in scalar_tags:
        events = ea.Scalars(tag)
        times = [e.wall_time for e in events]
        steps = [e.step for e in events]
        values = [e.value for e in events]
        scalar_data[tag] = {"times": times, "steps": steps, "values": values}

    return scalar_data


# Function for smoothing the curve (simple moving average for illustration)
def smooth_curve(points, factor=0.97):
    smoothed_points = np.zeros_like(points)
    for i in range(len(points)):
        if i > 0:
            smoothed_points[i] = (smoothed_points[i - 1] * factor) + (points[i] * (1 - factor))
        else:
            smoothed_points[i] = points[i]
    return smoothed_points


# Path to your TensorBoard log file or directory
# Assuming `all_data` is a structured dictionary containing all runs, organized by algorithm and then by seed
# e.g., all_data = {'algorithm1': {'seed1': data1, 'seed2': data2, ...},
#                   'algorithm2': {'seed1': data1, 'seed2': data2, ...}, ...}

log_file_paths = {
    "RA-RLHF": [
        "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/OLID/2024-02-19_01-09-26/trl",
        "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/OLID/2024-02-19_01-24-53/trl",
        "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/OLID/2024-02-19_01-40-24/trl",
    ],
    "RLHF": [
        "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/OLID/2024-02-19_01-08-02/trl",
        "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/OLID/2024-02-19_01-27-30/trl",
        "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/OLID/2024-02-19_01-46-52/trl",
    ],
}

# IMDB Sanity Check
# log_file_paths = {
#     "RA-RLHF": [
#         "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/IMDB/2024-02-18_23-09-57/trl",
#         "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/IMDB/2024-02-19_00-16-45/trl",
#         "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/IMDB/2024-02-19_01-24-12/trl",
#     ],
#     "RLHF": [
#         "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/IMDB/2024-02-19_01-15-25/trl",
#         "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/IMDB/2024-02-19_02-24-58/trl",
#         "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/IMDB/2024-02-19_03-36-47/trl",
#     ],
# }

# Extract data
all_data = {}
for algorithm, paths in log_file_paths.items():
    all_data[algorithm] = {}
    for i, path in enumerate(paths):
        seed_key = f"seed{i+1}"
        # load data
        all_data[algorithm][seed_key] = extract_scalar_data_from_event_file(path)

# all_data['RLHF']['seed1']['objective/kl'].keys()
# dict_keys(['times', 'steps', 'values'])

desired_tags = {
    "env/reward_mean": "Environment Reward",
    "env/reward_std": "Reward std",
    "objective/entropy": "Policy Entropy",
    "objective/kl": "KL Divergence",
    "objective/kl_coef": "Beta",
    "ppo/returns/mean": "Return over Batch",
    "tokens/responses_len_mean": "Response Length",
}

# Step 1: Calculate mean and standard deviation for each tag for each algorithm
tag_stats_by_algorithm = {}
for algorithm, seeds_data in all_data.items():
    tag_grouped_data = {}
    for seed, data in seeds_data.items():
        print(seed)
        for tag, values in data.items():
            # pdb.set_trace()
            if tag in desired_tags.keys():
                print(tag)
                if tag not in tag_grouped_data:
                    # pdb.set_trace()
                    print("here")
                    tag_grouped_data[tag] = []
                tag_grouped_data[tag].append(values)

    # pdb.set_trace()
    tag_stats = {}
    for tag, values_list in tag_grouped_data.items():
        # Assuming all seed runs have the same number of steps and are aligned
        steps = values_list[0]["steps"]
        all_values = np.array([values["values"] for values in values_list])
        mean_values = np.mean(all_values, axis=0)
        std_values = np.std(all_values, axis=0)
        tag_stats[tag] = {"steps": steps, "mean": mean_values, "std": std_values}

    tag_stats_by_algorithm[algorithm] = tag_stats


# Step 2: Plot mean and standard deviation for each tag for each algorithm
for tag in tag_grouped_data.keys():
    fig, ax = plt.subplots(figsize=(5, 5))  # Create a figure and an axis object

    for algorithm, stats in tag_stats_by_algorithm.items():
        if tag in stats:
            print(tag)
            steps = stats[tag]["steps"]
            mean_values = smooth_curve(stats[tag]["mean"])
            std_values = smooth_curve(stats[tag]["std"])

            if algorithm == "RLHF":
                ax.plot(steps, mean_values, label=f"{algorithm}", color="red")
                ax.fill_between(
                    steps, mean_values - std_values, mean_values + std_values, color="red", alpha=0.2
                )  # , label=f'{algorithm} std dev')
            else:
                ax.plot(steps, mean_values, label=f"{algorithm}", color="green")
                ax.fill_between(
                    steps, mean_values - std_values, mean_values + std_values, color="green", alpha=0.2
                )  # , label=f'{algorithm} std dev')
            ax.set_xlabel("Training iteration", fontsize=24)  # Set label size on the axis object
            ax.set_ylabel(f"{desired_tags[tag]}", fontsize=24)  # Set label size on the axis object
            ax.tick_params(axis="both", which="major", labelsize=20)  # Adjust tick size on the axis object
            # ax.set_box_aspect(0.7)  # Commented out because this might not be available depending on your matplotlib version
            ax.grid(True)  # Add grid on the axis object
            ax.legend(loc="upper left", fontsize=16)  # Add legend on the axis object
            # plt.title(f'Mean and Standard Deviation for {tag} across Algorithms')
            plt.savefig(
                f"train_plots_olid/{desired_tags[tag].replace(' ','_')}.pdf", bbox_inches="tight", pad_inches=0
            )  # as png

File Path: examples/OLID/plotting/test_plotting_2.py
Content:
import matplotlib.pyplot as plt

# import plotly.express as px
import pandas as pd
import numpy as np
import pdb

# IMDB
imdb_df_test = pd.read_csv("jigsaw_generations_35_32.csv")

# tail_to_save = -2.8
# tail_df = imdb_df_test[imdb_df_test['prompt_score'] <= tail_to_save]
# tail_df = tail_df.drop(columns=['input_ids'])
# print(tail_df['R'].mean())
# print(tail_df['R_risk'].mean())
# tail_df.to_csv("tail/imdb_tail.csv", index=False)
# pdb.set_trace()

# Rename as needed in plots
# imdb_df_test = imdb_df_test.rename(columns={"R": "RLHF", "R_risk": "RLHF risk averse"})
# imdb_df_test = imdb_df_test.rename(columns={"R_bar": "RLHF returns", "R_bar_risk": "RLHF risk averse returns"})


# # CDF plots
# fig = px.ecdf(
#     imdb_df_test,
#     x=["RLHF", "RLHF risk averse"],
#     labels={
#         "value": "Reward",
#         "variable": "Model",
#     },
# )
# fig.update_layout(
#     title="Reward distribution for RLHF and RLHF risk averse models",
#     xaxis_title="Reward",
#     yaxis_title="CDF",
# )
# print("Average reward for RLHF: ", imdb_df_test["RLHF"].mean())
# print("Average reward for RLHF risk averse: ", imdb_df_test["RLHF risk averse"].mean())
# fig.write_image("imdb_R_cdf.png")

# fig = px.ecdf(
#     imdb_df_test,
#     x=["RLHF returns", "RLHF risk averse returns"],
#     labels={
#         "value": "Reward",
#         "variable": "Model",
#     },
# )
# fig.update_layout(
#     title="Returns distribution for RLHF and RLHF risk averse models",
#     xaxis_title="Returns",
#     yaxis_title="CDF",
# )
# print("Average return for RLHF: ", imdb_df_test["RLHF returns"].mean())
# print("Average return for RLHF risk averse: ", imdb_df_test["RLHF risk averse returns"].mean())
# fig.write_image("imdb_R_bar_cdf.png")

# pdb.set_trace()
# Histogram shift plots for imdb


# Function to plot only the top edges of a histogram
def plot_hist_top_edges(data, ax, bins, color, label):
    hist, bin_edges = np.histogram(data, bins=bins)
    for left, right, height in zip(bin_edges[:-1], bin_edges[1:], hist):
        ax.plot([left, right], [height, height], color=color, label=label)
        label = None  # To avoid duplicate labels in the legend


fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(30, 6))
word_map = {"prompt_score": "Prompt Score", "R": "RLHF", "R_risk": "RA-RLHF"}
for i, col in enumerate(["prompt_score", "R", "R_risk"]):
    imdb_df_test[imdb_df_test["toxic"] == 1][col].plot.hist(ax=axs[i], bins=30, alpha=0.5, label="Toxic")
    imdb_df_test[imdb_df_test["toxic"] == 0][col].plot.hist(ax=axs[i], bins=30, alpha=0.5, label="Non Toxic")

    axs[i].set_title(f"{word_map[col]}", fontsize=30)
    axs[i].set_xlabel("Reward", fontsize=30)  # Set label size on the axis object
    axs[i].set_ylabel("")
    axs[i].tick_params(axis="both", which="major", labelsize=28)
    axs[i].grid(True)  # Add grid on the axis object
    axs[i].legend(loc="upper left", fontsize=24)  # Add legend on the axis objec


# Find the maximum y-value among the first three histograms
max_count = max(axs[0].get_ylim()[1], axs[1].get_ylim()[1], axs[2].get_ylim()[1])

# Set the y-axis limits for the first three subplots to be the same
axs[0].set_ylim(0, max_count)
axs[1].set_ylim(0, max_count)
axs[2].set_ylim(0, max_count)
axs[0].set_ylabel(f"Count", fontsize=30)

# tail prompts
tail_filter_level = -6.0
filtered_df = imdb_df_test[imdb_df_test["prompt_score"] <= tail_filter_level]
print(filtered_df.shape[0])
filtered_df[filtered_df["toxic"] == 0]["prompt_score"].plot.hist(ax=axs[3], bins=30, alpha=0.5, label="Non Toxic")
filtered_df[filtered_df["toxic"] == 1]["prompt_score"].plot.hist(ax=axs[3], bins=30, alpha=0.5, label="Toxic")
axs[3].set_title("Tail Prompts", fontsize=30)
axs[3].set_xlabel("Reward", fontsize=30)  # Set label size on the axis object
axs[3].set_ylabel("")
axs[3].tick_params(axis="both", which="major", labelsize=28)
axs[3].grid(True)  # Add grid on the axis object
axs[3].legend(loc="upper left", fontsize=24)  # Add legend on the axis objec
axs[3].axvline(x=tail_filter_level, color="red", linestyle="-", linewidth=4)

# tail performance
filtered_df["R"].plot.hist(ax=axs[4], bins=30, alpha=0.3, label="RLHF", color="red")
filtered_df["R_risk"].plot.hist(ax=axs[4], bins=30, alpha=0.5, label="RA-RLHF", color="olive")
axs[4].set_title("Tail Performance", fontsize=30)
axs[4].set_xlabel("Reward", fontsize=30)  # Set label size on the axis object
axs[4].set_ylabel("")
axs[4].tick_params(axis="both", which="major", labelsize=28)
axs[4].grid(True)  # Add grid on the axis object
axs[4].legend(loc="upper right", fontsize=24)  # Add legend on the axis objec
axs[4].axvline(x=tail_filter_level, color="red", linestyle="-", linewidth=4)

# Find the maximum y-value among the first three histograms
max_count_2 = max(axs[3].get_ylim()[1], axs[4].get_ylim()[1])

# Set the y-axis limits for the first three subplots to be the same
axs[3].set_ylim(0, max_count_2)
axs[4].set_ylim(0, max_count_2)

fig.tight_layout()
plt.savefig("./test_plots_imdb/imdb_distribution_shifts.png")
plt.savefig("./test_plots_imdb/imdb_distribution_shifts.pdf")
plt.clf()

# Quantile plots of returns for imdb

fig, axs = plt.subplots(figsize=(10, 5))

word_map = {"R_bar": "RLHF returns", "R_bar_risk": "RA-RLHF returns"}

for i, col in enumerate(["R_bar", "R_bar_risk"]):
    x = np.linspace(0, 1, len(imdb_df_test[col])) * 100
    y = imdb_df_test[col].sort_values().values
    axs.plot(x, y, label=word_map[col])

axs.set_xlabel("Quantile (%)", fontsize=16)  # Set label size on the axis object
axs.set_ylabel(f"Returns", fontsize=16)
axs.tick_params(axis="both", which="major", labelsize=14)
axs.grid(True)  # Add grid on the axis object
axs.legend(loc="upper left")
axs.set_title(f"Quantile plot of returns")


fig.tight_layout()
plt.savefig("./test_plots_imdb/imdb_R_bar_quantiles.png")
plt.savefig("./test_plots_imdb/imdb_R_bar_quantiles.pdf")
plt.clf()


# Quantile plots of rewards for imdb

fig, axs = plt.subplots(figsize=(10, 5))

word_map = {"R": "RLHF", "R_risk": "RA-RLHF"}

for i, col in enumerate(["R", "R_risk"]):
    x = np.linspace(0, 1, len(imdb_df_test[col])) * 100
    y = imdb_df_test[col].sort_values().values
    axs.plot(x, y, label=word_map[col])

axs.set_xlabel("Quantile (%)", fontsize=16)  # Set label size on the axis object
axs.set_ylabel(f"Rewards", fontsize=16)
axs.tick_params(axis="both", which="major", labelsize=14)
axs.grid(True)  # Add grid on the axis object
axs.legend(loc="upper left")
axs.set_title(f"Quantile plot of rewards")


fig.tight_layout()
plt.savefig("./test_plots_imdb/imdb_R_quantiles.png")
plt.savefig("./test_plots_imdb/imdb_R_quantiles.pdf")
plt.clf()

# # Box plots of returns for imdb

# fig, axs = plt.subplots(figsize=(10, 5))


# # # Jigsaw
# jigsaw_df_test = pd.read_csv("jigsaw_generations.csv")

# # Rename as needed in plots
# # jigsaw_df_test = jigsaw_df_test.rename(columns={"R": "RLHF", "R_risk": "RLHF risk averse"})
# # jigsaw_df_test = jigsaw_df_test.rename(columns={"R_bar": "RLHF returns", "R_bar_risk": "RLHF risk averse returns"})

# # # CDF plots
# # fig = px.ecdf(
# #     jigsaw_df_test,
# #     x=["RLHF", "RLHF risk averse"],
# #     labels={
# #         "value": "Reward",
# #         "variable": "Model",
# #     },
# # )
# # fig.update_layout(
# #     title="Reward distribution for RLHF and RLHF risk averse models",
# #     xaxis_title="Reward",
# #     yaxis_title="CDF",
# # )
# # print("Average reward for RLHF: ", jigsaw_df_test["RLHF"].mean())
# # print("Average reward for RLHF risk averse: ", jigsaw_df_test["RLHF risk averse"].mean())
# # fig.write_image("jigsaw_R_cdf.png")

# # fig = px.ecdf(
# #     jigsaw_df_test,
# #     x=["RLHF returns", "RLHF risk averse returns"],
# #     labels={
# #         "value": "Reward",
# #         "variable": "Model",
# #     },
# # )
# # fig.update_layout(
# #     title="Returns distribution for RLHF and RLHF risk averse models",
# #     xaxis_title="Returns",
# #     yaxis_title="CDF",
# # )
# # print("Average return for RLHF: ", jigsaw_df_test["RLHF returns"].mean())
# # print("Average return for RLHF risk averse: ", jigsaw_df_test["RLHF risk averse returns"].mean())
# # fig.write_image("jigsaw_R_bar_cdf.png")

# # Plot histogram shifts for jigsaw
# fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(10, 5))
# word_map = {"prompt_score": "Prompt", "R": "RLHF", "R_risk": "RA-RLHF"}
# for i, col in enumerate(["prompt_score", "R", "R_risk"]):
#     jigsaw_df_test[jigsaw_df_test["toxic"] == 0][col].plot.hist(ax=axs[i], bins=30, alpha=0.5, label="Non Toxic")
#     jigsaw_df_test[jigsaw_df_test["toxic"] == 1][col].plot.hist(ax=axs[i], bins=30, alpha=0.5, label="Toxic")

#     axs[i].set_xlabel("Reward", fontsize=16)  # Set label size on the axis object
#     axs[i].set_ylabel(f"Count", fontsize=16)
#     axs[i].tick_params(axis="both", which="major", labelsize=14)
#     axs[i].grid(True)  # Add grid on the axis object
#     axs[i].legend(loc="upper left")  # Add legend on the axis objec
#     axs[i].set_title(f"{word_map[col]}")


# fig.tight_layout()
# plt.savefig("./test_plots_jigsaw/jigsaw_distribution_shifts.png")
# plt.savefig("./test_plots_jigsaw/jigsaw_distribution_shifts.pdf")
# plt.clf()

# # Quantile plots of returns for jigsaw

# fig, axs = plt.subplots(figsize=(10, 5))
# word_map = {"R_bar": "RLHF returns", "R_bar_risk": "RA-RLHF returns"}

# for i, col in enumerate(["R_bar", "R_bar_risk"]):
#     x = np.linspace(0, 1, len(jigsaw_df_test[col])) * 100
#     y = jigsaw_df_test[col].sort_values().values
#     axs.plot(x, y, label=word_map[col])

# axs.set_xlabel("Quantile (%)", fontsize=16)  # Set label size on the axis object
# axs.set_ylabel(f"Returns", fontsize=16)
# axs.tick_params(axis="both", which="major", labelsize=14)
# axs.grid(True)  # Add grid on the axis object
# axs.legend(loc="upper left")
# axs.set_title(f"Quantile plot of returns")


# fig.tight_layout()
# plt.savefig("./test_plots_jigsaw/jigsaw_R_bar_quantiles.png")
# plt.savefig("./test_plots_jigsaw/jigsaw_R_bar_quantiles.pdf")
# plt.clf()

# # Quantile plots of rewards for jigsaw

# fig, axs = plt.subplots(figsize=(10, 5))
# word_map = {"R": "RLHF", "R_risk": "RA-RLHF"}
# for i, col in enumerate(["R", "R_risk"]):
#     x = np.linspace(0, 1, len(jigsaw_df_test[col])) * 100
#     y = jigsaw_df_test[col].sort_values().values
#     axs.plot(x, y, label=word_map[col])

# axs.set_xlabel("Quantile (%)", fontsize=16)  # Set label size on the axis object
# axs.set_ylabel(f"Rewards", fontsize=16)
# axs.tick_params(axis="both", which="major", labelsize=14)
# axs.grid(True)  # Add grid on the axis object
# axs.legend(loc="upper left")
# axs.set_title(f"Quantile plot of rewards")


# fig.tight_layout()
# plt.savefig("./test_plots_jigsaw/jigsaw_R_quantiles.png")
# plt.savefig("./test_plots_jigsaw/jigsaw_R_quantiles.pdf")
# plt.clf()

File Path: examples/OLID/plotting/test_plotting_new.py
Content:
import matplotlib.pyplot as plt
import plotly.express as px
import pandas as pd
import numpy as np

# # IMDB
# jigsaw_df_test = pd.read_csv("imdb_generations.csv")

# # Rename as needed in plots
# # jigsaw_df_test = jigsaw_df_test.rename(columns={"R": "RLHF", "R_risk": "RLHF risk averse"})
# # jigsaw_df_test = jigsaw_df_test.rename(columns={"R_bar": "RLHF returns", "R_bar_risk": "RLHF risk averse returns"})


# # # CDF plots
# # fig = px.ecdf(
# #     jigsaw_df_test,
# #     x=["RLHF", "RLHF risk averse"],
# #     labels={
# #         "value": "Reward",
# #         "variable": "Model",
# #     },
# # )
# # fig.update_layout(
# #     title="Reward distribution for RLHF and RLHF risk averse models",
# #     xaxis_title="Reward",
# #     yaxis_title="CDF",
# # )
# # print("Average reward for RLHF: ", jigsaw_df_test["RLHF"].mean())
# # print("Average reward for RLHF risk averse: ", jigsaw_df_test["RLHF risk averse"].mean())
# # fig.write_image("imdb_R_cdf.png")

# # fig = px.ecdf(
# #     jigsaw_df_test,
# #     x=["RLHF returns", "RLHF risk averse returns"],
# #     labels={
# #         "value": "Reward",
# #         "variable": "Model",
# #     },
# # )
# # fig.update_layout(
# #     title="Returns distribution for RLHF and RLHF risk averse models",
# #     xaxis_title="Returns",
# #     yaxis_title="CDF",
# # )
# # print("Average return for RLHF: ", jigsaw_df_test["RLHF returns"].mean())
# # print("Average return for RLHF risk averse: ", jigsaw_df_test["RLHF risk averse returns"].mean())
# # fig.write_image("imdb_R_bar_cdf.png")


# # Histogram shift plots for imdb
# fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(10, 5))
# word_map = {"prompt_score": "Prompt", "R": "RLHF", "R_risk": "RA-RLHF"}
# for i, col in enumerate(["prompt_score", "R", "R_risk"]):
#     jigsaw_df_test[jigsaw_df_test["label"] == 0][col].plot.hist(ax=axs[i], bins=30, alpha=0.5, label="Negative")
#     jigsaw_df_test[jigsaw_df_test["label"] == 1][col].plot.hist(ax=axs[i], bins=30, alpha=0.5, label="Positive")

#     axs[i].set_title(f"{word_map[col]}")
#     axs[i].set_xlabel("Reward", fontsize=16)  # Set label size on the axis object
#     axs[i].set_ylabel(f"Count", fontsize=16)
#     axs[i].tick_params(axis="both", which="major", labelsize=14)
#     axs[i].grid(True)  # Add grid on the axis object
#     axs[i].legend(loc="upper left")  # Add legend on the axis objec

# fig.tight_layout()
# plt.savefig("./test_plots_imdb/imdb_distribution_shifts.png")
# plt.savefig("./test_plots_imdb/imdb_distribution_shifts.pdf")
# plt.clf()

# # Quantile plots of returns for imdb

# fig, axs = plt.subplots(figsize=(10, 5))

# word_map = {"R_bar": "RLHF returns", "R_bar_risk": "RA-RLHF returns"}

# for i, col in enumerate(["R_bar", "R_bar_risk"]):
#     x = np.linspace(0, 1, len(jigsaw_df_test[col])) * 100
#     y = jigsaw_df_test[col].sort_values().values
#     axs.plot(x, y, label=word_map[col])

# axs.set_xlabel("Quantile (%)", fontsize=16)  # Set label size on the axis object
# axs.set_ylabel(f"Returns", fontsize=16)
# axs.tick_params(axis="both", which="major", labelsize=14)
# axs.grid(True)  # Add grid on the axis object
# axs.legend(loc="upper left")
# axs.set_title(f"Quantile plot of returns")


# fig.tight_layout()
# plt.savefig("./test_plots_imdb/imdb_R_bar_quantiles.png")
# plt.savefig("./test_plots_imdb/imdb_R_bar_quantiles.pdf")
# plt.clf()


# # Quantile plots of rewards for imdb

# fig, axs = plt.subplots(figsize=(10, 5))

# word_map = {"R": "RLHF", "R_risk": "RA-RLHF"}

# for i, col in enumerate(["R", "R_risk"]):
#     x = np.linspace(0, 1, len(jigsaw_df_test[col])) * 100
#     y = jigsaw_df_test[col].sort_values().values
#     axs.plot(x, y, label=word_map[col])

# axs.set_xlabel("Quantile (%)", fontsize=16)  # Set label size on the axis object
# axs.set_ylabel(f"Rewards", fontsize=16)
# axs.tick_params(axis="both", which="major", labelsize=14)
# axs.grid(True)  # Add grid on the axis object
# axs.legend(loc="upper left")
# axs.set_title(f"Quantile plot of rewards")


# fig.tight_layout()
# plt.savefig("./test_plots_imdb/imdb_R_quantiles.png")
# plt.savefig("./test_plots_imdb/imdb_R_quantiles.pdf")
# plt.clf()

# # Box plots of returns for imdb

# fig, axs = plt.subplots(figsize=(10, 5))


# # Jigsaw
jigsaw_df_test = pd.read_csv("../dataset/olid_generations_8_22_alpha_20_seed_56.csv")
# jigsaw_df_test_2 = pd.read_csv("../dataset/jigsaw_generations_8_32_alpha_40_seed_2.csv")

# jigsaw_df_test_2.drop(columns=jigsaw_df_test.columns, inplace=True)
# jigsaw_df_test = pd.concat([jigsaw_df_test, jigsaw_df_test_2], axis=1)


# Rename as needed in plots
# jigsaw_df_test = jigsaw_df_test.rename(columns={"R": "RLHF", "R_risk": "RLHF risk averse"})
# jigsaw_df_test = jigsaw_df_test.rename(columns={"R_bar": "RLHF returns", "R_bar_risk": "RLHF risk averse returns"})

# # CDF plots
# fig = px.ecdf(
#     jigsaw_df_test,
#     x=["RLHF", "RLHF risk averse"],
#     labels={
#         "value": "Reward",
#         "variable": "Model",
#     },
# )
# fig.update_layout(
#     title="Reward distribution for RLHF and RLHF risk averse models",
#     xaxis_title="Reward",
#     yaxis_title="CDF",
# )
# print("Average reward for RLHF: ", jigsaw_df_test["RLHF"].mean())
# print("Average reward for RLHF risk averse: ", jigsaw_df_test["RLHF risk averse"].mean())
# fig.write_image("jigsaw_R_cdf.png")

# fig = px.ecdf(
#     jigsaw_df_test,
#     x=["RLHF returns", "RLHF risk averse returns"],
#     labels={
#         "value": "Reward",
#         "variable": "Model",
#     },
# )
# fig.update_layout(
#     title="Returns distribution for RLHF and RLHF risk averse models",
#     xaxis_title="Returns",
#     yaxis_title="CDF",
# )
# print("Average return for RLHF: ", jigsaw_df_test["RLHF returns"].mean())
# print("Average return for RLHF risk averse: ", jigsaw_df_test["RLHF risk averse returns"].mean())
# fig.write_image("jigsaw_R_bar_cdf.png")

# tail_to_save = -3

# tail_df = jigsaw_df_test[jigsaw_df_test["prompt_score"] <= tail_to_save]

# tail_df = tail_df.drop(columns=["input_ids"])

# print(tail_df["R"].mean())

# print(tail_df["R_risk"].mean())

zoom_border_color = "limegreen"


# Plot histogram shifts for jigsaw
def plot_hist_top_edges(data, ax, bins, color, label):
    hist, bin_edges = np.histogram(data, bins=bins)

    for left, right, height in zip(bin_edges[:-1], bin_edges[1:], hist):
        ax.plot([left, right], [height, height], color=color, label=label)

        label = None  # To avoid duplicate labels in the legend


fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(30, 6))

word_map = {"prompt_score": "Prompt Score", "R": "RLHF", "R_risk": "RA-RLHF", "R_sft": "SFT"}

for i, col in enumerate(["prompt_score", "R_sft", "R", "R_risk"]):
    jigsaw_df_test[jigsaw_df_test["subtask_a"] == "NOT"][col].plot.hist(
        ax=axs[i], bins=30, alpha=0.5, label="Not Offensive"
    )
    jigsaw_df_test[jigsaw_df_test["subtask_a"] == "OFF"][col].plot.hist(
        ax=axs[i], bins=30, alpha=0.5, label="Offensive"
    )

    axs[i].set_title(f"{word_map[col]}", fontsize=30)

    axs[i].set_xlabel("Reward", fontsize=30)  # Set label size on the axis object

    axs[i].set_ylabel("")

    axs[i].tick_params(axis="both", which="major", labelsize=28)

    axs[i].grid(True)  # Add grid on the axis object

    axs[i].legend(loc="upper left", fontsize=24)  # Add legend on the axis objec

    # zoomed in bit
    left_lim = -1.3
    right_lim = 0
    axs_zoom = axs[i].inset_axes([0.10, 0.20, 0.65, 0.40])  # left, bottom, width, height

    axs_zoom.hist(
        jigsaw_df_test[jigsaw_df_test["subtask_a"] == "NOT"][col],
        bins=30,
        range=[left_lim, right_lim],
        alpha=0.5,
        label="Not Offensive",
    )
    axs_zoom.hist(
        jigsaw_df_test[jigsaw_df_test["subtask_a"] == "OFF"][col],
        bins=30,
        range=[left_lim, right_lim],
        alpha=0.5,
        label="Offensive",
    )
    axs_zoom.set_ylim(0, 10)
    axs_zoom.set_xlim(left_lim, right_lim)
    axs_zoom.xaxis.set_major_locator(plt.MaxNLocator(3))
    axs_zoom.yaxis.set_major_locator(plt.MaxNLocator(3))
    axs_zoom.tick_params(axis="both", which="major", labelsize=20)
    # set border color to orange
    axs_zoom.spines["bottom"].set_color(zoom_border_color)
    axs_zoom.spines["top"].set_color(zoom_border_color)
    axs_zoom.spines["left"].set_color(zoom_border_color)
    axs_zoom.spines["right"].set_color(zoom_border_color)
    # set border width to zoom_thickness
    zoom_thickness = 3.0
    axs_zoom.spines["bottom"].set_linewidth(zoom_thickness)
    axs_zoom.spines["top"].set_linewidth(zoom_thickness)
    axs_zoom.spines["left"].set_linewidth(zoom_thickness)
    axs_zoom.spines["right"].set_linewidth(zoom_thickness)
    # rectangle coords in zoomed in plot
    rect_height = 15
    rectangle = plt.Rectangle(
        (left_lim, 0),
        right_lim - left_lim,
        rect_height,
        facecolor="none",
        edgecolor=zoom_border_color,
        lw=zoom_thickness,
    )
    axs[i].add_patch(rectangle)


# Find the maximum y-value among the first three histograms

max_count = max(axs[0].get_ylim()[1], axs[1].get_ylim()[1], axs[2].get_ylim()[1])


# Set the y-axis limits for the first three subplots to be the same
axs[0].set_ylim(0, max_count)
axs[1].set_ylim(0, max_count)
axs[2].set_ylim(0, max_count)
axs[3].set_ylim(0, max_count)
axs[0].set_ylabel(f"Count", fontsize=30)

# make quantile plot
# imdb_gen_df = imdb_df_test2
# imdb_gen_df2 = imdb_df_test
# imdb_gen_df.drop(columns=imdb_gen_df2.columns, inplace=True)
# imdb_gen_df = pd.concat([imdb_gen_df, imdb_gen_df2], axis=1)

quantile_resolution = 1

quantile_df = jigsaw_df_test.copy()
quantile_df = quantile_df.sort_values(by=["prompt_score"])
quantile_df = quantile_df.reset_index(drop=True)

# Stuff to plot
gpt2_quantiles = []
rlhf_quantiles = []
rlhf_risk_quantiles = []
sft_quantiles = []

for quantile_point in range(1, 101, quantile_resolution):
    # select first quantile_point% of the data
    relevant_df = quantile_df.iloc[: int((quantile_point / 100) * quantile_df.shape[0])]

    gpt2_quantiles.append(relevant_df["R_gpt2"].mean())
    rlhf_quantiles.append(relevant_df["R"].mean())
    rlhf_risk_quantiles.append(relevant_df["R_risk"].mean())
    sft_quantiles.append(relevant_df["R_sft"].mean())

axs[4].plot(range(1, 101, quantile_resolution), gpt2_quantiles, label="GPT2", color="black")
axs[4].plot(range(1, 101, quantile_resolution), sft_quantiles, label="SFT", color="blue")
axs[4].plot(range(1, 101, quantile_resolution), rlhf_quantiles, label="RLHF", color="red")
axs[4].plot(
    range(1, 101, quantile_resolution),
    rlhf_risk_quantiles,
    label="RA-RLHF",
    color="green",
)


axs[4].set_xlabel("Quantile (%)", fontsize=30)  # Set label size on the axis object
axs[4].set_ylabel(f"Average Reward", fontsize=30)
# axs[4].set_yscale("log")
axs[4].tick_params(axis="both", which="major", labelsize=28)
axs[4].grid(True)  # Add grid on the axis object
axs[4].legend(loc="lower right", fontsize=16)
axs[4].set_title(f"Reward vs Quantile", fontsize=30)


# Zoomed in bit for the quantile plot
axs_zoom = axs[4].inset_axes([0.65, 0.42, 0.30, 0.30])
axs_zoom.plot(range(1, 101, quantile_resolution), gpt2_quantiles, label="GPT2", color="black")
axs_zoom.plot(range(1, 101, quantile_resolution), sft_quantiles, label="SFT", color="blue")
axs_zoom.plot(range(1, 101, quantile_resolution), rlhf_quantiles, label="RLHF", color="red")
axs_zoom.plot(range(1, 101, quantile_resolution), rlhf_risk_quantiles, label="RA-RLHF", color="green")
# increase tick font size
axs_zoom.tick_params(axis="both", which="major", labelsize=18)
# set border color to orange
axs_zoom.spines["bottom"].set_color(zoom_border_color)
axs_zoom.spines["top"].set_color(zoom_border_color)
axs_zoom.spines["left"].set_color(zoom_border_color)
axs_zoom.spines["right"].set_color(zoom_border_color)
# set border width to zoom_thickness
zoom_thickness = 3.0
axs_zoom.spines["bottom"].set_linewidth(zoom_thickness)
axs_zoom.spines["top"].set_linewidth(zoom_thickness)
axs_zoom.spines["left"].set_linewidth(zoom_thickness)
axs_zoom.spines["right"].set_linewidth(zoom_thickness)


# rectangle coords in zoomed in plot
x1, x2, y1, y2 = 0.3, 10.0, -1.2, -0.8
axs_zoom.set_xlim(x1, x2)
axs_zoom.set_ylim(y1, y2)
rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, facecolor="none", edgecolor=zoom_border_color, lw=zoom_thickness)
axs[4].add_patch(rect)

fig.tight_layout()
plt.savefig("./test_plots_olid/olid_distribution_shifts.png")
plt.savefig("./test_plots_olid/olid_distribution_shifts.pdf")
plt.clf()


print("\n\n")
print("Average reward for RLHF: ", jigsaw_df_test["R"].mean())
print("Average reward for RA-RLHF: ", jigsaw_df_test["R_risk"].mean())
print("Average reward for SFT: ", jigsaw_df_test["R_sft"].mean())
print("Average reward for GPT2:", jigsaw_df_test["R_gpt2"].mean())
print("Average Perplexity for RLHF: ", jigsaw_df_test["perplexity"].mean())
print("Average Perplexity for RA-RLHF: ", jigsaw_df_test["perplexity_risk"].mean())
print("Average Perplexity for SFT: ", jigsaw_df_test["perplexity_sft"].mean())
print("Average Perplexity for GPT2: ", jigsaw_df_test["perplexity_gpt2"].mean())
print("\n\n")
# print("Tail Average reward for RLHF: ", filtered_df["R"].mean())
# print("Tail Average reward for RA-RLHF: ", filtered_df["R_risk"].mean())
# print("Tail Average reward for SFT: ", filtered_df["R_sft"].mean())
# print("Tail Average reward for GPT2: ", filtered_df["R_gpt2"].mean())
# print("Tail Average Perplexity for RLHF: ", filtered_df["perplexity"].mean())
# print("Tail Average Perplexity for RA-RLHF: ", filtered_df["perplexity_risk"].mean())
# print("Tail Average Perplexity for SFT: ", filtered_df["perplexity_sft"].mean())
# print("Tail Average Perplexity for GPT2: ", filtered_df["perplexity_gpt2"].mean())


# Quantile plots of returns for jigsaw

rlhf_returns_quantiles = []

rlhf_risk_returns_quantiles = []

for quantile_point in range(1, 101, quantile_resolution):
    # select first quantile_point% of the data
    relevant_df = quantile_df.iloc[: int((quantile_point / 100) * quantile_df.shape[0])]

    rlhf_returns_quantiles.append(relevant_df["R_bar"].mean())
    rlhf_risk_returns_quantiles.append(relevant_df["R_bar_risk"].mean())


fig, axs = plt.subplots(figsize=(10, 5))
word_map = {"R_bar": "RLHF returns", "R_bar_risk": "RA-RLHF returns"}

axs.plot(range(1, 101, quantile_resolution), rlhf_returns_quantiles, label="RLHF", color="red")
axs.plot(range(1, 101, quantile_resolution), rlhf_risk_returns_quantiles, label="RA-RLHF", color="green")

axs.set_xlabel("Quantile (%)", fontsize=30)  # Set label size on the axis object
axs.set_ylabel(f"Returns", fontsize=30)
axs.tick_params(axis="both", which="major", labelsize=14)
axs.grid(True)  # Add grid on the axis object
axs.legend(loc="upper left", fontsize=16)
axs.set_title(f"Quantile plot of returns", fontsize=30)


fig.tight_layout()
plt.savefig("./test_plots_olid/olid_R_bar_quantiles.png")
plt.savefig("./test_plots_olid/olid_R_bar_quantiles.pdf")
plt.clf()

File Path: examples/OLID/plotting/test_plotting_new_imdb.py
Content:
import matplotlib.pyplot as plt
import plotly.express as px
import pandas as pd
import numpy as np

# # IMDB
# jigsaw_df_test = pd.read_csv("imdb_generations.csv")

# # Rename as needed in plots
# # jigsaw_df_test = jigsaw_df_test.rename(columns={"R": "RLHF", "R_risk": "RLHF risk averse"})
# # jigsaw_df_test = jigsaw_df_test.rename(columns={"R_bar": "RLHF returns", "R_bar_risk": "RLHF risk averse returns"})


# # # CDF plots
# # fig = px.ecdf(
# #     jigsaw_df_test,
# #     x=["RLHF", "RLHF risk averse"],
# #     labels={
# #         "value": "Reward",
# #         "variable": "Model",
# #     },
# # )
# # fig.update_layout(
# #     title="Reward distribution for RLHF and RLHF risk averse models",
# #     xaxis_title="Reward",
# #     yaxis_title="CDF",
# # )
# # print("Average reward for RLHF: ", jigsaw_df_test["RLHF"].mean())
# # print("Average reward for RLHF risk averse: ", jigsaw_df_test["RLHF risk averse"].mean())
# # fig.write_image("imdb_R_cdf.png")

# # fig = px.ecdf(
# #     jigsaw_df_test,
# #     x=["RLHF returns", "RLHF risk averse returns"],
# #     labels={
# #         "value": "Reward",
# #         "variable": "Model",
# #     },
# # )
# # fig.update_layout(
# #     title="Returns distribution for RLHF and RLHF risk averse models",
# #     xaxis_title="Returns",
# #     yaxis_title="CDF",
# # )
# # print("Average return for RLHF: ", jigsaw_df_test["RLHF returns"].mean())
# # print("Average return for RLHF risk averse: ", jigsaw_df_test["RLHF risk averse returns"].mean())
# # fig.write_image("imdb_R_bar_cdf.png")


# # Histogram shift plots for imdb
# fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(10, 5))
# word_map = {"prompt_score": "Prompt", "R": "RLHF", "R_risk": "RA-RLHF"}
# for i, col in enumerate(["prompt_score", "R", "R_risk"]):
#     jigsaw_df_test[jigsaw_df_test["label"] == 0][col].plot.hist(ax=axs[i], bins=30, alpha=0.5, label="Negative")
#     jigsaw_df_test[jigsaw_df_test["label"] == 1][col].plot.hist(ax=axs[i], bins=30, alpha=0.5, label="Positive")

#     axs[i].set_title(f"{word_map[col]}")
#     axs[i].set_xlabel("Reward", fontsize=16)  # Set label size on the axis object
#     axs[i].set_ylabel(f"Count", fontsize=16)
#     axs[i].tick_params(axis="both", which="major", labelsize=14)
#     axs[i].grid(True)  # Add grid on the axis object
#     axs[i].legend(loc="upper left")  # Add legend on the axis objec

# fig.tight_layout()
# plt.savefig("./test_plots_imdb/imdb_distribution_shifts.png")
# plt.savefig("./test_plots_imdb/imdb_distribution_shifts.pdf")
# plt.clf()

# # Quantile plots of returns for imdb

# fig, axs = plt.subplots(figsize=(10, 5))

# word_map = {"R_bar": "RLHF returns", "R_bar_risk": "RA-RLHF returns"}

# for i, col in enumerate(["R_bar", "R_bar_risk"]):
#     x = np.linspace(0, 1, len(jigsaw_df_test[col])) * 100
#     y = jigsaw_df_test[col].sort_values().values
#     axs.plot(x, y, label=word_map[col])

# axs.set_xlabel("Quantile (%)", fontsize=16)  # Set label size on the axis object
# axs.set_ylabel(f"Returns", fontsize=16)
# axs.tick_params(axis="both", which="major", labelsize=14)
# axs.grid(True)  # Add grid on the axis object
# axs.legend(loc="upper left")
# axs.set_title(f"Quantile plot of returns")


# fig.tight_layout()
# plt.savefig("./test_plots_imdb/imdb_R_bar_quantiles.png")
# plt.savefig("./test_plots_imdb/imdb_R_bar_quantiles.pdf")
# plt.clf()


# # Quantile plots of rewards for imdb

# fig, axs = plt.subplots(figsize=(10, 5))

# word_map = {"R": "RLHF", "R_risk": "RA-RLHF"}

# for i, col in enumerate(["R", "R_risk"]):
#     x = np.linspace(0, 1, len(jigsaw_df_test[col])) * 100
#     y = jigsaw_df_test[col].sort_values().values
#     axs.plot(x, y, label=word_map[col])

# axs.set_xlabel("Quantile (%)", fontsize=16)  # Set label size on the axis object
# axs.set_ylabel(f"Rewards", fontsize=16)
# axs.tick_params(axis="both", which="major", labelsize=14)
# axs.grid(True)  # Add grid on the axis object
# axs.legend(loc="upper left")
# axs.set_title(f"Quantile plot of rewards")


# fig.tight_layout()
# plt.savefig("./test_plots_imdb/imdb_R_quantiles.png")
# plt.savefig("./test_plots_imdb/imdb_R_quantiles.pdf")
# plt.clf()

# # Box plots of returns for imdb

# fig, axs = plt.subplots(figsize=(10, 5))


# # Jigsaw
jigsaw_df_test = pd.read_csv(
    "/mnt/research/Anon2/Students/auth2/repos/trl/examples/OLID/dataset/imdb_generations_64_48_alpha_40_seed_4_gptj.csv"
)
# jigsaw_df_test_2 = pd.read_csv("../dataset/jigsaw_generations_8_32_alpha_40_seed_2.csv")

# jigsaw_df_test_2.drop(columns=jigsaw_df_test.columns, inplace=True)
# jigsaw_df_test = pd.concat([jigsaw_df_test, jigsaw_df_test_2], axis=1)


# Rename as needed in plots
# jigsaw_df_test = jigsaw_df_test.rename(columns={"R": "RLHF", "R_risk": "RLHF risk averse"})
# jigsaw_df_test = jigsaw_df_test.rename(columns={"R_bar": "RLHF returns", "R_bar_risk": "RLHF risk averse returns"})

# # CDF plots
# fig = px.ecdf(
#     jigsaw_df_test,
#     x=["RLHF", "RLHF risk averse"],
#     labels={
#         "value": "Reward",
#         "variable": "Model",
#     },
# )
# fig.update_layout(
#     title="Reward distribution for RLHF and RLHF risk averse models",
#     xaxis_title="Reward",
#     yaxis_title="CDF",
# )
# print("Average reward for RLHF: ", jigsaw_df_test["RLHF"].mean())
# print("Average reward for RLHF risk averse: ", jigsaw_df_test["RLHF risk averse"].mean())
# fig.write_image("jigsaw_R_cdf.png")

# fig = px.ecdf(
#     jigsaw_df_test,
#     x=["RLHF returns", "RLHF risk averse returns"],
#     labels={
#         "value": "Reward",
#         "variable": "Model",
#     },
# )
# fig.update_layout(
#     title="Returns distribution for RLHF and RLHF risk averse models",
#     xaxis_title="Returns",
#     yaxis_title="CDF",
# )
# print("Average return for RLHF: ", jigsaw_df_test["RLHF returns"].mean())
# print("Average return for RLHF risk averse: ", jigsaw_df_test["RLHF risk averse returns"].mean())
# fig.write_image("jigsaw_R_bar_cdf.png")

# tail_to_save = -3

# tail_df = jigsaw_df_test[jigsaw_df_test["prompt_score"] <= tail_to_save]

# tail_df = tail_df.drop(columns=["input_ids"])

# print(tail_df["R"].mean())

# print(tail_df["R_risk"].mean())

zoom_border_color = "limegreen"


# Plot histogram shifts for jigsaw
def plot_hist_top_edges(data, ax, bins, color, label):
    hist, bin_edges = np.histogram(data, bins=bins)

    for left, right, height in zip(bin_edges[:-1], bin_edges[1:], hist):
        ax.plot([left, right], [height, height], color=color, label=label)

        label = None  # To avoid duplicate labels in the legend


fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(30, 6))

word_map = {
    "prompt_score": "Prompt Score",
    "R": "RLHF",
    "R_risk": "RA-RLHF",
    "R_sft": "SFT",
}

for i, col in enumerate(["prompt_score", "R_sft", "R", "R_risk"]):
    jigsaw_df_test[jigsaw_df_test["label"] == 0][col].plot.hist(ax=axs[i], bins=30, alpha=0.5, label="Not Offensive")
    jigsaw_df_test[jigsaw_df_test["label"] == 1][col].plot.hist(ax=axs[i], bins=30, alpha=0.5, label="Offensive")

    axs[i].set_title(f"{word_map[col]}", fontsize=30)

    axs[i].set_xlabel("Reward", fontsize=30)  # Set label size on the axis object

    axs[i].set_ylabel("")

    axs[i].tick_params(axis="both", which="major", labelsize=28)

    axs[i].grid(True)  # Add grid on the axis object

    axs[i].legend(loc="upper left", fontsize=24)  # Add legend on the axis objec

    # # zoomed in bit
    # left_lim = -1.3
    # right_lim = 0
    # axs_zoom = axs[i].inset_axes([0.10, 0.20, 0.65, 0.40])  # left, bottom, width, height

    # axs_zoom.hist(
    #     jigsaw_df_test[jigsaw_df_test["label"] == 0][col],
    #     bins=30,
    #     range=[left_lim, right_lim],
    #     alpha=0.5,
    #     label="Not Offensive",
    # )
    # axs_zoom.hist(
    #     jigsaw_df_test[jigsaw_df_test["label"] == 1][col],
    #     bins=30,
    #     range=[left_lim, right_lim],
    #     alpha=0.5,
    #     label="Offensive",
    # )
    # axs_zoom.set_ylim(0, 10)
    # axs_zoom.set_xlim(left_lim, right_lim)
    # axs_zoom.xaxis.set_major_locator(plt.MaxNLocator(3))
    # axs_zoom.yaxis.set_major_locator(plt.MaxNLocator(3))
    # axs_zoom.tick_params(axis="both", which="major", labelsize=20)
    # # set border color to orange
    # axs_zoom.spines["bottom"].set_color(zoom_border_color)
    # axs_zoom.spines["top"].set_color(zoom_border_color)
    # axs_zoom.spines["left"].set_color(zoom_border_color)
    # axs_zoom.spines["right"].set_color(zoom_border_color)
    # # set border width to zoom_thickness
    # zoom_thickness = 3.0
    # axs_zoom.spines["bottom"].set_linewidth(zoom_thickness)
    # axs_zoom.spines["top"].set_linewidth(zoom_thickness)
    # axs_zoom.spines["left"].set_linewidth(zoom_thickness)
    # axs_zoom.spines["right"].set_linewidth(zoom_thickness)
    # # rectangle coords in zoomed in plot
    # rect_height = 15
    # rectangle = plt.Rectangle(
    #     (left_lim, 0),
    #     right_lim - left_lim,
    #     rect_height,
    #     facecolor="none",
    #     edgecolor=zoom_border_color,
    #     lw=zoom_thickness,
    # )
    # axs[i].add_patch(rectangle)


# Find the maximum y-value among the first three histograms

max_count = max(axs[0].get_ylim()[1], axs[1].get_ylim()[1], axs[2].get_ylim()[1])


# Set the y-axis limits for the first three subplots to be the same
axs[0].set_ylim(0, max_count)
axs[1].set_ylim(0, max_count)
axs[2].set_ylim(0, max_count)
axs[3].set_ylim(0, max_count)
axs[0].set_ylabel(f"Count", fontsize=30)

# make quantile plot
# imdb_gen_df = imdb_df_test2
# imdb_gen_df2 = imdb_df_test
# imdb_gen_df.drop(columns=imdb_gen_df2.columns, inplace=True)
# imdb_gen_df = pd.concat([imdb_gen_df, imdb_gen_df2], axis=1)

quantile_resolution = 1

quantile_df = jigsaw_df_test.copy()
quantile_df = quantile_df.sort_values(by=["prompt_score"])
quantile_df = quantile_df.reset_index(drop=True)

# Stuff to plot
gpt2_quantiles = []
gptj_quantiles = []
rlhf_quantiles = []
rlhf_risk_quantiles = []
sft_quantiles = []

for quantile_point in range(1, 101, quantile_resolution):
    # select first quantile_point% of the data
    relevant_df = quantile_df.iloc[: int((quantile_point / 100) * quantile_df.shape[0])]

    gpt2_quantiles.append(relevant_df["R_gpt2"].mean())
    gptj_quantiles.append(relevant_df["R_gptj"].mean())
    rlhf_quantiles.append(relevant_df["R"].mean())
    rlhf_risk_quantiles.append(relevant_df["R_risk"].mean())
    sft_quantiles.append(relevant_df["R_sft"].mean())

axs[4].plot(range(1, 101, quantile_resolution), gpt2_quantiles, label="GPT2", color="black")
axs[4].plot(range(1, 101, quantile_resolution), gptj_quantiles, label="GPTJ", color="pink")
axs[4].plot(range(1, 101, quantile_resolution), sft_quantiles, label="SFT", color="blue")
axs[4].plot(range(1, 101, quantile_resolution), rlhf_quantiles, label="RLHF", color="red")
axs[4].plot(
    range(1, 101, quantile_resolution),
    rlhf_risk_quantiles,
    label="RA-RLHF",
    color="green",
)


axs[4].set_xlabel("Quantile (%)", fontsize=30)  # Set label size on the axis object
axs[4].set_ylabel(f"Average Reward", fontsize=30)
# axs[4].set_yscale("log")
axs[4].tick_params(axis="both", which="major", labelsize=28)
axs[4].grid(True)  # Add grid on the axis object
axs[4].legend(loc="lower right", fontsize=16)
axs[4].set_title(f"Reward vs Quantile", fontsize=30)


# # Zoomed in bit for the quantile plot
# axs_zoom = axs[4].inset_axes([0.65, 0.42, 0.30, 0.30])
# axs_zoom.plot(range(1, 101, quantile_resolution), gpt2_quantiles, label="GPT2", color="black")
# axs_zoom.plot(range(1, 101, quantile_resolution), sft_quantiles, label="SFT", color="blue")
# axs_zoom.plot(range(1, 101, quantile_resolution), rlhf_quantiles, label="RLHF", color="red")
# axs_zoom.plot(range(1, 101, quantile_resolution), rlhf_risk_quantiles, label="RA-RLHF", color="green")
# # increase tick font size
# axs_zoom.tick_params(axis="both", which="major", labelsize=18)
# # set border color to orange
# axs_zoom.spines["bottom"].set_color(zoom_border_color)
# axs_zoom.spines["top"].set_color(zoom_border_color)
# axs_zoom.spines["left"].set_color(zoom_border_color)
# axs_zoom.spines["right"].set_color(zoom_border_color)
# # set border width to zoom_thickness
# zoom_thickness = 3.0
# axs_zoom.spines["bottom"].set_linewidth(zoom_thickness)
# axs_zoom.spines["top"].set_linewidth(zoom_thickness)
# axs_zoom.spines["left"].set_linewidth(zoom_thickness)
# axs_zoom.spines["right"].set_linewidth(zoom_thickness)


# # rectangle coords in zoomed in plot
# x1, x2, y1, y2 = 0.3, 10.0, -1.2, -0.8
# axs_zoom.set_xlim(x1, x2)
# axs_zoom.set_ylim(y1, y2)
# rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, facecolor="none", edgecolor=zoom_border_color, lw=zoom_thickness)
# axs[4].add_patch(rect)

fig.tight_layout()
plt.savefig("./test_plots_imdb/imdb_distribution_shifts.png")
plt.savefig("./test_plots_imdb/imdb_distribution_shifts.pdf")
plt.clf()


print("\n\n")
print("Average reward for RLHF: ", jigsaw_df_test["R"].mean())
print("Average reward for RA-RLHF: ", jigsaw_df_test["R_risk"].mean())
print("Average reward for SFT: ", jigsaw_df_test["R_sft"].mean())
print("Average reward for GPTJ: ", jigsaw_df_test["R_gptj"].mean())
print("Average reward for GPT2:", jigsaw_df_test["R_gpt2"].mean())
# print("Average Perplexity for RLHF: ", jigsaw_df_test["perplexity"].mean())
# print("Average Perplexity for RA-RLHF: ", jigsaw_df_test["perplexity_risk"].mean())
# print("Average Perplexity for SFT: ", jigsaw_df_test["perplexity_sft"].mean())
# print("Average Perplexity for GPT2: ", jigsaw_df_test["perplexity_gpt2"].mean())
print("\n\n")

filtered_df = jigsaw_df_test[jigsaw_df_test["prompt_score"] < -2.5]
print("Tail Average reward for RLHF: ", filtered_df["R"].mean())
print("Tail Average reward for RA-RLHF: ", filtered_df["R_risk"].mean())
print("Tail Average reward for SFT: ", filtered_df["R_sft"].mean())
print("Tail Average reward for GPT2: ", filtered_df["R_gpt2"].mean())
print("Tail Average reward for GPTJ: ", filtered_df["R_gptj"].mean())
# print("Tail Average Perplexity for RLHF: ", filtered_df["perplexity"].mean())
# print("Tail Average Perplexity for RA-RLHF: ", filtered_df["perplexity_risk"].mean())
# print("Tail Average Perplexity for SFT: ", filtered_df["perplexity_sft"].mean())
# print("Tail Average Perplexity for GPT2: ", filtered_df["perplexity_gpt2"].mean())


# Quantile plots of returns for jigsaw

# rlhf_returns_quantiles = []

# rlhf_risk_returns_quantiles = []

# for quantile_point in range(1, 101, quantile_resolution):
#     # select first quantile_point% of the data
#     relevant_df = quantile_df.iloc[: int((quantile_point / 100) * quantile_df.shape[0])]

#     rlhf_returns_quantiles.append(relevant_df["R_bar"].mean())
#     rlhf_risk_returns_quantiles.append(relevant_df["R_bar_risk"].mean())


# fig, axs = plt.subplots(figsize=(10, 5))
# word_map = {"R_bar": "RLHF returns", "R_bar_risk": "RA-RLHF returns"}

# axs.plot(range(1, 101, quantile_resolution), rlhf_returns_quantiles, label="RLHF", color="red")
# axs.plot(range(1, 101, quantile_resolution), rlhf_risk_returns_quantiles, label="RA-RLHF", color="green")

# axs.set_xlabel("Quantile (%)", fontsize=30)  # Set label size on the axis object
# axs.set_ylabel(f"Returns", fontsize=30)
# axs.tick_params(axis="both", which="major", labelsize=14)
# axs.grid(True)  # Add grid on the axis object
# axs.legend(loc="upper left", fontsize=16)
# axs.set_title(f"Quantile plot of returns", fontsize=30)


# fig.tight_layout()
# plt.savefig("./test_plots_imdb/imdb_R_bar_quantiles.png")
# plt.savefig("./test_plots_imdb/imdb_R_bar_quantiles.pdf")
# plt.clf()

File Path: examples/OLID/plotting/test_plotting_old.py
Content:
import matplotlib.pyplot as plt
import plotly.express as px
import pandas as pd
import numpy as np

# # IMDB
# jigsaw_df_test = pd.read_csv("imdb_generations.csv")

# # Rename as needed in plots
# # jigsaw_df_test = jigsaw_df_test.rename(columns={"R": "RLHF", "R_risk": "RLHF risk averse"})
# # jigsaw_df_test = jigsaw_df_test.rename(columns={"R_bar": "RLHF returns", "R_bar_risk": "RLHF risk averse returns"})


# # # CDF plots
# # fig = px.ecdf(
# #     jigsaw_df_test,
# #     x=["RLHF", "RLHF risk averse"],
# #     labels={
# #         "value": "Reward",
# #         "variable": "Model",
# #     },
# # )
# # fig.update_layout(
# #     title="Reward distribution for RLHF and RLHF risk averse models",
# #     xaxis_title="Reward",
# #     yaxis_title="CDF",
# # )
# # print("Average reward for RLHF: ", jigsaw_df_test["RLHF"].mean())
# # print("Average reward for RLHF risk averse: ", jigsaw_df_test["RLHF risk averse"].mean())
# # fig.write_image("imdb_R_cdf.png")

# # fig = px.ecdf(
# #     jigsaw_df_test,
# #     x=["RLHF returns", "RLHF risk averse returns"],
# #     labels={
# #         "value": "Reward",
# #         "variable": "Model",
# #     },
# # )
# # fig.update_layout(
# #     title="Returns distribution for RLHF and RLHF risk averse models",
# #     xaxis_title="Returns",
# #     yaxis_title="CDF",
# # )
# # print("Average return for RLHF: ", jigsaw_df_test["RLHF returns"].mean())
# # print("Average return for RLHF risk averse: ", jigsaw_df_test["RLHF risk averse returns"].mean())
# # fig.write_image("imdb_R_bar_cdf.png")


# # Histogram shift plots for imdb
# fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(10, 5))
# word_map = {"prompt_score": "Prompt", "R": "RLHF", "R_risk": "RA-RLHF"}
# for i, col in enumerate(["prompt_score", "R", "R_risk"]):
#     jigsaw_df_test[jigsaw_df_test["label"] == 0][col].plot.hist(ax=axs[i], bins=30, alpha=0.5, label="Negative")
#     jigsaw_df_test[jigsaw_df_test["label"] == 1][col].plot.hist(ax=axs[i], bins=30, alpha=0.5, label="Positive")

#     axs[i].set_title(f"{word_map[col]}")
#     axs[i].set_xlabel("Reward", fontsize=16)  # Set label size on the axis object
#     axs[i].set_ylabel(f"Count", fontsize=16)
#     axs[i].tick_params(axis="both", which="major", labelsize=14)
#     axs[i].grid(True)  # Add grid on the axis object
#     axs[i].legend(loc="upper left")  # Add legend on the axis objec

# fig.tight_layout()
# plt.savefig("./test_plots_imdb/imdb_distribution_shifts.png")
# plt.savefig("./test_plots_imdb/imdb_distribution_shifts.pdf")
# plt.clf()

# # Quantile plots of returns for imdb

# fig, axs = plt.subplots(figsize=(10, 5))

# word_map = {"R_bar": "RLHF returns", "R_bar_risk": "RA-RLHF returns"}

# for i, col in enumerate(["R_bar", "R_bar_risk"]):
#     x = np.linspace(0, 1, len(jigsaw_df_test[col])) * 100
#     y = jigsaw_df_test[col].sort_values().values
#     axs.plot(x, y, label=word_map[col])

# axs.set_xlabel("Quantile (%)", fontsize=16)  # Set label size on the axis object
# axs.set_ylabel(f"Returns", fontsize=16)
# axs.tick_params(axis="both", which="major", labelsize=14)
# axs.grid(True)  # Add grid on the axis object
# axs.legend(loc="upper left")
# axs.set_title(f"Quantile plot of returns")


# fig.tight_layout()
# plt.savefig("./test_plots_imdb/imdb_R_bar_quantiles.png")
# plt.savefig("./test_plots_imdb/imdb_R_bar_quantiles.pdf")
# plt.clf()


# # Quantile plots of rewards for imdb

# fig, axs = plt.subplots(figsize=(10, 5))

# word_map = {"R": "RLHF", "R_risk": "RA-RLHF"}

# for i, col in enumerate(["R", "R_risk"]):
#     x = np.linspace(0, 1, len(jigsaw_df_test[col])) * 100
#     y = jigsaw_df_test[col].sort_values().values
#     axs.plot(x, y, label=word_map[col])

# axs.set_xlabel("Quantile (%)", fontsize=16)  # Set label size on the axis object
# axs.set_ylabel(f"Rewards", fontsize=16)
# axs.tick_params(axis="both", which="major", labelsize=14)
# axs.grid(True)  # Add grid on the axis object
# axs.legend(loc="upper left")
# axs.set_title(f"Quantile plot of rewards")


# fig.tight_layout()
# plt.savefig("./test_plots_imdb/imdb_R_quantiles.png")
# plt.savefig("./test_plots_imdb/imdb_R_quantiles.pdf")
# plt.clf()

# # Box plots of returns for imdb

# fig, axs = plt.subplots(figsize=(10, 5))


# # Jigsaw
jigsaw_df_test = pd.read_csv("../dataset/jigsaw_generations_8_32_alpha_40_seed_2.csv")

# Rename as needed in plots
# jigsaw_df_test = jigsaw_df_test.rename(columns={"R": "RLHF", "R_risk": "RLHF risk averse"})
# jigsaw_df_test = jigsaw_df_test.rename(columns={"R_bar": "RLHF returns", "R_bar_risk": "RLHF risk averse returns"})

# # CDF plots
# fig = px.ecdf(
#     jigsaw_df_test,
#     x=["RLHF", "RLHF risk averse"],
#     labels={
#         "value": "Reward",
#         "variable": "Model",
#     },
# )
# fig.update_layout(
#     title="Reward distribution for RLHF and RLHF risk averse models",
#     xaxis_title="Reward",
#     yaxis_title="CDF",
# )
# print("Average reward for RLHF: ", jigsaw_df_test["RLHF"].mean())
# print("Average reward for RLHF risk averse: ", jigsaw_df_test["RLHF risk averse"].mean())
# fig.write_image("jigsaw_R_cdf.png")

# fig = px.ecdf(
#     jigsaw_df_test,
#     x=["RLHF returns", "RLHF risk averse returns"],
#     labels={
#         "value": "Reward",
#         "variable": "Model",
#     },
# )
# fig.update_layout(
#     title="Returns distribution for RLHF and RLHF risk averse models",
#     xaxis_title="Returns",
#     yaxis_title="CDF",
# )
# print("Average return for RLHF: ", jigsaw_df_test["RLHF returns"].mean())
# print("Average return for RLHF risk averse: ", jigsaw_df_test["RLHF risk averse returns"].mean())
# fig.write_image("jigsaw_R_bar_cdf.png")

tail_to_save = -3

tail_df = jigsaw_df_test[jigsaw_df_test["prompt_score"] <= tail_to_save]

tail_df = tail_df.drop(columns=["input_ids"])

print(tail_df["R"].mean())

print(tail_df["R_risk"].mean())


# Plot histogram shifts for jigsaw
def plot_hist_top_edges(data, ax, bins, color, label):
    hist, bin_edges = np.histogram(data, bins=bins)

    for left, right, height in zip(bin_edges[:-1], bin_edges[1:], hist):
        ax.plot([left, right], [height, height], color=color, label=label)

        label = None  # To avoid duplicate labels in the legend


fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(30, 6))

word_map = {"prompt_score": "Prompt Score", "R": "RLHF", "R_risk": "RA-RLHF"}

for i, col in enumerate(["prompt_score", "R", "R_risk"]):
    jigsaw_df_test[jigsaw_df_test["toxic"] == 1][col].plot.hist(ax=axs[i], bins=30, alpha=0.5, label="Toxic")
    jigsaw_df_test[jigsaw_df_test["toxic"] == 0][col].plot.hist(ax=axs[i], bins=30, alpha=0.5, label="Non Toxic")

    axs[i].set_title(f"{word_map[col]}", fontsize=30)

    axs[i].set_xlabel("Reward", fontsize=30)  # Set label size on the axis object

    axs[i].set_ylabel("")

    axs[i].tick_params(axis="both", which="major", labelsize=28)

    axs[i].grid(True)  # Add grid on the axis object

    axs[i].legend(loc="upper left", fontsize=24)  # Add legend on the axis objec


# Find the maximum y-value among the first three histograms

max_count = max(axs[0].get_ylim()[1], axs[1].get_ylim()[1], axs[2].get_ylim()[1])


# Set the y-axis limits for the first three subplots to be the same

axs[0].set_ylim(0, max_count)

axs[1].set_ylim(0, max_count)

axs[2].set_ylim(0, max_count)

axs[0].set_ylabel(f"Count", fontsize=30)


# tail prompts

tail_filter_level = tail_to_save

filtered_df = jigsaw_df_test[jigsaw_df_test["prompt_score"] <= tail_filter_level]

print(filtered_df.shape[0])

filtered_df[filtered_df["toxic"] == 1]["prompt_score"].plot.hist(ax=axs[3], bins=30, alpha=0.5, label="Toxic")
filtered_df[filtered_df["toxic"] == 0]["prompt_score"].plot.hist(ax=axs[3], bins=30, alpha=0.5, label="Non Toxic")


axs[3].set_title("Tail Prompts", fontsize=30)

axs[3].set_xlabel("Reward", fontsize=30)  # Set label size on the axis object

axs[3].set_ylabel("")

axs[3].tick_params(axis="both", which="major", labelsize=28)

axs[3].grid(True)  # Add grid on the axis object

axs[3].legend(loc="upper left", fontsize=24)  # Add legend on the axis objec

axs[3].axvline(x=tail_filter_level, color="red", linestyle="-", linewidth=4)


# tail performance

filtered_df["R"].plot.hist(ax=axs[4], bins=30, alpha=0.3, label="RLHF", color="red")

filtered_df["R_risk"].plot.hist(ax=axs[4], bins=30, alpha=0.5, label="RA-RLHF", color="olive")

axs[4].set_title("Tail Performance", fontsize=30)

axs[4].set_xlabel("Reward", fontsize=30)  # Set label size on the axis object

axs[4].set_ylabel("")

axs[4].tick_params(axis="both", which="major", labelsize=28)

axs[4].grid(True)  # Add grid on the axis object

axs[4].legend(loc="upper right", fontsize=24)  # Add legend on the axis objec

axs[4].axvline(x=tail_filter_level, color="red", linestyle="-", linewidth=4)


# Find the maximum y-value among the first three histograms

max_count_2 = max(axs[3].get_ylim()[1], axs[4].get_ylim()[1])


# Set the y-axis limits for the first three subplots to be the same

axs[3].set_ylim(0, max_count_2)

axs[4].set_ylim(0, max_count_2)

# axs[3].set_xlim(-7.5, -5.9)
# axs[4].set_xlim(-8, 0)


fig.tight_layout()

plt.savefig("./test_plots_jigsaw/jigsaw_distribution_shifts.png")

plt.savefig("./test_plots_jigsaw/jigsaw_distribution_shifts.pdf")

plt.clf()


print("\n\n")
print("Average reward for RLHF: ", jigsaw_df_test["R"].mean())
print("Average reward for RA-RLHF: ", jigsaw_df_test["R_risk"].mean())
print("Average reward for SFT: ", jigsaw_df_test["R_sft"].mean())
print("Average reward for GPT2:", jigsaw_df_test["R_gpt2"].mean())
print("Average Perplexity for RLHF: ", jigsaw_df_test["perplexity"].mean())
print("Average Perplexity for RA-RLHF: ", jigsaw_df_test["perplexity_risk"].mean())
print("Average Perplexity for SFT: ", jigsaw_df_test["perplexity_sft"].mean())
print("Average Perplexity for GPT2: ", jigsaw_df_test["perplexity_gpt2"].mean())
print("\n\n")
print("Tail Average reward for RLHF: ", filtered_df["R"].mean())
print("Tail Average reward for RA-RLHF: ", filtered_df["R_risk"].mean())
print("Tail Average reward for SFT: ", filtered_df["R_sft"].mean())
print("Tail Average reward for GPT2: ", filtered_df["R_gpt2"].mean())
print("Tail Average Perplexity for RLHF: ", filtered_df["perplexity"].mean())
print("Tail Average Perplexity for RA-RLHF: ", filtered_df["perplexity_risk"].mean())
print("Tail Average Perplexity for SFT: ", filtered_df["perplexity_sft"].mean())
print("Tail Average Perplexity for GPT2: ", filtered_df["perplexity_gpt2"].mean())


# Quantile plots of returns for jigsaw

fig, axs = plt.subplots(figsize=(10, 5))
word_map = {"R_bar": "RLHF returns", "R_bar_risk": "RA-RLHF returns"}

for i, col in enumerate(["R_bar", "R_bar_risk"]):
    x = np.linspace(0, 1, len(jigsaw_df_test[col])) * 100
    y = jigsaw_df_test[col].sort_values().values
    axs.plot(x, y, label=word_map[col])

axs.set_xlabel("Quantile (%)", fontsize=16)  # Set label size on the axis object
axs.set_ylabel(f"Returns", fontsize=16)
axs.tick_params(axis="both", which="major", labelsize=14)
axs.grid(True)  # Add grid on the axis object
axs.legend(loc="upper left")
axs.set_title(f"Quantile plot of returns")


fig.tight_layout()
plt.savefig("./test_plots_jigsaw/jigsaw_R_bar_quantiles.png")
plt.savefig("./test_plots_jigsaw/jigsaw_R_bar_quantiles.pdf")
plt.clf()

# Quantile plots of rewards for jigsaw

fig, axs = plt.subplots(figsize=(10, 5))
word_map = {"R": "RLHF", "R_risk": "RA-RLHF"}
for i, col in enumerate(["R", "R_risk"]):
    x = np.linspace(0, 1, len(jigsaw_df_test[col])) * 100
    y = jigsaw_df_test[col].sort_values().values
    axs.plot(x, y, label=word_map[col])

axs.set_xlabel("Quantile (%)", fontsize=16)  # Set label size on the axis object
axs.set_ylabel(f"Rewards", fontsize=16)
axs.tick_params(axis="both", which="major", labelsize=14)
axs.grid(True)  # Add grid on the axis object
axs.legend(loc="upper left")
axs.set_title(f"Quantile plot of rewards")


fig.tight_layout()
plt.savefig("./test_plots_jigsaw/jigsaw_R_quantiles.png")
plt.savefig("./test_plots_jigsaw/jigsaw_R_quantiles.pdf")
plt.clf()

File Path: examples/OLID/training/appendix/imdb_analysis.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, set_seed
from trl.trainer.ppo_trainer_original import PPOTrainer
from trl.core import LengthSampler


tqdm.pandas()
import matplotlib.pyplot as plt
import itertools

import pdb
import pickle
import datetime
import os
from evaluate import load
from transformers import AutoModel, AutoTokenizer, AutoConfig
from transformers import OpenAIGPTTokenizer, OpenAIGPTModel
import random

import pandas as pd
import pdb

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
print(date_n_time)
logging_dir = f"/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy_old/{date_n_time}"
os.makedirs(logging_dir)


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name="lvwerra/gpt2-imdb",
            query_dataset="imdb",
            reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            remove_unused_columns=False,
        )
    )
    query_dataset: str = field(default="imdb", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="less", metadata={"help": "the dataset to query"})


args = tyro.cli(ScriptArguments)

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8, data_split="train"):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # load imdb with datasets
    # to load partial dataset, just change data split to split=data_split+'[:200]'
    ds = load_dataset(query_dataset, split=data_split)
    ds = ds.rename_columns({"text": "review"})
    ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)

    input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# get test dataset
test_dataset = build_dataset(args.ppo_config, args.query_dataset, data_split="train")
# Take 5k random samples
# random.seed(10)
# sample_size = 2500
# random_indices = random.sample(range(len(test_dataset)), sample_size)
# test_dataset = test_dataset.select(random_indices)

# pdb.set_trace()


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

zero_shot_model = trl_model_class.from_pretrained(
    "gpt2",
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)

sft_model = trl_model_class.from_pretrained(
    "lvwerra/gpt2-imdb",
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)

# For the following to work I should have saved the model using huggingface
# ppo_model = trl_model_class.from_pretrained(
#     '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-09-26_11-27-23/model.pt',
#     trust_remote_code=True,
#     device_map=device_map,
#     peft_config=peft_config,
# )

# sft_ppo_model = trl_model_class.from_pretrained(
#     "/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/trl_sft_ppo/2023-10-23_23-30-47",
#     trust_remote_code=True,
#     device_map=device_map,
#     peft_config=peft_config,
# )
"""
ppo_model_PATH = '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-09-26_11-27-23/model.pt'
ppo_model = torch.load(ppo_model_PATH)  

sft_ppo_model_PATH = '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_ppo/2023-09-26_10-36-41/model.pt'
sft_ppo_model = torch.load(sft_ppo_model_PATH)
"""
tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"
tokenizer.truncation_side = "left"
tokenizer.pad_token_as_eos_token = True
tokenizer.max_length = 64

# Define all the tester classes
zero_shot_tester = PPOTrainer(
    args.ppo_config, zero_shot_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator
)
sft_tester = PPOTrainer(args.ppo_config, sft_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)
# ppo_tester = PPOTrainer(args.ppo_config, ppo_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)
# sft_ppo_tester = PPOTrainer(
#     args.ppo_config, sft_ppo_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator
# )

# pdb.set_trace()
# ppo_tester._save_pretrained('/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-09-26_11-27-23')
# sft_ppo_tester._save_pretrained('/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_ppo/2023-09-26_10-36-41')

# Build the perplexity evaluation pipeline
perplexity = load("perplexity", module_type="measurement")

# DO I NEED TO TAKE CARE OF PAD TOKEN FOR PERPLEXITY PIPELINE AS WELL??
"""
# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id
"""
device = sft_tester.accelerator.device
if sft_tester.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = sft_tester.accelerator.state.deepspeed_plugin
task, model_name = args.ppo_config.reward_model.split(":")
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id


# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs = {
    "min_length": 48,
    "top_k": 0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": 48,
}


# date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
# os.makedirs(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/evaluation/{date_n_time}')

model_names_all = ["zero_shot_model", "sft_model", "ppo_model", "sft_ppo_model"]
testers_all = [
    zero_shot_tester,
    sft_tester,
]  # sft_ppo_tester]  # ppo_tester,

# infer on test dataset
# test_scores_1, test_scores_2, test_scores_3, test_scores_4 = [], [], [], []
positive_score_all = []
negative_score_all = []
label_all = []
critical_pos_texts = []
critical_neg_texts = []
critical_neg_scores = []
critical_pos_scores = []
total_critical_texts = []
total_critical_scores = []
total_critical_labels = []

for epoch, batch in tqdm(enumerate(zero_shot_tester.dataloader)):
    query_tensors = batch["input_ids"]

    texts = batch["review"]
    texts = [text[:200] for text in texts]
    # texts = ["lorem ipsum", "Happy Birthday!", "Bienvenue"]

    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
    pos_score = [output[1]["score"] for output in pipe_outputs]
    positive_score_all.append(pos_score)

    neg_score = [output[0]["score"] for output in pipe_outputs]
    negative_score_all.append(neg_score)

    labels = [label.item() for label in batch["label"]]
    label_all.append(labels)

    for ii in range(len(label_all[0])):
        if labels[ii] == 0 and pos_score[ii] < -2.8:
            critical_neg_texts.append(texts[ii])
            critical_neg_scores.append(pos_score[ii])
            total_critical_texts.append(texts[ii])
            total_critical_scores.append(pos_score[ii])
            total_critical_labels.append(labels[ii])
        if labels[ii] == 1 and pos_score[ii] < -2.8:
            critical_pos_texts.append(texts[ii])
            critical_pos_scores.append(pos_score[ii])
            total_critical_texts.append(texts[ii])
            total_critical_scores.append(pos_score[ii])
            total_critical_labels.append(labels[ii])

print(f"Total critical reviews: {len(total_critical_labels)}")
# get flattened lists
pos_scores = list(itertools.chain(*positive_score_all))
neg_scores = list(itertools.chain(*negative_score_all))
labels = list(itertools.chain(*label_all))


# Plot histograms
df = pd.DataFrame(list(zip(neg_scores, pos_scores, labels)), columns=["neg score", "pos score", "label"])
fig2, axs2 = plt.subplots(figsize=(10, 5))

df[df["label"] == 1]["pos score"].plot.hist(ax=axs2, bins=30, alpha=0.5, label="Positive")
df[df["label"] == 0]["pos score"].plot.hist(ax=axs2, bins=30, alpha=0.5, label="Negative")
axs2.legend()
axs2.set_ylabel(r"No. of input prompts", fontsize=16)  # Setting label size
axs2.set_xlabel(r"Enviroment Score", fontsize=16)
axs2.tick_params(axis="both", which="major", labelsize=14)  # Adjusting tick size
axs2.set_box_aspect(0.7)
axs2.grid(True)
fig2.savefig("histogram_plot.pdf", bbox_inches="tight")

# save critical reviews as csv
df1 = pd.DataFrame({"score": critical_neg_scores, "review": critical_neg_texts})
df2 = pd.DataFrame({"score": critical_pos_scores, "review": critical_pos_texts})
df3 = pd.DataFrame({"review": total_critical_texts})

df1 = df1.sort_values("score")
df2 = df2.sort_values("score")

df1.to_csv("critical_neg_texts.csv")
df2.to_csv("critical_pos_texts.csv")
df3.to_csv("total_critical_texts.csv")


# histogram plot for scores of critical reviews
df4 = pd.DataFrame(list(zip(total_critical_scores, total_critical_labels)), columns=["pos score", "label"])
fig3, axs3 = plt.subplots(figsize=(10, 5))

df4[df4["label"] == 1]["pos score"].plot.hist(ax=axs3, bins=30, alpha=0.5, label="Positive")
df4[df4["label"] == 0]["pos score"].plot.hist(ax=axs3, bins=30, alpha=0.5, label="Negative")
axs3.legend()
axs3.set_ylabel(r"No. of input prompts", fontsize=16)  # Setting label size
axs3.set_xlabel(r"Enviroment Score", fontsize=16)
axs3.tick_params(axis="both", which="major", labelsize=14)  # Adjusting tick size
axs3.set_box_aspect(0.7)
axs3.grid(True)

fig3.savefig("critical_context_scores.pdf", bbox_inches="tight")

File Path: examples/OLID/training/appendix/jigsaw_analysis.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset, Dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, set_seed
from trl.trainer.ppo_trainer_original import PPOTrainer
from trl.core import LengthSampler


tqdm.pandas()
import matplotlib.pyplot as plt
import itertools

import pdb
import pickle
import datetime
import os
from evaluate import load
from transformers import AutoModel, AutoTokenizer, AutoConfig
from transformers import OpenAIGPTTokenizer, OpenAIGPTModel
import random

import pandas as pd
import pdb

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")

logging_dir = f"/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy_old/{date_n_time}"
os.makedirs(logging_dir)


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name="lvwerra/gpt2-imdb",
            query_dataset="jigsaw_toxicity_pred",
            reward_model="unitary/toxic-bert",
            # reward_model="citizenlab/twitter-xlm-roberta-base-sentiment-finetunned",
            learning_rate=1.41e-5,
            # log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            steps=128000,
            exp_name="ppo",
            remove_unused_columns=False,
        )
    )
    query_dataset: str = field(default="jigsaw_toxicity_pred", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="more", metadata={"help": "the dataset to query"})

    generation_kwargs_min_length: Optional[int] = field(
        default=-1, metadata={"help": "minimum number of tokens while generation"}
    )
    generation_kwargs_top_k: Optional[int] = field(default=0, metadata={"help": "gneration top k"})
    generation_kwargs_top_p: Optional[float] = field(default=1.0, metadata={"help": "gneration top p"})
    generation_kwargs_max_new_tokens: Optional[int] = field(default=32, metadata={"help": "gneration top p"})

    change_tokenizer_args: bool = field(default=False, metadata={"help": "whether to use modify tokenizer settings"})
    tokenizer_kwargs: dict = field(
        default_factory=lambda: {
            "padding_side": "left",
            "truncation_side": "left",
            "pad_token_as_eos_token": True,
            "max_length": 64,
        }
    )


args = tyro.cli(ScriptArguments)

generation_kwargs = {
    "min_length": args.generation_kwargs_min_length,
    "top_k": args.generation_kwargs_top_k,
    "top_p": args.generation_kwargs_top_p,
    "do_sample": True,
    "max_new_tokens": args.generation_kwargs_max_new_tokens,
}

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # Kaggle datasets have to be downloaded manually (using Kaggle API)
    # datasets.load_dataset("jigsaw_toxicity_pred", data_dir="<path/to/manual/data>")
    # ds = load_dataset(
    #     query_dataset,
    #     data_dir="/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/original_dataset",
    #     split="train",
    # )  # Hardcoding path for now TODO: change this

    # ds = ds.rename_columns({"comment_text": "review"})
    # ds = ds.filter(lambda x: len(x["review"]) > 64, batched=False)

    # df = pd.DataFrame(ds)
    # num_toxic = df["toxic"].sum()

    # toxic_df = df[df["toxic"] == True]
    # non_toxic_df = df[df["toxic"] == False]

    # non_toxic_df = non_toxic_df.sample(n=num_toxic, random_state=config.seed)

    # # Recombine into dataset
    # ds = ds.from_pandas(pd.concat([toxic_df, non_toxic_df], ignore_index=True))
    df = pd.read_csv(
        "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw_2/dataset/jigsaw_train_70_30.csv",
    )
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"comment_text": "review"})

    # pdb.set_trace()

    if args.input_query_size == "more":

        def input_size():
            return 20

    else:
        input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    args.ppo_config.model_name,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id

# pdb.set_trace()

if args.change_tokenizer_args == True:
    tokenizer.padding_side = args.tokenizer_kwargs["padding_side"]
    tokenizer.truncation_side = args.tokenizer_kwargs["truncation_side"]
    tokenizer.pad_token_as_eos_token = args.tokenizer_kwargs["pad_token_as_eos_token"]
    tokenizer.max_length = args.tokenizer_kwargs["max_length"]

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
# task, model_name = args.ppo_config.reward_model.split(":")
task = "text-classification"
model_name = args.ppo_config.reward_model
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs["pad_token_id"] = tokenizer.eos_token_id

# empty list to store text data
text_train_data = []

# infer on test dataset
# test_scores_1, test_scores_2, test_scores_3, test_scores_4 = [], [], [], []
positive_score_all = []
negative_score_all = []
label_all = []
critical_pos_texts = []
critical_neg_texts = []
critical_neg_scores = []
critical_pos_scores = []
total_critical_texts = []
total_critical_scores = []
total_critical_labels = []

for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    texts = batch["query"]
    texts = [text[:200] for text in texts]

    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
    pos_score = [-output[0]["score"] for output in pipe_outputs]
    positive_score_all.append(pos_score)
    # rewards = [torch.tensor(-output[0]["score"]) for output in pipe_outputs]

    neg_score = [-output[1]["score"] for output in pipe_outputs]
    negative_score_all.append(neg_score)

    labels = [label.item() for label in batch["toxic"]]
    label_all.append(labels)

    for ii in range(len(label_all[0])):
        if labels[ii] == 0 and pos_score[ii] < -5.0:
            critical_neg_texts.append(texts[ii])
            critical_neg_scores.append(pos_score[ii])
            total_critical_texts.append(texts[ii])
            total_critical_scores.append(pos_score[ii])
            total_critical_labels.append(labels[ii])
        if labels[ii] == 1 and pos_score[ii] < -5.0:
            critical_pos_texts.append(texts[ii])
            critical_pos_scores.append(pos_score[ii])
            total_critical_texts.append(texts[ii])
            total_critical_scores.append(pos_score[ii])
            total_critical_labels.append(labels[ii])

print(f"Total critical reviews: {len(total_critical_labels)}")
# get flattened lists
pos_scores = list(itertools.chain(*positive_score_all))
neg_scores = list(itertools.chain(*negative_score_all))
labels = list(itertools.chain(*label_all))


# Plot histograms
df = pd.DataFrame(list(zip(neg_scores, pos_scores, labels)), columns=["neg score", "pos score", "label"])
fig2, axs2 = plt.subplots(figsize=(10, 5))

df[df["label"] == 0]["pos score"].plot.hist(ax=axs2, bins=30, alpha=0.5, label="Non-toxic")
df[df["label"] == 1]["pos score"].plot.hist(ax=axs2, bins=30, alpha=0.5, label="Toxic")
axs2.legend()
axs2.set_ylabel(r"No. of input prompts", fontsize=16)  # Setting label size
axs2.set_xlabel(r"Enviroment Score", fontsize=16)
axs2.tick_params(axis="both", which="major", labelsize=14)  # Adjusting tick size
axs2.set_box_aspect(0.7)
axs2.grid(True)
fig2.savefig("histogram_plot.pdf", bbox_inches="tight")

# save critical reviews as csv
df1 = pd.DataFrame({"score": critical_neg_scores, "review": critical_neg_texts})
df2 = pd.DataFrame({"score": critical_pos_scores, "review": critical_pos_texts})
df3 = pd.DataFrame({"review": total_critical_texts})

df1 = df1.sort_values("score")
df2 = df2.sort_values("score")

df1.to_csv("critical_neg_texts.csv")
df2.to_csv("critical_pos_texts.csv")
df3.to_csv("total_critical_texts.csv")


# histogram plot for scores of critical reviews
df4 = pd.DataFrame(list(zip(total_critical_scores, total_critical_labels)), columns=["pos score", "label"])
fig3, axs3 = plt.subplots(figsize=(10, 5))

df4[df4["label"] == 0]["pos score"].plot.hist(ax=axs3, bins=30, alpha=0.5, label="Non-toxic")
df4[df4["label"] == 1]["pos score"].plot.hist(ax=axs3, bins=30, alpha=0.5, label="Toxic")
axs3.legend()
axs3.set_ylabel(r"No. of input prompts", fontsize=16)  # Setting label size
axs3.set_xlabel(r"Enviroment Score", fontsize=16)
axs3.tick_params(axis="both", which="major", labelsize=14)  # Adjusting tick size
axs3.set_box_aspect(0.7)
axs3.grid(True)

fig3.savefig("critical_context_scores.pdf", bbox_inches="tight")

File Path: examples/OLID/training/ppo_auth2.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset, Dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, set_seed
from trl.core import LengthSampler
from trl.trainer.ppo_trainer_original import PPOTrainer

tqdm.pandas()

import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import pdb
import pandas as pd

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")

logging_dir = f"/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/{date_n_time}"
os.makedirs(logging_dir)


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-20_20-32-35",  # sft_both
            model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-25_16-31-51",  # sft_positive
            query_dataset="jigsaw_toxicity_pred",
            reward_model="unitary/toxic-bert",
            # reward_model="citizenlab/twitter-xlm-roberta-base-sentiment-finetunned",
            learning_rate=1.41e-5,
            # log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            # adap_kl_ctrl=False,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            steps=128000,
            exp_name="ppo",
        )
    )
    query_dataset: str = field(default="jigsaw_toxicity_pred", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="more", metadata={"help": "the dataset to query"})

    generation_kwargs_min_length: Optional[int] = field(
        default=32, metadata={"help": "minimum number of tokens while generation"}
    )
    generation_kwargs_top_k: Optional[int] = field(default=0, metadata={"help": "gneration top k"})
    generation_kwargs_top_p: Optional[float] = field(default=1.0, metadata={"help": "gneration top p"})
    generation_kwargs_max_new_tokens: Optional[int] = field(default=32, metadata={"help": "gneration top p"})

    change_tokenizer_args: bool = field(default=False, metadata={"help": "whether to use modify tokenizer settings"})
    tokenizer_kwargs: dict = field(
        default_factory=lambda: {
            "padding_side": "left",
            "truncation_side": "left",
            "pad_token_as_eos_token": True,
            "max_length": 64,
        }
    )
    prompt_len: Optional[int] = field(default=32, metadata={"help": "prompt length"})


args = tyro.cli(ScriptArguments)


generation_kwargs = {
    "min_length": args.generation_kwargs_min_length,
    "top_k": args.generation_kwargs_top_k,
    "top_p": args.generation_kwargs_top_p,
    "do_sample": True,
    "max_new_tokens": args.generation_kwargs_max_new_tokens,
}

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # # Kaggle datasets have to be downloaded manually (using Kaggle API)
    # # datasets.load_dataset("jigsaw_toxicity_pred", data_dir="<path/to/manual/data>")
    # ds = load_dataset(
    #     query_dataset,
    #     data_dir="/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/original_dataset",
    #     split="train",
    # )  # Hardcoding path for now TODO: change this

    # ds = ds.rename_columns({"comment_text": "review"})
    # ds = ds.filter(lambda x: len(x["review"]) > 64, batched=False)

    # df = pd.DataFrame(ds)
    # num_toxic = df["toxic"].sum()

    # toxic_df = df[df["toxic"] == True]
    # non_toxic_df = df[df["toxic"] == False]

    # non_toxic_df = non_toxic_df.sample(n=num_toxic, random_state=config.seed)

    # # Recombine into dataset
    # ds = ds.from_pandas(pd.concat([toxic_df, non_toxic_df], ignore_index=True))
    df = pd.read_csv(
        "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/jigsaw_train.csv",
    )
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"comment_text": "review"})

    # pdb.set_trace()

    if args.input_query_size == "more":

        def input_size():
            return args.prompt_len

    else:
        input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    args.ppo_config.model_name,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id

# pdb.set_trace()

if args.change_tokenizer_args == True:
    tokenizer.padding_side = args.tokenizer_kwargs["padding_side"]
    tokenizer.truncation_side = args.tokenizer_kwargs["truncation_side"]
    tokenizer.pad_token_as_eos_token = args.tokenizer_kwargs["pad_token_as_eos_token"]
    tokenizer.max_length = args.tokenizer_kwargs["max_length"]

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
# task, model_name = args.ppo_config.reward_model.split(":")
task = "text-classification"
model_name = args.ppo_config.reward_model
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs["pad_token_id"] = tokenizer.eos_token_id

# empty list to store text data
text_train_data = []

for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(
        texts,
        **sent_kwargs,
        # truncation=True,
    )

    if args.reward_function == "trl":
        rewards = [torch.tensor(-output[0]["score"]) for output in pipe_outputs]
    else:  # depreciated for jigsaw task
        raise NotImplementedError
        rewards = [
            -torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"], output[2]["score"]]))[2]
            for output in pipe_outputs
        ]
    # pdb.set_trace()
    for ii in range(3):
        data_row = {
            "iter": epoch,
            "input": batch["query"][ii],
            "output": batch["response"][ii],
            "score": rewards[ii].item(),
        }
        text_train_data.append(data_row)

    # pdb.set_trace()

    # Run PPO step
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
    ppo_trainer.log_stats(stats, batch, rewards)
    print(epoch)

ppo_trainer._save_pretrained(logging_dir + "/save_pretrained")
df = pd.DataFrame(text_train_data)
df.to_csv(logging_dir + "/textual_data.csv", index=False)

File Path: examples/OLID/training/ppo_auth2_seed12.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset, Dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, set_seed
from trl.core import LengthSampler
from trl.trainer.ppo_trainer_original import PPOTrainer

tqdm.pandas()

import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import pdb
import pandas as pd

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")

logging_dir = f"/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/{date_n_time}"
os.makedirs(logging_dir)


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-20_20-32-35",  # sft_both
            model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-30_00-30-38",  # sft_positive seed 12
            query_dataset="jigsaw_toxicity_pred",
            reward_model="unitary/toxic-bert",
            # reward_model="citizenlab/twitter-xlm-roberta-base-sentiment-finetunned",
            learning_rate=1.41e-5,
            # log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            # adap_kl_ctrl=False,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            steps=192000,
            exp_name="ppo",
        )
    )
    query_dataset: str = field(default="jigsaw_toxicity_pred", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="more", metadata={"help": "the dataset to query"})

    generation_kwargs_min_length: Optional[int] = field(
        default=32, metadata={"help": "minimum number of tokens while generation"}
    )
    generation_kwargs_top_k: Optional[int] = field(default=0, metadata={"help": "gneration top k"})
    generation_kwargs_top_p: Optional[float] = field(default=1.0, metadata={"help": "gneration top p"})
    generation_kwargs_max_new_tokens: Optional[int] = field(default=32, metadata={"help": "gneration top p"})

    change_tokenizer_args: bool = field(default=False, metadata={"help": "whether to use modify tokenizer settings"})
    tokenizer_kwargs: dict = field(
        default_factory=lambda: {
            "padding_side": "left",
            "truncation_side": "left",
            "pad_token_as_eos_token": True,
            "max_length": 64,
        }
    )
    prompt_len: Optional[int] = field(default=32, metadata={"help": "prompt length"})


args = tyro.cli(ScriptArguments)


generation_kwargs = {
    "min_length": args.generation_kwargs_min_length,
    "top_k": args.generation_kwargs_top_k,
    "top_p": args.generation_kwargs_top_p,
    "do_sample": True,
    "max_new_tokens": args.generation_kwargs_max_new_tokens,
}

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # # Kaggle datasets have to be downloaded manually (using Kaggle API)
    # # datasets.load_dataset("jigsaw_toxicity_pred", data_dir="<path/to/manual/data>")
    # ds = load_dataset(
    #     query_dataset,
    #     data_dir="/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/original_dataset",
    #     split="train",
    # )  # Hardcoding path for now TODO: change this

    # ds = ds.rename_columns({"comment_text": "review"})
    # ds = ds.filter(lambda x: len(x["review"]) > 64, batched=False)

    # df = pd.DataFrame(ds)
    # num_toxic = df["toxic"].sum()

    # toxic_df = df[df["toxic"] == True]
    # non_toxic_df = df[df["toxic"] == False]

    # non_toxic_df = non_toxic_df.sample(n=num_toxic, random_state=config.seed)

    # # Recombine into dataset
    # ds = ds.from_pandas(pd.concat([toxic_df, non_toxic_df], ignore_index=True))
    df = pd.read_csv(
        "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/jigsaw_train.csv",
    )
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"comment_text": "review"})

    # pdb.set_trace()

    if args.input_query_size == "more":

        def input_size():
            return args.prompt_len

    else:
        input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    args.ppo_config.model_name,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id

# pdb.set_trace()

if args.change_tokenizer_args == True:
    tokenizer.padding_side = args.tokenizer_kwargs["padding_side"]
    tokenizer.truncation_side = args.tokenizer_kwargs["truncation_side"]
    tokenizer.pad_token_as_eos_token = args.tokenizer_kwargs["pad_token_as_eos_token"]
    tokenizer.max_length = args.tokenizer_kwargs["max_length"]

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
# task, model_name = args.ppo_config.reward_model.split(":")
task = "text-classification"
model_name = args.ppo_config.reward_model
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs["pad_token_id"] = tokenizer.eos_token_id

# empty list to store text data
text_train_data = []

for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(
        texts,
        **sent_kwargs,
        # truncation=True,
    )

    if args.reward_function == "trl":
        rewards = [torch.tensor(-output[0]["score"]) for output in pipe_outputs]
    else:  # depreciated for jigsaw task
        raise NotImplementedError
        rewards = [
            -torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"], output[2]["score"]]))[2]
            for output in pipe_outputs
        ]
    # pdb.set_trace()
    for ii in range(3):
        data_row = {
            "iter": epoch,
            "input": batch["query"][ii],
            "output": batch["response"][ii],
            "score": rewards[ii].item(),
        }
        text_train_data.append(data_row)

    # pdb.set_trace()

    # Run PPO step
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
    ppo_trainer.log_stats(stats, batch, rewards)
    print(epoch)

ppo_trainer._save_pretrained(logging_dir + "/save_pretrained")
df = pd.DataFrame(text_train_data)
df.to_csv(logging_dir + "/textual_data.csv", index=False)

File Path: examples/OLID/training/ppo_auth2_seed2.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset, Dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, set_seed
from trl.core import LengthSampler
from trl.trainer.ppo_trainer_original import PPOTrainer

tqdm.pandas()

import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import pdb
import pandas as pd

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")

logging_dir = f"/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/OLID/{date_n_time}"
os.makedirs(logging_dir)


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-20_20-32-35",  # sft_both
            model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/OLID/2024-02-18_15-25-28",  # OLID sft_positive
            query_dataset="olid",
            reward_model="pigeon-phobia/bertweet-base_finetuned_olid_a",
            # reward_model="citizenlab/twitter-xlm-roberta-base-sentiment-finetunned",
            learning_rate=1.41e-5,
            # log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            # adap_kl_ctrl=False,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            steps=192000,
            exp_name="ppo",
        )
    )
    query_dataset: str = field(default="olid", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="more", metadata={"help": "the dataset to query"})

    generation_kwargs_min_length: Optional[int] = field(
        default=32, metadata={"help": "minimum number of tokens while generation"}
    )
    generation_kwargs_top_k: Optional[int] = field(default=0, metadata={"help": "gneration top k"})
    generation_kwargs_top_p: Optional[float] = field(default=1.0, metadata={"help": "gneration top p"})
    generation_kwargs_max_new_tokens: Optional[int] = field(default=32, metadata={"help": "gneration top p"})

    change_tokenizer_args: bool = field(default=False, metadata={"help": "whether to use modify tokenizer settings"})
    tokenizer_kwargs: dict = field(
        default_factory=lambda: {
            "padding_side": "left",
            "truncation_side": "left",
            "pad_token_as_eos_token": True,
            "max_length": 64,
        }
    )
    prompt_len: Optional[int] = field(default=32, metadata={"help": "prompt length"})


args = tyro.cli(ScriptArguments)


generation_kwargs = {
    "min_length": args.generation_kwargs_min_length,
    "top_k": args.generation_kwargs_top_k,
    "top_p": args.generation_kwargs_top_p,
    "do_sample": True,
    "max_new_tokens": args.generation_kwargs_max_new_tokens,
}

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # # Kaggle datasets have to be downloaded manually (using Kaggle API)
    # # datasets.load_dataset("jigsaw_toxicity_pred", data_dir="<path/to/manual/data>")
    # ds = load_dataset(
    #     query_dataset,
    #     data_dir="/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/original_dataset",
    #     split="train",
    # )  # Hardcoding path for now TODO: change this

    # ds = ds.rename_columns({"comment_text": "review"})
    # ds = ds.filter(lambda x: len(x["review"]) > 64, batched=False)

    # df = pd.DataFrame(ds)
    # num_toxic = df["toxic"].sum()

    # toxic_df = df[df["toxic"] == True]
    # non_toxic_df = df[df["toxic"] == False]

    # non_toxic_df = non_toxic_df.sample(n=num_toxic, random_state=config.seed)

    # # Recombine into dataset
    # ds = ds.from_pandas(pd.concat([toxic_df, non_toxic_df], ignore_index=True))
    df = pd.read_csv(
        "/mnt/research/Anon2/Students/auth2/repos/trl/examples/OLID/dataset/OLIDv1.0/olid-train.csv",
    )
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"tweet": "review"})

    # pdb.set_trace()

    if args.input_query_size == "more":

        def input_size():
            return args.prompt_len

    else:
        input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    args.ppo_config.model_name,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id

# pdb.set_trace()

if args.change_tokenizer_args == True:
    tokenizer.padding_side = args.tokenizer_kwargs["padding_side"]
    tokenizer.truncation_side = args.tokenizer_kwargs["truncation_side"]
    tokenizer.pad_token_as_eos_token = args.tokenizer_kwargs["pad_token_as_eos_token"]
    tokenizer.max_length = args.tokenizer_kwargs["max_length"]

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
# task, model_name = args.ppo_config.reward_model.split(":")
task = "text-classification"
model_name = args.ppo_config.reward_model
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs["pad_token_id"] = tokenizer.eos_token_id

# empty list to store text data
text_train_data = []

for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(
        texts,
        **sent_kwargs,
        truncation=True,
    )

    if args.reward_function == "trl":
        # pdb.set_trace()
        rewards = [torch.tensor(output[0]["score"]) for output in pipe_outputs]
    else:  # depreciated for jigsaw task
        raise NotImplementedError
        rewards = [
            -torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"], output[2]["score"]]))[2]
            for output in pipe_outputs
        ]
    # pdb.set_trace()
    for ii in range(3):
        data_row = {
            "iter": epoch,
            "input": batch["query"][ii],
            "output": batch["response"][ii],
            "score": rewards[ii].item(),
        }
        text_train_data.append(data_row)

    # pdb.set_trace()

    # Run PPO step
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
    ppo_trainer.log_stats(stats, batch, rewards)
    print(epoch)

ppo_trainer._save_pretrained(logging_dir + "/save_pretrained")
df = pd.DataFrame(text_train_data)
df.to_csv(logging_dir + "/textual_data.csv", index=False)

File Path: examples/OLID/training/ppo_auth2_seed36.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset, Dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, set_seed
from trl.core import LengthSampler
from trl.trainer.ppo_trainer_original import PPOTrainer

tqdm.pandas()

import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import pdb
import pandas as pd

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")

logging_dir = f"/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/{date_n_time}"
os.makedirs(logging_dir)


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-20_20-32-35",  # sft_both
            model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-30_00-24-10",  # sft_positive seed 36
            query_dataset="jigsaw_toxicity_pred",
            reward_model="unitary/toxic-bert",
            # reward_model="citizenlab/twitter-xlm-roberta-base-sentiment-finetunned",
            learning_rate=1.41e-5,
            # log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            # adap_kl_ctrl=False,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            steps=192000,
            exp_name="ppo",
        )
    )
    query_dataset: str = field(default="jigsaw_toxicity_pred", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="more", metadata={"help": "the dataset to query"})

    generation_kwargs_min_length: Optional[int] = field(
        default=32, metadata={"help": "minimum number of tokens while generation"}
    )
    generation_kwargs_top_k: Optional[int] = field(default=0, metadata={"help": "gneration top k"})
    generation_kwargs_top_p: Optional[float] = field(default=1.0, metadata={"help": "gneration top p"})
    generation_kwargs_max_new_tokens: Optional[int] = field(default=32, metadata={"help": "gneration top p"})

    change_tokenizer_args: bool = field(default=False, metadata={"help": "whether to use modify tokenizer settings"})
    tokenizer_kwargs: dict = field(
        default_factory=lambda: {
            "padding_side": "left",
            "truncation_side": "left",
            "pad_token_as_eos_token": True,
            "max_length": 64,
        }
    )
    prompt_len: Optional[int] = field(default=32, metadata={"help": "prompt length"})


args = tyro.cli(ScriptArguments)


generation_kwargs = {
    "min_length": args.generation_kwargs_min_length,
    "top_k": args.generation_kwargs_top_k,
    "top_p": args.generation_kwargs_top_p,
    "do_sample": True,
    "max_new_tokens": args.generation_kwargs_max_new_tokens,
}

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # # Kaggle datasets have to be downloaded manually (using Kaggle API)
    # # datasets.load_dataset("jigsaw_toxicity_pred", data_dir="<path/to/manual/data>")
    # ds = load_dataset(
    #     query_dataset,
    #     data_dir="/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/original_dataset",
    #     split="train",
    # )  # Hardcoding path for now TODO: change this

    # ds = ds.rename_columns({"comment_text": "review"})
    # ds = ds.filter(lambda x: len(x["review"]) > 64, batched=False)

    # df = pd.DataFrame(ds)
    # num_toxic = df["toxic"].sum()

    # toxic_df = df[df["toxic"] == True]
    # non_toxic_df = df[df["toxic"] == False]

    # non_toxic_df = non_toxic_df.sample(n=num_toxic, random_state=config.seed)

    # # Recombine into dataset
    # ds = ds.from_pandas(pd.concat([toxic_df, non_toxic_df], ignore_index=True))
    df = pd.read_csv(
        "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/jigsaw_train.csv",
    )
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"comment_text": "review"})

    # pdb.set_trace()

    if args.input_query_size == "more":

        def input_size():
            return args.prompt_len

    else:
        input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    args.ppo_config.model_name,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id

# pdb.set_trace()

if args.change_tokenizer_args == True:
    tokenizer.padding_side = args.tokenizer_kwargs["padding_side"]
    tokenizer.truncation_side = args.tokenizer_kwargs["truncation_side"]
    tokenizer.pad_token_as_eos_token = args.tokenizer_kwargs["pad_token_as_eos_token"]
    tokenizer.max_length = args.tokenizer_kwargs["max_length"]

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
# task, model_name = args.ppo_config.reward_model.split(":")
task = "text-classification"
model_name = args.ppo_config.reward_model
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs["pad_token_id"] = tokenizer.eos_token_id

# empty list to store text data
text_train_data = []

for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(
        texts,
        **sent_kwargs,
        # truncation=True,
    )

    if args.reward_function == "trl":
        rewards = [torch.tensor(-output[0]["score"]) for output in pipe_outputs]
    else:  # depreciated for jigsaw task
        raise NotImplementedError
        rewards = [
            -torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"], output[2]["score"]]))[2]
            for output in pipe_outputs
        ]
    # pdb.set_trace()
    for ii in range(3):
        data_row = {
            "iter": epoch,
            "input": batch["query"][ii],
            "output": batch["response"][ii],
            "score": rewards[ii].item(),
        }
        text_train_data.append(data_row)

    # pdb.set_trace()

    # Run PPO step
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
    ppo_trainer.log_stats(stats, batch, rewards)
    print(epoch)

ppo_trainer._save_pretrained(logging_dir + "/save_pretrained")
df = pd.DataFrame(text_train_data)
df.to_csv(logging_dir + "/textual_data.csv", index=False)

File Path: examples/OLID/training/sft_auth2.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional
import pandas as pd
import torch
from accelerate import Accelerator
from datasets import load_dataset, Dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoModelForCausalLM, BitsAndBytesConfig, HfArgumentParser, TrainingArguments

from trl import SFTTrainer


tqdm.pandas()

import pdb
from transformers import GenerationConfig
from transformers import AutoTokenizer, pipeline, Seq2SeqTrainingArguments
import datetime
import os
from trl import set_seed

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
logging_dir = f"/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/OLID/{date_n_time}"
os.makedirs(logging_dir)


# Define and parse arguments.
@dataclass
class ScriptArguments:
    """
    The name of the Casual LM model we wish to fine-tune with SFTTrainer
    """

    model_name: Optional[str] = field(default="gpt2", metadata={"help": "the model name"})
    dataset_name: Optional[str] = field(default="jigsaw_toxicity_pred", metadata={"help": "the dataset name"})
    dataset_text_field: Optional[str] = field(default="tweet", metadata={"help": "the text field of the dataset"})
    log_with: Optional[str] = field(default="tensorboard", metadata={"help": "use 'wandb' to log with wandb"})

    # learning params
    learning_rate: Optional[float] = field(default=0.00001, metadata={"help": "the learning rate"})  # default=1.41e-5
    batch_size: Optional[int] = field(default=64, metadata={"help": "the batch size"})
    # Using tokenizer max length=64 from RL4LLMs Table 4
    seq_length: Optional[int] = field(default=64, metadata={"help": "Input sequence length"})  # default=512
    gradient_accumulation_steps: Optional[int] = field(
        default=16, metadata={"help": "the number of gradient accumulation steps"}
    )
    num_train_epochs: Optional[int] = field(
        default=10, metadata={"help": "the number of training epochs"}
    )  # default=3
    max_steps: Optional[int] = field(default=-1, metadata={"help": "the number of training steps"})

    # model download, quantization and peft params
    trust_remote_code: Optional[bool] = field(default=True, metadata={"help": "Enable `trust_remote_code`"})
    use_auth_token: Optional[bool] = field(default=True, metadata={"help": "Use HF auth token to access the model"})
    load_in_8bit: Optional[bool] = field(default=False, metadata={"help": "load the model in 8 bits precision"})
    load_in_4bit: Optional[bool] = field(default=False, metadata={"help": "load the model in 4 bits precision"})
    use_peft: Optional[bool] = field(default=False, metadata={"help": "Wether to use PEFT or not to train adapters"})
    peft_lora_r: Optional[int] = field(default=64, metadata={"help": "the r parameter of the LoRA adapters"})
    peft_lora_alpha: Optional[int] = field(default=16, metadata={"help": "the alpha parameter of the LoRA adapters"})

    # logging
    output_dir: Optional[str] = field(default=logging_dir, metadata={"help": "the output directory"})
    logging_steps: Optional[int] = field(default=1, metadata={"help": "the number of logging steps"})
    save_steps: Optional[int] = field(
        default=100, metadata={"help": "Number of updates steps before two checkpoint saves"}
    )
    save_total_limit: Optional[int] = field(default=10, metadata={"help": "Limits total number of checkpoints."})

    # post processing
    push_to_hub: Optional[bool] = field(default=False, metadata={"help": "Push the model to HF Hub"})
    hub_model_id: Optional[str] = field(default=None, metadata={"help": "The name of the model on HF Hub"})

    # added by auth1
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="less", metadata={"help": "the dataset to query"})
    generation_kwargs_min_length: Optional[int] = field(
        default=-1, metadata={"help": "minimum number of tokens while generation"}
    )
    generation_kwargs_top_k: Optional[int] = field(default=0, metadata={"help": "gneration top k"})
    generation_kwargs_top_p: Optional[float] = field(default=1.0, metadata={"help": "gneration top p"})
    generation_kwargs_max_new_tokens: Optional[int] = field(default=32, metadata={"help": "gneration top p"})

    change_tokenizer_args: bool = field(default=False, metadata={"help": "whether to use modify tokenizer settings"})
    tokenizer_kwargs: dict = field(
        default_factory=lambda: {
            "padding_side": "left",
            "truncation_side": "left",
            "pad_token_as_eos_token": True,
            "max_length": 64,
        }
    )
    exp_name: str = field(default="sft-positive", metadata={"help": "experiment name"})
    seed: Optional[int] = field(default=0, metadata={"help": "seed"})


parser = HfArgumentParser(ScriptArguments)
script_args = parser.parse_args_into_dataclasses()[0]

set_seed(script_args.seed)

generation_kwargs = {
    "min_length": script_args.generation_kwargs_min_length,
    "top_k": script_args.generation_kwargs_top_k,
    "top_p": script_args.generation_kwargs_top_p,
    "do_sample": True,
    "max_new_tokens": script_args.generation_kwargs_max_new_tokens,
}

# Step 1: Load the model
if script_args.load_in_8bit and script_args.load_in_4bit:
    raise ValueError("You can't load the model in 8 bits and 4 bits at the same time")
elif script_args.load_in_8bit or script_args.load_in_4bit:
    quantization_config = BitsAndBytesConfig(
        load_in_8bit=script_args.load_in_8bit, load_in_4bit=script_args.load_in_4bit
    )
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}
    torch_dtype = torch.bfloat16
else:
    device_map = None
    quantization_config = None
    torch_dtype = None

model = AutoModelForCausalLM.from_pretrained(
    script_args.model_name,
    quantization_config=quantization_config,
    device_map=device_map,
    trust_remote_code=script_args.trust_remote_code,
    torch_dtype=torch_dtype,
)

# Removing huggingface authorization token param from model loading command above
#    use_auth_token=script_args.use_auth_token,
# Make sure you have logged in to the Hugging Face Hub using huggingface-cli login
# or by calling huggingface_hub.login() in Python. This will save an authentication
# token that Transformers will pick up automatically.

# Step 2: Load the dataset
df = pd.read_csv(
    "/mnt/research/Anon2/Students/auth2/repos/trl/examples/OLID/dataset/OLIDv1.0/olid-train.csv",
)
df = df[df["subtask_a"] == "NOT"]  # SFT only on positive(non-toxic) dataset
dataset = Dataset.from_pandas(df)


# Balance the dataset

# Filter dataset for only positive reviews
# dataset = dataset.filter(lambda x: x["label"] == 1, batched=False)


# Step 3: Define generation arguments
# generation_kwargs = GenerationConfig(
#     min_length = 48,
#     top_k = 50,
#     top_p = 1.0,
#     do_sample = True,
#     max_new_tokens = 48,
# )
# pad_token_id = tokenizer.eos_token_id,

# Step 3: Define the training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir=script_args.output_dir,
    per_device_train_batch_size=script_args.batch_size,
    gradient_accumulation_steps=script_args.gradient_accumulation_steps,
    learning_rate=script_args.learning_rate,
    logging_steps=script_args.logging_steps,
    num_train_epochs=script_args.num_train_epochs,
    max_steps=script_args.max_steps,
    report_to=script_args.log_with,
    save_steps=script_args.save_steps,
    save_total_limit=script_args.save_total_limit,
    push_to_hub=script_args.push_to_hub,
    hub_model_id=script_args.hub_model_id,
    generation_config=generation_kwargs,
)

# Step 4: Define the LoraConfig
if script_args.use_peft:
    peft_config = LoraConfig(
        r=script_args.peft_lora_r,
        lora_alpha=script_args.peft_lora_alpha,
        bias="none",
        task_type="CAUSAL_LM",
    )
else:
    peft_config = None


# trl sft trainer default tokenizer and pad_token
"""
if tokenizer is None:
            tokenizer = AutoTokenizer.from_pretrained(model.config._name_or_path)
            if getattr(tokenizer, "pad_token", None) is None:
                tokenizer.pad_token = tokenizer.eos_token

if max_seq_length is None:
            # to overcome some issues with broken tokenizers
            max_seq_length = min(tokenizer.model_max_length, 1024)

            warnings.warn(
                f"You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to {max_seq_length}"
            )

"""

# Looks like padding_side = "right" by default
# Hence, defining our tokenizer here
tokenizer = AutoTokenizer.from_pretrained(script_args.model_name)
tokenizer.pad_token = tokenizer.eos_token

if script_args.change_tokenizer_args == True:
    tokenizer.padding_side = script_args.tokenizer_kwargs["padding_side"]
    tokenizer.truncation_side = script_args.tokenizer_kwargs["truncation_side"]
    tokenizer.pad_token_as_eos_token = script_args.tokenizer_kwargs["pad_token_as_eos_token"]
    tokenizer.max_length = script_args.tokenizer_kwargs["max_length"]

# tokenizer.padding_side = "left"
# tokenizer.truncation_side = "left"
# tokenizer.pad_token_as_eos_token = True
# tokenizer.max_length = 64

# Step 5: Define the Trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    max_seq_length=script_args.seq_length,
    train_dataset=dataset,
    dataset_text_field=script_args.dataset_text_field,
    peft_config=peft_config,
    tokenizer=tokenizer,
)

trainer.train()

# Step 6: Save the model
trainer.save_model(logging_dir)

File Path: examples/OLID/training/soft_risk_ppo_auth2_2.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset, Dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import (
    AutoModelForCausalLMWithValueHead,
    AutoModelForSeq2SeqLMWithValueHead,
    PPOConfig,
    set_seed,
)  # ,PPOTrainer
from trl.trainer.ppo_trainer import PPOTrainer
from trl.core import LengthSampler


tqdm.pandas()

import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import pdb
import pandas as pd
import math

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
logging_dir = f"/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/{date_n_time}"
os.makedirs(logging_dir)


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="gpt2",
            # model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-20_20-32-35",  # sft_both
            model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-31_18-06-18",  # sft_positive seed2 70_30
            query_dataset="jigsaw_toxicity_pred",
            reward_model="unitary/toxic-bert",
            learning_rate=1.41e-5,
            # log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            # adap_kl_ctrl=False,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            steps=192000,
            exp_name="ppo",
        )
    )
    query_dataset: str = field(default="jigsaw_toxicity_pred", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="less", metadata={"help": "the dataset to query"})

    generation_kwargs_min_length: Optional[int] = field(
        default=-1, metadata={"help": "minimum number of tokens while generation"}
    )
    generation_kwargs_top_k: Optional[int] = field(default=0, metadata={"help": "gneration top k"})
    generation_kwargs_top_p: Optional[float] = field(default=1.0, metadata={"help": "gneration top p"})
    generation_kwargs_max_new_tokens: Optional[int] = field(default=32, metadata={"help": "gneration top p"})

    change_tokenizer_args: bool = field(default=False, metadata={"help": "whether to use modify tokenizer settings"})
    tokenizer_kwargs: dict = field(
        default_factory=lambda: {
            "padding_side": "left",
            "truncation_side": "left",
            "pad_token_as_eos_token": True,
            "max_length": 64,
        }
    )
    risk_scheduler: str = field(default="old", metadata={"help": "old risk scheduler that doesn't go down to alpha"})
    risk_n: int = field(
        default=240, metadata={"help": "240 (> batch size): no RA; 70: RA begins after iter 70; 1: RA throughout"}
    )
    risk_alpha: Optional[float] = field(default=0.2, metadata={"help": "risk alpha value = 20 percent"})
    risk_rho: Optional[float] = field(
        default=0.8, metadata={"help": "risk alpha reaches a value = 20 percent at 80 percent of total iterations"}
    )
    prompt_len: Optional[int] = field(default=32, metadata={"help": "prompt length"})


args = tyro.cli(ScriptArguments)


generation_kwargs = {
    "min_length": args.generation_kwargs_min_length,
    "top_k": args.generation_kwargs_top_k,
    "top_p": args.generation_kwargs_top_p,
    "do_sample": True,
    "max_new_tokens": args.generation_kwargs_max_new_tokens,
}

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # Kaggle datasets have to be downloaded manually (using Kaggle API)
    # datasets.load_dataset("jigsaw_toxicity_pred", data_dir="<path/to/manual/data>")
    # ds = load_dataset(
    #     query_dataset,
    #     data_dir="/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/original_dataset",
    #     split="train",
    # )  # Hardcoding path for now TODO: change this

    # ds = ds.rename_columns({"comment_text": "review"})
    # ds = ds.filter(lambda x: len(x["review"]) > 64, batched=False)

    # df = pd.DataFrame(ds)
    # num_toxic = df["toxic"].sum()

    # toxic_df = df[df["toxic"] == True]
    # non_toxic_df = df[df["toxic"] == False]

    # non_toxic_df = non_toxic_df.sample(n=num_toxic, random_state=config.seed)

    # # Recombine into dataset
    # ds = ds.from_pandas(pd.concat([toxic_df, non_toxic_df], ignore_index=True))
    df = pd.read_csv(
        "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/jigsaw_train_70_30.csv",
    )
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"comment_text": "review"})

    # pdb.set_trace()

    if args.input_query_size == "more":

        def input_size():
            return args.prompt_len

    else:
        input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    args.ppo_config.model_name,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id

# pdb.set_trace()

if args.change_tokenizer_args == True:
    tokenizer.padding_side = args.tokenizer_kwargs["padding_side"]
    tokenizer.truncation_side = args.tokenizer_kwargs["truncation_side"]
    tokenizer.pad_token_as_eos_token = args.tokenizer_kwargs["pad_token_as_eos_token"]
    tokenizer.max_length = args.tokenizer_kwargs["max_length"]

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
task = "text-classification"
model_name = args.ppo_config.reward_model
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs["pad_token_id"] = tokenizer.eos_token_id

# empty list to store text data
text_train_data = []


def get_current_risk_level(alpha, rho, m, M):
    """
    Get current risk_level_alpha*N value

    Args:
        alpha (float):
            Risk level alpha
        rho (float):
            Risk level scheduler: 0.8 means the soft risk level reaches α after ρ = 80% of the training
        m (int):
            current training step/epoch
        M (int):
            total policy training steps

    Returns:
        alpha_N (int):
            risk_level_alpha*N value
    """
    alpha = args.risk_alpha
    rho = args.risk_rho

    if args.risk_scheduler == "old":
        if m <= args.risk_n:
            return args.ppo_config.batch_size
        else:
            return math.ceil(args.ppo_config.batch_size * max(alpha, 1 - (1 - alpha) * (m - args.risk_n) / (rho * M)))
    else:
        print("here")
        if m <= args.risk_n:
            val = args.ppo_config.batch_size
        elif m >= math.ceil(rho * M):
            val = math.ceil(alpha * args.ppo_config.batch_size)
        else:
            K = (1 - alpha) / (math.ceil(rho * M) - args.risk_n)
            val = math.ceil(args.ppo_config.batch_size * max(alpha, 1 - K * (m - args.risk_n)))
        return val


def get_current_risk_level_2(alpha, rho, m, M):
    """
    Get current risk_level_alpha*N value

    Args:
        alpha (float):
            Risk level alpha
        rho (float):
            Risk level scheduler: 0.8 means the soft risk level reaches α after ρ = 80% of the training
        m (int):
            current training step/epoch
        M (int):
            total policy training steps

    Returns:
        alpha_N (int):
            risk_level_alpha*N value
    """
    alpha = 0.2
    rho = 0.8
    n = 70
    M = 194

    if m <= n:
        val = args.ppo_config.batch_size
    elif m >= math.ceil(rho * M):
        val = alpha * 100
    else:
        K = (1 - alpha) / (math.ceil(rho * M) - n)
        val = math.ceil(args.ppo_config.batch_size * max(alpha, 1 - K * (m - n)))
    return val


def apply_risk_modification_to_batch(batch, query_tensors, response_tensors, rewards, alpha_N):
    query_tensors = list(map(lambda t: t.cpu(), query_tensors))
    response_tensors = list(map(lambda t: t.cpu(), response_tensors))
    rewards = list(map(lambda t: t.cpu(), rewards))
    # response_lengths = [response_tensors[i].shape[0] for i in response_tensors]

    step_dict = {"query_t": query_tensors, "resp_t": response_tensors, "rewards": rewards}

    # batch = {k: [t.cpu() for t in v] if isinstance(v, list) else v.cpu() for k, v in batch.items()}
    batch = {
        k: [t.cpu() for t in v]
        if isinstance(v, list) and all(isinstance(t, torch.Tensor) for t in v)
        else v.cpu()
        if isinstance(v, torch.Tensor)
        else v
        for k, v in batch.items()
    }
    # pdb.set_trace()

    df_step_dict = pd.DataFrame(step_dict)
    df_batch = pd.DataFrame(batch)

    df_step_dict = df_step_dict.sort_values(by="rewards")  # .head(alpha_N)
    df_batch = df_batch.sort_values(by="rewards")  # .head(alpha_N).drop('rewards', axis=1)
    batch = df_batch.to_dict(orient="list")
    # line #996 in ppo_trainer already does response_masks_batch[j] = torch.cat((torch.zeros_like(query_batch[j]), response_masks_batch[j]))[1:]
    # pdb.set_trace()
    # mask = [torch.cat((torch.ones_like(t, dtype =int), torch.zeros([48 - t.shape[0]], dtype =int))) for t in response_tensors]  #torch.zeros([64], dtype =int),
    mask = [torch.ones([32], dtype=int) for _ in response_tensors]  # torch.zeros([64], dtype =int),
    for ii in range(args.ppo_config.batch_size - alpha_N):
        mask[-ii - 1] = torch.zeros([32], dtype=int)

    sr_query_tensors = list(map(lambda t: t.to(device), df_step_dict["query_t"].tolist()))
    sr_response_tensors = list(map(lambda t: t.to(device), df_step_dict["resp_t"].tolist()))
    sr_rewards = list(map(lambda t: t.to(device), df_step_dict["rewards"].tolist()))
    batch = {
        k: [t.to(device) for t in v]
        if isinstance(v, list) and all(isinstance(t, torch.Tensor) for t in v)
        else v.to(device)
        if isinstance(v, torch.Tensor)
        else v
        for k, v in batch.items()
    }

    return batch, sr_query_tensors, sr_response_tensors, sr_rewards, mask


for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)

    if args.reward_function == "trl":
        rewards = [torch.tensor(-output[0]["score"]) for output in pipe_outputs]
    else:  # depreciated
        raise NotImplementedError
        rewards = [
            torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"]]))[1]
            for output in pipe_outputs
        ]

    for ii in range(3):
        data_row = {
            "iter": epoch,
            "input": batch["query"][ii],
            "output": batch["response"][ii],
            "score": rewards[ii].item(),
        }
        text_train_data.append(data_row)

    batch["rewards"] = rewards
    alpha_N_ceil = get_current_risk_level(PPOConfig.soft_risk_alpha, PPOConfig.risk_level_scheduler, epoch, 288)
    # sr_batch, sr_query_t, sr_resp_t, sr_rewards, mask = apply_risk_modification_to_batch(batch, query_tensors, response_tensors, rewards, alpha_N_ceil)

    # Run PPO step
    print(alpha_N_ceil)
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards, alpha_N_ceil)
    ppo_trainer.log_stats(stats, batch, rewards)
    print(epoch)

ppo_trainer._save_pretrained(logging_dir + "/save_pretrained")
df = pd.DataFrame(text_train_data)
df.to_csv(logging_dir + "/textual_data.csv", index=False)

File Path: examples/OLID/training/soft_risk_ppo_auth2_seed12.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset, Dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import (
    AutoModelForCausalLMWithValueHead,
    AutoModelForSeq2SeqLMWithValueHead,
    PPOConfig,
    set_seed,
)  # ,PPOTrainer
from trl.trainer.ppo_trainer import PPOTrainer
from trl.core import LengthSampler


tqdm.pandas()

import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import pdb
import pandas as pd
import math

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
logging_dir = f"/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/{date_n_time}"
os.makedirs(logging_dir)


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="gpt2",
            # model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-20_20-32-35",  # sft_both
            model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-30_00-30-38",  # sft_positive seed 12
            query_dataset="jigsaw_toxicity_pred",
            reward_model="unitary/toxic-bert",
            learning_rate=1.41e-5,
            # log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            # adap_kl_ctrl=False,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            steps=192000,
            exp_name="ppo",
        )
    )
    query_dataset: str = field(default="jigsaw_toxicity_pred", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="less", metadata={"help": "the dataset to query"})

    generation_kwargs_min_length: Optional[int] = field(
        default=-1, metadata={"help": "minimum number of tokens while generation"}
    )
    generation_kwargs_top_k: Optional[int] = field(default=0, metadata={"help": "gneration top k"})
    generation_kwargs_top_p: Optional[float] = field(default=1.0, metadata={"help": "gneration top p"})
    generation_kwargs_max_new_tokens: Optional[int] = field(default=32, metadata={"help": "gneration top p"})

    change_tokenizer_args: bool = field(default=False, metadata={"help": "whether to use modify tokenizer settings"})
    tokenizer_kwargs: dict = field(
        default_factory=lambda: {
            "padding_side": "left",
            "truncation_side": "left",
            "pad_token_as_eos_token": True,
            "max_length": 64,
        }
    )
    risk_scheduler: str = field(default="old", metadata={"help": "old risk scheduler that doesn't go down to alpha"})
    risk_n: int = field(
        default=240, metadata={"help": "240 (> batch size): no RA; 70: RA begins after iter 70; 1: RA throughout"}
    )
    risk_alpha: Optional[float] = field(default=0.2, metadata={"help": "risk alpha value = 20 percent"})
    risk_rho: Optional[float] = field(
        default=0.8, metadata={"help": "risk alpha reaches a value = 20 percent at 80 percent of total iterations"}
    )
    prompt_len: Optional[int] = field(default=32, metadata={"help": "prompt length"})


args = tyro.cli(ScriptArguments)


generation_kwargs = {
    "min_length": args.generation_kwargs_min_length,
    "top_k": args.generation_kwargs_top_k,
    "top_p": args.generation_kwargs_top_p,
    "do_sample": True,
    "max_new_tokens": args.generation_kwargs_max_new_tokens,
}

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # Kaggle datasets have to be downloaded manually (using Kaggle API)
    # datasets.load_dataset("jigsaw_toxicity_pred", data_dir="<path/to/manual/data>")
    # ds = load_dataset(
    #     query_dataset,
    #     data_dir="/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/original_dataset",
    #     split="train",
    # )  # Hardcoding path for now TODO: change this

    # ds = ds.rename_columns({"comment_text": "review"})
    # ds = ds.filter(lambda x: len(x["review"]) > 64, batched=False)

    # df = pd.DataFrame(ds)
    # num_toxic = df["toxic"].sum()

    # toxic_df = df[df["toxic"] == True]
    # non_toxic_df = df[df["toxic"] == False]

    # non_toxic_df = non_toxic_df.sample(n=num_toxic, random_state=config.seed)

    # # Recombine into dataset
    # ds = ds.from_pandas(pd.concat([toxic_df, non_toxic_df], ignore_index=True))
    df = pd.read_csv(
        "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/jigsaw_train.csv",
    )
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"comment_text": "review"})

    # pdb.set_trace()

    if args.input_query_size == "more":

        def input_size():
            return args.prompt_len

    else:
        input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    args.ppo_config.model_name,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id

# pdb.set_trace()

if args.change_tokenizer_args == True:
    tokenizer.padding_side = args.tokenizer_kwargs["padding_side"]
    tokenizer.truncation_side = args.tokenizer_kwargs["truncation_side"]
    tokenizer.pad_token_as_eos_token = args.tokenizer_kwargs["pad_token_as_eos_token"]
    tokenizer.max_length = args.tokenizer_kwargs["max_length"]

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
task = "text-classification"
model_name = args.ppo_config.reward_model
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs["pad_token_id"] = tokenizer.eos_token_id

# empty list to store text data
text_train_data = []


def get_current_risk_level(alpha, rho, m, M):
    """
    Get current risk_level_alpha*N value

    Args:
        alpha (float):
            Risk level alpha
        rho (float):
            Risk level scheduler: 0.8 means the soft risk level reaches α after ρ = 80% of the training
        m (int):
            current training step/epoch
        M (int):
            total policy training steps

    Returns:
        alpha_N (int):
            risk_level_alpha*N value
    """
    alpha = args.risk_alpha
    rho = args.risk_rho

    if args.risk_scheduler == "old":
        if m <= args.risk_n:
            return args.ppo_config.batch_size
        else:
            return math.ceil(args.ppo_config.batch_size * max(alpha, 1 - (1 - alpha) * (m - args.risk_n) / (rho * M)))
    else:
        print("here")
        if m <= args.risk_n:
            val = args.ppo_config.batch_size
        elif m >= math.ceil(rho * M):
            val = math.ceil(alpha * args.ppo_config.batch_size)
        else:
            K = (1 - alpha) / (math.ceil(rho * M) - args.risk_n)
            val = math.ceil(args.ppo_config.batch_size * max(alpha, 1 - K * (m - args.risk_n)))
        return val


def get_current_risk_level_2(alpha, rho, m, M):
    """
    Get current risk_level_alpha*N value

    Args:
        alpha (float):
            Risk level alpha
        rho (float):
            Risk level scheduler: 0.8 means the soft risk level reaches α after ρ = 80% of the training
        m (int):
            current training step/epoch
        M (int):
            total policy training steps

    Returns:
        alpha_N (int):
            risk_level_alpha*N value
    """
    alpha = 0.2
    rho = 0.8
    n = 70
    M = 194

    if m <= n:
        val = args.ppo_config.batch_size
    elif m >= math.ceil(rho * M):
        val = alpha * 100
    else:
        K = (1 - alpha) / (math.ceil(rho * M) - n)
        val = math.ceil(args.ppo_config.batch_size * max(alpha, 1 - K * (m - n)))
    return val


def apply_risk_modification_to_batch(batch, query_tensors, response_tensors, rewards, alpha_N):
    query_tensors = list(map(lambda t: t.cpu(), query_tensors))
    response_tensors = list(map(lambda t: t.cpu(), response_tensors))
    rewards = list(map(lambda t: t.cpu(), rewards))
    # response_lengths = [response_tensors[i].shape[0] for i in response_tensors]

    step_dict = {"query_t": query_tensors, "resp_t": response_tensors, "rewards": rewards}

    # batch = {k: [t.cpu() for t in v] if isinstance(v, list) else v.cpu() for k, v in batch.items()}
    batch = {
        k: [t.cpu() for t in v]
        if isinstance(v, list) and all(isinstance(t, torch.Tensor) for t in v)
        else v.cpu()
        if isinstance(v, torch.Tensor)
        else v
        for k, v in batch.items()
    }
    # pdb.set_trace()

    df_step_dict = pd.DataFrame(step_dict)
    df_batch = pd.DataFrame(batch)

    df_step_dict = df_step_dict.sort_values(by="rewards")  # .head(alpha_N)
    df_batch = df_batch.sort_values(by="rewards")  # .head(alpha_N).drop('rewards', axis=1)
    batch = df_batch.to_dict(orient="list")
    # line #996 in ppo_trainer already does response_masks_batch[j] = torch.cat((torch.zeros_like(query_batch[j]), response_masks_batch[j]))[1:]
    # pdb.set_trace()
    # mask = [torch.cat((torch.ones_like(t, dtype =int), torch.zeros([48 - t.shape[0]], dtype =int))) for t in response_tensors]  #torch.zeros([64], dtype =int),
    mask = [torch.ones([32], dtype=int) for _ in response_tensors]  # torch.zeros([64], dtype =int),
    for ii in range(args.ppo_config.batch_size - alpha_N):
        mask[-ii - 1] = torch.zeros([32], dtype=int)

    sr_query_tensors = list(map(lambda t: t.to(device), df_step_dict["query_t"].tolist()))
    sr_response_tensors = list(map(lambda t: t.to(device), df_step_dict["resp_t"].tolist()))
    sr_rewards = list(map(lambda t: t.to(device), df_step_dict["rewards"].tolist()))
    batch = {
        k: [t.to(device) for t in v]
        if isinstance(v, list) and all(isinstance(t, torch.Tensor) for t in v)
        else v.to(device)
        if isinstance(v, torch.Tensor)
        else v
        for k, v in batch.items()
    }

    return batch, sr_query_tensors, sr_response_tensors, sr_rewards, mask


for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)

    if args.reward_function == "trl":
        rewards = [torch.tensor(-output[0]["score"]) for output in pipe_outputs]
    else:  # depreciated
        raise NotImplementedError
        rewards = [
            torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"]]))[1]
            for output in pipe_outputs
        ]

    for ii in range(3):
        data_row = {
            "iter": epoch,
            "input": batch["query"][ii],
            "output": batch["response"][ii],
            "score": rewards[ii].item(),
        }
        text_train_data.append(data_row)

    batch["rewards"] = rewards
    alpha_N_ceil = get_current_risk_level(PPOConfig.soft_risk_alpha, PPOConfig.risk_level_scheduler, epoch, 264)
    # sr_batch, sr_query_t, sr_resp_t, sr_rewards, mask = apply_risk_modification_to_batch(batch, query_tensors, response_tensors, rewards, alpha_N_ceil)

    # Run PPO step
    print(alpha_N_ceil)
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards, alpha_N_ceil)
    ppo_trainer.log_stats(stats, batch, rewards)
    print(epoch)

ppo_trainer._save_pretrained(logging_dir + "/save_pretrained")
df = pd.DataFrame(text_train_data)
df.to_csv(logging_dir + "/textual_data.csv", index=False)

File Path: examples/OLID/training/soft_risk_ppo_auth2_seed2.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset, Dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import (
    AutoModelForCausalLMWithValueHead,
    AutoModelForSeq2SeqLMWithValueHead,
    PPOConfig,
    set_seed,
)  # ,PPOTrainer
from trl.trainer.ppo_trainer import PPOTrainer
from trl.core import LengthSampler


tqdm.pandas()

import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import pdb
import pandas as pd
import math

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
logging_dir = f"/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/OLID/{date_n_time}"
os.makedirs(logging_dir)


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="gpt2",
            # model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-20_20-32-35",  # sft_both
            model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/OLID/2024-02-18_15-25-28",  # OLID sft_positive
            query_dataset="bblah",
            reward_model="pigeon-phobia/bertweet-base_finetuned_olid_a",
            learning_rate=1.41e-5,
            # log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            # adap_kl_ctrl=False,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            steps=192000,
            exp_name="ppo",
        )
    )
    query_dataset: str = field(default="blah_blah", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="less", metadata={"help": "the dataset to query"})

    generation_kwargs_min_length: Optional[int] = field(
        default=-1, metadata={"help": "minimum number of tokens while generation"}
    )
    generation_kwargs_top_k: Optional[int] = field(default=0, metadata={"help": "gneration top k"})
    generation_kwargs_top_p: Optional[float] = field(default=1.0, metadata={"help": "gneration top p"})
    generation_kwargs_max_new_tokens: Optional[int] = field(default=32, metadata={"help": "gneration top p"})

    change_tokenizer_args: bool = field(default=False, metadata={"help": "whether to use modify tokenizer settings"})
    tokenizer_kwargs: dict = field(
        default_factory=lambda: {
            "padding_side": "left",
            "truncation_side": "left",
            "pad_token_as_eos_token": True,
            "max_length": 64,
        }
    )
    risk_scheduler: str = field(default="old", metadata={"help": "old risk scheduler that doesn't go down to alpha"})
    risk_n: int = field(
        default=240, metadata={"help": "240 (> batch size): no RA; 70: RA begins after iter 70; 1: RA throughout"}
    )
    risk_alpha: Optional[float] = field(default=0.2, metadata={"help": "risk alpha value = 20 percent"})
    risk_rho: Optional[float] = field(
        default=0.8, metadata={"help": "risk alpha reaches a value = 20 percent at 80 percent of total iterations"}
    )
    prompt_len: Optional[int] = field(default=32, metadata={"help": "prompt length"})


args = tyro.cli(ScriptArguments)


generation_kwargs = {
    "min_length": args.generation_kwargs_min_length,
    "top_k": args.generation_kwargs_top_k,
    "top_p": args.generation_kwargs_top_p,
    "do_sample": True,
    "max_new_tokens": args.generation_kwargs_max_new_tokens,
}

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # Kaggle datasets have to be downloaded manually (using Kaggle API)
    # datasets.load_dataset("jigsaw_toxicity_pred", data_dir="<path/to/manual/data>")
    # ds = load_dataset(
    #     query_dataset,
    #     data_dir="/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/original_dataset",
    #     split="train",
    # )  # Hardcoding path for now TODO: change this

    # ds = ds.rename_columns({"comment_text": "review"})
    # ds = ds.filter(lambda x: len(x["review"]) > 64, batched=False)

    # df = pd.DataFrame(ds)
    # num_toxic = df["toxic"].sum()

    # toxic_df = df[df["toxic"] == True]
    # non_toxic_df = df[df["toxic"] == False]

    # non_toxic_df = non_toxic_df.sample(n=num_toxic, random_state=config.seed)

    # # Recombine into dataset
    # ds = ds.from_pandas(pd.concat([toxic_df, non_toxic_df], ignore_index=True))
    df = pd.read_csv(
        "/mnt/research/Anon2/Students/auth2/repos/trl/examples/OLID/dataset/OLIDv1.0/olid-train.csv",
    )
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"tweet": "review"})

    # pdb.set_trace()

    if args.input_query_size == "more":

        def input_size():
            return args.prompt_len

    else:
        input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    args.ppo_config.model_name,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id

# pdb.set_trace()

if args.change_tokenizer_args == True:
    tokenizer.padding_side = args.tokenizer_kwargs["padding_side"]
    tokenizer.truncation_side = args.tokenizer_kwargs["truncation_side"]
    tokenizer.pad_token_as_eos_token = args.tokenizer_kwargs["pad_token_as_eos_token"]
    tokenizer.max_length = args.tokenizer_kwargs["max_length"]

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
task = "text-classification"
model_name = args.ppo_config.reward_model
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs["pad_token_id"] = tokenizer.eos_token_id

# empty list to store text data
text_train_data = []


def get_current_risk_level(alpha, rho, m, M):
    """
    Get current risk_level_alpha*N value

    Args:
        alpha (float):
            Risk level alpha
        rho (float):
            Risk level scheduler: 0.8 means the soft risk level reaches α after ρ = 80% of the training
        m (int):
            current training step/epoch
        M (int):
            total policy training steps

    Returns:
        alpha_N (int):
            risk_level_alpha*N value
    """
    alpha = args.risk_alpha
    rho = args.risk_rho

    if args.risk_scheduler == "old":
        if m <= args.risk_n:
            return args.ppo_config.batch_size
        else:
            return math.ceil(args.ppo_config.batch_size * max(alpha, 1 - (1 - alpha) * (m - args.risk_n) / (rho * M)))
    else:
        print("here")
        if m <= args.risk_n:
            val = args.ppo_config.batch_size
        elif m >= math.ceil(rho * M):
            val = math.ceil(alpha * args.ppo_config.batch_size)
        else:
            K = (1 - alpha) / (math.ceil(rho * M) - args.risk_n)
            val = math.ceil(args.ppo_config.batch_size * max(alpha, 1 - K * (m - args.risk_n)))
        return val


def get_current_risk_level_2(alpha, rho, m, M):
    """
    Get current risk_level_alpha*N value

    Args:
        alpha (float):
            Risk level alpha
        rho (float):
            Risk level scheduler: 0.8 means the soft risk level reaches α after ρ = 80% of the training
        m (int):
            current training step/epoch
        M (int):
            total policy training steps

    Returns:
        alpha_N (int):
            risk_level_alpha*N value
    """
    alpha = 0.2
    rho = 0.8
    n = 70
    M = 194

    if m <= n:
        val = args.ppo_config.batch_size
    elif m >= math.ceil(rho * M):
        val = alpha * 100
    else:
        K = (1 - alpha) / (math.ceil(rho * M) - n)
        val = math.ceil(args.ppo_config.batch_size * max(alpha, 1 - K * (m - n)))
    return val


def apply_risk_modification_to_batch(batch, query_tensors, response_tensors, rewards, alpha_N):
    query_tensors = list(map(lambda t: t.cpu(), query_tensors))
    response_tensors = list(map(lambda t: t.cpu(), response_tensors))
    rewards = list(map(lambda t: t.cpu(), rewards))
    # response_lengths = [response_tensors[i].shape[0] for i in response_tensors]

    step_dict = {"query_t": query_tensors, "resp_t": response_tensors, "rewards": rewards}

    # batch = {k: [t.cpu() for t in v] if isinstance(v, list) else v.cpu() for k, v in batch.items()}
    batch = {
        k: (
            [t.cpu() for t in v]
            if isinstance(v, list) and all(isinstance(t, torch.Tensor) for t in v)
            else v.cpu() if isinstance(v, torch.Tensor) else v
        )
        for k, v in batch.items()
    }
    # pdb.set_trace()

    df_step_dict = pd.DataFrame(step_dict)
    df_batch = pd.DataFrame(batch)

    df_step_dict = df_step_dict.sort_values(by="rewards")  # .head(alpha_N)
    df_batch = df_batch.sort_values(by="rewards")  # .head(alpha_N).drop('rewards', axis=1)
    batch = df_batch.to_dict(orient="list")
    # line #996 in ppo_trainer already does response_masks_batch[j] = torch.cat((torch.zeros_like(query_batch[j]), response_masks_batch[j]))[1:]
    # pdb.set_trace()
    # mask = [torch.cat((torch.ones_like(t, dtype =int), torch.zeros([48 - t.shape[0]], dtype =int))) for t in response_tensors]  #torch.zeros([64], dtype =int),
    mask = [torch.ones([32], dtype=int) for _ in response_tensors]  # torch.zeros([64], dtype =int),
    for ii in range(args.ppo_config.batch_size - alpha_N):
        mask[-ii - 1] = torch.zeros([32], dtype=int)

    sr_query_tensors = list(map(lambda t: t.to(device), df_step_dict["query_t"].tolist()))
    sr_response_tensors = list(map(lambda t: t.to(device), df_step_dict["resp_t"].tolist()))
    sr_rewards = list(map(lambda t: t.to(device), df_step_dict["rewards"].tolist()))
    batch = {
        k: (
            [t.to(device) for t in v]
            if isinstance(v, list) and all(isinstance(t, torch.Tensor) for t in v)
            else v.to(device) if isinstance(v, torch.Tensor) else v
        )
        for k, v in batch.items()
    }

    return batch, sr_query_tensors, sr_response_tensors, sr_rewards, mask


for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs, truncation=True)

    if args.reward_function == "trl":
        rewards = [torch.tensor(output[0]["score"]) for output in pipe_outputs]
    else:  # depreciated
        raise NotImplementedError
        rewards = [
            torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"]]))[1]
            for output in pipe_outputs
        ]

    for ii in range(3):
        data_row = {
            "iter": epoch,
            "input": batch["query"][ii],
            "output": batch["response"][ii],
            "score": rewards[ii].item(),
        }
        text_train_data.append(data_row)

    batch["rewards"] = rewards
    alpha_N_ceil = get_current_risk_level(PPOConfig.soft_risk_alpha, PPOConfig.risk_level_scheduler, epoch, 103)
    # sr_batch, sr_query_t, sr_resp_t, sr_rewards, mask = apply_risk_modification_to_batch(batch, query_tensors, response_tensors, rewards, alpha_N_ceil)

    # Run PPO step
    print(alpha_N_ceil)
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards, alpha_N_ceil)
    ppo_trainer.log_stats(stats, batch, rewards)
    print(epoch)

ppo_trainer._save_pretrained(logging_dir + "/save_pretrained")
df = pd.DataFrame(text_train_data)
df.to_csv(logging_dir + "/textual_data.csv", index=False)

File Path: examples/OLID/training/soft_risk_ppo_auth2_seed36.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset, Dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import (
    AutoModelForCausalLMWithValueHead,
    AutoModelForSeq2SeqLMWithValueHead,
    PPOConfig,
    set_seed,
)  # ,PPOTrainer
from trl.trainer.ppo_trainer import PPOTrainer
from trl.core import LengthSampler


tqdm.pandas()

import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import pdb
import pandas as pd
import math

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
logging_dir = f"/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/{date_n_time}"
os.makedirs(logging_dir)


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="gpt2",
            # model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-20_20-32-35",  # sft_both
            model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-30_00-24-10",  # sft_positive seed 36
            query_dataset="jigsaw_toxicity_pred",
            reward_model="unitary/toxic-bert",
            learning_rate=1.41e-5,
            # log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            # adap_kl_ctrl=False,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            steps=192000,
            exp_name="ppo",
        )
    )
    query_dataset: str = field(default="jigsaw_toxicity_pred", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="less", metadata={"help": "the dataset to query"})

    generation_kwargs_min_length: Optional[int] = field(
        default=-1, metadata={"help": "minimum number of tokens while generation"}
    )
    generation_kwargs_top_k: Optional[int] = field(default=0, metadata={"help": "gneration top k"})
    generation_kwargs_top_p: Optional[float] = field(default=1.0, metadata={"help": "gneration top p"})
    generation_kwargs_max_new_tokens: Optional[int] = field(default=32, metadata={"help": "gneration top p"})

    change_tokenizer_args: bool = field(default=False, metadata={"help": "whether to use modify tokenizer settings"})
    tokenizer_kwargs: dict = field(
        default_factory=lambda: {
            "padding_side": "left",
            "truncation_side": "left",
            "pad_token_as_eos_token": True,
            "max_length": 64,
        }
    )
    risk_scheduler: str = field(default="old", metadata={"help": "old risk scheduler that doesn't go down to alpha"})
    risk_n: int = field(
        default=240, metadata={"help": "240 (> batch size): no RA; 70: RA begins after iter 70; 1: RA throughout"}
    )
    risk_alpha: Optional[float] = field(default=0.2, metadata={"help": "risk alpha value = 20 percent"})
    risk_rho: Optional[float] = field(
        default=0.8, metadata={"help": "risk alpha reaches a value = 20 percent at 80 percent of total iterations"}
    )
    prompt_len: Optional[int] = field(default=32, metadata={"help": "prompt length"})


args = tyro.cli(ScriptArguments)


generation_kwargs = {
    "min_length": args.generation_kwargs_min_length,
    "top_k": args.generation_kwargs_top_k,
    "top_p": args.generation_kwargs_top_p,
    "do_sample": True,
    "max_new_tokens": args.generation_kwargs_max_new_tokens,
}

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # Kaggle datasets have to be downloaded manually (using Kaggle API)
    # datasets.load_dataset("jigsaw_toxicity_pred", data_dir="<path/to/manual/data>")
    # ds = load_dataset(
    #     query_dataset,
    #     data_dir="/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/original_dataset",
    #     split="train",
    # )  # Hardcoding path for now TODO: change this

    # ds = ds.rename_columns({"comment_text": "review"})
    # ds = ds.filter(lambda x: len(x["review"]) > 64, batched=False)

    # df = pd.DataFrame(ds)
    # num_toxic = df["toxic"].sum()

    # toxic_df = df[df["toxic"] == True]
    # non_toxic_df = df[df["toxic"] == False]

    # non_toxic_df = non_toxic_df.sample(n=num_toxic, random_state=config.seed)

    # # Recombine into dataset
    # ds = ds.from_pandas(pd.concat([toxic_df, non_toxic_df], ignore_index=True))
    df = pd.read_csv(
        "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/jigsaw_train.csv",
    )
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"comment_text": "review"})

    # pdb.set_trace()

    if args.input_query_size == "more":

        def input_size():
            return args.prompt_len

    else:
        input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    args.ppo_config.model_name,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id

# pdb.set_trace()

if args.change_tokenizer_args == True:
    tokenizer.padding_side = args.tokenizer_kwargs["padding_side"]
    tokenizer.truncation_side = args.tokenizer_kwargs["truncation_side"]
    tokenizer.pad_token_as_eos_token = args.tokenizer_kwargs["pad_token_as_eos_token"]
    tokenizer.max_length = args.tokenizer_kwargs["max_length"]

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
task = "text-classification"
model_name = args.ppo_config.reward_model
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs["pad_token_id"] = tokenizer.eos_token_id

# empty list to store text data
text_train_data = []


def get_current_risk_level(alpha, rho, m, M):
    """
    Get current risk_level_alpha*N value

    Args:
        alpha (float):
            Risk level alpha
        rho (float):
            Risk level scheduler: 0.8 means the soft risk level reaches α after ρ = 80% of the training
        m (int):
            current training step/epoch
        M (int):
            total policy training steps

    Returns:
        alpha_N (int):
            risk_level_alpha*N value
    """
    alpha = args.risk_alpha
    rho = args.risk_rho

    if args.risk_scheduler == "old":
        if m <= args.risk_n:
            return args.ppo_config.batch_size
        else:
            return math.ceil(args.ppo_config.batch_size * max(alpha, 1 - (1 - alpha) * (m - args.risk_n) / (rho * M)))
    else:
        print("here")
        if m <= args.risk_n:
            val = args.ppo_config.batch_size
        elif m >= math.ceil(rho * M):
            val = math.ceil(alpha * args.ppo_config.batch_size)
        else:
            K = (1 - alpha) / (math.ceil(rho * M) - args.risk_n)
            val = math.ceil(args.ppo_config.batch_size * max(alpha, 1 - K * (m - args.risk_n)))
        return val


def get_current_risk_level_2(alpha, rho, m, M):
    """
    Get current risk_level_alpha*N value

    Args:
        alpha (float):
            Risk level alpha
        rho (float):
            Risk level scheduler: 0.8 means the soft risk level reaches α after ρ = 80% of the training
        m (int):
            current training step/epoch
        M (int):
            total policy training steps

    Returns:
        alpha_N (int):
            risk_level_alpha*N value
    """
    alpha = 0.2
    rho = 0.8
    n = 70
    M = 194

    if m <= n:
        val = args.ppo_config.batch_size
    elif m >= math.ceil(rho * M):
        val = alpha * 100
    else:
        K = (1 - alpha) / (math.ceil(rho * M) - n)
        val = math.ceil(args.ppo_config.batch_size * max(alpha, 1 - K * (m - n)))
    return val


def apply_risk_modification_to_batch(batch, query_tensors, response_tensors, rewards, alpha_N):
    query_tensors = list(map(lambda t: t.cpu(), query_tensors))
    response_tensors = list(map(lambda t: t.cpu(), response_tensors))
    rewards = list(map(lambda t: t.cpu(), rewards))
    # response_lengths = [response_tensors[i].shape[0] for i in response_tensors]

    step_dict = {"query_t": query_tensors, "resp_t": response_tensors, "rewards": rewards}

    # batch = {k: [t.cpu() for t in v] if isinstance(v, list) else v.cpu() for k, v in batch.items()}
    batch = {
        k: [t.cpu() for t in v]
        if isinstance(v, list) and all(isinstance(t, torch.Tensor) for t in v)
        else v.cpu()
        if isinstance(v, torch.Tensor)
        else v
        for k, v in batch.items()
    }
    # pdb.set_trace()

    df_step_dict = pd.DataFrame(step_dict)
    df_batch = pd.DataFrame(batch)

    df_step_dict = df_step_dict.sort_values(by="rewards")  # .head(alpha_N)
    df_batch = df_batch.sort_values(by="rewards")  # .head(alpha_N).drop('rewards', axis=1)
    batch = df_batch.to_dict(orient="list")
    # line #996 in ppo_trainer already does response_masks_batch[j] = torch.cat((torch.zeros_like(query_batch[j]), response_masks_batch[j]))[1:]
    # pdb.set_trace()
    # mask = [torch.cat((torch.ones_like(t, dtype =int), torch.zeros([48 - t.shape[0]], dtype =int))) for t in response_tensors]  #torch.zeros([64], dtype =int),
    mask = [torch.ones([32], dtype=int) for _ in response_tensors]  # torch.zeros([64], dtype =int),
    for ii in range(args.ppo_config.batch_size - alpha_N):
        mask[-ii - 1] = torch.zeros([32], dtype=int)

    sr_query_tensors = list(map(lambda t: t.to(device), df_step_dict["query_t"].tolist()))
    sr_response_tensors = list(map(lambda t: t.to(device), df_step_dict["resp_t"].tolist()))
    sr_rewards = list(map(lambda t: t.to(device), df_step_dict["rewards"].tolist()))
    batch = {
        k: [t.to(device) for t in v]
        if isinstance(v, list) and all(isinstance(t, torch.Tensor) for t in v)
        else v.to(device)
        if isinstance(v, torch.Tensor)
        else v
        for k, v in batch.items()
    }

    return batch, sr_query_tensors, sr_response_tensors, sr_rewards, mask


for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)

    if args.reward_function == "trl":
        rewards = [torch.tensor(-output[0]["score"]) for output in pipe_outputs]
    else:  # depreciated
        raise NotImplementedError
        rewards = [
            torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"]]))[1]
            for output in pipe_outputs
        ]

    for ii in range(3):
        data_row = {
            "iter": epoch,
            "input": batch["query"][ii],
            "output": batch["response"][ii],
            "score": rewards[ii].item(),
        }
        text_train_data.append(data_row)

    batch["rewards"] = rewards
    alpha_N_ceil = get_current_risk_level(PPOConfig.soft_risk_alpha, PPOConfig.risk_level_scheduler, epoch, 264)
    # sr_batch, sr_query_t, sr_resp_t, sr_rewards, mask = apply_risk_modification_to_batch(batch, query_tensors, response_tensors, rewards, alpha_N_ceil)

    # Run PPO step
    print(alpha_N_ceil)
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards, alpha_N_ceil)
    ppo_trainer.log_stats(stats, batch, rewards)
    print(epoch)

ppo_trainer._save_pretrained(logging_dir + "/save_pretrained")
df = pd.DataFrame(text_train_data)
df.to_csv(logging_dir + "/textual_data.csv", index=False)

File Path: examples/RealToxicity/dataset/eda.py
Content:
from datasets import load_dataset, load_from_disk, concatenate_datasets
import pdb
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

## Stage 1: Preprocess the dataset #################
# dataset = load_dataset("allenai/real-toxicity-prompts")


# def preprocess_function(examples):
#     # print(examples)
#     # pdb.set_trace()
#     return {
#         "Prompt": examples["prompt"]["text"],
#         "Continuation": examples["continuation"]["text"],
#         "Label": examples["prompt"]["toxicity"] > 0.5 if examples["prompt"]["toxicity"] else None,
#         "Perspective": examples["prompt"]["toxicity"],
#     }


# processed_dataset = (
#     dataset["train"].map(preprocess_function).select_columns(["Prompt", "Continuation", "Label", "Perspective"])
# )
# # Save the processed dataset
# processed_dataset.save_to_disk("real_toxicity.hf")


# # Stage 2: Append reward model scores #################
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# real_toxicity = load_from_disk("real_toxicity.hf")
# # Load the toxicity models
# model_unitary = AutoModelForSequenceClassification.from_pretrained("unitary/toxic-bert").to(device)
# tokenizer_unitary = AutoTokenizer.from_pretrained("unitary/toxic-bert")
# model_facebook = AutoModelForSequenceClassification.from_pretrained(
#     "facebook/roberta-hate-speech-dynabench-r4-target"
# ).to(device)
# tokenizer_facebook = AutoTokenizer.from_pretrained("facebook/roberta-hate-speech-dynabench-r4-target")


# def preprocess_function(examples):
#     # Tokenize the prompts
#     prompt_encodings_unitary = tokenizer_unitary(
#         examples["Prompt"], truncation=True, padding=True, return_tensors="pt"
#     ).to(device)
#     prompt_encodings_facebook = tokenizer_facebook(
#         examples["Prompt"], truncation=True, padding=True, return_tensors="pt"
#     ).to(device)

#     # Compute the toxicity scores
#     with torch.no_grad():
#         logits = model_unitary(**prompt_encodings_unitary).logits.float()
#         unitary_scores = (-logits[:, 1]).tolist()
#         logits = model_facebook(**prompt_encodings_facebook).logits.float()
#         facebook_scores = (logits[:, 0]).tolist()

#     # Create the labels

#     return {"Unitary": unitary_scores, "Facebook": facebook_scores}


# real_toxicity = real_toxicity.map(preprocess_function, batched=True, batch_size=1024)
# real_toxicity.save_to_disk("real_toxicity_prompts.hf")

# Stage 3: Balance dataset  and create train test split
real_toxicity = load_from_disk("real_toxicity_prompts.hf")

# Balance the dataset
real_toxicity = real_toxicity.filter(lambda x: x["Label"] is not None)
real_toxicity_pos = real_toxicity.filter(lambda x: x["Label"] == 0)
real_toxicity_neg = real_toxicity.filter(lambda x: x["Label"] == 1)

real_toxicity_neg_test = real_toxicity_neg.train_test_split(test_size=0.2, seed=42)["test"]
real_toxicity_neg_train = real_toxicity_neg.train_test_split(test_size=0.2, seed=42)["train"]

real_toxicity_pos_test = real_toxicity_pos.train_test_split(test_size=0.2, seed=42)["test"]
real_toxicity_pos_train = real_toxicity_pos.train_test_split(test_size=0.2, seed=42)["train"]

real_toxicity_train = concatenate_datasets(
    [
        real_toxicity_neg_train,
        real_toxicity_pos_train.shuffle(seed=42).select(range(int(len(real_toxicity_neg_train) * 70 / 30))),
    ]
)
real_toxicity_test = concatenate_datasets(
    [real_toxicity_neg_test, real_toxicity_pos_test.shuffle(seed=42).select(range(len(real_toxicity_neg_test)))]
)

print("Real Toxicity train size: ", len(real_toxicity_train))
print(
    "Ratio of positive to total samples: ",
    len(real_toxicity_train.filter(lambda x: x["Label"] == 0)) / len(real_toxicity_train),
)
print("Real Toxicity test size: ", len(real_toxicity_test))
print(
    "Ratio of positive to total samples: ",
    len(real_toxicity_test.filter(lambda x: x["Label"] == 0)) / len(real_toxicity_test),
)
a = input("Enter to save")
real_toxicity_train.save_to_disk(
    "/mnt/research/Anon2/Students/auth2/datasets/RealToxicity/real_toxicity_train.hf"
)
real_toxicity_test.save_to_disk(
    "/mnt/research/Anon2/Students/auth2/datasets/RealToxicity/real_toxicity_test.hf"
)

File Path: examples/RealToxicity/dataset/eda_plotting.py
Content:
from datasets import load_dataset, load_from_disk
from matplotlib import pyplot as plt
import pandas as pd

real_toxicity = load_from_disk(
    "/mnt/research/Anon2/Students/auth2/datasets/RealToxicity/real_toxicity_test.hf"
)


def plot_histogram(dataset, feature_name):
    fig, ax = plt.subplots()
    dataset_label_0 = dataset.filter(lambda x: x["Label"] == 0)
    dataset_label_1 = dataset.filter(lambda x: x["Label"] == 1)
    dataset_label_0 = pd.Series(dataset_label_0[feature_name])
    dataset_label_1 = pd.Series(dataset_label_1[feature_name])
    # print(dataset_label_0)
    dataset_label_0.plot.hist(ax=ax, bins=40, alpha=0.5, label="Non Toxic")
    dataset_label_1.plot.hist(ax=ax, bins=40, alpha=0.5, label="Toxic")
    ax.set_title(f"Histogram of {feature_name} - RealToxicity Test")
    ax.legend()
    plt.savefig(f"{feature_name}_histogram_RealToxicitytest.png")
    plt.clf()


# Perspective
plot_histogram(real_toxicity, "Perspective")

# Unitary
plot_histogram(real_toxicity, "Unitary")

# Facebook
plot_histogram(real_toxicity, "Facebook")

File Path: examples/RealToxicity/dataset/single_perplexity_realtox.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset, load_from_disk
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed
from trl.core import LengthSampler
import pandas as pd
from datasets import Dataset

tqdm.pandas()

import pdb
import pickle
import datetime
import os
from evaluate import load
from transformers import AutoModel, AutoTokenizer, AutoConfig
from transformers import OpenAIGPTTokenizer, OpenAIGPTModel
import random
import statistics as st

# date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
# print(date_n_time)
# logging_dir = f"/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/ppl_eval/{date_n_time}"
# os.makedirs(logging_dir)
prompt_len = 32
response_len = 32

rlhf_model = "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/real_toxicity/2024-02-22_14-39-57/save_pretrained"
ra_rlhf_model = "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/real_toxicity/2024-02-22_17-34-54/save_pretrained"
gpt2_model = "gpt2"
sft_model = "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/real_toxicity/2024-02-22_14-11-00"  # sft positive

# @dataclass
# class ScriptArguments:
#     ppo_config: PPOConfig = field(
#         default_factory=lambda: PPOConfig(
#             model_name=model_path,
#             query_dataset=None,
#             reward_model=None,
#             learning_rate=1.41e-5,
#             mini_batch_size=128,
#             batch_size=128,
#             gradient_accumulation_steps=1,
#             early_stopping=False,
#             target_kl=6.0,
#             kl_penalty="kl",
#             seed=0,
#             use_score_scaling=False,
#             use_score_norm=False,
#             score_clip=None,
#             log_with="tensorboard",
#             project_kwargs={"logging_dir": logging_dir},
#             remove_unused_columns=False,
#         )
#     )
#     query_dataset: str = field(default="imdb", metadata={"help": "the dataset to query"})
#     use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
#     use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
#     peft_config: Optional[LoraConfig] = field(
#         default_factory=lambda: LoraConfig(
#             r=16,
#             lora_alpha=16,
#             bias="none",
#             task_type="CAUSAL_LM",
#         ),
#     )
#     reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
#     input_query_size: str = field(default="less", metadata={"help": "the dataset to query"})


# args = tyro.cli(ScriptArguments)

# # We then define the arguments to pass to the sentiment analysis pipeline.
# # We set `return_all_scores` to True to get the sentiment score for each token.
# sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

# trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset():
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"

    ds = load_from_disk("/mnt/shared-scratch/Shakkottai_S/auth2d36/datasets/RealToxicity/real_toxicity_test.hf")
    ds = ds.rename_columns({"Prompt": "review"})
    # breakpoint()
    ds = ds.filter(lambda x: x["Label"] == 0, batched=False)  # Positive reviews only
    # ds = ds.shuffle(seed=0).select(range(5000))

    # pdb.set_trace()

    def input_size():
        return prompt_len

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# get test dataset
test_dataset = build_dataset()
# Take 5k random samples
# sample_size = 5000
# random_indices = random.sample(range(len(test_dataset)), sample_size)
# test_dataset = test_dataset.select(random_indices)


# def collator(data):
#     return dict((key, [d[key] for d in data]) for key in data[0])


# # set seed before initializing value head for deterministic eval
# set_seed(args.ppo_config.seed)

# # Now let's build the model, the reference model, and the tokenizer.
# if not args.use_peft:
#     ref_model = None  # trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
#     device_map = None
#     peft_config = None
# else:
#     peft_config = args.peft_config
#     ref_model = None
#     # Copy the model to each device
#     device_map = {"": Accelerator().local_process_index}

# # zero_shot_model = trl_model_class.from_pretrained(
# #     "gpt2",
# #     trust_remote_code=True,
# #     device_map=device_map,
# #     peft_config=peft_config,
# # )

# # sft_model = trl_model_class.from_pretrained(
# #     "lvwerra/gpt2-imdb",
# #     trust_remote_code=True,
# #     device_map=device_map,
# #     peft_config=peft_config,
# # )

# # For the following to work I should have saved the model using huggingface
# # ppo_model = trl_model_class.from_pretrained(
# #     '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-09-26_11-27-23/model.pt',
# #     trust_remote_code=True,
# #     device_map=device_map,
# #     peft_config=peft_config,
# # )

# # sft_ppo_model = trl_model_class.from_pretrained(
# #     "/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/trl_sft_ppo/2023-10-23_23-30-47",
# #     trust_remote_code=True,
# #     device_map=device_map,
# #     peft_config=peft_config,
# # )
# """
# ppo_model_PATH = '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-09-26_11-27-23/model.pt'
# ppo_model = torch.load(ppo_model_PATH)

# sft_ppo_model_PATH = '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_ppo/2023-09-26_10-36-41/model.pt'
# sft_ppo_model = torch.load(sft_ppo_model_PATH)
# """
# tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# # Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
# tokenizer.pad_token_id = tokenizer.eos_token_id
# # tokenizer.pad_token = tokenizer.eos_token
# # tokenizer.padding_side = "left"
# # tokenizer.truncation_side = "left"
# # tokenizer.pad_token_as_eos_token = True
# # tokenizer.max_length = 64
# model = trl_model_class.from_pretrained(
#     model_path,
#     trust_remote_code=True,
#     device_map=device_map,
#     peft_config=peft_config,
# )
# # Define all the tester classes
# zero_shot_tester = PPOTrainer(
#     args.ppo_config, model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator
# )
# # sft_tester = PPOTrainer(args.ppo_config, sft_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)
# # # ppo_tester = PPOTrainer(args.ppo_config, ppo_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)
# # sft_ppo_tester = PPOTrainer(
# #     args.ppo_config, sft_ppo_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator
# # )

# # pdb.set_trace()
# # ppo_tester._save_pretrained('/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-09-26_11-27-23')
# # sft_ppo_tester._save_pretrained('/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_ppo/2023-09-26_10-36-41')

# Build the perplexity evaluation pipeline
perplexity = load("perplexity", module_type="measurement")

# DO I NEED TO TAKE CARE OF PAD TOKEN FOR PERPLEXITY PIPELINE AS WELL??
"""
# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id
"""

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
# generation_kwargs = {
#     "min_length": response_len,
#     "top_k": 0,
#     "top_p": 1.0,
#     "do_sample": True,
#     "pad_token_id": tokenizer.eos_token_id,
#     "max_new_tokens": response_len,
# }


# # date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
# os.makedirs(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/evaluation/{date_n_time}')

# model_names_all = ["zero_shot_model", "sft_model", "ppo_model", "sft_ppo_model"]
# testers_all = [zero_shot_tester]  # ppo_tester,

# infer on test dataset
# test_scores_1, test_scores_2, test_scores_3, test_scores_4 = [], [], [], []

perplexity_scores = {"ra_rlhf": None, "rlhf": None, "gpt2": None, "sft": None}
# for epoch, batch in tqdm(enumerate(zero_shot_tester.dataloader)):
batch = test_dataset

texts = batch["review"]
texts = [text[:1000] for text in texts]
# texts = ["lorem ipsum", "Happy Birthday!", "Bienvenue"]

# ra-rlhf
perplexity_score = perplexity.compute(data=texts, model_id=ra_rlhf_model, batch_size=8)["perplexities"]
print(f"Epoch: {0}, Perplexity ra ralhf: {sum(perplexity_score)/len(perplexity_score)}")
perplexity_scores["ra_rlhf"] = perplexity_score

# rlhf
perplexity_score = perplexity.compute(data=texts, model_id=rlhf_model, batch_size=8)["perplexities"]
print(f"Epoch: {0}, Perplexity rlhf: {sum(perplexity_score)/len(perplexity_score)}")
perplexity_scores["rlhf"] = perplexity_score

# gpt2
perplexity_score = perplexity.compute(data=texts, model_id=gpt2_model, batch_size=8)["perplexities"]
print(f"Epoch: {0}, Perplexity gpt2: {sum(perplexity_score)/len(perplexity_score)}")
perplexity_scores["gpt2"] = perplexity_score

# sft
perplexity_score = perplexity.compute(data=texts, model_id=sft_model, batch_size=8)["perplexities"]
print(f"Epoch: {0}, Perplexity sft: {sum(perplexity_score)/len(perplexity_score)}")
perplexity_scores["sft"] = perplexity_score


# save perplexity scores as json in current directory
import json

with open("perplexity_scores.json", "w") as f:
    json.dump(perplexity_scores, f)

File Path: examples/RealToxicity/dataset/test_quantile_batched_real_toxicity.py
Content:
from transformers import pipeline, AutoTokenizer
from datasets import Dataset, load_from_disk
from dataclasses import dataclass, field
from typing import Optional
from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead
from peft import LoraConfig
import pandas as pd
from tqdm import tqdm
import matplotlib.pyplot as plt
import plotly.express as px
import pdb
import tyro
import torch
from evaluate import load

# load the dataset
# jigsaw_df_train = pd.read_csv("jigsaw_train.csv")
# jigsaw_df_test = pd.read_csv("jigsaw_test.csv")

# load reward model
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none"}
jigsaw_reward_model = pipeline("sentiment-analysis", model="unitary/toxic-bert", **sent_kwargs)

ref_model_path = "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/real_toxicity/2024-02-22_14-11-00"  # sft positive
# ref_model_path = (
# "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-20_20-32-35"  # sft both
# )


model_path = "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/real_toxicity/2024-02-22_14-39-57/save_pretrained"
risk_model_path = "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/real_toxicity/2024-02-22_17-34-54/save_pretrained"

# 2x stuff
# model_path = "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/real_toxicity/2024-02-23_11-16-51/save_pretrained"
# risk_model_path = "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/real_toxicity/2024-02-23_12-37-19/save_pretrained"

BETA = 0.2
BETA_risk = 0.2
GAMMA = 1.0

prompt_len = 32
response_len = 32
alpha_level = 20
seed = 56


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="gpt2",
            query_dataset=None,
            # reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            init_kl_coef=BETA,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
        )
    )
    ppo_config_risk: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="gpt2",
            query_dataset=None,
            # reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            init_kl_coef=BETA_risk,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
        )
    )
    query_dataset: str = field(default=None, metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})


args = tyro.cli(ScriptArguments)

tokenizer = AutoTokenizer.from_pretrained(ref_model_path)
tokenizer.pad_token = tokenizer.eos_token

ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(
    ref_model_path,
    trust_remote_code=True,
    device_map=None,
    peft_config=None,
)


model = AutoModelForCausalLMWithValueHead.from_pretrained(
    model_path,
    trust_remote_code=True,
    device_map=None,
    peft_config=None,
)

risk_model = AutoModelForCausalLMWithValueHead.from_pretrained(
    risk_model_path,
    trust_remote_code=True,
    device_map=None,
    peft_config=None,
)
gpt2_model = AutoModelForCausalLMWithValueHead.from_pretrained(
    "gpt2",
    trust_remote_code=True,
    device_map=None,
    peft_config=None,
)
ppo_trainer_gpt2 = PPOTrainer(args.ppo_config, model=gpt2_model, ref_model=ref_model, tokenizer=tokenizer)
ppo_trainer_sft = PPOTrainer(args.ppo_config, model=ref_model, ref_model=ref_model, tokenizer=tokenizer)
ppo_trainer = PPOTrainer(args.ppo_config, model=model, ref_model=ref_model, tokenizer=tokenizer)
ppo_trainer_risk = PPOTrainer(args.ppo_config_risk, model=risk_model, ref_model=ref_model, tokenizer=tokenizer)
perplexity = load("perplexity", module_type="measurement")


def build_dataset():
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"

    ds = load_from_disk("/mnt/shared-scratch/Shakkottai_S/auth2d36/datasets/RealToxicity/real_toxicity_test.hf")
    ds = ds.rename_columns({"Prompt": "review"})

    # pdb.set_trace()

    def input_size():
        return prompt_len

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side="left")
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"

generation_kwargs = {
    "min_length": response_len,
    "top_k": 0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": response_len,
}


batch_size = 128
ds = build_dataset()
df = pd.DataFrame()
for i in tqdm(range(0, len(ds), batch_size)):
    batch = ds[i : i + batch_size]
    query_tensors = batch["input_ids"]
    query_tensors = [q.to(ppo_trainer.current_device) for q in query_tensors]
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    response_tensors_risk = ppo_trainer_risk.generate(query_tensors, return_prompt=False, **generation_kwargs)
    response_tensors_sft = ppo_trainer_sft.generate(query_tensors, return_prompt=False, **generation_kwargs)
    response_tensors_gpt2 = ppo_trainer_gpt2.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)
    batch["response_risk"] = tokenizer.batch_decode(response_tensors_risk)
    batch["response_sft"] = tokenizer.batch_decode(response_tensors_sft)
    batch["response_gpt2"] = tokenizer.batch_decode(response_tensors_gpt2)

    # pdb.set_trace()
    text_data = [text[:500] for text in batch["review"]]
    # Compute query sentiment score
    pipe_outputs_prompt = jigsaw_reward_model(
        batch["query"],
        **sent_kwargs,
        # truncation=True,
    )
    prompt_scores = [torch.tensor(-output[0]["score"]) for output in pipe_outputs_prompt]
    batch["prompt_score"] = [s.item() for s in prompt_scores]

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = jigsaw_reward_model(
        texts,
        **sent_kwargs,
        truncation=True,
    )
    texts_risk = [q + r for q, r in zip(batch["query"], batch["response_risk"])]
    pipe_outputs_risk = jigsaw_reward_model(
        texts_risk,
        **sent_kwargs,
        truncation=True,
    )
    texts_sft = [q + r for q, r in zip(batch["query"], batch["response_sft"])]
    pipe_outputs_sft = jigsaw_reward_model(
        texts_sft,
        **sent_kwargs,
        truncation=True,
    )
    texts_gpt2 = [q + r for q, r in zip(batch["query"], batch["response_gpt2"])]
    pipe_outputs_gpt2 = jigsaw_reward_model(
        texts_gpt2,
        **sent_kwargs,
        truncation=True,
    )
    scores = [torch.tensor(-output[0]["score"]) for output in pipe_outputs]
    scores_risk = [torch.tensor(-output[0]["score"]) for output in pipe_outputs_risk]
    scores_sft = [torch.tensor(-output[0]["score"]) for output in pipe_outputs_sft]
    scores_gpt2 = [torch.tensor(-output[0]["score"]) for output in pipe_outputs_gpt2]
    batch["R"] = [s.item() for s in scores]
    batch["R_risk"] = [s.item() for s in scores_risk]
    batch["R_sft"] = [s.item() for s in scores_sft]
    batch["R_gpt2"] = [s.item() for s in scores_gpt2]

    # Compute R_bar
    scores = torch.tensor(scores, device=ppo_trainer.current_device)
    scores_risk = torch.tensor(scores_risk, device=ppo_trainer_risk.current_device)

    model_inputs = ppo_trainer.prepare_model_inputs(query_tensors, response_tensors)
    model_inputs_risk = ppo_trainer_risk.prepare_model_inputs(query_tensors, response_tensors_risk)

    model_inputs_names = list(model_inputs.keys())

    with torch.no_grad():
        all_logprobs, _, values, masks = ppo_trainer.batched_forward_pass(
            ppo_trainer.model,
            query_tensors,
            response_tensors,
            model_inputs,
            response_masks=None,
            return_logits=False,
        )
        all_logprobs_risk, _, values_risk, masks_risk = ppo_trainer_risk.batched_forward_pass(
            ppo_trainer_risk.model,
            query_tensors,
            response_tensors_risk,
            model_inputs_risk,
            response_masks=None,
            return_logits=False,
        )
        ref_logprobs, _, _, _ = ppo_trainer.batched_forward_pass(
            ppo_trainer.ref_model, query_tensors, response_tensors, model_inputs, return_logits=None
        )
        ref_logprobs_risk, _, _, _ = ppo_trainer_risk.batched_forward_pass(
            ppo_trainer_risk.ref_model, query_tensors, response_tensors_risk, model_inputs_risk, return_logits=None
        )
        rewards, non_score_reward = ppo_trainer.compute_rewards(scores, all_logprobs, ref_logprobs, masks)
        rewards_risk, non_score_reward_risk = ppo_trainer_risk.compute_rewards(
            scores_risk, all_logprobs_risk, ref_logprobs_risk, masks_risk
        )
    if GAMMA < 1.0:
        for t in range(rewards.shape[1]):
            rewards[:, t] = GAMMA**t * rewards[:, t]
            rewards_risk[:, t] = GAMMA**t * rewards_risk[:, t]

    batch["R_bar"] = rewards.sum(axis=1).tolist()
    batch["R_bar_risk"] = rewards_risk.sum(axis=1).tolist()

    # Compute perplexity
    batch["perplexity"] = perplexity.compute(data=text_data, model_id=model_path)["perplexities"]
    batch["perplexity_risk"] = perplexity.compute(data=text_data, model_id=risk_model_path)["perplexities"]
    batch["perplexity_sft"] = perplexity.compute(data=text_data, model_id=ref_model_path)["perplexities"]
    batch["perplexity_gpt2"] = perplexity.compute(data=text_data, model_id="gpt2", batch_size=64)["perplexities"]
    # pdb.set_trace()
    try:
        df = pd.concat([df, pd.DataFrame(batch)], ignore_index=True)
    except:
        batch["input_ids"] = [None] * len(batch["review"])
        df = pd.concat([df, pd.DataFrame(batch)], ignore_index=True)


print("Saving results")
df.to_csv(f"realtoxic_generations_{prompt_len}_{response_len}_alpha_{alpha_level}_seed_{seed}_final.csv", index=False)


print("Average RLHF reward:", df["R"].mean())
print("Average RA-RLHF reward:", df["R_risk"].mean())
print("Average SFT reward:", df["R_sft"].mean())
print("Average GPT2 reward:", df["R_gpt2"].mean())

print("Average RLHF perplexity:", df["perplexity"].mean())
print("Average RA-RLHF perplexity:", df["perplexity_risk"].mean())
print("Average SFT perplexity:", df["perplexity_sft"].mean())
print("Average GPT2 perplexity:", df["perplexity_gpt2"].mean())

File Path: examples/RealToxicity/plotting/new_quantile_plot_imdb.py
Content:
import matplotlib.pyplot as plt
import pandas as pd


# read imdb_gen_df
imdb_gen_df = pd.read_csv("imdb_generations_all_seed_12.csv")
imdb_gen_df2 = pd.read_csv("imdb_generations_seed_42.csv")

# Replace columns of imdb_gen_df with imdb_gen_df2

imdb_gen_df.drop(columns=imdb_gen_df2.columns, inplace=True)
imdb_gen_df = pd.concat([imdb_gen_df, imdb_gen_df2], axis=1)

# New quantile plot


# print(imdb_gen_df.columns)
# a = input("Press Enter to continue...")

quantile_resolution = 1

quantile_df = imdb_gen_df.copy()
quantile_df = quantile_df.sort_values(by=["prompt_score"])
quantile_df = quantile_df.reset_index(drop=True)

# Stuff to plot
gpt2_quantiles = []
rlhf_quantiles = []
rlhf_risk_quantiles = []
sft_quantiles = []

for quantile_point in range(1, 101, quantile_resolution):
    # selecct first quantile_point% of the data
    relevant_df = quantile_df.iloc[: int((quantile_point / 100) * quantile_df.shape[0])]

    gpt2_quantiles.append(relevant_df["R_gpt2"].mean())
    rlhf_quantiles.append(relevant_df["R_rlhf"].mean())
    rlhf_risk_quantiles.append(relevant_df["R_risk"].mean())
    sft_quantiles.append(relevant_df["R_sft"].mean())

# Plot
fig, axs = plt.subplots(figsize=(10, 5))

axs.plot(range(1, 101, quantile_resolution), gpt2_quantiles, label="GPT2", color="red")
axs.plot(range(1, 101, quantile_resolution), rlhf_quantiles, label="RLHF", color="blue")
axs.plot(range(1, 101, quantile_resolution), rlhf_risk_quantiles, label="RA-RLHF", color="green")
axs.plot(range(1, 101, quantile_resolution), sft_quantiles, label="SFT", color="orange")

axs.set_xlabel("Quantile (%)", fontsize=16)  # Set label size on the axis object
axs.set_ylabel(f"Average Reward", fontsize=16)
axs.tick_params(axis="both", which="major", labelsize=14)
axs.grid(True)  # Add grid on the axis object
axs.legend(loc="upper left")
axs.set_title(f"Average Reward vs Quantile")

plt.tight_layout()
plt.savefig("new_quantile_plot_imdb.png")
plt.savefig("new_quantile_plot_imdb.pdf")
print("Verify plot saved")
print("GPT2: ", gpt2_quantiles[-1])
print("RLHF: ", rlhf_quantiles[-1])
print("RA-RLHF: ", rlhf_risk_quantiles[-1])
print("SFT: ", sft_quantiles[-1])

File Path: examples/RealToxicity/plotting/new_quantile_plot_jigsaw.py
Content:
import matplotlib.pyplot as plt
import pandas as pd


# read jigsaw_gen_df
jigsaw_gen_df = pd.read_csv("../dataset/jigsaw_generations_8_32_alpha_40_seed_36.csv")


# New quantile plot


# print(jigsaw_gen_df.columns)
# a = input("Press Enter to continue...")

quantile_resolution = 1

quantile_df = jigsaw_gen_df.copy()
quantile_df = quantile_df.sort_values(by=["prompt_score"])
quantile_df = quantile_df.reset_index(drop=True)

# Stuff to plot
gpt2_quantiles = []
rlhf_quantiles = []
rlhf_risk_quantiles = []
sft_quantiles = []

for quantile_point in range(1, 101, quantile_resolution):
    # selecct first quantile_point% of the data
    relevant_df = quantile_df.iloc[: int((quantile_point / 100) * quantile_df.shape[0])]

    gpt2_quantiles.append(relevant_df["R_gpt2"].mean())
    rlhf_quantiles.append(relevant_df["R"].mean())
    rlhf_risk_quantiles.append(relevant_df["R_risk"].mean())
    sft_quantiles.append(relevant_df["R_sft"].mean())

# Plot
fig, axs = plt.subplots(figsize=(10, 5))

axs.plot(range(1, 101, quantile_resolution), gpt2_quantiles, label="GPT2", color="red")
axs.plot(range(1, 101, quantile_resolution), rlhf_quantiles, label="RLHF", color="blue")
axs.plot(range(1, 101, quantile_resolution), rlhf_risk_quantiles, label="RA-RLHF", color="green")
axs.plot(range(1, 101, quantile_resolution), sft_quantiles, label="SFT", color="orange")

axs.set_xlabel("Quantile (%)", fontsize=16)  # Set label size on the axis object
axs.set_ylabel(f"Average Reward", fontsize=16)
axs.tick_params(axis="both", which="major", labelsize=14)
axs.grid(True)  # Add grid on the axis object
axs.legend(loc="upper left")
axs.set_title(f"Average Reward vs Quantile")

plt.tight_layout()
plt.savefig("new_quantile_plot_jigsaw_see36.png")
plt.savefig("new_quantile_plot_jigsaw_see36.pdf")
print("Verify plot saved")
print("GPT2: ", gpt2_quantiles[-1])
print("RLHF: ", rlhf_quantiles[-1])
print("RA-RLHF: ", rlhf_risk_quantiles[-1])
print("SFT: ", sft_quantiles[-1])

File Path: examples/RealToxicity/plotting/pick_best_examples.py
Content:
import pandas as pd

df = pd.read_csv("../dataset/jigsaw_generations_8_32_alpha_20_seed_2.csv")
df = df.filter(["query", "prompt_score", "response", "R", "response_risk", "R_risk"])

# Sort by difference between R and R_risk
df["R_diff"] = df["R_risk"] - df["R"]
df = df.sort_values(by=["R_diff"], ascending=False)

# remove columns where prompt score bigger than R risk or R
df = df[df["prompt_score"] < df["R_risk"]]
df = df[df["prompt_score"] < df["R"]]
df = df[df["prompt_score"] < -2.5]

df.to_csv("examples.csv", index=False)

File Path: examples/RealToxicity/plotting/plot_from_multiple_tb_log.py
Content:
import matplotlib.pyplot as plt
import numpy as np
from tensorboard.backend.event_processing import event_accumulator

import pdb


# Function to extract scalar data from a TensorBoard log file
def extract_scalar_data_from_event_file(event_file_path):
    # Initialize an accumulator
    ea = event_accumulator.EventAccumulator(event_file_path)
    ea.Reload()  # Loads the log data from file

    # Get all scalar tags
    scalar_tags = ea.Tags()["scalars"]

    # Dictionary to hold the data
    scalar_data = {}

    # Extract data for each scalar tag
    for tag in scalar_tags:
        events = ea.Scalars(tag)
        times = [e.wall_time for e in events]
        steps = [e.step for e in events]
        values = [e.value for e in events]
        scalar_data[tag] = {"times": times, "steps": steps, "values": values}

    return scalar_data


# Function for smoothing the curve (simple moving average for illustration)
def smooth_curve(points, factor=0.97):
    smoothed_points = np.zeros_like(points)
    for i in range(len(points)):
        if i > 0:
            smoothed_points[i] = (smoothed_points[i - 1] * factor) + (points[i] * (1 - factor))
        else:
            smoothed_points[i] = points[i]
    return smoothed_points


# Path to your TensorBoard log file or directory
# Assuming `all_data` is a structured dictionary containing all runs, organized by algorithm and then by seed
# e.g., all_data = {'algorithm1': {'seed1': data1, 'seed2': data2, ...},
#                   'algorithm2': {'seed1': data1, 'seed2': data2, ...}, ...}

# log_file_paths = {
#     "RA-RLHF": [
#         "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/real_toxicity/2024-02-22_17-34-54/trl",
#         "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/real_toxicity/2024-02-22_19-17-19/trl",
#         "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/real_toxicity/2024-02-22_21-00-40/trl",
#     ],
#     "RLHF": [
#         "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/real_toxicity/2024-02-22_14-39-57/trl",
#         "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/real_toxicity/2024-02-22_16-43-04/trl",
#         "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/real_toxicity/2024-02-22_18-45-40/trl",
#     ],
# }

# 2x stuff
log_file_paths = {
    "RA-RLHF": [
        "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/real_toxicity/2024-02-23_11-16-41/trl",
        "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/real_toxicity/2024-02-23_12-37-19/trl",
        "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/real_toxicity/2024-02-23_13-58-01/trl",
    ],
    "RLHF": [
        "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/real_toxicity/2024-02-23_15-29-36/trl",
        "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/real_toxicity/2024-02-23_11-16-51/trl",
        "/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/real_toxicity/2024-02-23_12-37-13/trl",
    ],
}
# Extract data
all_data = {}
for algorithm, paths in log_file_paths.items():
    all_data[algorithm] = {}
    for i, path in enumerate(paths):
        seed_key = f"seed{i+1}"
        # load data
        all_data[algorithm][seed_key] = extract_scalar_data_from_event_file(path)

# all_data['RLHF']['seed1']['objective/kl'].keys()
# dict_keys(['times', 'steps', 'values'])

desired_tags = {
    "env/reward_mean": "Environment Reward",
    "env/reward_std": "Reward std",
    "objective/entropy": "Policy Entropy",
    "objective/kl": "KL Divergence",
    "objective/kl_coef": "Beta",
    "ppo/returns/mean": "Return over Batch",
    "tokens/responses_len_mean": "Response Length",
}

# Step 1: Calculate mean and standard deviation for each tag for each algorithm
tag_stats_by_algorithm = {}
for algorithm, seeds_data in all_data.items():
    tag_grouped_data = {}
    for seed, data in seeds_data.items():
        print(seed)
        for tag, values in data.items():
            # pdb.set_trace()
            if tag in desired_tags.keys():
                print(tag)
                if tag not in tag_grouped_data:
                    # pdb.set_trace()
                    print("here")
                    tag_grouped_data[tag] = []
                tag_grouped_data[tag].append(values)

    # pdb.set_trace()
    tag_stats = {}
    for tag, values_list in tag_grouped_data.items():
        # Assuming all seed runs have the same number of steps and are aligned
        steps = values_list[0]["steps"]
        all_values = np.array([values["values"] for values in values_list])
        mean_values = np.mean(all_values, axis=0)
        std_values = np.std(all_values, axis=0)
        tag_stats[tag] = {"steps": steps, "mean": mean_values, "std": std_values}

    tag_stats_by_algorithm[algorithm] = tag_stats


# Step 2: Plot mean and standard deviation for each tag for each algorithm
for tag in tag_grouped_data.keys():
    fig, ax = plt.subplots(figsize=(5, 5))  # Create a figure and an axis object

    for algorithm, stats in tag_stats_by_algorithm.items():
        if tag in stats:
            print(tag)
            steps = stats[tag]["steps"]
            mean_values = smooth_curve(stats[tag]["mean"])
            std_values = smooth_curve(stats[tag]["std"])

            if algorithm == "RLHF":
                ax.plot(steps, mean_values, label=f"{algorithm}", color="red")
                ax.fill_between(
                    steps, mean_values - std_values, mean_values + std_values, color="red", alpha=0.2
                )  # , label=f'{algorithm} std dev')
            else:
                ax.plot(steps, mean_values, label=f"{algorithm}", color="green")
                ax.fill_between(
                    steps, mean_values - std_values, mean_values + std_values, color="green", alpha=0.2
                )  # , label=f'{algorithm} std dev')
            ax.set_xlabel("Training iteration", fontsize=24)  # Set label size on the axis object
            ax.set_ylabel(f"{desired_tags[tag]}", fontsize=24)  # Set label size on the axis object
            ax.tick_params(axis="both", which="major", labelsize=20)  # Adjust tick size on the axis object
            # ax.set_box_aspect(0.7)  # Commented out because this might not be available depending on your matplotlib version
            ax.grid(True)  # Add grid on the axis object
            ax.legend(loc="upper left", fontsize=16)  # Add legend on the axis object
            # plt.title(f'Mean and Standard Deviation for {tag} across Algorithms')
            plt.savefig(
                f"train_plots_realtoxicity_2x/{desired_tags[tag].replace(' ','_')}.pdf",
                bbox_inches="tight",
                pad_inches=0,
            )  # as png

File Path: examples/RealToxicity/plotting/test_plotting_2.py
Content:
import matplotlib.pyplot as plt

# import plotly.express as px
import pandas as pd
import numpy as np
import pdb

# IMDB
imdb_df_test = pd.read_csv("jigsaw_generations_35_32.csv")

# tail_to_save = -2.8
# tail_df = imdb_df_test[imdb_df_test['prompt_score'] <= tail_to_save]
# tail_df = tail_df.drop(columns=['input_ids'])
# print(tail_df['R'].mean())
# print(tail_df['R_risk'].mean())
# tail_df.to_csv("tail/imdb_tail.csv", index=False)
# pdb.set_trace()

# Rename as needed in plots
# imdb_df_test = imdb_df_test.rename(columns={"R": "RLHF", "R_risk": "RLHF risk averse"})
# imdb_df_test = imdb_df_test.rename(columns={"R_bar": "RLHF returns", "R_bar_risk": "RLHF risk averse returns"})


# # CDF plots
# fig = px.ecdf(
#     imdb_df_test,
#     x=["RLHF", "RLHF risk averse"],
#     labels={
#         "value": "Reward",
#         "variable": "Model",
#     },
# )
# fig.update_layout(
#     title="Reward distribution for RLHF and RLHF risk averse models",
#     xaxis_title="Reward",
#     yaxis_title="CDF",
# )
# print("Average reward for RLHF: ", imdb_df_test["RLHF"].mean())
# print("Average reward for RLHF risk averse: ", imdb_df_test["RLHF risk averse"].mean())
# fig.write_image("imdb_R_cdf.png")

# fig = px.ecdf(
#     imdb_df_test,
#     x=["RLHF returns", "RLHF risk averse returns"],
#     labels={
#         "value": "Reward",
#         "variable": "Model",
#     },
# )
# fig.update_layout(
#     title="Returns distribution for RLHF and RLHF risk averse models",
#     xaxis_title="Returns",
#     yaxis_title="CDF",
# )
# print("Average return for RLHF: ", imdb_df_test["RLHF returns"].mean())
# print("Average return for RLHF risk averse: ", imdb_df_test["RLHF risk averse returns"].mean())
# fig.write_image("imdb_R_bar_cdf.png")

# pdb.set_trace()
# Histogram shift plots for imdb


# Function to plot only the top edges of a histogram
def plot_hist_top_edges(data, ax, bins, color, label):
    hist, bin_edges = np.histogram(data, bins=bins)
    for left, right, height in zip(bin_edges[:-1], bin_edges[1:], hist):
        ax.plot([left, right], [height, height], color=color, label=label)
        label = None  # To avoid duplicate labels in the legend


fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(30, 6))
word_map = {"prompt_score": "Prompt Score", "R": "RLHF", "R_risk": "RA-RLHF"}
for i, col in enumerate(["prompt_score", "R", "R_risk"]):
    imdb_df_test[imdb_df_test["toxic"] == 1][col].plot.hist(ax=axs[i], bins=30, alpha=0.5, label="Toxic")
    imdb_df_test[imdb_df_test["toxic"] == 0][col].plot.hist(ax=axs[i], bins=30, alpha=0.5, label="Non Toxic")

    axs[i].set_title(f"{word_map[col]}", fontsize=30)
    axs[i].set_xlabel("Reward", fontsize=30)  # Set label size on the axis object
    axs[i].set_ylabel("")
    axs[i].tick_params(axis="both", which="major", labelsize=28)
    axs[i].grid(True)  # Add grid on the axis object
    axs[i].legend(loc="upper left", fontsize=24)  # Add legend on the axis objec


# Find the maximum y-value among the first three histograms
max_count = max(axs[0].get_ylim()[1], axs[1].get_ylim()[1], axs[2].get_ylim()[1])

# Set the y-axis limits for the first three subplots to be the same
axs[0].set_ylim(0, max_count)
axs[1].set_ylim(0, max_count)
axs[2].set_ylim(0, max_count)
axs[0].set_ylabel(f"Count", fontsize=30)

# tail prompts
tail_filter_level = -6.0
filtered_df = imdb_df_test[imdb_df_test["prompt_score"] <= tail_filter_level]
print(filtered_df.shape[0])
filtered_df[filtered_df["toxic"] == 0]["prompt_score"].plot.hist(ax=axs[3], bins=30, alpha=0.5, label="Non Toxic")
filtered_df[filtered_df["toxic"] == 1]["prompt_score"].plot.hist(ax=axs[3], bins=30, alpha=0.5, label="Toxic")
axs[3].set_title("Tail Prompts", fontsize=30)
axs[3].set_xlabel("Reward", fontsize=30)  # Set label size on the axis object
axs[3].set_ylabel("")
axs[3].tick_params(axis="both", which="major", labelsize=28)
axs[3].grid(True)  # Add grid on the axis object
axs[3].legend(loc="upper left", fontsize=24)  # Add legend on the axis objec
axs[3].axvline(x=tail_filter_level, color="red", linestyle="-", linewidth=4)

# tail performance
filtered_df["R"].plot.hist(ax=axs[4], bins=30, alpha=0.3, label="RLHF", color="red")
filtered_df["R_risk"].plot.hist(ax=axs[4], bins=30, alpha=0.5, label="RA-RLHF", color="olive")
axs[4].set_title("Tail Performance", fontsize=30)
axs[4].set_xlabel("Reward", fontsize=30)  # Set label size on the axis object
axs[4].set_ylabel("")
axs[4].tick_params(axis="both", which="major", labelsize=28)
axs[4].grid(True)  # Add grid on the axis object
axs[4].legend(loc="upper right", fontsize=24)  # Add legend on the axis objec
axs[4].axvline(x=tail_filter_level, color="red", linestyle="-", linewidth=4)

# Find the maximum y-value among the first three histograms
max_count_2 = max(axs[3].get_ylim()[1], axs[4].get_ylim()[1])

# Set the y-axis limits for the first three subplots to be the same
axs[3].set_ylim(0, max_count_2)
axs[4].set_ylim(0, max_count_2)

fig.tight_layout()
plt.savefig("./test_plots_imdb/imdb_distribution_shifts.png")
plt.savefig("./test_plots_imdb/imdb_distribution_shifts.pdf")
plt.clf()

# Quantile plots of returns for imdb

fig, axs = plt.subplots(figsize=(10, 5))

word_map = {"R_bar": "RLHF returns", "R_bar_risk": "RA-RLHF returns"}

for i, col in enumerate(["R_bar", "R_bar_risk"]):
    x = np.linspace(0, 1, len(imdb_df_test[col])) * 100
    y = imdb_df_test[col].sort_values().values
    axs.plot(x, y, label=word_map[col])

axs.set_xlabel("Quantile (%)", fontsize=16)  # Set label size on the axis object
axs.set_ylabel(f"Returns", fontsize=16)
axs.tick_params(axis="both", which="major", labelsize=14)
axs.grid(True)  # Add grid on the axis object
axs.legend(loc="upper left")
axs.set_title(f"Quantile plot of returns")


fig.tight_layout()
plt.savefig("./test_plots_imdb/imdb_R_bar_quantiles.png")
plt.savefig("./test_plots_imdb/imdb_R_bar_quantiles.pdf")
plt.clf()


# Quantile plots of rewards for imdb

fig, axs = plt.subplots(figsize=(10, 5))

word_map = {"R": "RLHF", "R_risk": "RA-RLHF"}

for i, col in enumerate(["R", "R_risk"]):
    x = np.linspace(0, 1, len(imdb_df_test[col])) * 100
    y = imdb_df_test[col].sort_values().values
    axs.plot(x, y, label=word_map[col])

axs.set_xlabel("Quantile (%)", fontsize=16)  # Set label size on the axis object
axs.set_ylabel(f"Rewards", fontsize=16)
axs.tick_params(axis="both", which="major", labelsize=14)
axs.grid(True)  # Add grid on the axis object
axs.legend(loc="upper left")
axs.set_title(f"Quantile plot of rewards")


fig.tight_layout()
plt.savefig("./test_plots_imdb/imdb_R_quantiles.png")
plt.savefig("./test_plots_imdb/imdb_R_quantiles.pdf")
plt.clf()

# # Box plots of returns for imdb

# fig, axs = plt.subplots(figsize=(10, 5))


# # # Jigsaw
# jigsaw_df_test = pd.read_csv("jigsaw_generations.csv")

# # Rename as needed in plots
# # jigsaw_df_test = jigsaw_df_test.rename(columns={"R": "RLHF", "R_risk": "RLHF risk averse"})
# # jigsaw_df_test = jigsaw_df_test.rename(columns={"R_bar": "RLHF returns", "R_bar_risk": "RLHF risk averse returns"})

# # # CDF plots
# # fig = px.ecdf(
# #     jigsaw_df_test,
# #     x=["RLHF", "RLHF risk averse"],
# #     labels={
# #         "value": "Reward",
# #         "variable": "Model",
# #     },
# # )
# # fig.update_layout(
# #     title="Reward distribution for RLHF and RLHF risk averse models",
# #     xaxis_title="Reward",
# #     yaxis_title="CDF",
# # )
# # print("Average reward for RLHF: ", jigsaw_df_test["RLHF"].mean())
# # print("Average reward for RLHF risk averse: ", jigsaw_df_test["RLHF risk averse"].mean())
# # fig.write_image("jigsaw_R_cdf.png")

# # fig = px.ecdf(
# #     jigsaw_df_test,
# #     x=["RLHF returns", "RLHF risk averse returns"],
# #     labels={
# #         "value": "Reward",
# #         "variable": "Model",
# #     },
# # )
# # fig.update_layout(
# #     title="Returns distribution for RLHF and RLHF risk averse models",
# #     xaxis_title="Returns",
# #     yaxis_title="CDF",
# # )
# # print("Average return for RLHF: ", jigsaw_df_test["RLHF returns"].mean())
# # print("Average return for RLHF risk averse: ", jigsaw_df_test["RLHF risk averse returns"].mean())
# # fig.write_image("jigsaw_R_bar_cdf.png")

# # Plot histogram shifts for jigsaw
# fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(10, 5))
# word_map = {"prompt_score": "Prompt", "R": "RLHF", "R_risk": "RA-RLHF"}
# for i, col in enumerate(["prompt_score", "R", "R_risk"]):
#     jigsaw_df_test[jigsaw_df_test["toxic"] == 0][col].plot.hist(ax=axs[i], bins=30, alpha=0.5, label="Non Toxic")
#     jigsaw_df_test[jigsaw_df_test["toxic"] == 1][col].plot.hist(ax=axs[i], bins=30, alpha=0.5, label="Toxic")

#     axs[i].set_xlabel("Reward", fontsize=16)  # Set label size on the axis object
#     axs[i].set_ylabel(f"Count", fontsize=16)
#     axs[i].tick_params(axis="both", which="major", labelsize=14)
#     axs[i].grid(True)  # Add grid on the axis object
#     axs[i].legend(loc="upper left")  # Add legend on the axis objec
#     axs[i].set_title(f"{word_map[col]}")


# fig.tight_layout()
# plt.savefig("./test_plots_jigsaw/jigsaw_distribution_shifts.png")
# plt.savefig("./test_plots_jigsaw/jigsaw_distribution_shifts.pdf")
# plt.clf()

# # Quantile plots of returns for jigsaw

# fig, axs = plt.subplots(figsize=(10, 5))
# word_map = {"R_bar": "RLHF returns", "R_bar_risk": "RA-RLHF returns"}

# for i, col in enumerate(["R_bar", "R_bar_risk"]):
#     x = np.linspace(0, 1, len(jigsaw_df_test[col])) * 100
#     y = jigsaw_df_test[col].sort_values().values
#     axs.plot(x, y, label=word_map[col])

# axs.set_xlabel("Quantile (%)", fontsize=16)  # Set label size on the axis object
# axs.set_ylabel(f"Returns", fontsize=16)
# axs.tick_params(axis="both", which="major", labelsize=14)
# axs.grid(True)  # Add grid on the axis object
# axs.legend(loc="upper left")
# axs.set_title(f"Quantile plot of returns")


# fig.tight_layout()
# plt.savefig("./test_plots_jigsaw/jigsaw_R_bar_quantiles.png")
# plt.savefig("./test_plots_jigsaw/jigsaw_R_bar_quantiles.pdf")
# plt.clf()

# # Quantile plots of rewards for jigsaw

# fig, axs = plt.subplots(figsize=(10, 5))
# word_map = {"R": "RLHF", "R_risk": "RA-RLHF"}
# for i, col in enumerate(["R", "R_risk"]):
#     x = np.linspace(0, 1, len(jigsaw_df_test[col])) * 100
#     y = jigsaw_df_test[col].sort_values().values
#     axs.plot(x, y, label=word_map[col])

# axs.set_xlabel("Quantile (%)", fontsize=16)  # Set label size on the axis object
# axs.set_ylabel(f"Rewards", fontsize=16)
# axs.tick_params(axis="both", which="major", labelsize=14)
# axs.grid(True)  # Add grid on the axis object
# axs.legend(loc="upper left")
# axs.set_title(f"Quantile plot of rewards")


# fig.tight_layout()
# plt.savefig("./test_plots_jigsaw/jigsaw_R_quantiles.png")
# plt.savefig("./test_plots_jigsaw/jigsaw_R_quantiles.pdf")
# plt.clf()

File Path: examples/RealToxicity/plotting/test_plotting_new.py
Content:
import matplotlib.pyplot as plt
import plotly.express as px
import pandas as pd
import numpy as np

# # IMDB
# jigsaw_df_test = pd.read_csv("imdb_generations.csv")

# # Rename as needed in plots
# # jigsaw_df_test = jigsaw_df_test.rename(columns={"R": "RLHF", "R_risk": "RLHF risk averse"})
# # jigsaw_df_test = jigsaw_df_test.rename(columns={"R_bar": "RLHF returns", "R_bar_risk": "RLHF risk averse returns"})


# # # CDF plots
# # fig = px.ecdf(
# #     jigsaw_df_test,
# #     x=["RLHF", "RLHF risk averse"],
# #     labels={
# #         "value": "Reward",
# #         "variable": "Model",
# #     },
# # )
# # fig.update_layout(
# #     title="Reward distribution for RLHF and RLHF risk averse models",
# #     xaxis_title="Reward",
# #     yaxis_title="CDF",
# # )
# # print("Average reward for RLHF: ", jigsaw_df_test["RLHF"].mean())
# # print("Average reward for RLHF risk averse: ", jigsaw_df_test["RLHF risk averse"].mean())
# # fig.write_image("imdb_R_cdf.png")

# # fig = px.ecdf(
# #     jigsaw_df_test,
# #     x=["RLHF returns", "RLHF risk averse returns"],
# #     labels={
# #         "value": "Reward",
# #         "variable": "Model",
# #     },
# # )
# # fig.update_layout(
# #     title="Returns distribution for RLHF and RLHF risk averse models",
# #     xaxis_title="Returns",
# #     yaxis_title="CDF",
# # )
# # print("Average return for RLHF: ", jigsaw_df_test["RLHF returns"].mean())
# # print("Average return for RLHF risk averse: ", jigsaw_df_test["RLHF risk averse returns"].mean())
# # fig.write_image("imdb_R_bar_cdf.png")


# # Histogram shift plots for imdb
# fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(10, 5))
# word_map = {"prompt_score": "Prompt", "R": "RLHF", "R_risk": "RA-RLHF"}
# for i, col in enumerate(["prompt_score", "R", "R_risk"]):
#     jigsaw_df_test[jigsaw_df_test["label"] == 0][col].plot.hist(ax=axs[i], bins=30, alpha=0.5, label="Negative")
#     jigsaw_df_test[jigsaw_df_test["label"] == 1][col].plot.hist(ax=axs[i], bins=30, alpha=0.5, label="Positive")

#     axs[i].set_title(f"{word_map[col]}")
#     axs[i].set_xlabel("Reward", fontsize=16)  # Set label size on the axis object
#     axs[i].set_ylabel(f"Count", fontsize=16)
#     axs[i].tick_params(axis="both", which="major", labelsize=14)
#     axs[i].grid(True)  # Add grid on the axis object
#     axs[i].legend(loc="upper left")  # Add legend on the axis objec

# fig.tight_layout()
# plt.savefig("./test_plots_imdb/imdb_distribution_shifts.png")
# plt.savefig("./test_plots_imdb/imdb_distribution_shifts.pdf")
# plt.clf()

# # Quantile plots of returns for imdb

# fig, axs = plt.subplots(figsize=(10, 5))

# word_map = {"R_bar": "RLHF returns", "R_bar_risk": "RA-RLHF returns"}

# for i, col in enumerate(["R_bar", "R_bar_risk"]):
#     x = np.linspace(0, 1, len(jigsaw_df_test[col])) * 100
#     y = jigsaw_df_test[col].sort_values().values
#     axs.plot(x, y, label=word_map[col])

# axs.set_xlabel("Quantile (%)", fontsize=16)  # Set label size on the axis object
# axs.set_ylabel(f"Returns", fontsize=16)
# axs.tick_params(axis="both", which="major", labelsize=14)
# axs.grid(True)  # Add grid on the axis object
# axs.legend(loc="upper left")
# axs.set_title(f"Quantile plot of returns")


# fig.tight_layout()
# plt.savefig("./test_plots_imdb/imdb_R_bar_quantiles.png")
# plt.savefig("./test_plots_imdb/imdb_R_bar_quantiles.pdf")
# plt.clf()


# # Quantile plots of rewards for imdb

# fig, axs = plt.subplots(figsize=(10, 5))

# word_map = {"R": "RLHF", "R_risk": "RA-RLHF"}

# for i, col in enumerate(["R", "R_risk"]):
#     x = np.linspace(0, 1, len(jigsaw_df_test[col])) * 100
#     y = jigsaw_df_test[col].sort_values().values
#     axs.plot(x, y, label=word_map[col])

# axs.set_xlabel("Quantile (%)", fontsize=16)  # Set label size on the axis object
# axs.set_ylabel(f"Rewards", fontsize=16)
# axs.tick_params(axis="both", which="major", labelsize=14)
# axs.grid(True)  # Add grid on the axis object
# axs.legend(loc="upper left")
# axs.set_title(f"Quantile plot of rewards")


# fig.tight_layout()
# plt.savefig("./test_plots_imdb/imdb_R_quantiles.png")
# plt.savefig("./test_plots_imdb/imdb_R_quantiles.pdf")
# plt.clf()

# # Box plots of returns for imdb

# fig, axs = plt.subplots(figsize=(10, 5))


# # Jigsaw
jigsaw_df_test = pd.read_csv(
    "/mnt/research/Anon2/Students/auth2/repos/trl/examples/RealToxicity/dataset/realtoxic_generations_32_32_alpha_20_seed_56_final.csv"
)
# jigsaw_df_test_2 = pd.read_csv("../dataset/jigsaw_generations_8_32_alpha_40_seed_2.csv")

# jigsaw_df_test_2.drop(columns=jigsaw_df_test.columns, inplace=True)
# jigsaw_df_test = pd.concat([jigsaw_df_test, jigsaw_df_test_2], axis=1)


# Rename as needed in plots
# jigsaw_df_test = jigsaw_df_test.rename(columns={"R": "RLHF", "R_risk": "RLHF risk averse"})
# jigsaw_df_test = jigsaw_df_test.rename(columns={"R_bar": "RLHF returns", "R_bar_risk": "RLHF risk averse returns"})

# # CDF plots
# fig = px.ecdf(
#     jigsaw_df_test,
#     x=["RLHF", "RLHF risk averse"],
#     labels={
#         "value": "Reward",
#         "variable": "Model",
#     },
# )
# fig.update_layout(
#     title="Reward distribution for RLHF and RLHF risk averse models",
#     xaxis_title="Reward",
#     yaxis_title="CDF",
# )
# print("Average reward for RLHF: ", jigsaw_df_test["RLHF"].mean())
# print("Average reward for RLHF risk averse: ", jigsaw_df_test["RLHF risk averse"].mean())
# fig.write_image("jigsaw_R_cdf.png")

# fig = px.ecdf(
#     jigsaw_df_test,
#     x=["RLHF returns", "RLHF risk averse returns"],
#     labels={
#         "value": "Reward",
#         "variable": "Model",
#     },
# )
# fig.update_layout(
#     title="Returns distribution for RLHF and RLHF risk averse models",
#     xaxis_title="Returns",
#     yaxis_title="CDF",
# )
# print("Average return for RLHF: ", jigsaw_df_test["RLHF returns"].mean())
# print("Average return for RLHF risk averse: ", jigsaw_df_test["RLHF risk averse returns"].mean())
# fig.write_image("jigsaw_R_bar_cdf.png")

# tail_to_save = -3

# tail_df = jigsaw_df_test[jigsaw_df_test["prompt_score"] <= tail_to_save]

# tail_df = tail_df.drop(columns=["input_ids"])

# print(tail_df["R"].mean())

# print(tail_df["R_risk"].mean())

zoom_border_color = "limegreen"


# Plot histogram shifts for jigsaw
def plot_hist_top_edges(data, ax, bins, color, label):
    hist, bin_edges = np.histogram(data, bins=bins)

    for left, right, height in zip(bin_edges[:-1], bin_edges[1:], hist):
        ax.plot([left, right], [height, height], color=color, label=label)

        label = None  # To avoid duplicate labels in the legend


fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(30, 6))

word_map = {"prompt_score": "Prompt Score", "R": "RLHF", "R_risk": "RA-RLHF", "R_sft": "SFT"}

for i, col in enumerate(["prompt_score", "R_sft", "R", "R_risk"]):
    jigsaw_df_test[jigsaw_df_test["Label"] == 0][col].plot.hist(ax=axs[i], bins=30, alpha=0.5, label="Non Toxic")
    jigsaw_df_test[jigsaw_df_test["Label"] == 1][col].plot.hist(ax=axs[i], bins=30, alpha=0.5, label="Toxic")

    axs[i].set_title(f"{word_map[col]}", fontsize=30)

    axs[i].set_xlabel("Reward", fontsize=30)  # Set label size on the axis object

    axs[i].set_ylabel("")

    axs[i].tick_params(axis="both", which="major", labelsize=28)

    axs[i].grid(True)  # Add grid on the axis object

    axs[i].legend(loc="upper left", fontsize=24)  # Add legend on the axis objec

    # zoomed in bit
    left_lim = -7
    right_lim = 5
    axs_zoom = axs[i].inset_axes([0.15, 0.20, 0.65, 0.40])  # left, bottom, width, height

    axs_zoom.hist(
        jigsaw_df_test[jigsaw_df_test["Label"] == 0][col],
        bins=30,
        range=[left_lim, right_lim],
        alpha=0.5,
        label="Non Toxic",
    )
    axs_zoom.hist(
        jigsaw_df_test[jigsaw_df_test["Label"] == 1][col],
        bins=30,
        range=[left_lim, right_lim],
        alpha=0.5,
        label="Label",
    )
    axs_zoom.set_ylim(0, 150)
    axs_zoom.set_xlim(left_lim, right_lim)
    axs_zoom.xaxis.set_major_locator(plt.MaxNLocator(3))
    axs_zoom.yaxis.set_major_locator(plt.MaxNLocator(3))
    axs_zoom.tick_params(axis="both", which="major", labelsize=20)
    # set border color to orange
    axs_zoom.spines["bottom"].set_color(zoom_border_color)
    axs_zoom.spines["top"].set_color(zoom_border_color)
    axs_zoom.spines["left"].set_color(zoom_border_color)
    axs_zoom.spines["right"].set_color(zoom_border_color)
    # set border width to zoom_thickness
    zoom_thickness = 3.0
    axs_zoom.spines["bottom"].set_linewidth(zoom_thickness)
    axs_zoom.spines["top"].set_linewidth(zoom_thickness)
    axs_zoom.spines["left"].set_linewidth(zoom_thickness)
    axs_zoom.spines["right"].set_linewidth(zoom_thickness)
    # rectangle coords in zoomed in plot
    rect_height = 300
    rectangle = plt.Rectangle(
        (left_lim, 0),
        right_lim - left_lim,
        rect_height,
        facecolor="none",
        edgecolor=zoom_border_color,
        lw=zoom_thickness,
    )
    axs[i].add_patch(rectangle)


# Find the maximum y-value among the first three histograms

max_count = max(axs[0].get_ylim()[1], axs[1].get_ylim()[1], axs[2].get_ylim()[1])


# Set the y-axis limits for the first three subplots to be the same
axs[0].set_ylim(0, max_count)
axs[1].set_ylim(0, max_count)
axs[2].set_ylim(0, max_count)
axs[3].set_ylim(0, max_count)
axs[0].set_ylabel(f"Count", fontsize=30)

# make quantile plot
# imdb_gen_df = imdb_df_test2
# imdb_gen_df2 = imdb_df_test
# imdb_gen_df.drop(columns=imdb_gen_df2.columns, inplace=True)
# imdb_gen_df = pd.concat([imdb_gen_df, imdb_gen_df2], axis=1)

quantile_resolution = 1

quantile_df = jigsaw_df_test.copy()
quantile_df = quantile_df.sort_values(by=["prompt_score"])
quantile_df = quantile_df.reset_index(drop=True)

# Stuff to plot
gpt2_quantiles = []
rlhf_quantiles = []
rlhf_risk_quantiles = []
sft_quantiles = []

for quantile_point in range(1, 101, quantile_resolution):
    # select first quantile_point% of the data
    relevant_df = quantile_df.iloc[: int((quantile_point / 100) * quantile_df.shape[0])]

    gpt2_quantiles.append(relevant_df["R_gpt2"].mean())
    rlhf_quantiles.append(relevant_df["R"].mean())
    rlhf_risk_quantiles.append(relevant_df["R_risk"].mean())
    sft_quantiles.append(relevant_df["R_sft"].mean())

axs[4].plot(range(1, 101, quantile_resolution), gpt2_quantiles, label="GPT2", color="black")
axs[4].plot(range(1, 101, quantile_resolution), sft_quantiles, label="SFT", color="blue")
axs[4].plot(range(1, 101, quantile_resolution), rlhf_quantiles, label="RLHF", color="red")
axs[4].plot(
    range(1, 101, quantile_resolution),
    rlhf_risk_quantiles,
    label="RA-RLHF",
    color="green",
)


axs[4].set_xlabel("Quantile (%)", fontsize=30)  # Set label size on the axis object
axs[4].set_ylabel(f"Average Reward", fontsize=30)
# axs[4].set_yscale("log")
axs[4].tick_params(axis="both", which="major", labelsize=28)
axs[4].grid(True)  # Add grid on the axis object
axs[4].legend(loc="lower right", fontsize=16)
axs[4].set_title(f"Reward vs Quantile", fontsize=30)


# Zoomed in bit for the quantile plot
axs_zoom = axs[4].inset_axes([0.65, 0.42, 0.30, 0.30])
axs_zoom.plot(range(1, 101, quantile_resolution), gpt2_quantiles, label="GPT2", color="black")
axs_zoom.plot(range(1, 101, quantile_resolution), sft_quantiles, label="SFT", color="blue")
axs_zoom.plot(range(1, 101, quantile_resolution), rlhf_quantiles, label="RLHF", color="red")
axs_zoom.plot(range(1, 101, quantile_resolution), rlhf_risk_quantiles, label="RA-RLHF", color="green")
# increase tick font size
axs_zoom.tick_params(axis="both", which="major", labelsize=18)
# set border color to orange
axs_zoom.spines["bottom"].set_color(zoom_border_color)
axs_zoom.spines["top"].set_color(zoom_border_color)
axs_zoom.spines["left"].set_color(zoom_border_color)
axs_zoom.spines["right"].set_color(zoom_border_color)
# set border width to zoom_thickness
zoom_thickness = 3.0
axs_zoom.spines["bottom"].set_linewidth(zoom_thickness)
axs_zoom.spines["top"].set_linewidth(zoom_thickness)
axs_zoom.spines["left"].set_linewidth(zoom_thickness)
axs_zoom.spines["right"].set_linewidth(zoom_thickness)


# rectangle coords in zoomed in plot
x1, x2, y1, y2 = 0.3, 4.0, -5.2, -4.3
axs_zoom.set_xlim(x1, x2)
axs_zoom.set_ylim(y1, y2)
rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, facecolor="none", edgecolor=zoom_border_color, lw=zoom_thickness)
axs[4].add_patch(rect)

fig.tight_layout()
plt.savefig("./test_plots_realtoxicity_2x/jigsaw_distribution_shifts.png")
plt.savefig("./test_plots_realtoxicity_2x/jigsaw_distribution_shifts.pdf")
plt.clf()


print("\n\n")
print("Average reward for RLHF: ", jigsaw_df_test["R"].mean())
print("Average reward for RA-RLHF: ", jigsaw_df_test["R_risk"].mean())
print("Average reward for SFT: ", jigsaw_df_test["R_sft"].mean())
print("Average reward for GPT2:", jigsaw_df_test["R_gpt2"].mean())
# print("Average Perplexity for RLHF: ", jigsaw_df_test["perplexity"].mean())
# print("Average Perplexity for RA-RLHF: ", jigsaw_df_test["perplexity_risk"].mean())
# print("Average Perplexity for SFT: ", jigsaw_df_test["perplexity_sft"].mean())
# print("Average Perplexity for GPT2: ", jigsaw_df_test["perplexity_gpt2"].mean())
print("\n\n")
filtered_df = jigsaw_df_test[jigsaw_df_test["prompt_score"] < 5]
print("Tail Average reward for RLHF: ", filtered_df["R"].mean())
print("Tail Average reward for RA-RLHF: ", filtered_df["R_risk"].mean())
print("Tail Average reward for SFT: ", filtered_df["R_sft"].mean())
print("Tail Average reward for GPT2: ", filtered_df["R_gpt2"].mean())
# print("Tail Average Perplexity for RLHF: ", filtered_df["perplexity"].mean())
# print("Tail Average Perplexity for RA-RLHF: ", filtered_df["perplexity_risk"].mean())
# print("Tail Average Perplexity for SFT: ", filtered_df["perplexity_sft"].mean())
# print("Tail Average Perplexity for GPT2: ", filtered_df["perplexity_gpt2"].mean())


# Quantile plots of returns for jigsaw

rlhf_returns_quantiles = []

rlhf_risk_returns_quantiles = []

for quantile_point in range(1, 101, quantile_resolution):
    # select first quantile_point% of the data
    relevant_df = quantile_df.iloc[: int((quantile_point / 100) * quantile_df.shape[0])]

    rlhf_returns_quantiles.append(relevant_df["R_bar"].mean())
    rlhf_risk_returns_quantiles.append(relevant_df["R_bar_risk"].mean())


fig, axs = plt.subplots(figsize=(10, 5))
word_map = {"R_bar": "RLHF returns", "R_bar_risk": "RA-RLHF returns"}

axs.plot(range(1, 101, quantile_resolution), rlhf_returns_quantiles, label="RLHF", color="red")
axs.plot(range(1, 101, quantile_resolution), rlhf_risk_returns_quantiles, label="RA-RLHF", color="green")

axs.set_xlabel("Quantile (%)", fontsize=30)  # Set label size on the axis object
axs.set_ylabel(f"Returns", fontsize=30)
axs.tick_params(axis="both", which="major", labelsize=14)
axs.grid(True)  # Add grid on the axis object
axs.legend(loc="upper left", fontsize=16)
axs.set_title(f"Quantile plot of returns", fontsize=30)


fig.tight_layout()
plt.savefig("./test_plots_realtoxicity_2x/jigsaw_R_bar_quantiles.png")
plt.savefig("./test_plots_realtoxicity_2x/jigsaw_R_bar_quantiles.pdf")
plt.clf()

File Path: examples/RealToxicity/plotting/test_plotting_old.py
Content:
import matplotlib.pyplot as plt
import plotly.express as px
import pandas as pd
import numpy as np

# # IMDB
# jigsaw_df_test = pd.read_csv("imdb_generations.csv")

# # Rename as needed in plots
# # jigsaw_df_test = jigsaw_df_test.rename(columns={"R": "RLHF", "R_risk": "RLHF risk averse"})
# # jigsaw_df_test = jigsaw_df_test.rename(columns={"R_bar": "RLHF returns", "R_bar_risk": "RLHF risk averse returns"})


# # # CDF plots
# # fig = px.ecdf(
# #     jigsaw_df_test,
# #     x=["RLHF", "RLHF risk averse"],
# #     labels={
# #         "value": "Reward",
# #         "variable": "Model",
# #     },
# # )
# # fig.update_layout(
# #     title="Reward distribution for RLHF and RLHF risk averse models",
# #     xaxis_title="Reward",
# #     yaxis_title="CDF",
# # )
# # print("Average reward for RLHF: ", jigsaw_df_test["RLHF"].mean())
# # print("Average reward for RLHF risk averse: ", jigsaw_df_test["RLHF risk averse"].mean())
# # fig.write_image("imdb_R_cdf.png")

# # fig = px.ecdf(
# #     jigsaw_df_test,
# #     x=["RLHF returns", "RLHF risk averse returns"],
# #     labels={
# #         "value": "Reward",
# #         "variable": "Model",
# #     },
# # )
# # fig.update_layout(
# #     title="Returns distribution for RLHF and RLHF risk averse models",
# #     xaxis_title="Returns",
# #     yaxis_title="CDF",
# # )
# # print("Average return for RLHF: ", jigsaw_df_test["RLHF returns"].mean())
# # print("Average return for RLHF risk averse: ", jigsaw_df_test["RLHF risk averse returns"].mean())
# # fig.write_image("imdb_R_bar_cdf.png")


# # Histogram shift plots for imdb
# fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(10, 5))
# word_map = {"prompt_score": "Prompt", "R": "RLHF", "R_risk": "RA-RLHF"}
# for i, col in enumerate(["prompt_score", "R", "R_risk"]):
#     jigsaw_df_test[jigsaw_df_test["label"] == 0][col].plot.hist(ax=axs[i], bins=30, alpha=0.5, label="Negative")
#     jigsaw_df_test[jigsaw_df_test["label"] == 1][col].plot.hist(ax=axs[i], bins=30, alpha=0.5, label="Positive")

#     axs[i].set_title(f"{word_map[col]}")
#     axs[i].set_xlabel("Reward", fontsize=16)  # Set label size on the axis object
#     axs[i].set_ylabel(f"Count", fontsize=16)
#     axs[i].tick_params(axis="both", which="major", labelsize=14)
#     axs[i].grid(True)  # Add grid on the axis object
#     axs[i].legend(loc="upper left")  # Add legend on the axis objec

# fig.tight_layout()
# plt.savefig("./test_plots_imdb/imdb_distribution_shifts.png")
# plt.savefig("./test_plots_imdb/imdb_distribution_shifts.pdf")
# plt.clf()

# # Quantile plots of returns for imdb

# fig, axs = plt.subplots(figsize=(10, 5))

# word_map = {"R_bar": "RLHF returns", "R_bar_risk": "RA-RLHF returns"}

# for i, col in enumerate(["R_bar", "R_bar_risk"]):
#     x = np.linspace(0, 1, len(jigsaw_df_test[col])) * 100
#     y = jigsaw_df_test[col].sort_values().values
#     axs.plot(x, y, label=word_map[col])

# axs.set_xlabel("Quantile (%)", fontsize=16)  # Set label size on the axis object
# axs.set_ylabel(f"Returns", fontsize=16)
# axs.tick_params(axis="both", which="major", labelsize=14)
# axs.grid(True)  # Add grid on the axis object
# axs.legend(loc="upper left")
# axs.set_title(f"Quantile plot of returns")


# fig.tight_layout()
# plt.savefig("./test_plots_imdb/imdb_R_bar_quantiles.png")
# plt.savefig("./test_plots_imdb/imdb_R_bar_quantiles.pdf")
# plt.clf()


# # Quantile plots of rewards for imdb

# fig, axs = plt.subplots(figsize=(10, 5))

# word_map = {"R": "RLHF", "R_risk": "RA-RLHF"}

# for i, col in enumerate(["R", "R_risk"]):
#     x = np.linspace(0, 1, len(jigsaw_df_test[col])) * 100
#     y = jigsaw_df_test[col].sort_values().values
#     axs.plot(x, y, label=word_map[col])

# axs.set_xlabel("Quantile (%)", fontsize=16)  # Set label size on the axis object
# axs.set_ylabel(f"Rewards", fontsize=16)
# axs.tick_params(axis="both", which="major", labelsize=14)
# axs.grid(True)  # Add grid on the axis object
# axs.legend(loc="upper left")
# axs.set_title(f"Quantile plot of rewards")


# fig.tight_layout()
# plt.savefig("./test_plots_imdb/imdb_R_quantiles.png")
# plt.savefig("./test_plots_imdb/imdb_R_quantiles.pdf")
# plt.clf()

# # Box plots of returns for imdb

# fig, axs = plt.subplots(figsize=(10, 5))


# # Jigsaw
jigsaw_df_test = pd.read_csv("../dataset/jigsaw_generations_8_32_alpha_40_seed_2.csv")

# Rename as needed in plots
# jigsaw_df_test = jigsaw_df_test.rename(columns={"R": "RLHF", "R_risk": "RLHF risk averse"})
# jigsaw_df_test = jigsaw_df_test.rename(columns={"R_bar": "RLHF returns", "R_bar_risk": "RLHF risk averse returns"})

# # CDF plots
# fig = px.ecdf(
#     jigsaw_df_test,
#     x=["RLHF", "RLHF risk averse"],
#     labels={
#         "value": "Reward",
#         "variable": "Model",
#     },
# )
# fig.update_layout(
#     title="Reward distribution for RLHF and RLHF risk averse models",
#     xaxis_title="Reward",
#     yaxis_title="CDF",
# )
# print("Average reward for RLHF: ", jigsaw_df_test["RLHF"].mean())
# print("Average reward for RLHF risk averse: ", jigsaw_df_test["RLHF risk averse"].mean())
# fig.write_image("jigsaw_R_cdf.png")

# fig = px.ecdf(
#     jigsaw_df_test,
#     x=["RLHF returns", "RLHF risk averse returns"],
#     labels={
#         "value": "Reward",
#         "variable": "Model",
#     },
# )
# fig.update_layout(
#     title="Returns distribution for RLHF and RLHF risk averse models",
#     xaxis_title="Returns",
#     yaxis_title="CDF",
# )
# print("Average return for RLHF: ", jigsaw_df_test["RLHF returns"].mean())
# print("Average return for RLHF risk averse: ", jigsaw_df_test["RLHF risk averse returns"].mean())
# fig.write_image("jigsaw_R_bar_cdf.png")

tail_to_save = -3

tail_df = jigsaw_df_test[jigsaw_df_test["prompt_score"] <= tail_to_save]

tail_df = tail_df.drop(columns=["input_ids"])

print(tail_df["R"].mean())

print(tail_df["R_risk"].mean())


# Plot histogram shifts for jigsaw
def plot_hist_top_edges(data, ax, bins, color, label):
    hist, bin_edges = np.histogram(data, bins=bins)

    for left, right, height in zip(bin_edges[:-1], bin_edges[1:], hist):
        ax.plot([left, right], [height, height], color=color, label=label)

        label = None  # To avoid duplicate labels in the legend


fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(30, 6))

word_map = {"prompt_score": "Prompt Score", "R": "RLHF", "R_risk": "RA-RLHF"}

for i, col in enumerate(["prompt_score", "R", "R_risk"]):
    jigsaw_df_test[jigsaw_df_test["toxic"] == 1][col].plot.hist(ax=axs[i], bins=30, alpha=0.5, label="Toxic")
    jigsaw_df_test[jigsaw_df_test["toxic"] == 0][col].plot.hist(ax=axs[i], bins=30, alpha=0.5, label="Non Toxic")

    axs[i].set_title(f"{word_map[col]}", fontsize=30)

    axs[i].set_xlabel("Reward", fontsize=30)  # Set label size on the axis object

    axs[i].set_ylabel("")

    axs[i].tick_params(axis="both", which="major", labelsize=28)

    axs[i].grid(True)  # Add grid on the axis object

    axs[i].legend(loc="upper left", fontsize=24)  # Add legend on the axis objec


# Find the maximum y-value among the first three histograms

max_count = max(axs[0].get_ylim()[1], axs[1].get_ylim()[1], axs[2].get_ylim()[1])


# Set the y-axis limits for the first three subplots to be the same

axs[0].set_ylim(0, max_count)

axs[1].set_ylim(0, max_count)

axs[2].set_ylim(0, max_count)

axs[0].set_ylabel(f"Count", fontsize=30)


# tail prompts

tail_filter_level = tail_to_save

filtered_df = jigsaw_df_test[jigsaw_df_test["prompt_score"] <= tail_filter_level]

print(filtered_df.shape[0])

filtered_df[filtered_df["toxic"] == 1]["prompt_score"].plot.hist(ax=axs[3], bins=30, alpha=0.5, label="Toxic")
filtered_df[filtered_df["toxic"] == 0]["prompt_score"].plot.hist(ax=axs[3], bins=30, alpha=0.5, label="Non Toxic")


axs[3].set_title("Tail Prompts", fontsize=30)

axs[3].set_xlabel("Reward", fontsize=30)  # Set label size on the axis object

axs[3].set_ylabel("")

axs[3].tick_params(axis="both", which="major", labelsize=28)

axs[3].grid(True)  # Add grid on the axis object

axs[3].legend(loc="upper left", fontsize=24)  # Add legend on the axis objec

axs[3].axvline(x=tail_filter_level, color="red", linestyle="-", linewidth=4)


# tail performance

filtered_df["R"].plot.hist(ax=axs[4], bins=30, alpha=0.3, label="RLHF", color="red")

filtered_df["R_risk"].plot.hist(ax=axs[4], bins=30, alpha=0.5, label="RA-RLHF", color="olive")

axs[4].set_title("Tail Performance", fontsize=30)

axs[4].set_xlabel("Reward", fontsize=30)  # Set label size on the axis object

axs[4].set_ylabel("")

axs[4].tick_params(axis="both", which="major", labelsize=28)

axs[4].grid(True)  # Add grid on the axis object

axs[4].legend(loc="upper right", fontsize=24)  # Add legend on the axis objec

axs[4].axvline(x=tail_filter_level, color="red", linestyle="-", linewidth=4)


# Find the maximum y-value among the first three histograms

max_count_2 = max(axs[3].get_ylim()[1], axs[4].get_ylim()[1])


# Set the y-axis limits for the first three subplots to be the same

axs[3].set_ylim(0, max_count_2)

axs[4].set_ylim(0, max_count_2)

# axs[3].set_xlim(-7.5, -5.9)
# axs[4].set_xlim(-8, 0)


fig.tight_layout()

plt.savefig("./test_plots_jigsaw/jigsaw_distribution_shifts.png")

plt.savefig("./test_plots_jigsaw/jigsaw_distribution_shifts.pdf")

plt.clf()


print("\n\n")
print("Average reward for RLHF: ", jigsaw_df_test["R"].mean())
print("Average reward for RA-RLHF: ", jigsaw_df_test["R_risk"].mean())
print("Average reward for SFT: ", jigsaw_df_test["R_sft"].mean())
print("Average reward for GPT2:", jigsaw_df_test["R_gpt2"].mean())
print("Average Perplexity for RLHF: ", jigsaw_df_test["perplexity"].mean())
print("Average Perplexity for RA-RLHF: ", jigsaw_df_test["perplexity_risk"].mean())
print("Average Perplexity for SFT: ", jigsaw_df_test["perplexity_sft"].mean())
print("Average Perplexity for GPT2: ", jigsaw_df_test["perplexity_gpt2"].mean())
print("\n\n")
print("Tail Average reward for RLHF: ", filtered_df["R"].mean())
print("Tail Average reward for RA-RLHF: ", filtered_df["R_risk"].mean())
print("Tail Average reward for SFT: ", filtered_df["R_sft"].mean())
print("Tail Average reward for GPT2: ", filtered_df["R_gpt2"].mean())
print("Tail Average Perplexity for RLHF: ", filtered_df["perplexity"].mean())
print("Tail Average Perplexity for RA-RLHF: ", filtered_df["perplexity_risk"].mean())
print("Tail Average Perplexity for SFT: ", filtered_df["perplexity_sft"].mean())
print("Tail Average Perplexity for GPT2: ", filtered_df["perplexity_gpt2"].mean())


# Quantile plots of returns for jigsaw

fig, axs = plt.subplots(figsize=(10, 5))
word_map = {"R_bar": "RLHF returns", "R_bar_risk": "RA-RLHF returns"}

for i, col in enumerate(["R_bar", "R_bar_risk"]):
    x = np.linspace(0, 1, len(jigsaw_df_test[col])) * 100
    y = jigsaw_df_test[col].sort_values().values
    axs.plot(x, y, label=word_map[col])

axs.set_xlabel("Quantile (%)", fontsize=16)  # Set label size on the axis object
axs.set_ylabel(f"Returns", fontsize=16)
axs.tick_params(axis="both", which="major", labelsize=14)
axs.grid(True)  # Add grid on the axis object
axs.legend(loc="upper left")
axs.set_title(f"Quantile plot of returns")


fig.tight_layout()
plt.savefig("./test_plots_jigsaw/jigsaw_R_bar_quantiles.png")
plt.savefig("./test_plots_jigsaw/jigsaw_R_bar_quantiles.pdf")
plt.clf()

# Quantile plots of rewards for jigsaw

fig, axs = plt.subplots(figsize=(10, 5))
word_map = {"R": "RLHF", "R_risk": "RA-RLHF"}
for i, col in enumerate(["R", "R_risk"]):
    x = np.linspace(0, 1, len(jigsaw_df_test[col])) * 100
    y = jigsaw_df_test[col].sort_values().values
    axs.plot(x, y, label=word_map[col])

axs.set_xlabel("Quantile (%)", fontsize=16)  # Set label size on the axis object
axs.set_ylabel(f"Rewards", fontsize=16)
axs.tick_params(axis="both", which="major", labelsize=14)
axs.grid(True)  # Add grid on the axis object
axs.legend(loc="upper left")
axs.set_title(f"Quantile plot of rewards")


fig.tight_layout()
plt.savefig("./test_plots_jigsaw/jigsaw_R_quantiles.png")
plt.savefig("./test_plots_jigsaw/jigsaw_R_quantiles.pdf")
plt.clf()

File Path: examples/RealToxicity/training/appendix/imdb_analysis.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, set_seed
from trl.trainer.ppo_trainer_original import PPOTrainer
from trl.core import LengthSampler


tqdm.pandas()
import matplotlib.pyplot as plt
import itertools

import pdb
import pickle
import datetime
import os
from evaluate import load
from transformers import AutoModel, AutoTokenizer, AutoConfig
from transformers import OpenAIGPTTokenizer, OpenAIGPTModel
import random

import pandas as pd
import pdb

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
print(date_n_time)
logging_dir = f"/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy_old/{date_n_time}"
os.makedirs(logging_dir)


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name="lvwerra/gpt2-imdb",
            query_dataset="imdb",
            reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            remove_unused_columns=False,
        )
    )
    query_dataset: str = field(default="imdb", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="less", metadata={"help": "the dataset to query"})


args = tyro.cli(ScriptArguments)

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8, data_split="train"):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # load imdb with datasets
    # to load partial dataset, just change data split to split=data_split+'[:200]'
    ds = load_dataset(query_dataset, split=data_split)
    ds = ds.rename_columns({"text": "review"})
    ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)

    input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# get test dataset
test_dataset = build_dataset(args.ppo_config, args.query_dataset, data_split="train")
# Take 5k random samples
# random.seed(10)
# sample_size = 2500
# random_indices = random.sample(range(len(test_dataset)), sample_size)
# test_dataset = test_dataset.select(random_indices)

# pdb.set_trace()


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

zero_shot_model = trl_model_class.from_pretrained(
    "gpt2",
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)

sft_model = trl_model_class.from_pretrained(
    "lvwerra/gpt2-imdb",
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)

# For the following to work I should have saved the model using huggingface
# ppo_model = trl_model_class.from_pretrained(
#     '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-09-26_11-27-23/model.pt',
#     trust_remote_code=True,
#     device_map=device_map,
#     peft_config=peft_config,
# )

# sft_ppo_model = trl_model_class.from_pretrained(
#     "/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/trl_sft_ppo/2023-10-23_23-30-47",
#     trust_remote_code=True,
#     device_map=device_map,
#     peft_config=peft_config,
# )
"""
ppo_model_PATH = '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-09-26_11-27-23/model.pt'
ppo_model = torch.load(ppo_model_PATH)  

sft_ppo_model_PATH = '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_ppo/2023-09-26_10-36-41/model.pt'
sft_ppo_model = torch.load(sft_ppo_model_PATH)
"""
tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"
tokenizer.truncation_side = "left"
tokenizer.pad_token_as_eos_token = True
tokenizer.max_length = 64

# Define all the tester classes
zero_shot_tester = PPOTrainer(
    args.ppo_config, zero_shot_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator
)
sft_tester = PPOTrainer(args.ppo_config, sft_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)
# ppo_tester = PPOTrainer(args.ppo_config, ppo_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)
# sft_ppo_tester = PPOTrainer(
#     args.ppo_config, sft_ppo_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator
# )

# pdb.set_trace()
# ppo_tester._save_pretrained('/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-09-26_11-27-23')
# sft_ppo_tester._save_pretrained('/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_ppo/2023-09-26_10-36-41')

# Build the perplexity evaluation pipeline
perplexity = load("perplexity", module_type="measurement")

# DO I NEED TO TAKE CARE OF PAD TOKEN FOR PERPLEXITY PIPELINE AS WELL??
"""
# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id
"""
device = sft_tester.accelerator.device
if sft_tester.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = sft_tester.accelerator.state.deepspeed_plugin
task, model_name = args.ppo_config.reward_model.split(":")
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id


# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs = {
    "min_length": 48,
    "top_k": 0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": 48,
}


# date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
# os.makedirs(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/evaluation/{date_n_time}')

model_names_all = ["zero_shot_model", "sft_model", "ppo_model", "sft_ppo_model"]
testers_all = [
    zero_shot_tester,
    sft_tester,
]  # sft_ppo_tester]  # ppo_tester,

# infer on test dataset
# test_scores_1, test_scores_2, test_scores_3, test_scores_4 = [], [], [], []
positive_score_all = []
negative_score_all = []
label_all = []
critical_pos_texts = []
critical_neg_texts = []
critical_neg_scores = []
critical_pos_scores = []
total_critical_texts = []
total_critical_scores = []
total_critical_labels = []

for epoch, batch in tqdm(enumerate(zero_shot_tester.dataloader)):
    query_tensors = batch["input_ids"]

    texts = batch["review"]
    texts = [text[:200] for text in texts]
    # texts = ["lorem ipsum", "Happy Birthday!", "Bienvenue"]

    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
    pos_score = [output[1]["score"] for output in pipe_outputs]
    positive_score_all.append(pos_score)

    neg_score = [output[0]["score"] for output in pipe_outputs]
    negative_score_all.append(neg_score)

    labels = [label.item() for label in batch["label"]]
    label_all.append(labels)

    for ii in range(len(label_all[0])):
        if labels[ii] == 0 and pos_score[ii] < -2.8:
            critical_neg_texts.append(texts[ii])
            critical_neg_scores.append(pos_score[ii])
            total_critical_texts.append(texts[ii])
            total_critical_scores.append(pos_score[ii])
            total_critical_labels.append(labels[ii])
        if labels[ii] == 1 and pos_score[ii] < -2.8:
            critical_pos_texts.append(texts[ii])
            critical_pos_scores.append(pos_score[ii])
            total_critical_texts.append(texts[ii])
            total_critical_scores.append(pos_score[ii])
            total_critical_labels.append(labels[ii])

print(f"Total critical reviews: {len(total_critical_labels)}")
# get flattened lists
pos_scores = list(itertools.chain(*positive_score_all))
neg_scores = list(itertools.chain(*negative_score_all))
labels = list(itertools.chain(*label_all))


# Plot histograms
df = pd.DataFrame(list(zip(neg_scores, pos_scores, labels)), columns=["neg score", "pos score", "label"])
fig2, axs2 = plt.subplots(figsize=(10, 5))

df[df["label"] == 1]["pos score"].plot.hist(ax=axs2, bins=30, alpha=0.5, label="Positive")
df[df["label"] == 0]["pos score"].plot.hist(ax=axs2, bins=30, alpha=0.5, label="Negative")
axs2.legend()
axs2.set_ylabel(r"No. of input prompts", fontsize=16)  # Setting label size
axs2.set_xlabel(r"Enviroment Score", fontsize=16)
axs2.tick_params(axis="both", which="major", labelsize=14)  # Adjusting tick size
axs2.set_box_aspect(0.7)
axs2.grid(True)
fig2.savefig("histogram_plot.pdf", bbox_inches="tight")

# save critical reviews as csv
df1 = pd.DataFrame({"score": critical_neg_scores, "review": critical_neg_texts})
df2 = pd.DataFrame({"score": critical_pos_scores, "review": critical_pos_texts})
df3 = pd.DataFrame({"review": total_critical_texts})

df1 = df1.sort_values("score")
df2 = df2.sort_values("score")

df1.to_csv("critical_neg_texts.csv")
df2.to_csv("critical_pos_texts.csv")
df3.to_csv("total_critical_texts.csv")


# histogram plot for scores of critical reviews
df4 = pd.DataFrame(list(zip(total_critical_scores, total_critical_labels)), columns=["pos score", "label"])
fig3, axs3 = plt.subplots(figsize=(10, 5))

df4[df4["label"] == 1]["pos score"].plot.hist(ax=axs3, bins=30, alpha=0.5, label="Positive")
df4[df4["label"] == 0]["pos score"].plot.hist(ax=axs3, bins=30, alpha=0.5, label="Negative")
axs3.legend()
axs3.set_ylabel(r"No. of input prompts", fontsize=16)  # Setting label size
axs3.set_xlabel(r"Enviroment Score", fontsize=16)
axs3.tick_params(axis="both", which="major", labelsize=14)  # Adjusting tick size
axs3.set_box_aspect(0.7)
axs3.grid(True)

fig3.savefig("critical_context_scores.pdf", bbox_inches="tight")

File Path: examples/RealToxicity/training/appendix/jigsaw_analysis.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset, Dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, set_seed
from trl.trainer.ppo_trainer_original import PPOTrainer
from trl.core import LengthSampler


tqdm.pandas()
import matplotlib.pyplot as plt
import itertools

import pdb
import pickle
import datetime
import os
from evaluate import load
from transformers import AutoModel, AutoTokenizer, AutoConfig
from transformers import OpenAIGPTTokenizer, OpenAIGPTModel
import random

import pandas as pd
import pdb

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")

logging_dir = f"/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy_old/{date_n_time}"
os.makedirs(logging_dir)


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name="lvwerra/gpt2-imdb",
            query_dataset="jigsaw_toxicity_pred",
            reward_model="unitary/toxic-bert",
            # reward_model="citizenlab/twitter-xlm-roberta-base-sentiment-finetunned",
            learning_rate=1.41e-5,
            # log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            steps=128000,
            exp_name="ppo",
            remove_unused_columns=False,
        )
    )
    query_dataset: str = field(default="jigsaw_toxicity_pred", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="more", metadata={"help": "the dataset to query"})

    generation_kwargs_min_length: Optional[int] = field(
        default=-1, metadata={"help": "minimum number of tokens while generation"}
    )
    generation_kwargs_top_k: Optional[int] = field(default=0, metadata={"help": "gneration top k"})
    generation_kwargs_top_p: Optional[float] = field(default=1.0, metadata={"help": "gneration top p"})
    generation_kwargs_max_new_tokens: Optional[int] = field(default=32, metadata={"help": "gneration top p"})

    change_tokenizer_args: bool = field(default=False, metadata={"help": "whether to use modify tokenizer settings"})
    tokenizer_kwargs: dict = field(
        default_factory=lambda: {
            "padding_side": "left",
            "truncation_side": "left",
            "pad_token_as_eos_token": True,
            "max_length": 64,
        }
    )


args = tyro.cli(ScriptArguments)

generation_kwargs = {
    "min_length": args.generation_kwargs_min_length,
    "top_k": args.generation_kwargs_top_k,
    "top_p": args.generation_kwargs_top_p,
    "do_sample": True,
    "max_new_tokens": args.generation_kwargs_max_new_tokens,
}

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # Kaggle datasets have to be downloaded manually (using Kaggle API)
    # datasets.load_dataset("jigsaw_toxicity_pred", data_dir="<path/to/manual/data>")
    # ds = load_dataset(
    #     query_dataset,
    #     data_dir="/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/original_dataset",
    #     split="train",
    # )  # Hardcoding path for now TODO: change this

    # ds = ds.rename_columns({"comment_text": "review"})
    # ds = ds.filter(lambda x: len(x["review"]) > 64, batched=False)

    # df = pd.DataFrame(ds)
    # num_toxic = df["toxic"].sum()

    # toxic_df = df[df["toxic"] == True]
    # non_toxic_df = df[df["toxic"] == False]

    # non_toxic_df = non_toxic_df.sample(n=num_toxic, random_state=config.seed)

    # # Recombine into dataset
    # ds = ds.from_pandas(pd.concat([toxic_df, non_toxic_df], ignore_index=True))
    df = pd.read_csv(
        "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw_2/dataset/jigsaw_train_70_30.csv",
    )
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"comment_text": "review"})

    # pdb.set_trace()

    if args.input_query_size == "more":

        def input_size():
            return 20

    else:
        input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    args.ppo_config.model_name,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id

# pdb.set_trace()

if args.change_tokenizer_args == True:
    tokenizer.padding_side = args.tokenizer_kwargs["padding_side"]
    tokenizer.truncation_side = args.tokenizer_kwargs["truncation_side"]
    tokenizer.pad_token_as_eos_token = args.tokenizer_kwargs["pad_token_as_eos_token"]
    tokenizer.max_length = args.tokenizer_kwargs["max_length"]

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
# task, model_name = args.ppo_config.reward_model.split(":")
task = "text-classification"
model_name = args.ppo_config.reward_model
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs["pad_token_id"] = tokenizer.eos_token_id

# empty list to store text data
text_train_data = []

# infer on test dataset
# test_scores_1, test_scores_2, test_scores_3, test_scores_4 = [], [], [], []
positive_score_all = []
negative_score_all = []
label_all = []
critical_pos_texts = []
critical_neg_texts = []
critical_neg_scores = []
critical_pos_scores = []
total_critical_texts = []
total_critical_scores = []
total_critical_labels = []

for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    texts = batch["query"]
    texts = [text[:200] for text in texts]

    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
    pos_score = [-output[0]["score"] for output in pipe_outputs]
    positive_score_all.append(pos_score)
    # rewards = [torch.tensor(-output[0]["score"]) for output in pipe_outputs]

    neg_score = [-output[1]["score"] for output in pipe_outputs]
    negative_score_all.append(neg_score)

    labels = [label.item() for label in batch["toxic"]]
    label_all.append(labels)

    for ii in range(len(label_all[0])):
        if labels[ii] == 0 and pos_score[ii] < -5.0:
            critical_neg_texts.append(texts[ii])
            critical_neg_scores.append(pos_score[ii])
            total_critical_texts.append(texts[ii])
            total_critical_scores.append(pos_score[ii])
            total_critical_labels.append(labels[ii])
        if labels[ii] == 1 and pos_score[ii] < -5.0:
            critical_pos_texts.append(texts[ii])
            critical_pos_scores.append(pos_score[ii])
            total_critical_texts.append(texts[ii])
            total_critical_scores.append(pos_score[ii])
            total_critical_labels.append(labels[ii])

print(f"Total critical reviews: {len(total_critical_labels)}")
# get flattened lists
pos_scores = list(itertools.chain(*positive_score_all))
neg_scores = list(itertools.chain(*negative_score_all))
labels = list(itertools.chain(*label_all))


# Plot histograms
df = pd.DataFrame(list(zip(neg_scores, pos_scores, labels)), columns=["neg score", "pos score", "label"])
fig2, axs2 = plt.subplots(figsize=(10, 5))

df[df["label"] == 0]["pos score"].plot.hist(ax=axs2, bins=30, alpha=0.5, label="Non-toxic")
df[df["label"] == 1]["pos score"].plot.hist(ax=axs2, bins=30, alpha=0.5, label="Toxic")
axs2.legend()
axs2.set_ylabel(r"No. of input prompts", fontsize=16)  # Setting label size
axs2.set_xlabel(r"Enviroment Score", fontsize=16)
axs2.tick_params(axis="both", which="major", labelsize=14)  # Adjusting tick size
axs2.set_box_aspect(0.7)
axs2.grid(True)
fig2.savefig("histogram_plot.pdf", bbox_inches="tight")

# save critical reviews as csv
df1 = pd.DataFrame({"score": critical_neg_scores, "review": critical_neg_texts})
df2 = pd.DataFrame({"score": critical_pos_scores, "review": critical_pos_texts})
df3 = pd.DataFrame({"review": total_critical_texts})

df1 = df1.sort_values("score")
df2 = df2.sort_values("score")

df1.to_csv("critical_neg_texts.csv")
df2.to_csv("critical_pos_texts.csv")
df3.to_csv("total_critical_texts.csv")


# histogram plot for scores of critical reviews
df4 = pd.DataFrame(list(zip(total_critical_scores, total_critical_labels)), columns=["pos score", "label"])
fig3, axs3 = plt.subplots(figsize=(10, 5))

df4[df4["label"] == 0]["pos score"].plot.hist(ax=axs3, bins=30, alpha=0.5, label="Non-toxic")
df4[df4["label"] == 1]["pos score"].plot.hist(ax=axs3, bins=30, alpha=0.5, label="Toxic")
axs3.legend()
axs3.set_ylabel(r"No. of input prompts", fontsize=16)  # Setting label size
axs3.set_xlabel(r"Enviroment Score", fontsize=16)
axs3.tick_params(axis="both", which="major", labelsize=14)  # Adjusting tick size
axs3.set_box_aspect(0.7)
axs3.grid(True)

fig3.savefig("critical_context_scores.pdf", bbox_inches="tight")

File Path: examples/RealToxicity/training/ppo_auth2.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset, Dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, set_seed
from trl.core import LengthSampler
from trl.trainer.ppo_trainer_original import PPOTrainer

tqdm.pandas()

import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import pdb
import pandas as pd

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")

logging_dir = f"/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/{date_n_time}"
os.makedirs(logging_dir)


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-20_20-32-35",  # sft_both
            model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-25_16-31-51",  # sft_positive
            query_dataset="jigsaw_toxicity_pred",
            reward_model="unitary/toxic-bert",
            # reward_model="citizenlab/twitter-xlm-roberta-base-sentiment-finetunned",
            learning_rate=1.41e-5,
            # log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            # adap_kl_ctrl=False,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            steps=128000,
            exp_name="ppo",
        )
    )
    query_dataset: str = field(default="jigsaw_toxicity_pred", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="more", metadata={"help": "the dataset to query"})

    generation_kwargs_min_length: Optional[int] = field(
        default=32, metadata={"help": "minimum number of tokens while generation"}
    )
    generation_kwargs_top_k: Optional[int] = field(default=0, metadata={"help": "gneration top k"})
    generation_kwargs_top_p: Optional[float] = field(default=1.0, metadata={"help": "gneration top p"})
    generation_kwargs_max_new_tokens: Optional[int] = field(default=32, metadata={"help": "gneration top p"})

    change_tokenizer_args: bool = field(default=False, metadata={"help": "whether to use modify tokenizer settings"})
    tokenizer_kwargs: dict = field(
        default_factory=lambda: {
            "padding_side": "left",
            "truncation_side": "left",
            "pad_token_as_eos_token": True,
            "max_length": 64,
        }
    )
    prompt_len: Optional[int] = field(default=32, metadata={"help": "prompt length"})


args = tyro.cli(ScriptArguments)


generation_kwargs = {
    "min_length": args.generation_kwargs_min_length,
    "top_k": args.generation_kwargs_top_k,
    "top_p": args.generation_kwargs_top_p,
    "do_sample": True,
    "max_new_tokens": args.generation_kwargs_max_new_tokens,
}

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # # Kaggle datasets have to be downloaded manually (using Kaggle API)
    # # datasets.load_dataset("jigsaw_toxicity_pred", data_dir="<path/to/manual/data>")
    # ds = load_dataset(
    #     query_dataset,
    #     data_dir="/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/original_dataset",
    #     split="train",
    # )  # Hardcoding path for now TODO: change this

    # ds = ds.rename_columns({"comment_text": "review"})
    # ds = ds.filter(lambda x: len(x["review"]) > 64, batched=False)

    # df = pd.DataFrame(ds)
    # num_toxic = df["toxic"].sum()

    # toxic_df = df[df["toxic"] == True]
    # non_toxic_df = df[df["toxic"] == False]

    # non_toxic_df = non_toxic_df.sample(n=num_toxic, random_state=config.seed)

    # # Recombine into dataset
    # ds = ds.from_pandas(pd.concat([toxic_df, non_toxic_df], ignore_index=True))
    df = pd.read_csv(
        "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/jigsaw_train.csv",
    )
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"comment_text": "review"})

    # pdb.set_trace()

    if args.input_query_size == "more":

        def input_size():
            return args.prompt_len

    else:
        input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    args.ppo_config.model_name,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id

# pdb.set_trace()

if args.change_tokenizer_args == True:
    tokenizer.padding_side = args.tokenizer_kwargs["padding_side"]
    tokenizer.truncation_side = args.tokenizer_kwargs["truncation_side"]
    tokenizer.pad_token_as_eos_token = args.tokenizer_kwargs["pad_token_as_eos_token"]
    tokenizer.max_length = args.tokenizer_kwargs["max_length"]

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
# task, model_name = args.ppo_config.reward_model.split(":")
task = "text-classification"
model_name = args.ppo_config.reward_model
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs["pad_token_id"] = tokenizer.eos_token_id

# empty list to store text data
text_train_data = []

for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(
        texts,
        **sent_kwargs,
        # truncation=True,
    )

    if args.reward_function == "trl":
        rewards = [torch.tensor(-output[0]["score"]) for output in pipe_outputs]
    else:  # depreciated for jigsaw task
        raise NotImplementedError
        rewards = [
            -torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"], output[2]["score"]]))[2]
            for output in pipe_outputs
        ]
    # pdb.set_trace()
    for ii in range(3):
        data_row = {
            "iter": epoch,
            "input": batch["query"][ii],
            "output": batch["response"][ii],
            "score": rewards[ii].item(),
        }
        text_train_data.append(data_row)

    # pdb.set_trace()

    # Run PPO step
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
    ppo_trainer.log_stats(stats, batch, rewards)
    print(epoch)

ppo_trainer._save_pretrained(logging_dir + "/save_pretrained")
df = pd.DataFrame(text_train_data)
df.to_csv(logging_dir + "/textual_data.csv", index=False)

File Path: examples/RealToxicity/training/ppo_auth2_seed12.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset, Dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, set_seed
from trl.core import LengthSampler
from trl.trainer.ppo_trainer_original import PPOTrainer

tqdm.pandas()

import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import pdb
import pandas as pd

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")

logging_dir = f"/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/{date_n_time}"
os.makedirs(logging_dir)


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-20_20-32-35",  # sft_both
            model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-30_00-30-38",  # sft_positive seed 12
            query_dataset="jigsaw_toxicity_pred",
            reward_model="unitary/toxic-bert",
            # reward_model="citizenlab/twitter-xlm-roberta-base-sentiment-finetunned",
            learning_rate=1.41e-5,
            # log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            # adap_kl_ctrl=False,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            steps=192000,
            exp_name="ppo",
        )
    )
    query_dataset: str = field(default="jigsaw_toxicity_pred", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="more", metadata={"help": "the dataset to query"})

    generation_kwargs_min_length: Optional[int] = field(
        default=32, metadata={"help": "minimum number of tokens while generation"}
    )
    generation_kwargs_top_k: Optional[int] = field(default=0, metadata={"help": "gneration top k"})
    generation_kwargs_top_p: Optional[float] = field(default=1.0, metadata={"help": "gneration top p"})
    generation_kwargs_max_new_tokens: Optional[int] = field(default=32, metadata={"help": "gneration top p"})

    change_tokenizer_args: bool = field(default=False, metadata={"help": "whether to use modify tokenizer settings"})
    tokenizer_kwargs: dict = field(
        default_factory=lambda: {
            "padding_side": "left",
            "truncation_side": "left",
            "pad_token_as_eos_token": True,
            "max_length": 64,
        }
    )
    prompt_len: Optional[int] = field(default=32, metadata={"help": "prompt length"})


args = tyro.cli(ScriptArguments)


generation_kwargs = {
    "min_length": args.generation_kwargs_min_length,
    "top_k": args.generation_kwargs_top_k,
    "top_p": args.generation_kwargs_top_p,
    "do_sample": True,
    "max_new_tokens": args.generation_kwargs_max_new_tokens,
}

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # # Kaggle datasets have to be downloaded manually (using Kaggle API)
    # # datasets.load_dataset("jigsaw_toxicity_pred", data_dir="<path/to/manual/data>")
    # ds = load_dataset(
    #     query_dataset,
    #     data_dir="/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/original_dataset",
    #     split="train",
    # )  # Hardcoding path for now TODO: change this

    # ds = ds.rename_columns({"comment_text": "review"})
    # ds = ds.filter(lambda x: len(x["review"]) > 64, batched=False)

    # df = pd.DataFrame(ds)
    # num_toxic = df["toxic"].sum()

    # toxic_df = df[df["toxic"] == True]
    # non_toxic_df = df[df["toxic"] == False]

    # non_toxic_df = non_toxic_df.sample(n=num_toxic, random_state=config.seed)

    # # Recombine into dataset
    # ds = ds.from_pandas(pd.concat([toxic_df, non_toxic_df], ignore_index=True))
    df = pd.read_csv(
        "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/jigsaw_train.csv",
    )
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"comment_text": "review"})

    # pdb.set_trace()

    if args.input_query_size == "more":

        def input_size():
            return args.prompt_len

    else:
        input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    args.ppo_config.model_name,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id

# pdb.set_trace()

if args.change_tokenizer_args == True:
    tokenizer.padding_side = args.tokenizer_kwargs["padding_side"]
    tokenizer.truncation_side = args.tokenizer_kwargs["truncation_side"]
    tokenizer.pad_token_as_eos_token = args.tokenizer_kwargs["pad_token_as_eos_token"]
    tokenizer.max_length = args.tokenizer_kwargs["max_length"]

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
# task, model_name = args.ppo_config.reward_model.split(":")
task = "text-classification"
model_name = args.ppo_config.reward_model
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs["pad_token_id"] = tokenizer.eos_token_id

# empty list to store text data
text_train_data = []

for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(
        texts,
        **sent_kwargs,
        # truncation=True,
    )

    if args.reward_function == "trl":
        rewards = [torch.tensor(-output[0]["score"]) for output in pipe_outputs]
    else:  # depreciated for jigsaw task
        raise NotImplementedError
        rewards = [
            -torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"], output[2]["score"]]))[2]
            for output in pipe_outputs
        ]
    # pdb.set_trace()
    for ii in range(3):
        data_row = {
            "iter": epoch,
            "input": batch["query"][ii],
            "output": batch["response"][ii],
            "score": rewards[ii].item(),
        }
        text_train_data.append(data_row)

    # pdb.set_trace()

    # Run PPO step
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
    ppo_trainer.log_stats(stats, batch, rewards)
    print(epoch)

ppo_trainer._save_pretrained(logging_dir + "/save_pretrained")
df = pd.DataFrame(text_train_data)
df.to_csv(logging_dir + "/textual_data.csv", index=False)

File Path: examples/RealToxicity/training/ppo_auth2_seed2.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset, Dataset, load_from_disk
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, set_seed
from trl.core import LengthSampler
from trl.trainer.ppo_trainer_original import PPOTrainer

tqdm.pandas()

import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import pdb
import pandas as pd

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")

logging_dir = (
    f"/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/real_toxicity/{date_n_time}"
)
os.makedirs(logging_dir)


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-20_20-32-35",  # sft_both
            model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/real_toxicity/2024-02-22_14-11-00",  # sft_positive real_toxicity
            query_dataset="real_toxicity_train",
            reward_model="unitary/toxic-bert",
            # reward_model="citizenlab/twitter-xlm-roberta-base-sentiment-finetunned",
            learning_rate=2*1.41e-5,
            # log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            # adap_kl_ctrl=False,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            steps=192000,
            exp_name="ppo",
        )
    )
    query_dataset: str = field(default="jigsaw_toxicity_pred", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="more", metadata={"help": "the dataset to query"})

    generation_kwargs_min_length: Optional[int] = field(
        default=32, metadata={"help": "minimum number of tokens while generation"}
    )
    generation_kwargs_top_k: Optional[int] = field(default=0, metadata={"help": "gneration top k"})
    generation_kwargs_top_p: Optional[float] = field(default=1.0, metadata={"help": "gneration top p"})
    generation_kwargs_max_new_tokens: Optional[int] = field(default=32, metadata={"help": "gneration top p"})

    change_tokenizer_args: bool = field(default=False, metadata={"help": "whether to use modify tokenizer settings"})
    tokenizer_kwargs: dict = field(
        default_factory=lambda: {
            "padding_side": "left",
            "truncation_side": "left",
            "pad_token_as_eos_token": True,
            "max_length": 64,
        }
    )
    prompt_len: Optional[int] = field(default=32, metadata={"help": "prompt length"})


args = tyro.cli(ScriptArguments)


generation_kwargs = {
    "min_length": args.generation_kwargs_min_length,
    "top_k": args.generation_kwargs_top_k,
    "top_p": args.generation_kwargs_top_p,
    "do_sample": True,
    "max_new_tokens": args.generation_kwargs_max_new_tokens,
}

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token

    ds = load_from_disk("/mnt/shared-scratch/Shakkottai_S/auth2d36/datasets/RealToxicity/real_toxicity_train.hf")
    ds = ds.rename_columns({"Prompt": "review"})

    # pdb.set_trace()

    if args.input_query_size == "more":

        def input_size():
            return args.prompt_len

    else:
        input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    args.ppo_config.model_name,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id

# pdb.set_trace()

if args.change_tokenizer_args == True:
    tokenizer.padding_side = args.tokenizer_kwargs["padding_side"]
    tokenizer.truncation_side = args.tokenizer_kwargs["truncation_side"]
    tokenizer.pad_token_as_eos_token = args.tokenizer_kwargs["pad_token_as_eos_token"]
    tokenizer.max_length = args.tokenizer_kwargs["max_length"]

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
# task, model_name = args.ppo_config.reward_model.split(":")
task = "sentiment-analysis"
model_name = args.ppo_config.reward_model
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs["pad_token_id"] = tokenizer.eos_token_id

# empty list to store text data
text_train_data = []

for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(
        texts,
        **sent_kwargs,
        # truncation=True,
    )

    if args.reward_function == "trl":
        rewards = [torch.tensor(-output[0]["score"]) for output in pipe_outputs]
    else:  # depreciated for jigsaw task
        raise NotImplementedError
        rewards = [
            -torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"], output[2]["score"]]))[2]
            for output in pipe_outputs
        ]
    # pdb.set_trace()
    for ii in range(3):
        data_row = {
            "iter": epoch,
            "input": batch["query"][ii],
            "output": batch["response"][ii],
            "score": rewards[ii].item(),
        }
        text_train_data.append(data_row)

    # pdb.set_trace()

    # Run PPO step
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
    ppo_trainer.log_stats(stats, batch, rewards)
    print(epoch)

ppo_trainer._save_pretrained(logging_dir + "/save_pretrained")
df = pd.DataFrame(text_train_data)
df.to_csv(logging_dir + "/textual_data.csv", index=False)

File Path: examples/RealToxicity/training/ppo_auth2_seed36.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset, Dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, set_seed
from trl.core import LengthSampler
from trl.trainer.ppo_trainer_original import PPOTrainer

tqdm.pandas()

import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import pdb
import pandas as pd

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")

logging_dir = f"/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/{date_n_time}"
os.makedirs(logging_dir)


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-20_20-32-35",  # sft_both
            model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-30_00-24-10",  # sft_positive seed 36
            query_dataset="jigsaw_toxicity_pred",
            reward_model="unitary/toxic-bert",
            # reward_model="citizenlab/twitter-xlm-roberta-base-sentiment-finetunned",
            learning_rate=1.41e-5,
            # log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            # adap_kl_ctrl=False,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            steps=192000,
            exp_name="ppo",
        )
    )
    query_dataset: str = field(default="jigsaw_toxicity_pred", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="more", metadata={"help": "the dataset to query"})

    generation_kwargs_min_length: Optional[int] = field(
        default=32, metadata={"help": "minimum number of tokens while generation"}
    )
    generation_kwargs_top_k: Optional[int] = field(default=0, metadata={"help": "gneration top k"})
    generation_kwargs_top_p: Optional[float] = field(default=1.0, metadata={"help": "gneration top p"})
    generation_kwargs_max_new_tokens: Optional[int] = field(default=32, metadata={"help": "gneration top p"})

    change_tokenizer_args: bool = field(default=False, metadata={"help": "whether to use modify tokenizer settings"})
    tokenizer_kwargs: dict = field(
        default_factory=lambda: {
            "padding_side": "left",
            "truncation_side": "left",
            "pad_token_as_eos_token": True,
            "max_length": 64,
        }
    )
    prompt_len: Optional[int] = field(default=32, metadata={"help": "prompt length"})


args = tyro.cli(ScriptArguments)


generation_kwargs = {
    "min_length": args.generation_kwargs_min_length,
    "top_k": args.generation_kwargs_top_k,
    "top_p": args.generation_kwargs_top_p,
    "do_sample": True,
    "max_new_tokens": args.generation_kwargs_max_new_tokens,
}

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # # Kaggle datasets have to be downloaded manually (using Kaggle API)
    # # datasets.load_dataset("jigsaw_toxicity_pred", data_dir="<path/to/manual/data>")
    # ds = load_dataset(
    #     query_dataset,
    #     data_dir="/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/original_dataset",
    #     split="train",
    # )  # Hardcoding path for now TODO: change this

    # ds = ds.rename_columns({"comment_text": "review"})
    # ds = ds.filter(lambda x: len(x["review"]) > 64, batched=False)

    # df = pd.DataFrame(ds)
    # num_toxic = df["toxic"].sum()

    # toxic_df = df[df["toxic"] == True]
    # non_toxic_df = df[df["toxic"] == False]

    # non_toxic_df = non_toxic_df.sample(n=num_toxic, random_state=config.seed)

    # # Recombine into dataset
    # ds = ds.from_pandas(pd.concat([toxic_df, non_toxic_df], ignore_index=True))
    df = pd.read_csv(
        "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/jigsaw_train.csv",
    )
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"comment_text": "review"})

    # pdb.set_trace()

    if args.input_query_size == "more":

        def input_size():
            return args.prompt_len

    else:
        input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    args.ppo_config.model_name,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id

# pdb.set_trace()

if args.change_tokenizer_args == True:
    tokenizer.padding_side = args.tokenizer_kwargs["padding_side"]
    tokenizer.truncation_side = args.tokenizer_kwargs["truncation_side"]
    tokenizer.pad_token_as_eos_token = args.tokenizer_kwargs["pad_token_as_eos_token"]
    tokenizer.max_length = args.tokenizer_kwargs["max_length"]

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
# task, model_name = args.ppo_config.reward_model.split(":")
task = "text-classification"
model_name = args.ppo_config.reward_model
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs["pad_token_id"] = tokenizer.eos_token_id

# empty list to store text data
text_train_data = []

for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(
        texts,
        **sent_kwargs,
        # truncation=True,
    )

    if args.reward_function == "trl":
        rewards = [torch.tensor(-output[0]["score"]) for output in pipe_outputs]
    else:  # depreciated for jigsaw task
        raise NotImplementedError
        rewards = [
            -torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"], output[2]["score"]]))[2]
            for output in pipe_outputs
        ]
    # pdb.set_trace()
    for ii in range(3):
        data_row = {
            "iter": epoch,
            "input": batch["query"][ii],
            "output": batch["response"][ii],
            "score": rewards[ii].item(),
        }
        text_train_data.append(data_row)

    # pdb.set_trace()

    # Run PPO step
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
    ppo_trainer.log_stats(stats, batch, rewards)
    print(epoch)

ppo_trainer._save_pretrained(logging_dir + "/save_pretrained")
df = pd.DataFrame(text_train_data)
df.to_csv(logging_dir + "/textual_data.csv", index=False)

File Path: examples/RealToxicity/training/sft_auth2.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional
import pandas as pd
import torch
from accelerate import Accelerator
from datasets import load_dataset, Dataset, load_from_disk
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoModelForCausalLM, BitsAndBytesConfig, HfArgumentParser, TrainingArguments

from trl import SFTTrainer


tqdm.pandas()

import pdb
from transformers import GenerationConfig
from transformers import AutoTokenizer, pipeline, Seq2SeqTrainingArguments
import datetime
import os
from trl import set_seed

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
logging_dir = (
    f"/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/real_toxicity/{date_n_time}"
)
os.makedirs(logging_dir)


# Define and parse arguments.
@dataclass
class ScriptArguments:
    """
    The name of the Casual LM model we wish to fine-tune with SFTTrainer
    """

    model_name: Optional[str] = field(default="gpt2", metadata={"help": "the model name"})
    dataset_name: Optional[str] = field(default="jigsaw_toxicity_pred", metadata={"help": "the dataset name"})
    dataset_text_field: Optional[str] = field(
        default="Prompt", metadata={"help": "the text field of the dataset"}
    )
    log_with: Optional[str] = field(default="tensorboard", metadata={"help": "use 'wandb' to log with wandb"})

    # learning params
    learning_rate: Optional[float] = field(default=0.00001, metadata={"help": "the learning rate"})  # default=1.41e-5
    batch_size: Optional[int] = field(default=64, metadata={"help": "the batch size"})
    # Using tokenizer max length=64 from RL4LLMs Table 4
    seq_length: Optional[int] = field(default=64, metadata={"help": "Input sequence length"})  # default=512
    gradient_accumulation_steps: Optional[int] = field(
        default=16, metadata={"help": "the number of gradient accumulation steps"}
    )
    num_train_epochs: Optional[int] = field(
        default=10, metadata={"help": "the number of training epochs"}
    )  # default=3
    max_steps: Optional[int] = field(default=-1, metadata={"help": "the number of training steps"})

    # model download, quantization and peft params
    trust_remote_code: Optional[bool] = field(default=True, metadata={"help": "Enable `trust_remote_code`"})
    use_auth_token: Optional[bool] = field(default=True, metadata={"help": "Use HF auth token to access the model"})
    load_in_8bit: Optional[bool] = field(default=False, metadata={"help": "load the model in 8 bits precision"})
    load_in_4bit: Optional[bool] = field(default=False, metadata={"help": "load the model in 4 bits precision"})
    use_peft: Optional[bool] = field(default=False, metadata={"help": "Wether to use PEFT or not to train adapters"})
    peft_lora_r: Optional[int] = field(default=64, metadata={"help": "the r parameter of the LoRA adapters"})
    peft_lora_alpha: Optional[int] = field(default=16, metadata={"help": "the alpha parameter of the LoRA adapters"})

    # logging
    output_dir: Optional[str] = field(default=logging_dir, metadata={"help": "the output directory"})
    logging_steps: Optional[int] = field(default=1, metadata={"help": "the number of logging steps"})
    save_steps: Optional[int] = field(
        default=100, metadata={"help": "Number of updates steps before two checkpoint saves"}
    )
    save_total_limit: Optional[int] = field(default=10, metadata={"help": "Limits total number of checkpoints."})

    # post processing
    push_to_hub: Optional[bool] = field(default=False, metadata={"help": "Push the model to HF Hub"})
    hub_model_id: Optional[str] = field(default=None, metadata={"help": "The name of the model on HF Hub"})

    # added by auth1
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="less", metadata={"help": "the dataset to query"})
    generation_kwargs_min_length: Optional[int] = field(
        default=-1, metadata={"help": "minimum number of tokens while generation"}
    )
    generation_kwargs_top_k: Optional[int] = field(default=0, metadata={"help": "gneration top k"})
    generation_kwargs_top_p: Optional[float] = field(default=1.0, metadata={"help": "gneration top p"})
    generation_kwargs_max_new_tokens: Optional[int] = field(default=32, metadata={"help": "gneration top p"})

    change_tokenizer_args: bool = field(default=False, metadata={"help": "whether to use modify tokenizer settings"})
    tokenizer_kwargs: dict = field(
        default_factory=lambda: {
            "padding_side": "left",
            "truncation_side": "left",
            "pad_token_as_eos_token": True,
            "max_length": 64,
        }
    )
    exp_name: str = field(default="sft-positive", metadata={"help": "experiment name"})
    seed: Optional[int] = field(default=0, metadata={"help": "seed"})


parser = HfArgumentParser(ScriptArguments)
script_args = parser.parse_args_into_dataclasses()[0]

set_seed(script_args.seed)

generation_kwargs = {
    "min_length": script_args.generation_kwargs_min_length,
    "top_k": script_args.generation_kwargs_top_k,
    "top_p": script_args.generation_kwargs_top_p,
    "do_sample": True,
    "max_new_tokens": script_args.generation_kwargs_max_new_tokens,
}

# Step 1: Load the model
if script_args.load_in_8bit and script_args.load_in_4bit:
    raise ValueError("You can't load the model in 8 bits and 4 bits at the same time")
elif script_args.load_in_8bit or script_args.load_in_4bit:
    quantization_config = BitsAndBytesConfig(
        load_in_8bit=script_args.load_in_8bit, load_in_4bit=script_args.load_in_4bit
    )
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}
    torch_dtype = torch.bfloat16
else:
    device_map = None
    quantization_config = None
    torch_dtype = None

model = AutoModelForCausalLM.from_pretrained(
    script_args.model_name,
    quantization_config=quantization_config,
    device_map=device_map,
    trust_remote_code=script_args.trust_remote_code,
    torch_dtype=torch_dtype,
)

# Removing huggingface authorization token param from model loading command above
#    use_auth_token=script_args.use_auth_token,
# Make sure you have logged in to the Hugging Face Hub using huggingface-cli login
# or by calling huggingface_hub.login() in Python. This will save an authentication
# token that Transformers will pick up automatically.

# Step 2: Load the dataset
dataset = load_from_disk("/mnt/shared-scratch/Shakkottai_S/auth2d36/datasets/RealToxicity/real_toxicity_train.hf")
dataset = dataset.filter(lambda x:x["Label"]==0)  # SFT only on positive(non-toxic) dataset



# Balance the dataset

# Filter dataset for only positive reviews
# dataset = dataset.filter(lambda x: x["label"] == 1, batched=False)


# Step 3: Define generation arguments
# generation_kwargs = GenerationConfig(
#     min_length = 48,
#     top_k = 50,
#     top_p = 1.0,
#     do_sample = True,
#     max_new_tokens = 48,
# )
# pad_token_id = tokenizer.eos_token_id,

# Step 3: Define the training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir=script_args.output_dir,
    per_device_train_batch_size=script_args.batch_size,
    gradient_accumulation_steps=script_args.gradient_accumulation_steps,
    learning_rate=script_args.learning_rate,
    logging_steps=script_args.logging_steps,
    num_train_epochs=script_args.num_train_epochs,
    max_steps=script_args.max_steps,
    report_to=script_args.log_with,
    save_steps=script_args.save_steps,
    save_total_limit=script_args.save_total_limit,
    push_to_hub=script_args.push_to_hub,
    hub_model_id=script_args.hub_model_id,
    generation_config=generation_kwargs,
)

# Step 4: Define the LoraConfig
if script_args.use_peft:
    peft_config = LoraConfig(
        r=script_args.peft_lora_r,
        lora_alpha=script_args.peft_lora_alpha,
        bias="none",
        task_type="CAUSAL_LM",
    )
else:
    peft_config = None


# trl sft trainer default tokenizer and pad_token
"""
if tokenizer is None:
            tokenizer = AutoTokenizer.from_pretrained(model.config._name_or_path)
            if getattr(tokenizer, "pad_token", None) is None:
                tokenizer.pad_token = tokenizer.eos_token

if max_seq_length is None:
            # to overcome some issues with broken tokenizers
            max_seq_length = min(tokenizer.model_max_length, 1024)

            warnings.warn(
                f"You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to {max_seq_length}"
            )

"""

# Looks like padding_side = "right" by default
# Hence, defining our tokenizer here
tokenizer = AutoTokenizer.from_pretrained(script_args.model_name)
tokenizer.pad_token = tokenizer.eos_token

if script_args.change_tokenizer_args == True:
    tokenizer.padding_side = script_args.tokenizer_kwargs["padding_side"]
    tokenizer.truncation_side = script_args.tokenizer_kwargs["truncation_side"]
    tokenizer.pad_token_as_eos_token = script_args.tokenizer_kwargs["pad_token_as_eos_token"]
    tokenizer.max_length = script_args.tokenizer_kwargs["max_length"]

# tokenizer.padding_side = "left"
# tokenizer.truncation_side = "left"
# tokenizer.pad_token_as_eos_token = True
# tokenizer.max_length = 64

# Step 5: Define the Trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    max_seq_length=script_args.seq_length,
    train_dataset=dataset,
    dataset_text_field=script_args.dataset_text_field,
    peft_config=peft_config,
    tokenizer=tokenizer,
)

trainer.train()

# Step 6: Save the model
trainer.save_model(logging_dir)

File Path: examples/RealToxicity/training/soft_risk_ppo_auth2_2.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset, Dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import (
    AutoModelForCausalLMWithValueHead,
    AutoModelForSeq2SeqLMWithValueHead,
    PPOConfig,
    set_seed,
)  # ,PPOTrainer
from trl.trainer.ppo_trainer import PPOTrainer
from trl.core import LengthSampler


tqdm.pandas()

import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import pdb
import pandas as pd
import math

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
logging_dir = f"/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/{date_n_time}"
os.makedirs(logging_dir)


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="gpt2",
            # model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-20_20-32-35",  # sft_both
            model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-31_18-06-18",  # sft_positive seed2 70_30
            query_dataset="jigsaw_toxicity_pred",
            reward_model="unitary/toxic-bert",
            learning_rate=1.41e-5,
            # log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            # adap_kl_ctrl=False,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            steps=192000,
            exp_name="ppo",
        )
    )
    query_dataset: str = field(default="jigsaw_toxicity_pred", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="less", metadata={"help": "the dataset to query"})

    generation_kwargs_min_length: Optional[int] = field(
        default=-1, metadata={"help": "minimum number of tokens while generation"}
    )
    generation_kwargs_top_k: Optional[int] = field(default=0, metadata={"help": "gneration top k"})
    generation_kwargs_top_p: Optional[float] = field(default=1.0, metadata={"help": "gneration top p"})
    generation_kwargs_max_new_tokens: Optional[int] = field(default=32, metadata={"help": "gneration top p"})

    change_tokenizer_args: bool = field(default=False, metadata={"help": "whether to use modify tokenizer settings"})
    tokenizer_kwargs: dict = field(
        default_factory=lambda: {
            "padding_side": "left",
            "truncation_side": "left",
            "pad_token_as_eos_token": True,
            "max_length": 64,
        }
    )
    risk_scheduler: str = field(default="old", metadata={"help": "old risk scheduler that doesn't go down to alpha"})
    risk_n: int = field(
        default=240, metadata={"help": "240 (> batch size): no RA; 70: RA begins after iter 70; 1: RA throughout"}
    )
    risk_alpha: Optional[float] = field(default=0.2, metadata={"help": "risk alpha value = 20 percent"})
    risk_rho: Optional[float] = field(
        default=0.8, metadata={"help": "risk alpha reaches a value = 20 percent at 80 percent of total iterations"}
    )
    prompt_len: Optional[int] = field(default=32, metadata={"help": "prompt length"})


args = tyro.cli(ScriptArguments)


generation_kwargs = {
    "min_length": args.generation_kwargs_min_length,
    "top_k": args.generation_kwargs_top_k,
    "top_p": args.generation_kwargs_top_p,
    "do_sample": True,
    "max_new_tokens": args.generation_kwargs_max_new_tokens,
}

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # Kaggle datasets have to be downloaded manually (using Kaggle API)
    # datasets.load_dataset("jigsaw_toxicity_pred", data_dir="<path/to/manual/data>")
    # ds = load_dataset(
    #     query_dataset,
    #     data_dir="/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/original_dataset",
    #     split="train",
    # )  # Hardcoding path for now TODO: change this

    # ds = ds.rename_columns({"comment_text": "review"})
    # ds = ds.filter(lambda x: len(x["review"]) > 64, batched=False)

    # df = pd.DataFrame(ds)
    # num_toxic = df["toxic"].sum()

    # toxic_df = df[df["toxic"] == True]
    # non_toxic_df = df[df["toxic"] == False]

    # non_toxic_df = non_toxic_df.sample(n=num_toxic, random_state=config.seed)

    # # Recombine into dataset
    # ds = ds.from_pandas(pd.concat([toxic_df, non_toxic_df], ignore_index=True))
    df = pd.read_csv(
        "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/jigsaw_train_70_30.csv",
    )
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"comment_text": "review"})

    # pdb.set_trace()

    if args.input_query_size == "more":

        def input_size():
            return args.prompt_len

    else:
        input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    args.ppo_config.model_name,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id

# pdb.set_trace()

if args.change_tokenizer_args == True:
    tokenizer.padding_side = args.tokenizer_kwargs["padding_side"]
    tokenizer.truncation_side = args.tokenizer_kwargs["truncation_side"]
    tokenizer.pad_token_as_eos_token = args.tokenizer_kwargs["pad_token_as_eos_token"]
    tokenizer.max_length = args.tokenizer_kwargs["max_length"]

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
task = "text-classification"
model_name = args.ppo_config.reward_model
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs["pad_token_id"] = tokenizer.eos_token_id

# empty list to store text data
text_train_data = []


def get_current_risk_level(alpha, rho, m, M):
    """
    Get current risk_level_alpha*N value

    Args:
        alpha (float):
            Risk level alpha
        rho (float):
            Risk level scheduler: 0.8 means the soft risk level reaches α after ρ = 80% of the training
        m (int):
            current training step/epoch
        M (int):
            total policy training steps

    Returns:
        alpha_N (int):
            risk_level_alpha*N value
    """
    alpha = args.risk_alpha
    rho = args.risk_rho

    if args.risk_scheduler == "old":
        if m <= args.risk_n:
            return args.ppo_config.batch_size
        else:
            return math.ceil(args.ppo_config.batch_size * max(alpha, 1 - (1 - alpha) * (m - args.risk_n) / (rho * M)))
    else:
        print("here")
        if m <= args.risk_n:
            val = args.ppo_config.batch_size
        elif m >= math.ceil(rho * M):
            val = math.ceil(alpha * args.ppo_config.batch_size)
        else:
            K = (1 - alpha) / (math.ceil(rho * M) - args.risk_n)
            val = math.ceil(args.ppo_config.batch_size * max(alpha, 1 - K * (m - args.risk_n)))
        return val


def get_current_risk_level_2(alpha, rho, m, M):
    """
    Get current risk_level_alpha*N value

    Args:
        alpha (float):
            Risk level alpha
        rho (float):
            Risk level scheduler: 0.8 means the soft risk level reaches α after ρ = 80% of the training
        m (int):
            current training step/epoch
        M (int):
            total policy training steps

    Returns:
        alpha_N (int):
            risk_level_alpha*N value
    """
    alpha = 0.2
    rho = 0.8
    n = 70
    M = 194

    if m <= n:
        val = args.ppo_config.batch_size
    elif m >= math.ceil(rho * M):
        val = alpha * 100
    else:
        K = (1 - alpha) / (math.ceil(rho * M) - n)
        val = math.ceil(args.ppo_config.batch_size * max(alpha, 1 - K * (m - n)))
    return val


def apply_risk_modification_to_batch(batch, query_tensors, response_tensors, rewards, alpha_N):
    query_tensors = list(map(lambda t: t.cpu(), query_tensors))
    response_tensors = list(map(lambda t: t.cpu(), response_tensors))
    rewards = list(map(lambda t: t.cpu(), rewards))
    # response_lengths = [response_tensors[i].shape[0] for i in response_tensors]

    step_dict = {"query_t": query_tensors, "resp_t": response_tensors, "rewards": rewards}

    # batch = {k: [t.cpu() for t in v] if isinstance(v, list) else v.cpu() for k, v in batch.items()}
    batch = {
        k: [t.cpu() for t in v]
        if isinstance(v, list) and all(isinstance(t, torch.Tensor) for t in v)
        else v.cpu()
        if isinstance(v, torch.Tensor)
        else v
        for k, v in batch.items()
    }
    # pdb.set_trace()

    df_step_dict = pd.DataFrame(step_dict)
    df_batch = pd.DataFrame(batch)

    df_step_dict = df_step_dict.sort_values(by="rewards")  # .head(alpha_N)
    df_batch = df_batch.sort_values(by="rewards")  # .head(alpha_N).drop('rewards', axis=1)
    batch = df_batch.to_dict(orient="list")
    # line #996 in ppo_trainer already does response_masks_batch[j] = torch.cat((torch.zeros_like(query_batch[j]), response_masks_batch[j]))[1:]
    # pdb.set_trace()
    # mask = [torch.cat((torch.ones_like(t, dtype =int), torch.zeros([48 - t.shape[0]], dtype =int))) for t in response_tensors]  #torch.zeros([64], dtype =int),
    mask = [torch.ones([32], dtype=int) for _ in response_tensors]  # torch.zeros([64], dtype =int),
    for ii in range(args.ppo_config.batch_size - alpha_N):
        mask[-ii - 1] = torch.zeros([32], dtype=int)

    sr_query_tensors = list(map(lambda t: t.to(device), df_step_dict["query_t"].tolist()))
    sr_response_tensors = list(map(lambda t: t.to(device), df_step_dict["resp_t"].tolist()))
    sr_rewards = list(map(lambda t: t.to(device), df_step_dict["rewards"].tolist()))
    batch = {
        k: [t.to(device) for t in v]
        if isinstance(v, list) and all(isinstance(t, torch.Tensor) for t in v)
        else v.to(device)
        if isinstance(v, torch.Tensor)
        else v
        for k, v in batch.items()
    }

    return batch, sr_query_tensors, sr_response_tensors, sr_rewards, mask


for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)

    if args.reward_function == "trl":
        rewards = [torch.tensor(-output[0]["score"]) for output in pipe_outputs]
    else:  # depreciated
        raise NotImplementedError
        rewards = [
            torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"]]))[1]
            for output in pipe_outputs
        ]

    for ii in range(3):
        data_row = {
            "iter": epoch,
            "input": batch["query"][ii],
            "output": batch["response"][ii],
            "score": rewards[ii].item(),
        }
        text_train_data.append(data_row)

    batch["rewards"] = rewards
    alpha_N_ceil = get_current_risk_level(PPOConfig.soft_risk_alpha, PPOConfig.risk_level_scheduler, epoch, 288)
    # sr_batch, sr_query_t, sr_resp_t, sr_rewards, mask = apply_risk_modification_to_batch(batch, query_tensors, response_tensors, rewards, alpha_N_ceil)

    # Run PPO step
    print(alpha_N_ceil)
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards, alpha_N_ceil)
    ppo_trainer.log_stats(stats, batch, rewards)
    print(epoch)

ppo_trainer._save_pretrained(logging_dir + "/save_pretrained")
df = pd.DataFrame(text_train_data)
df.to_csv(logging_dir + "/textual_data.csv", index=False)

File Path: examples/RealToxicity/training/soft_risk_ppo_auth2_seed12.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset, Dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import (
    AutoModelForCausalLMWithValueHead,
    AutoModelForSeq2SeqLMWithValueHead,
    PPOConfig,
    set_seed,
)  # ,PPOTrainer
from trl.trainer.ppo_trainer import PPOTrainer
from trl.core import LengthSampler


tqdm.pandas()

import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import pdb
import pandas as pd
import math

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
logging_dir = f"/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/{date_n_time}"
os.makedirs(logging_dir)


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="gpt2",
            # model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-20_20-32-35",  # sft_both
            model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-30_00-30-38",  # sft_positive seed 12
            query_dataset="jigsaw_toxicity_pred",
            reward_model="unitary/toxic-bert",
            learning_rate=1.41e-5,
            # log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            # adap_kl_ctrl=False,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            steps=192000,
            exp_name="ppo",
        )
    )
    query_dataset: str = field(default="jigsaw_toxicity_pred", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="less", metadata={"help": "the dataset to query"})

    generation_kwargs_min_length: Optional[int] = field(
        default=-1, metadata={"help": "minimum number of tokens while generation"}
    )
    generation_kwargs_top_k: Optional[int] = field(default=0, metadata={"help": "gneration top k"})
    generation_kwargs_top_p: Optional[float] = field(default=1.0, metadata={"help": "gneration top p"})
    generation_kwargs_max_new_tokens: Optional[int] = field(default=32, metadata={"help": "gneration top p"})

    change_tokenizer_args: bool = field(default=False, metadata={"help": "whether to use modify tokenizer settings"})
    tokenizer_kwargs: dict = field(
        default_factory=lambda: {
            "padding_side": "left",
            "truncation_side": "left",
            "pad_token_as_eos_token": True,
            "max_length": 64,
        }
    )
    risk_scheduler: str = field(default="old", metadata={"help": "old risk scheduler that doesn't go down to alpha"})
    risk_n: int = field(
        default=240, metadata={"help": "240 (> batch size): no RA; 70: RA begins after iter 70; 1: RA throughout"}
    )
    risk_alpha: Optional[float] = field(default=0.2, metadata={"help": "risk alpha value = 20 percent"})
    risk_rho: Optional[float] = field(
        default=0.8, metadata={"help": "risk alpha reaches a value = 20 percent at 80 percent of total iterations"}
    )
    prompt_len: Optional[int] = field(default=32, metadata={"help": "prompt length"})


args = tyro.cli(ScriptArguments)


generation_kwargs = {
    "min_length": args.generation_kwargs_min_length,
    "top_k": args.generation_kwargs_top_k,
    "top_p": args.generation_kwargs_top_p,
    "do_sample": True,
    "max_new_tokens": args.generation_kwargs_max_new_tokens,
}

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # Kaggle datasets have to be downloaded manually (using Kaggle API)
    # datasets.load_dataset("jigsaw_toxicity_pred", data_dir="<path/to/manual/data>")
    # ds = load_dataset(
    #     query_dataset,
    #     data_dir="/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/original_dataset",
    #     split="train",
    # )  # Hardcoding path for now TODO: change this

    # ds = ds.rename_columns({"comment_text": "review"})
    # ds = ds.filter(lambda x: len(x["review"]) > 64, batched=False)

    # df = pd.DataFrame(ds)
    # num_toxic = df["toxic"].sum()

    # toxic_df = df[df["toxic"] == True]
    # non_toxic_df = df[df["toxic"] == False]

    # non_toxic_df = non_toxic_df.sample(n=num_toxic, random_state=config.seed)

    # # Recombine into dataset
    # ds = ds.from_pandas(pd.concat([toxic_df, non_toxic_df], ignore_index=True))
    df = pd.read_csv(
        "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/jigsaw_train.csv",
    )
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"comment_text": "review"})

    # pdb.set_trace()

    if args.input_query_size == "more":

        def input_size():
            return args.prompt_len

    else:
        input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    args.ppo_config.model_name,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id

# pdb.set_trace()

if args.change_tokenizer_args == True:
    tokenizer.padding_side = args.tokenizer_kwargs["padding_side"]
    tokenizer.truncation_side = args.tokenizer_kwargs["truncation_side"]
    tokenizer.pad_token_as_eos_token = args.tokenizer_kwargs["pad_token_as_eos_token"]
    tokenizer.max_length = args.tokenizer_kwargs["max_length"]

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
task = "text-classification"
model_name = args.ppo_config.reward_model
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs["pad_token_id"] = tokenizer.eos_token_id

# empty list to store text data
text_train_data = []


def get_current_risk_level(alpha, rho, m, M):
    """
    Get current risk_level_alpha*N value

    Args:
        alpha (float):
            Risk level alpha
        rho (float):
            Risk level scheduler: 0.8 means the soft risk level reaches α after ρ = 80% of the training
        m (int):
            current training step/epoch
        M (int):
            total policy training steps

    Returns:
        alpha_N (int):
            risk_level_alpha*N value
    """
    alpha = args.risk_alpha
    rho = args.risk_rho

    if args.risk_scheduler == "old":
        if m <= args.risk_n:
            return args.ppo_config.batch_size
        else:
            return math.ceil(args.ppo_config.batch_size * max(alpha, 1 - (1 - alpha) * (m - args.risk_n) / (rho * M)))
    else:
        print("here")
        if m <= args.risk_n:
            val = args.ppo_config.batch_size
        elif m >= math.ceil(rho * M):
            val = math.ceil(alpha * args.ppo_config.batch_size)
        else:
            K = (1 - alpha) / (math.ceil(rho * M) - args.risk_n)
            val = math.ceil(args.ppo_config.batch_size * max(alpha, 1 - K * (m - args.risk_n)))
        return val


def get_current_risk_level_2(alpha, rho, m, M):
    """
    Get current risk_level_alpha*N value

    Args:
        alpha (float):
            Risk level alpha
        rho (float):
            Risk level scheduler: 0.8 means the soft risk level reaches α after ρ = 80% of the training
        m (int):
            current training step/epoch
        M (int):
            total policy training steps

    Returns:
        alpha_N (int):
            risk_level_alpha*N value
    """
    alpha = 0.2
    rho = 0.8
    n = 70
    M = 194

    if m <= n:
        val = args.ppo_config.batch_size
    elif m >= math.ceil(rho * M):
        val = alpha * 100
    else:
        K = (1 - alpha) / (math.ceil(rho * M) - n)
        val = math.ceil(args.ppo_config.batch_size * max(alpha, 1 - K * (m - n)))
    return val


def apply_risk_modification_to_batch(batch, query_tensors, response_tensors, rewards, alpha_N):
    query_tensors = list(map(lambda t: t.cpu(), query_tensors))
    response_tensors = list(map(lambda t: t.cpu(), response_tensors))
    rewards = list(map(lambda t: t.cpu(), rewards))
    # response_lengths = [response_tensors[i].shape[0] for i in response_tensors]

    step_dict = {"query_t": query_tensors, "resp_t": response_tensors, "rewards": rewards}

    # batch = {k: [t.cpu() for t in v] if isinstance(v, list) else v.cpu() for k, v in batch.items()}
    batch = {
        k: [t.cpu() for t in v]
        if isinstance(v, list) and all(isinstance(t, torch.Tensor) for t in v)
        else v.cpu()
        if isinstance(v, torch.Tensor)
        else v
        for k, v in batch.items()
    }
    # pdb.set_trace()

    df_step_dict = pd.DataFrame(step_dict)
    df_batch = pd.DataFrame(batch)

    df_step_dict = df_step_dict.sort_values(by="rewards")  # .head(alpha_N)
    df_batch = df_batch.sort_values(by="rewards")  # .head(alpha_N).drop('rewards', axis=1)
    batch = df_batch.to_dict(orient="list")
    # line #996 in ppo_trainer already does response_masks_batch[j] = torch.cat((torch.zeros_like(query_batch[j]), response_masks_batch[j]))[1:]
    # pdb.set_trace()
    # mask = [torch.cat((torch.ones_like(t, dtype =int), torch.zeros([48 - t.shape[0]], dtype =int))) for t in response_tensors]  #torch.zeros([64], dtype =int),
    mask = [torch.ones([32], dtype=int) for _ in response_tensors]  # torch.zeros([64], dtype =int),
    for ii in range(args.ppo_config.batch_size - alpha_N):
        mask[-ii - 1] = torch.zeros([32], dtype=int)

    sr_query_tensors = list(map(lambda t: t.to(device), df_step_dict["query_t"].tolist()))
    sr_response_tensors = list(map(lambda t: t.to(device), df_step_dict["resp_t"].tolist()))
    sr_rewards = list(map(lambda t: t.to(device), df_step_dict["rewards"].tolist()))
    batch = {
        k: [t.to(device) for t in v]
        if isinstance(v, list) and all(isinstance(t, torch.Tensor) for t in v)
        else v.to(device)
        if isinstance(v, torch.Tensor)
        else v
        for k, v in batch.items()
    }

    return batch, sr_query_tensors, sr_response_tensors, sr_rewards, mask


for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)

    if args.reward_function == "trl":
        rewards = [torch.tensor(-output[0]["score"]) for output in pipe_outputs]
    else:  # depreciated
        raise NotImplementedError
        rewards = [
            torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"]]))[1]
            for output in pipe_outputs
        ]

    for ii in range(3):
        data_row = {
            "iter": epoch,
            "input": batch["query"][ii],
            "output": batch["response"][ii],
            "score": rewards[ii].item(),
        }
        text_train_data.append(data_row)

    batch["rewards"] = rewards
    alpha_N_ceil = get_current_risk_level(PPOConfig.soft_risk_alpha, PPOConfig.risk_level_scheduler, epoch, 264)
    # sr_batch, sr_query_t, sr_resp_t, sr_rewards, mask = apply_risk_modification_to_batch(batch, query_tensors, response_tensors, rewards, alpha_N_ceil)

    # Run PPO step
    print(alpha_N_ceil)
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards, alpha_N_ceil)
    ppo_trainer.log_stats(stats, batch, rewards)
    print(epoch)

ppo_trainer._save_pretrained(logging_dir + "/save_pretrained")
df = pd.DataFrame(text_train_data)
df.to_csv(logging_dir + "/textual_data.csv", index=False)

File Path: examples/RealToxicity/training/soft_risk_ppo_auth2_seed2.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset, Dataset, load_from_disk
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import (
    AutoModelForCausalLMWithValueHead,
    AutoModelForSeq2SeqLMWithValueHead,
    PPOConfig,
    set_seed,
)  # ,PPOTrainer
from trl.trainer.ppo_trainer import PPOTrainer
from trl.core import LengthSampler


tqdm.pandas()

import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import pdb
import pandas as pd
import math

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
logging_dir = (
    f"/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/real_toxicity/{date_n_time}"
)
os.makedirs(logging_dir)


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="gpt2",
            # model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-20_20-32-35",  # sft_both
            model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/real_toxicity/2024-02-22_14-11-00",  # sft_positive real_toxicity
            query_dataset="real_toxicity_train",
            reward_model="unitary/toxic-bert",
            learning_rate=2*1.41e-5,
            # log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            # adap_kl_ctrl=False,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            steps=192000,
            exp_name="ppo",
        )
    )
    query_dataset: str = field(default="jigsaw_toxicity_pred", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="less", metadata={"help": "the dataset to query"})

    generation_kwargs_min_length: Optional[int] = field(
        default=-1, metadata={"help": "minimum number of tokens while generation"}
    )
    generation_kwargs_top_k: Optional[int] = field(default=0, metadata={"help": "gneration top k"})
    generation_kwargs_top_p: Optional[float] = field(default=1.0, metadata={"help": "gneration top p"})
    generation_kwargs_max_new_tokens: Optional[int] = field(default=32, metadata={"help": "gneration top p"})

    change_tokenizer_args: bool = field(default=False, metadata={"help": "whether to use modify tokenizer settings"})
    tokenizer_kwargs: dict = field(
        default_factory=lambda: {
            "padding_side": "left",
            "truncation_side": "left",
            "pad_token_as_eos_token": True,
            "max_length": 64,
        }
    )
    risk_scheduler: str = field(default="old", metadata={"help": "old risk scheduler that doesn't go down to alpha"})
    risk_n: int = field(
        default=240, metadata={"help": "240 (> batch size): no RA; 70: RA begins after iter 70; 1: RA throughout"}
    )
    risk_alpha: Optional[float] = field(default=0.2, metadata={"help": "risk alpha value = 20 percent"})
    risk_rho: Optional[float] = field(
        default=0.8, metadata={"help": "risk alpha reaches a value = 20 percent at 80 percent of total iterations"}
    )
    prompt_len: Optional[int] = field(default=32, metadata={"help": "prompt length"})


args = tyro.cli(ScriptArguments)


generation_kwargs = {
    "min_length": args.generation_kwargs_min_length,
    "top_k": args.generation_kwargs_top_k,
    "top_p": args.generation_kwargs_top_p,
    "do_sample": True,
    "max_new_tokens": args.generation_kwargs_max_new_tokens,
}

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token

    ds = load_from_disk("/mnt/shared-scratch/Shakkottai_S/auth2d36/datasets/RealToxicity/real_toxicity_train.hf")
    ds = ds.rename_columns({"Prompt": "review"})

    # pdb.set_trace()

    if args.input_query_size == "more":

        def input_size():
            return args.prompt_len

    else:
        input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    args.ppo_config.model_name,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id

# pdb.set_trace()

if args.change_tokenizer_args == True:
    tokenizer.padding_side = args.tokenizer_kwargs["padding_side"]
    tokenizer.truncation_side = args.tokenizer_kwargs["truncation_side"]
    tokenizer.pad_token_as_eos_token = args.tokenizer_kwargs["pad_token_as_eos_token"]
    tokenizer.max_length = args.tokenizer_kwargs["max_length"]

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
task = "text-classification"
model_name = args.ppo_config.reward_model
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs["pad_token_id"] = tokenizer.eos_token_id

# empty list to store text data
text_train_data = []


def get_current_risk_level(alpha, rho, m, M):
    """
    Get current risk_level_alpha*N value

    Args:
        alpha (float):
            Risk level alpha
        rho (float):
            Risk level scheduler: 0.8 means the soft risk level reaches α after ρ = 80% of the training
        m (int):
            current training step/epoch
        M (int):
            total policy training steps

    Returns:
        alpha_N (int):
            risk_level_alpha*N value
    """
    alpha = args.risk_alpha
    rho = args.risk_rho

    if args.risk_scheduler == "old":
        if m <= args.risk_n:
            return args.ppo_config.batch_size
        else:
            return math.ceil(args.ppo_config.batch_size * max(alpha, 1 - (1 - alpha) * (m - args.risk_n) / (rho * M)))
    else:
        print("here")
        if m <= args.risk_n:
            val = args.ppo_config.batch_size
        elif m >= math.ceil(rho * M):
            val = math.ceil(alpha * args.ppo_config.batch_size)
        else:
            K = (1 - alpha) / (math.ceil(rho * M) - args.risk_n)
            val = math.ceil(args.ppo_config.batch_size * max(alpha, 1 - K * (m - args.risk_n)))
        return val


def get_current_risk_level_2(alpha, rho, m, M):
    """
    Get current risk_level_alpha*N value

    Args:
        alpha (float):
            Risk level alpha
        rho (float):
            Risk level scheduler: 0.8 means the soft risk level reaches α after ρ = 80% of the training
        m (int):
            current training step/epoch
        M (int):
            total policy training steps

    Returns:
        alpha_N (int):
            risk_level_alpha*N value
    """
    alpha = 0.2
    rho = 0.8
    n = 70
    M = 194

    if m <= n:
        val = args.ppo_config.batch_size
    elif m >= math.ceil(rho * M):
        val = alpha * 100
    else:
        K = (1 - alpha) / (math.ceil(rho * M) - n)
        val = math.ceil(args.ppo_config.batch_size * max(alpha, 1 - K * (m - n)))
    return val


def apply_risk_modification_to_batch(batch, query_tensors, response_tensors, rewards, alpha_N):
    query_tensors = list(map(lambda t: t.cpu(), query_tensors))
    response_tensors = list(map(lambda t: t.cpu(), response_tensors))
    rewards = list(map(lambda t: t.cpu(), rewards))
    # response_lengths = [response_tensors[i].shape[0] for i in response_tensors]

    step_dict = {"query_t": query_tensors, "resp_t": response_tensors, "rewards": rewards}

    # batch = {k: [t.cpu() for t in v] if isinstance(v, list) else v.cpu() for k, v in batch.items()}
    batch = {
        k: (
            [t.cpu() for t in v]
            if isinstance(v, list) and all(isinstance(t, torch.Tensor) for t in v)
            else v.cpu() if isinstance(v, torch.Tensor) else v
        )
        for k, v in batch.items()
    }
    # pdb.set_trace()

    df_step_dict = pd.DataFrame(step_dict)
    df_batch = pd.DataFrame(batch)

    df_step_dict = df_step_dict.sort_values(by="rewards")  # .head(alpha_N)
    df_batch = df_batch.sort_values(by="rewards")  # .head(alpha_N).drop('rewards', axis=1)
    batch = df_batch.to_dict(orient="list")
    # line #996 in ppo_trainer already does response_masks_batch[j] = torch.cat((torch.zeros_like(query_batch[j]), response_masks_batch[j]))[1:]
    # pdb.set_trace()
    # mask = [torch.cat((torch.ones_like(t, dtype =int), torch.zeros([48 - t.shape[0]], dtype =int))) for t in response_tensors]  #torch.zeros([64], dtype =int),
    mask = [torch.ones([32], dtype=int) for _ in response_tensors]  # torch.zeros([64], dtype =int),
    for ii in range(args.ppo_config.batch_size - alpha_N):
        mask[-ii - 1] = torch.zeros([32], dtype=int)

    sr_query_tensors = list(map(lambda t: t.to(device), df_step_dict["query_t"].tolist()))
    sr_response_tensors = list(map(lambda t: t.to(device), df_step_dict["resp_t"].tolist()))
    sr_rewards = list(map(lambda t: t.to(device), df_step_dict["rewards"].tolist()))
    batch = {
        k: (
            [t.to(device) for t in v]
            if isinstance(v, list) and all(isinstance(t, torch.Tensor) for t in v)
            else v.to(device) if isinstance(v, torch.Tensor) else v
        )
        for k, v in batch.items()
    }

    return batch, sr_query_tensors, sr_response_tensors, sr_rewards, mask


for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs, truncation=True)

    if args.reward_function == "trl":
        rewards = [torch.tensor(-output[0]["score"]) for output in pipe_outputs]
    else:  # depreciated
        raise NotImplementedError
        rewards = [
            torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"]]))[1]
            for output in pipe_outputs
        ]

    for ii in range(3):
        data_row = {
            "iter": epoch,
            "input": batch["query"][ii],
            "output": batch["response"][ii],
            "score": rewards[ii].item(),
        }
        text_train_data.append(data_row)

    batch["rewards"] = rewards
    alpha_N_ceil = get_current_risk_level(PPOConfig.soft_risk_alpha, PPOConfig.risk_level_scheduler, epoch, 452)
    # sr_batch, sr_query_t, sr_resp_t, sr_rewards, mask = apply_risk_modification_to_batch(batch, query_tensors, response_tensors, rewards, alpha_N_ceil)

    # Run PPO step
    print(alpha_N_ceil)
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards, alpha_N_ceil)
    ppo_trainer.log_stats(stats, batch, rewards)
    print(epoch)

ppo_trainer._save_pretrained(logging_dir + "/save_pretrained")
df = pd.DataFrame(text_train_data)
df.to_csv(logging_dir + "/textual_data.csv", index=False)

File Path: examples/RealToxicity/training/soft_risk_ppo_auth2_seed36.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset, Dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import (
    AutoModelForCausalLMWithValueHead,
    AutoModelForSeq2SeqLMWithValueHead,
    PPOConfig,
    set_seed,
)  # ,PPOTrainer
from trl.trainer.ppo_trainer import PPOTrainer
from trl.core import LengthSampler


tqdm.pandas()

import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import pdb
import pandas as pd
import math

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
logging_dir = f"/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/dummy/{date_n_time}"
os.makedirs(logging_dir)


@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            # model_name="gpt2",
            # model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-20_20-32-35",  # sft_both
            model_name="/mnt/research/Anon2/Students/auth2/rlhf/trl/logs/sft_auth2/2024-01-30_00-24-10",  # sft_positive seed 36
            query_dataset="jigsaw_toxicity_pred",
            reward_model="unitary/toxic-bert",
            learning_rate=1.41e-5,
            # log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            # adap_kl_ctrl=False,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            steps=192000,
            exp_name="ppo",
        )
    )
    query_dataset: str = field(default="jigsaw_toxicity_pred", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="less", metadata={"help": "the dataset to query"})

    generation_kwargs_min_length: Optional[int] = field(
        default=-1, metadata={"help": "minimum number of tokens while generation"}
    )
    generation_kwargs_top_k: Optional[int] = field(default=0, metadata={"help": "gneration top k"})
    generation_kwargs_top_p: Optional[float] = field(default=1.0, metadata={"help": "gneration top p"})
    generation_kwargs_max_new_tokens: Optional[int] = field(default=32, metadata={"help": "gneration top p"})

    change_tokenizer_args: bool = field(default=False, metadata={"help": "whether to use modify tokenizer settings"})
    tokenizer_kwargs: dict = field(
        default_factory=lambda: {
            "padding_side": "left",
            "truncation_side": "left",
            "pad_token_as_eos_token": True,
            "max_length": 64,
        }
    )
    risk_scheduler: str = field(default="old", metadata={"help": "old risk scheduler that doesn't go down to alpha"})
    risk_n: int = field(
        default=240, metadata={"help": "240 (> batch size): no RA; 70: RA begins after iter 70; 1: RA throughout"}
    )
    risk_alpha: Optional[float] = field(default=0.2, metadata={"help": "risk alpha value = 20 percent"})
    risk_rho: Optional[float] = field(
        default=0.8, metadata={"help": "risk alpha reaches a value = 20 percent at 80 percent of total iterations"}
    )
    prompt_len: Optional[int] = field(default=32, metadata={"help": "prompt length"})


args = tyro.cli(ScriptArguments)


generation_kwargs = {
    "min_length": args.generation_kwargs_min_length,
    "top_k": args.generation_kwargs_top_k,
    "top_p": args.generation_kwargs_top_p,
    "do_sample": True,
    "max_new_tokens": args.generation_kwargs_max_new_tokens,
}

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # Kaggle datasets have to be downloaded manually (using Kaggle API)
    # datasets.load_dataset("jigsaw_toxicity_pred", data_dir="<path/to/manual/data>")
    # ds = load_dataset(
    #     query_dataset,
    #     data_dir="/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/original_dataset",
    #     split="train",
    # )  # Hardcoding path for now TODO: change this

    # ds = ds.rename_columns({"comment_text": "review"})
    # ds = ds.filter(lambda x: len(x["review"]) > 64, batched=False)

    # df = pd.DataFrame(ds)
    # num_toxic = df["toxic"].sum()

    # toxic_df = df[df["toxic"] == True]
    # non_toxic_df = df[df["toxic"] == False]

    # non_toxic_df = non_toxic_df.sample(n=num_toxic, random_state=config.seed)

    # # Recombine into dataset
    # ds = ds.from_pandas(pd.concat([toxic_df, non_toxic_df], ignore_index=True))
    df = pd.read_csv(
        "/mnt/research/Anon2/Students/auth2/repos/trl/examples/Jigsaw/dataset/jigsaw_train.csv",
    )
    ds = Dataset.from_pandas(df)
    ds = ds.rename_columns({"comment_text": "review"})

    # pdb.set_trace()

    if args.input_query_size == "more":

        def input_size():
            return args.prompt_len

    else:
        input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    args.ppo_config.model_name,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id

# pdb.set_trace()

if args.change_tokenizer_args == True:
    tokenizer.padding_side = args.tokenizer_kwargs["padding_side"]
    tokenizer.truncation_side = args.tokenizer_kwargs["truncation_side"]
    tokenizer.pad_token_as_eos_token = args.tokenizer_kwargs["pad_token_as_eos_token"]
    tokenizer.max_length = args.tokenizer_kwargs["max_length"]

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
task = "text-classification"
model_name = args.ppo_config.reward_model
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs["pad_token_id"] = tokenizer.eos_token_id

# empty list to store text data
text_train_data = []


def get_current_risk_level(alpha, rho, m, M):
    """
    Get current risk_level_alpha*N value

    Args:
        alpha (float):
            Risk level alpha
        rho (float):
            Risk level scheduler: 0.8 means the soft risk level reaches α after ρ = 80% of the training
        m (int):
            current training step/epoch
        M (int):
            total policy training steps

    Returns:
        alpha_N (int):
            risk_level_alpha*N value
    """
    alpha = args.risk_alpha
    rho = args.risk_rho

    if args.risk_scheduler == "old":
        if m <= args.risk_n:
            return args.ppo_config.batch_size
        else:
            return math.ceil(args.ppo_config.batch_size * max(alpha, 1 - (1 - alpha) * (m - args.risk_n) / (rho * M)))
    else:
        print("here")
        if m <= args.risk_n:
            val = args.ppo_config.batch_size
        elif m >= math.ceil(rho * M):
            val = math.ceil(alpha * args.ppo_config.batch_size)
        else:
            K = (1 - alpha) / (math.ceil(rho * M) - args.risk_n)
            val = math.ceil(args.ppo_config.batch_size * max(alpha, 1 - K * (m - args.risk_n)))
        return val


def get_current_risk_level_2(alpha, rho, m, M):
    """
    Get current risk_level_alpha*N value

    Args:
        alpha (float):
            Risk level alpha
        rho (float):
            Risk level scheduler: 0.8 means the soft risk level reaches α after ρ = 80% of the training
        m (int):
            current training step/epoch
        M (int):
            total policy training steps

    Returns:
        alpha_N (int):
            risk_level_alpha*N value
    """
    alpha = 0.2
    rho = 0.8
    n = 70
    M = 194

    if m <= n:
        val = args.ppo_config.batch_size
    elif m >= math.ceil(rho * M):
        val = alpha * 100
    else:
        K = (1 - alpha) / (math.ceil(rho * M) - n)
        val = math.ceil(args.ppo_config.batch_size * max(alpha, 1 - K * (m - n)))
    return val


def apply_risk_modification_to_batch(batch, query_tensors, response_tensors, rewards, alpha_N):
    query_tensors = list(map(lambda t: t.cpu(), query_tensors))
    response_tensors = list(map(lambda t: t.cpu(), response_tensors))
    rewards = list(map(lambda t: t.cpu(), rewards))
    # response_lengths = [response_tensors[i].shape[0] for i in response_tensors]

    step_dict = {"query_t": query_tensors, "resp_t": response_tensors, "rewards": rewards}

    # batch = {k: [t.cpu() for t in v] if isinstance(v, list) else v.cpu() for k, v in batch.items()}
    batch = {
        k: [t.cpu() for t in v]
        if isinstance(v, list) and all(isinstance(t, torch.Tensor) for t in v)
        else v.cpu()
        if isinstance(v, torch.Tensor)
        else v
        for k, v in batch.items()
    }
    # pdb.set_trace()

    df_step_dict = pd.DataFrame(step_dict)
    df_batch = pd.DataFrame(batch)

    df_step_dict = df_step_dict.sort_values(by="rewards")  # .head(alpha_N)
    df_batch = df_batch.sort_values(by="rewards")  # .head(alpha_N).drop('rewards', axis=1)
    batch = df_batch.to_dict(orient="list")
    # line #996 in ppo_trainer already does response_masks_batch[j] = torch.cat((torch.zeros_like(query_batch[j]), response_masks_batch[j]))[1:]
    # pdb.set_trace()
    # mask = [torch.cat((torch.ones_like(t, dtype =int), torch.zeros([48 - t.shape[0]], dtype =int))) for t in response_tensors]  #torch.zeros([64], dtype =int),
    mask = [torch.ones([32], dtype=int) for _ in response_tensors]  # torch.zeros([64], dtype =int),
    for ii in range(args.ppo_config.batch_size - alpha_N):
        mask[-ii - 1] = torch.zeros([32], dtype=int)

    sr_query_tensors = list(map(lambda t: t.to(device), df_step_dict["query_t"].tolist()))
    sr_response_tensors = list(map(lambda t: t.to(device), df_step_dict["resp_t"].tolist()))
    sr_rewards = list(map(lambda t: t.to(device), df_step_dict["rewards"].tolist()))
    batch = {
        k: [t.to(device) for t in v]
        if isinstance(v, list) and all(isinstance(t, torch.Tensor) for t in v)
        else v.to(device)
        if isinstance(v, torch.Tensor)
        else v
        for k, v in batch.items()
    }

    return batch, sr_query_tensors, sr_response_tensors, sr_rewards, mask


for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)

    if args.reward_function == "trl":
        rewards = [torch.tensor(-output[0]["score"]) for output in pipe_outputs]
    else:  # depreciated
        raise NotImplementedError
        rewards = [
            torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"]]))[1]
            for output in pipe_outputs
        ]

    for ii in range(3):
        data_row = {
            "iter": epoch,
            "input": batch["query"][ii],
            "output": batch["response"][ii],
            "score": rewards[ii].item(),
        }
        text_train_data.append(data_row)

    batch["rewards"] = rewards
    alpha_N_ceil = get_current_risk_level(PPOConfig.soft_risk_alpha, PPOConfig.risk_level_scheduler, epoch, 264)
    # sr_batch, sr_query_t, sr_resp_t, sr_rewards, mask = apply_risk_modification_to_batch(batch, query_tensors, response_tensors, rewards, alpha_N_ceil)

    # Run PPO step
    print(alpha_N_ceil)
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards, alpha_N_ceil)
    ppo_trainer.log_stats(stats, batch, rewards)
    print(epoch)

ppo_trainer._save_pretrained(logging_dir + "/save_pretrained")
df = pd.DataFrame(text_train_data)
df.to_csv(logging_dir + "/textual_data.csv", index=False)

File Path: examples/dpo.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# 0. imports
from dataclasses import dataclass, field
from typing import Dict, Optional

import torch
from datasets import Dataset, load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, HfArgumentParser, TrainingArguments

from trl import DPOTrainer


# Define and parse arguments.
@dataclass
class ScriptArguments:
    """
    The arguments for the DPO training script.
    """

    # data parameters
    beta: Optional[float] = field(default=0.1, metadata={"help": "the beta parameter for DPO loss"})

    # training parameters
    model_name_or_path: Optional[str] = field(default="gpt2", metadata={"help": "the model name"})
    learning_rate: Optional[float] = field(default=1e-3, metadata={"help": "optimizer learning rate"})
    per_device_train_batch_size: Optional[int] = field(default=4, metadata={"help": "batch size per device"})
    gradient_accumulation_steps: Optional[int] = field(
        default=1, metadata={"help": "the number of gradient accumulation steps"}
    )
    max_length: Optional[int] = field(default=512, metadata={"help": "max length of each sample"})
    max_prompt_length: Optional[int] = field(default=128, metadata={"help": "max length of each sample's prompt"})
    max_target_length: Optional[int] = field(
        default=128, metadata={"help": "Only used for encoder decoder model. Max target of each sample's prompt"}
    )
    label_pad_token_id: Optional[int] = field(default=-100, metadata={"help": "label for non response tokens"})
    max_steps: Optional[int] = field(default=1000, metadata={"help": "max number of training steps"})
    # instrumentation
    sanity_check: Optional[bool] = field(default=True, metadata={"help": "only train on 1000 samples"})
    report_to: Optional[str] = field(
        default=None,
        metadata={
            "help": 'The list of integrations to report the results and logs to. Supported platforms are `"azure_ml"`,'
            '`"comet_ml"`, `"mlflow"`, `"neptune"`, `"tensorboard"`,`"clearml"` and `"wandb"`. '
            'Use `"all"` to report to all integrations installed, `"none"` for no integrations.'
        },
    )
    # debug argument for distributed training
    ignore_bias_buffers: Optional[bool] = field(
        default=False,
        metadata={
            "help": "fix for DDP issues with LM bias/mask buffers - invalid scalar type,`inplace operation. See"
            "https://github.com/huggingface/transformers/issues/22482#issuecomment-1595790992"
        },
    )


def extract_anthropic_prompt(prompt_and_response):
    """Extract the anthropic prompt from a prompt and response pair."""
    search_term = "\n\nAssistant:"
    search_term_idx = prompt_and_response.rfind(search_term)
    assert search_term_idx != -1, f"Prompt and response does not contain '{search_term}'"
    return prompt_and_response[: search_term_idx + len(search_term)]


def get_hh(split: str, sanity_check: bool = False, silent: bool = False, cache_dir: str = None) -> Dataset:
    """Load the Anthropic Helpful-Harmless dataset from Hugging Face and convert it to the necessary format.

    The dataset is converted to a dictionary with the following structure:
    {
        'prompt': List[str],
        'chosen': List[str],
        'rejected': List[str],
    }

    Prompts should be structured as follows:
      \n\nHuman: <prompt>\n\nAssistant:
    Multiple turns are allowed, but the prompt should always start with \n\nHuman: and end with \n\nAssistant:.
    """
    dataset = load_dataset("Anthropic/hh-rlhf", split=split, cache_dir=cache_dir)
    if sanity_check:
        dataset = dataset.select(range(min(len(dataset), 1000)))

    def split_prompt_and_responses(sample) -> Dict[str, str]:
        prompt = extract_anthropic_prompt(sample["chosen"])
        return {
            "prompt": prompt,
            "chosen": sample["chosen"][len(prompt) :],
            "rejected": sample["rejected"][len(prompt) :],
        }

    return dataset.map(split_prompt_and_responses)


if __name__ == "__main__":
    parser = HfArgumentParser(ScriptArguments)
    script_args = parser.parse_args_into_dataclasses()[0]

    # 1. load a pretrained model
    model = AutoModelForCausalLM.from_pretrained(script_args.model_name_or_path)

    if script_args.ignore_bias_buffers:
        # torch distributed hack
        model._ddp_params_and_buffers_to_ignore = [
            name for name, buffer in model.named_buffers() if buffer.dtype == torch.bool
        ]

    model_ref = AutoModelForCausalLM.from_pretrained(script_args.model_name_or_path)

    tokenizer = AutoTokenizer.from_pretrained(script_args.model_name_or_path)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # 2. Load the Anthropic Helpful-Harmless dataset
    train_dataset = get_hh("train", sanity_check=script_args.sanity_check)

    # 3. Load evaluation dataset
    eval_dataset = get_hh("test", sanity_check=script_args.sanity_check)

    # 4. initialize training arguments:
    training_args = TrainingArguments(
        per_device_train_batch_size=script_args.per_device_train_batch_size,
        max_steps=script_args.max_steps,
        remove_unused_columns=False,
        gradient_accumulation_steps=script_args.gradient_accumulation_steps,
        learning_rate=script_args.learning_rate,
        evaluation_strategy="steps",
        logging_first_step=True,
        logging_steps=10,  # match results in blog post
        eval_steps=500,
        output_dir="./test",
        report_to=script_args.report_to,
    )

    # 5. initialize the DPO trainer
    dpo_trainer = DPOTrainer(
        model,
        model_ref,
        args=training_args,
        beta=script_args.beta,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        tokenizer=tokenizer,
        max_length=script_args.max_length,
        max_target_length=script_args.max_target_length,
        max_prompt_length=script_args.max_prompt_length,
    )

    # 6. train
    dpo_trainer.train()

File Path: examples/hello_world.py
Content:
# 0. imports
import torch
from transformers import GPT2Tokenizer

from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer


# 1. load a pretrained model
model = AutoModelForCausalLMWithValueHead.from_pretrained("gpt2")
model_ref = AutoModelForCausalLMWithValueHead.from_pretrained("gpt2")
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token

# 2. initialize trainer
ppo_config = {"batch_size": 1}
config = PPOConfig(**ppo_config)
ppo_trainer = PPOTrainer(config, model, model_ref, tokenizer)

# 3. encode a query
query_txt = "This morning I went to the "
query_tensor = tokenizer.encode(query_txt, return_tensors="pt").to(model.pretrained_model.device)

# 4. generate model response
generation_kwargs = {
    "min_length": -1,
    "top_k": 0.0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": 20,
}
response_tensor = ppo_trainer.generate([item for item in query_tensor], return_prompt=False, **generation_kwargs)
response_txt = tokenizer.decode(response_tensor[0])

# 5. define a reward for response
# (this could be any reward such as human feedback or output from another model)
reward = [torch.tensor(1.0, device=model.pretrained_model.device)]

# 6. train model with ppo
train_stats = ppo_trainer.step([query_tensor[0]], [response_tensor[0]], reward)

File Path: examples/llama/evaluation/evaluate_perplexity.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed
from trl.core import LengthSampler


tqdm.pandas()

import pdb
import pickle
import datetime
import os
from evaluate import load
from transformers import AutoModel, AutoTokenizer, AutoConfig
from transformers import OpenAIGPTTokenizer, OpenAIGPTModel
import random

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
print(date_n_time)
logging_dir = f"/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/eval/{date_n_time}"
os.makedirs(logging_dir)

@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name="lvwerra/gpt2-imdb",
            query_dataset="imdb",
            reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            remove_unused_columns=False
        )
    )
    query_dataset: str = field(default="imdb", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="less", metadata={"help": "the dataset to query"})


args = tyro.cli(ScriptArguments)

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead

# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8, data_split = 'train'):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # load imdb with datasets
    # to load partial dataset, just change data split to split=data_split+'[:200]'
    ds = load_dataset(query_dataset, split=data_split)
    ds = ds.rename_columns({"text": "review"})
    ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)

    input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# get test dataset
test_dataset = build_dataset(args.ppo_config, args.query_dataset, data_split='test')
# Take 5k random samples
sample_size = 5000
random_indices = random.sample(range(len(test_dataset)), sample_size)
test_dataset = test_dataset.select(random_indices)

def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])

# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

zero_shot_model = trl_model_class.from_pretrained(
    'gpt2',
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)

sft_model = trl_model_class.from_pretrained(
    'lvwerra/gpt2-imdb',
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)

# For the following to work I should have saved the model using huggingface
# ppo_model = trl_model_class.from_pretrained(
#     '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-09-26_11-27-23/model.pt',
#     trust_remote_code=True,
#     device_map=device_map,
#     peft_config=peft_config,
# )

sft_ppo_model = trl_model_class.from_pretrained(
    '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/trl_sft_ppo/2023-10-23_23-30-47',
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)
"""
ppo_model_PATH = '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-09-26_11-27-23/model.pt'
ppo_model = torch.load(ppo_model_PATH)  

sft_ppo_model_PATH = '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_ppo/2023-09-26_10-36-41/model.pt'
sft_ppo_model = torch.load(sft_ppo_model_PATH)
"""
tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"
tokenizer.truncation_side = "left"
tokenizer.pad_token_as_eos_token = True
tokenizer.max_length = 64

# Define all the tester classes 
zero_shot_tester = PPOTrainer(args.ppo_config, zero_shot_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)
sft_tester = PPOTrainer(args.ppo_config, sft_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)
# ppo_tester = PPOTrainer(args.ppo_config, ppo_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)
sft_ppo_tester = PPOTrainer(args.ppo_config, sft_ppo_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)

#pdb.set_trace()
#ppo_tester._save_pretrained('/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-09-26_11-27-23')
#sft_ppo_tester._save_pretrained('/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_ppo/2023-09-26_10-36-41')

# Build the perplexity evaluation pipeline
perplexity = load("perplexity", module_type="measurement")

# DO I NEED TO TAKE CARE OF PAD TOKEN FOR PERPLEXITY PIPELINE AS WELL??
"""
# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id
"""

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs = {
    "min_length": 48,
    "top_k": 50,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": 48,
}


#date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
#os.makedirs(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/evaluation/{date_n_time}')

model_names_all = ['zero_shot_model', 'sft_model', 'ppo_model', 'sft_ppo_model']
testers_all = [zero_shot_tester, sft_tester, sft_ppo_tester] #ppo_tester,

# infer on test dataset
#test_scores_1, test_scores_2, test_scores_3, test_scores_4 = [], [], [], []
test_scores_all = [[], [], []]

for epoch, batch in tqdm(enumerate(zero_shot_tester.dataloader)):
    query_tensors = batch["input_ids"]
    for ii, tester in enumerate(testers_all):
        #pdb.set_trace()
        # Get response from the current model
        #response_tensors = tester.generate(query_tensors, return_prompt=False, **generation_kwargs)
        #batch["response"] = tokenizer.batch_decode(response_tensors)

        #print(f'model: {model_names_all[ii]}\n\n input:{batch["query"][:3]}, output:{batch["response"][:3]} \n\n')

        # Compute sentiment score
        
        #texts = [q + r for q, r in zip(batch["query"], batch["response"])]
        texts = batch["review"]
        texts = [text[:1000] for text in texts]
        #texts = ["lorem ipsum", "Happy Birthday!", "Bienvenue"]

        if ii == 0:
            perplexity_scores = perplexity.compute(data=texts, model_id='gpt2')['perplexities']
        elif ii == 1:
            perplexity_scores = perplexity.compute(data=texts, model_id='lvwerra/gpt2-imdb')['perplexities']
        elif ii == 2:
            perplexity_scores = perplexity.compute(data=texts, model_id='/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/trl_sft_ppo/2023-10-23_23-30-47')['perplexities']

        test_scores_all[ii].append(sum(perplexity_scores)/len(perplexity_scores))
        print(f'Epoch: {epoch} - Perplexity for {model_names_all[ii]}: {sum(perplexity_scores)/len(perplexity_scores)}')
        


# print and save the final score  
final_test_scores = [sum(test_scores)/len(test_scores) for test_scores in test_scores_all] 
print(f'Total test perelxity-scores for zero-shot, sft, ppo, sft-ppo are: {final_test_scores}')
#pickle.dump(final_test_score.item(), open(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_ppo/{date_n_time}/final_test_score', "wb"))



    





File Path: examples/llama/evaluation/evaluate_sentiment.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed
from trl.core import LengthSampler


tqdm.pandas()

import pdb
import pickle
import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import statistics
import random

@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name="gpt2",
            query_dataset="imdb",
            reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
        )
    )
    query_dataset: str = field(default="imdb", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )


args = tyro.cli(ScriptArguments)


# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8, data_split = 'train'):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # load imdb with datasets
    # to load partial dataset, just change data split to split=data_split+'[:200]'
    ds = load_dataset(query_dataset, split=data_split)
    ds = ds.rename_columns({"text": "review"})
    ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)

    def input_size():
        return 64 #LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)

# build and sample test dataset
test_dataset = build_dataset(args.ppo_config, args.query_dataset, data_split='test')
# Take 5k random samples
sample_size = 5000
random_indices = random.sample(range(len(test_dataset)), sample_size)
test_dataset = test_dataset.select(random_indices)

def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_positive_1/oct_2_afternoon/checkpoint-120',
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.padding_side = "left"
tokenizer.truncation_side = "left"
tokenizer.pad_token_as_eos_token = True
tokenizer.max_length = 64

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)
# Creating a tester object here because dataloader is only available in the PPOTrainer class
ppo_tester = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
task, model_name = args.ppo_config.reward_model.split(":")
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs = {
    "min_length": 48,
    "top_k": 50,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": 48,
}


# infer on test dataset
test_scores = []
for epoch, batch in tqdm(enumerate(ppo_tester.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
    #rewards = [torch.tensor(output[1]["score"]) for output in pipe_outputs]
    rewards = [
        torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"]]))[1]
        for output in pipe_outputs
    ]

    test_scores.append(sum(rewards)/len(rewards))
    print(sum(rewards)/len(rewards))

# print and save the final score  
date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
os.makedirs(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/zero_shot/{date_n_time}')
final_test_score = sum(test_scores)/len(test_scores)     
test_scores_list = [t.item() for t in test_scores]
print(f'Total test sentiment-score and std is: {final_test_score.item(), statistics.stdev(test_scores_list)}')
pickle.dump(final_test_score.item(), open(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/zero_shot/{date_n_time}/final_test_score', "wb"))




File Path: examples/llama/evaluation/single_model_generic.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed
from trl.core import LengthSampler


tqdm.pandas()

import pdb
import pickle
import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import statistics
import random
from evaluate import load

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
logging_dir = f"/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/eval_generic/{date_n_time}"
os.makedirs(logging_dir)
import pandas as pd

@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name="gpt2",
            query_dataset="imdb",
            reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            #log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            exp_name="ppo",
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
        )
    )
    query_dataset: str = field(default="imdb", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="more", metadata={"help": "the dataset to query"})
    
    generation_kwargs_min_length: Optional[int] = field(default=-1, metadata={"help": "minimum number of tokens while generation"})
    generation_kwargs_top_k: Optional[int] = field(default=0, metadata={"help": "gneration top k"})
    generation_kwargs_top_p: Optional[float] = field(default=1.0, metadata={"help": "gneration top p"})
    generation_kwargs_max_new_tokens: Optional[int] = field(default=32, metadata={"help": "gneration top p"})

    change_tokenizer_args: bool = field(default=False, metadata={"help": "whether to use modify tokenizer settings"})
    tokenizer_kwargs: dict = field(
    default_factory=lambda: {
      "padding_side": "left", 
      "truncation_side": "left",
      "pad_token_as_eos_token": True,
      "max_length": 64
    }
  )


args = tyro.cli(ScriptArguments)

generation_kwargs = {
      "min_length": args.generation_kwargs_min_length, 
      "top_k": args.generation_kwargs_top_k,
      "top_p": args.generation_kwargs_top_p,
      "do_sample": True,
      "max_new_tokens": args.generation_kwargs_max_new_tokens
    }

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8, data_split = 'train'):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # load imdb with datasets
    # to load partial dataset, just change data split to split=data_split+'[:200]'
    ds = load_dataset(query_dataset, split=data_split)
    ds = ds.rename_columns({"text": "review"})
    ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)

    if args.input_query_size == "more":
        def input_size():
            return 64 
    else: 
        input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
#dataset = build_dataset(args.ppo_config, args.query_dataset)

# build and sample test dataset
test_dataset = build_dataset(args.ppo_config, args.query_dataset, data_split='test')
# Take 5k random samples
sample_size = 640 #5000
random_indices = random.sample(range(len(test_dataset)), sample_size)
test_dataset = test_dataset.select(random_indices)

def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}


model = trl_model_class.from_pretrained(
    args.ppo_config.model_name,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)

#'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_positive_1/oct_2_afternoon/checkpoint-120'
# ppo_model_PATH = '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-10-16_19-15-40/model.pt'
# model = torch.load(ppo_model_PATH) 

tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id
if args.change_tokenizer_args == True: 
    tokenizer.padding_side = args.tokenizer_kwargs['padding_side']
    tokenizer.truncation_side = args.tokenizer_kwargs['truncation_side']
    tokenizer.pad_token_as_eos_token = args.tokenizer_kwargs['pad_token_as_eos_token']
    tokenizer.max_length = args.tokenizer_kwargs['max_length']

#pdb.set_trace()
# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
#ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)
# Creating a tester object here because dataloader is only available in the PPOTrainer class
ppo_tester = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_tester.accelerator.device
if ppo_tester.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_tester.accelerator.state.deepspeed_plugin
task, model_name = args.ppo_config.reward_model.split(":")
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs = {
    "min_length": 48,
    "top_k": 50,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": 48,
}

# empty list to store text data
text_test_data = []

# Build the perplexity evaluation pipeline
perplexity = load("perplexity", module_type="metric")

# infer on test dataset
test_scores = []
perplexity_scores_all = []

for epoch, batch in tqdm(enumerate(ppo_tester.dataloader)):
    #pdb.set_trace()
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_tester.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    print(f'model: input:{batch["query"][:3]}, output:{batch["response"][:3]} \n\n')

    #pdb.set_trace()
    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
    
    # if args.reward_function == "trl":
    #     rewards = [torch.tensor(output[1]["score"]) for output in pipe_outputs]
    # else: 
        # pdb.set_trace()
    rewards = [
        torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"]]))[1]
        for output in pipe_outputs
    ]
    
    #pdb.set_trace()
    #perplexity_scores = perplexity.compute(predictions=texts, model_id=args.ppo_config.model_name)['perplexities']
    #perplexity_scores_all.append(sum(perplexity_scores)/len(perplexity_scores))

    for ii in range(3): 
        data_row = {'iter': epoch, 'input':batch["query"][ii], 'output':batch["response"][ii], 'score':rewards[ii].item()}
        text_test_data.append(data_row)
    
    test_scores.extend(rewards)

    #test_scores.append(sum(rewards)/len(rewards))
    print(sum(rewards)/len(rewards))

# print and save the final score  
final_test_score = sum(test_scores)/len(test_scores)     
test_scores_list = [t.item() for t in test_scores]
print(f'Total test sentiment-score and std is: {final_test_score.item(), statistics.stdev(test_scores_list)}')
pickle.dump(final_test_score.item(), open(logging_dir+'final_test_score', "wb"))

df = pd.DataFrame(text_test_data)
df.to_csv(logging_dir+'/textual_data.csv', index=False)
#print('perplexity:', sum(perplexity_scores_all)/len(perplexity_scores_all))


File Path: examples/llama/ploting/plot_score.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed
from trl.core import LengthSampler


tqdm.pandas()
import matplotlib.pyplot as plt
import itertools

import pdb
import pickle
import datetime
import os
from evaluate import load
from transformers import AutoModel, AutoTokenizer, AutoConfig
from transformers import OpenAIGPTTokenizer, OpenAIGPTModel
import random

import pandas as pd

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
print(date_n_time)
logging_dir = f"/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/plot_score/{date_n_time}"
os.makedirs(logging_dir)

@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name="lvwerra/gpt2-imdb",
            query_dataset="imdb",
            reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            remove_unused_columns=False
        )
    )
    query_dataset: str = field(default="imdb", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="less", metadata={"help": "the dataset to query"})


args = tyro.cli(ScriptArguments)

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead

# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8, data_split = 'train'):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # load imdb with datasets
    # to load partial dataset, just change data split to split=data_split+'[:200]'
    ds = load_dataset(query_dataset, split=data_split)
    ds = ds.rename_columns({"text": "review"})
    ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)

    input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# get test dataset
test_dataset = build_dataset(args.ppo_config, args.query_dataset, data_split='train')
# Take 5k random samples
sample_size = 2500
random_indices = random.sample(range(len(test_dataset)), sample_size)
test_dataset = test_dataset.select(random_indices)

#pdb.set_trace()

def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])

# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

zero_shot_model = trl_model_class.from_pretrained(
    'gpt2',
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)

sft_model = trl_model_class.from_pretrained(
    'lvwerra/gpt2-imdb',
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)

# For the following to work I should have saved the model using huggingface
# ppo_model = trl_model_class.from_pretrained(
#     '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-09-26_11-27-23/model.pt',
#     trust_remote_code=True,
#     device_map=device_map,
#     peft_config=peft_config,
# )

sft_ppo_model = trl_model_class.from_pretrained(
    '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/trl_sft_ppo/2023-10-23_23-30-47',
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)
"""
ppo_model_PATH = '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-09-26_11-27-23/model.pt'
ppo_model = torch.load(ppo_model_PATH)  

sft_ppo_model_PATH = '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_ppo/2023-09-26_10-36-41/model.pt'
sft_ppo_model = torch.load(sft_ppo_model_PATH)
"""
tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"
tokenizer.truncation_side = "left"
tokenizer.pad_token_as_eos_token = True
tokenizer.max_length = 64

# Define all the tester classes 
zero_shot_tester = PPOTrainer(args.ppo_config, zero_shot_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)
sft_tester = PPOTrainer(args.ppo_config, sft_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)
# ppo_tester = PPOTrainer(args.ppo_config, ppo_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)
sft_ppo_tester = PPOTrainer(args.ppo_config, sft_ppo_model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)

#pdb.set_trace()
#ppo_tester._save_pretrained('/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/2023-09-26_11-27-23')
#sft_ppo_tester._save_pretrained('/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_ppo/2023-09-26_10-36-41')

# Build the perplexity evaluation pipeline
perplexity = load("perplexity", module_type="measurement")

# DO I NEED TO TAKE CARE OF PAD TOKEN FOR PERPLEXITY PIPELINE AS WELL??
"""
# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id
"""
device = sft_tester.accelerator.device
if sft_tester.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = sft_tester.accelerator.state.deepspeed_plugin
task, model_name = args.ppo_config.reward_model.split(":")
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id


# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs = {
    "min_length": 48,
    "top_k": 50,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": 48,
}


#date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
#os.makedirs(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/evaluation/{date_n_time}')

model_names_all = ['zero_shot_model', 'sft_model', 'ppo_model', 'sft_ppo_model']
testers_all = [zero_shot_tester, sft_tester, sft_ppo_tester] #ppo_tester,

# infer on test dataset
#test_scores_1, test_scores_2, test_scores_3, test_scores_4 = [], [], [], []
positive_score_all = []
negative_score_all = []
label_all = []

for epoch, batch in tqdm(enumerate(zero_shot_tester.dataloader)):
    query_tensors = batch["input_ids"]
    
    texts = batch["review"]
    texts = [text[:1000] for text in texts]
    #texts = ["lorem ipsum", "Happy Birthday!", "Bienvenue"]

    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
    #pdb.set_trace()
    positive_score_all.append([output[1]["score"] for output in pipe_outputs])
    negative_score_all.append([output[0]["score"] for output in pipe_outputs])
    label_all.append([label.item() for label in batch["label"]])

    #test_scores_all[ii].append(sum(perplexity_scores)/len(perplexity_scores))
    #print(f'Epoch: {epoch} - Perplexity for {model_names_all[ii]}: {sum(perplexity_scores)/len(perplexity_scores)}')
        

# get flattened lists 
pos_scores = list(itertools.chain(*positive_score_all))
neg_scores = list(itertools.chain(*negative_score_all))
labels = list(itertools.chain(*label_all))

# plot positive and negative score and save the plot
df = pd.DataFrame(list(zip(neg_scores, pos_scores, labels)), columns = ['neg score', 'pos score', 'label'])
ax = df.plot.scatter(x='neg score', y='pos score', c='label', colormap='viridis')
#fig = plt.scatter(x=df['neg score'], y=df['pos score'], c=df['label'], colormap='viridis')
ax.grid(True)
fig = ax.get_figure()
fig.savefig('scatter_plot.png')

# Plot histograms
fig2, axs2 = plt.subplots(1, 2, figsize=(10,5))

df[df['label'] == 0]['neg score'].plot.hist(ax=axs2[0], bins=10, alpha=0.5, label='Class 0')
df[df['label'] == 1]['neg score'].plot.hist(ax=axs2[0], bins=10, alpha=0.5, label='Class 1')
axs2[0].set_title('Negative Score Distribution')
axs2[0].legend()

df[df['label'] == 0]['pos score'].plot.hist(ax=axs2[1], bins=10, alpha=0.5, label='Class 0') 
df[df['label'] == 1]['pos score'].plot.hist(ax=axs2[1], bins=10, alpha=0.5, label='Class 1')
axs2[1].set_title('Positive Score Distribution')
axs2[1].legend()

fig2.savefig('histograms_plot.png')
File Path: examples/llama/training/hf_example.py
Content:
# load the base model in 4-bit quantization
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)

base_model = AutoModelForCausalLM.from_pretrained(
    script_args.model_name,        # "meta-llama/Llama-2-7b-hf"
    quantization_config=bnb_config,
    device_map={"": 0},
    trust_remote_code=True,
    use_auth_token=True,
)
base_model.config.use_cache = False

# add LoRA layers on top of the quantized base model
peft_config = LoraConfig(
    r=script_args.lora_r,
    lora_alpha=script_args.lora_alpha,
    lora_dropout=script_args.lora_dropout,
    target_modules=["q_proj", "v_proj"],
    bias="none",
    task_type="CAUSAL_LM",
)
...
trainer = SFTTrainer(
    model=base_model,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    peft_config=peft_config,
    packing=True,
    max_seq_length=None,
    tokenizer=tokenizer,
    args=training_args,         # HF Trainer arguments
)
trainer.train()

File Path: examples/llama/training/lm_sft_auth1.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoModelForCausalLM, BitsAndBytesConfig, HfArgumentParser, TrainingArguments

from trl import SFTTrainer


tqdm.pandas()

import pdb
from transformers import GenerationConfig
from transformers import AutoTokenizer, pipeline, Seq2SeqTrainingArguments
import datetime
import os
from trl import set_seed

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
logging_dir = f"/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_auth1/{date_n_time}"
os.makedirs(logging_dir)

# Define and parse arguments.
@dataclass
class ScriptArguments:
    """
    The name of the Casual LM model we wish to fine-tune with SFTTrainer
    """
    #EleutherAI/pythia-2.8b
    model_name: Optional[str] = field(default="meta-llama/Llama-2-7b-hf", metadata={"help": "the model name"})
    dataset_name: Optional[str] = field(
        default="imdb", metadata={"help": "the dataset name"}
    )
    dataset_text_field: Optional[str] = field(default="text", metadata={"help": "the text field of the dataset"})
    #log_with: Optional[str] = field(default="tensorboard", metadata={"help": "use 'wandb' to log with wandb"})
    log_with: Optional[str] = field(default="none", metadata={"help": "use 'wandb' to log with wandb"})

    # learning params
    learning_rate: Optional[float] = field(default=0.00001, metadata={"help": "the learning rate"}) #default=1.41e-5
    batch_size: Optional[int] = field(default=8, metadata={"help": "the batch size"})
    # Using tokenizer max length=64 from RL4LLMs Table 4
    seq_length: Optional[int] = field(default=64, metadata={"help": "Input sequence length"}) #default=512
    gradient_accumulation_steps: Optional[int] = field(
        default=16, metadata={"help": "the number of gradient accumulation steps"}
    )
    num_train_epochs: Optional[int] = field(default=10, metadata={"help": "the number of training epochs"}) #default=3
    max_steps: Optional[int] = field(default=-1, metadata={"help": "the number of training steps"})

    # model download, quantization and peft params
    trust_remote_code: Optional[bool] = field(default=True, metadata={"help": "Enable `trust_remote_code`"})
    use_auth_token: Optional[bool] = field(default=True, metadata={"help": "Use HF auth token to access the model"})
    load_in_8bit: Optional[bool] = field(default=False, metadata={"help": "load the model in 8 bits precision"})
    load_in_4bit: Optional[bool] = field(default=False, metadata={"help": "load the model in 4 bits precision"})
    use_peft: Optional[bool] = field(default=False, metadata={"help": "Wether to use PEFT or not to train adapters"})
    peft_lora_r: Optional[int] = field(default=64, metadata={"help": "the r parameter of the LoRA adapters"})
    peft_lora_alpha: Optional[int] = field(default=16, metadata={"help": "the alpha parameter of the LoRA adapters"})

    # logging
    output_dir: Optional[str] = field(default=logging_dir, metadata={"help": "the output directory"})
    logging_steps: Optional[int] = field(default=1, metadata={"help": "the number of logging steps"})
    save_steps: Optional[int] = field(
        default=100, metadata={"help": "Number of updates steps before two checkpoint saves"}
    )
    save_total_limit: Optional[int] = field(default=10, metadata={"help": "Limits total number of checkpoints."})

    # post processing
    push_to_hub: Optional[bool] = field(default=False, metadata={"help": "Push the model to HF Hub"})
    hub_model_id: Optional[str] = field(default=None, metadata={"help": "The name of the model on HF Hub"})

    # added by auth1
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="less", metadata={"help": "the dataset to query"})
    generation_kwargs_min_length: Optional[int] = field(default=-1, metadata={"help": "minimum number of tokens while generation"})
    generation_kwargs_top_k: Optional[int] = field(default=0, metadata={"help": "gneration top k"})
    generation_kwargs_top_p: Optional[float] = field(default=1.0, metadata={"help": "gneration top p"})
    generation_kwargs_max_new_tokens: Optional[int] = field(default=32, metadata={"help": "gneration top p"})
    
    change_tokenizer_args: bool = field(default=False, metadata={"help": "whether to use modify tokenizer settings"})
    tokenizer_kwargs: dict = field(
    default_factory=lambda: {
      "padding_side": "left", 
      "truncation_side": "left",
      "pad_token_as_eos_token": True,
      "max_length": 64
    }
  )
    exp_name: str = field(default="sft-positive", metadata={"help": "experiment name"}) 
    seed: Optional[int] = field(default=0, metadata={"help": "seed"})


parser = HfArgumentParser(ScriptArguments)
script_args = parser.parse_args_into_dataclasses()[0]

set_seed(script_args.seed)

generation_kwargs = {
      "min_length": script_args.generation_kwargs_min_length, 
      "top_k": script_args.generation_kwargs_top_k,
      "top_p": script_args.generation_kwargs_top_p,
      "do_sample": True,
      "max_new_tokens": script_args.generation_kwargs_max_new_tokens
    }

# Step 1: Load the model
if script_args.load_in_8bit and script_args.load_in_4bit:
    raise ValueError("You can't load the model in 8 bits and 4 bits at the same time")
elif script_args.load_in_8bit or script_args.load_in_4bit:
    quantization_config = BitsAndBytesConfig(
        load_in_8bit=script_args.load_in_8bit, load_in_4bit=script_args.load_in_4bit
    )
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}
    torch_dtype = torch.bfloat16
else:
    device_map = None
    quantization_config = None
    torch_dtype = None

model = AutoModelForCausalLM.from_pretrained(
    script_args.model_name,
    quantization_config=quantization_config,
    device_map=device_map,
    trust_remote_code=script_args.trust_remote_code,
    torch_dtype=torch_dtype)

# Removing huggingface authorization token param from model loading command above 
#    use_auth_token=script_args.use_auth_token,
# Make sure you have logged in to the Hugging Face Hub using huggingface-cli login 
# or by calling huggingface_hub.login() in Python. This will save an authentication 
# token that Transformers will pick up automatically.

# Step 2: Load the dataset
dataset = load_dataset(script_args.dataset_name, split="train")

# Filter dataset for only positive reviews
dataset = dataset.filter(lambda x: x["label"] == 1, batched=False)


# Step 3: Define generation arguments
# generation_kwargs = GenerationConfig(
#     min_length = 48,
#     top_k = 50,
#     top_p = 1.0,
#     do_sample = True,
#     max_new_tokens = 48,
# )
#pad_token_id = tokenizer.eos_token_id,

# Step 3: Define the training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir=script_args.output_dir,
    per_device_train_batch_size=script_args.batch_size,
    gradient_accumulation_steps=script_args.gradient_accumulation_steps,
    learning_rate=script_args.learning_rate,
    logging_steps=script_args.logging_steps,
    num_train_epochs=script_args.num_train_epochs,
    max_steps=script_args.max_steps,
    report_to=script_args.log_with,
    save_steps=script_args.save_steps,
    save_total_limit=script_args.save_total_limit,
    push_to_hub=script_args.push_to_hub,
    hub_model_id=script_args.hub_model_id,
    generation_config=generation_kwargs
)

# Step 4: Define the LoraConfig
if script_args.use_peft:
    peft_config = LoraConfig(
        r=script_args.peft_lora_r,
        lora_alpha=script_args.peft_lora_alpha,
        bias="none",
        task_type="CAUSAL_LM",
    )
else:
    peft_config = None


# trl sft trainer default tokenizer and pad_token
"""
if tokenizer is None:
            tokenizer = AutoTokenizer.from_pretrained(model.config._name_or_path)
            if getattr(tokenizer, "pad_token", None) is None:
                tokenizer.pad_token = tokenizer.eos_token

if max_seq_length is None:
            # to overcome some issues with broken tokenizers
            max_seq_length = min(tokenizer.model_max_length, 1024)

            warnings.warn(
                f"You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to {max_seq_length}"
            )

"""

# Looks like padding_side = "right" by default
# Hence, defining our tokenizer here
tokenizer = AutoTokenizer.from_pretrained(script_args.model_name)
tokenizer.pad_token = tokenizer.eos_token

if script_args.change_tokenizer_args == True: 
    tokenizer.padding_side = script_args.tokenizer_kwargs['padding_side']
    tokenizer.truncation_side = script_args.tokenizer_kwargs['truncation_side']
    tokenizer.pad_token_as_eos_token = script_args.tokenizer_kwargs['pad_token_as_eos_token']
    tokenizer.max_length = script_args.tokenizer_kwargs['max_length']

# tokenizer.padding_side = "left"
# tokenizer.truncation_side = "left"
# tokenizer.pad_token_as_eos_token = True
# tokenizer.max_length = 64

# Step 5: Define the Trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    max_seq_length=script_args.seq_length,
    train_dataset=dataset,
    dataset_text_field=script_args.dataset_text_field,
    peft_config=peft_config,
    tokenizer=tokenizer
)

trainer.train()

# Step 6: Save the model
trainer.save_model(logging_dir)

File Path: examples/llama/training/old/ppo.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed
from trl.core import LengthSampler

tqdm.pandas()

import pdb
import pickle
import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
print(date_n_time)
logging_dir = f"/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/{date_n_time}"
os.makedirs(logging_dir)

@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name="gpt2",
            query_dataset="imdb",
            reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5, #1e-6, #
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1, #1280, #The number of gradient accumulation steps
            early_stopping=False,
            target_kl=6.0, #1.0,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            #steps= 128000, #64000,
            #init_kl_coef=0.1,
            #vf_coef=0.5,
            #ppo_epochs=5,
            #gamma=0.99, #discount factor = gamma? 
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            remove_unused_columns=False
        )
    )
    query_dataset: str = field(default="imdb", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )


args = tyro.cli(ScriptArguments)


# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8, data_split = 'train'):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # load imdb with datasets
    # to load partial dataset, just change data split to split=data_split+'[:200]'
    ds = load_dataset(query_dataset, split=data_split)
    ds = ds.rename_columns({"text": "review"})
    ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)

    def input_size():
        return 64 #LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)
test_dataset = build_dataset(args.ppo_config, args.query_dataset, data_split='test')

def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    args.ppo_config.model_name,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.padding_side = "left"
tokenizer.truncation_side = "left"
tokenizer.pad_token_as_eos_token = True
tokenizer.max_length = 64

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)
# Creating a tester object here because dataloader is only available in the PPOTrainer class
ppo_tester = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
task, model_name = args.ppo_config.reward_model.split(":")
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs = {
    "min_length": 48,
    "top_k": 50,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": 48,
}

# stat lists
obj_entropy = []
policy_entropy = []

mean_non_score_reward = []
mean_scores = []
std_scores = []

mean_returns = []
var_returns = []

for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]

    # including the line below to deal with sentiment_pipeline's tokenizer
    texts = [text[:512] if len(text) > 512 else text for text in texts]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
    rewards = [torch.tensor(output[1]["score"]) for output in pipe_outputs]
    
    """
    rewards = [
        torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"]]))[1]
        for output in pipe_outputs
    ]
    """
    # Run PPO step
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
    ppo_trainer.log_stats(stats, batch, rewards)
    
    # print stats
    print(f'input:{batch["query"][:3]}, output:{batch["response"][:3]} \n\n')
    
    obj_entropy.append(stats['objective/entropy'])
    policy_entropy.append(stats['ppo/policy/entropy'])
    mean_non_score_reward.append(stats['ppo/mean_non_score_reward'])
    mean_scores.append(stats['ppo/mean_scores'])
    std_scores.append(stats['ppo/std_scores'])
    mean_returns.append(stats['ppo/returns/mean'])
    var_returns.append(stats['ppo/returns/var'])

    print(f'epoch: {epoch}')
    mean_return = stats['ppo/returns/mean']
    mean_score = stats['ppo/mean_scores']
    print(f'mean_returns: {mean_return}, mean_score: {mean_score}')


# save the final model - maybe this will unnecessarily take memory? What is a good way to go about it? 

#torch.save(model, logging_dir +'/model.pt')
#ppo_trainer.create_model_card(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/{date_n_time}')
#ppo_trainer._save_pretrained(logging_dir)

# save the train stats 
#train_stats_lists = [obj_entropy, policy_entropy, mean_non_score_reward, mean_scores, std_scores, mean_returns, var_returns]
#train_stats_lists_names = ['obj_entropy', 'policy_entropy', 'mean_non_score_reward', 'mean_scores', 'std_scores', 'mean_returns', 'var_returns']
#for ii in range(len(train_stats_lists)):
#    pickle.dump(train_stats_lists[ii], open(logging_dir + f'/{train_stats_lists_names[ii]}', "wb"))


"""
# infer on test dataset
test_scores = []
for epoch, batch in tqdm(enumerate(ppo_tester.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
    rewards = [
        torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"]]))[1]
        for output in pipe_outputs
    ]

    test_scores.append(sum(rewards)/len(rewards))
    print(sum(rewards)/len(rewards))

# print and save the final score  
final_test_score = sum(test_scores)/len(test_scores)     
print(f'Total test sentiment-score is: {final_test_score.item()}')
pickle.dump(final_test_score.item(), open(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/{date_n_time}/final_test_score', "wb"))
"""

    





File Path: examples/llama/training/old/sft_lvwerra.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed
from trl.core import LengthSampler


tqdm.pandas()

import pdb
import pickle
import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import random
import statistics
from transformers import GPT2Config
import re

@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name="lvwerra/gpt2-imdb",
            query_dataset="imdb",
            reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
        )
    )
    query_dataset: str = field(default="imdb", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )


args = tyro.cli(ScriptArguments)


# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8, data_split = 'train'):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # load imdb with datasets
    # to load partial dataset, just change data split to split=data_split+'[:200]'
    ds = load_dataset(query_dataset, split=data_split)
    ds = ds.rename_columns({"text": "review"})
    ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)

    def input_size():
        return 64 #LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)

# build and sample test dataset
test_dataset = build_dataset(args.ppo_config, args.query_dataset, data_split='test')
# Take 5k random samples
sample_size = 5000
random_indices = random.sample(range(len(test_dataset)), sample_size)
test_dataset = test_dataset.select(random_indices)

def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

"""
model = trl_model_class.from_pretrained(
    args.ppo_config.model_name,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)
model = trl_model_class.from_pretrained(
    '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_positive/2023-10-03_00-17-00/pytorch_model.bin',
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)
"""

pdb.set_trace()
config = GPT2Config.from_json_file('/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_positive/2023-10-03_00-17-00/config.json')
#config = GPT2Config.from_pretrained('/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_positive/2023-10-03_00-17-00/config.json')
state_dict = torch.load('/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_positive/2023-10-03_00-17-00/pytorch_model.bin')

model = trl_model_class.from_config(config) 
model.load_state_dict(state_dict)
"""# Strip transformer. prefix from state dict 
new_state_dict = {}
for key, value in state_dict.items():
    new_key = key.replace("transformer.", "")
    new_state_dict[new_key] = value

# Remove 'lm_head' keys
cleaned_dict = {k: v for k, v in state_dict.items() if not re.search(r'lm_head.', k)} 

# Load state dict
model.load_state_dict(cleaned_dict)
"""

tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.padding_side = "left"
tokenizer.truncation_side = "left"
tokenizer.pad_token_as_eos_token = True
tokenizer.max_length = 64

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)
# Creating a tester object here because dataloader is only available in the PPOTrainer class
ppo_tester = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
task, model_name = args.ppo_config.reward_model.split(":")
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs = {
    "min_length": 48,
    "top_k": 50,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": 48,
}


# infer on test dataset
test_scores = []
for epoch, batch in tqdm(enumerate(ppo_tester.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_tester.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
    #rewards = [torch.tensor(output[1]["score"]) for output in pipe_outputs]
    rewards = [
        torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"]]))[1]
        for output in pipe_outputs
    ]

    test_scores.append(sum(rewards)/len(rewards))
    print(sum(rewards)/len(rewards))

# print and save the final score  
date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
os.makedirs(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_lvwerra/{date_n_time}')
final_test_score = sum(test_scores)/len(test_scores)     
test_scores_list = [t.item() for t in test_scores]
print(f'Total test sentiment-score and std is: {final_test_score.item(), statistics.stdev(test_scores_list)}')
pickle.dump([final_test_score.item(), statistics.stdev(test_scores_list)], open(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_lvwerra/{date_n_time}/final_test_score', "wb"))




File Path: examples/llama/training/old/sft_positive.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoModelForCausalLM, BitsAndBytesConfig, HfArgumentParser, TrainingArguments

from trl import SFTTrainer


tqdm.pandas()

import pdb
from transformers import GenerationConfig
from transformers import AutoTokenizer, pipeline, Seq2SeqTrainingArguments
import datetime
import os

# Define and parse arguments.
@dataclass
class ScriptArguments:
    """
    The name of the Casual LM model we wish to fine-tune with SFTTrainer
    """

    model_name: Optional[str] = field(default="gpt2", metadata={"help": "the model name"})
    dataset_name: Optional[str] = field(
        default="imdb", metadata={"help": "the dataset name"}
    )
    dataset_text_field: Optional[str] = field(default="text", metadata={"help": "the text field of the dataset"})
    log_with: Optional[str] = field(default="none", metadata={"help": "use 'wandb' to log with wandb"})
    
    # learning params
    learning_rate: Optional[float] = field(default=0.00001, metadata={"help": "the learning rate"}) #default=1.41e-5
    batch_size: Optional[int] = field(default=64, metadata={"help": "the batch size"})
    # Using tokenizer max length=64 from RL4LLMs Table 4
    seq_length: Optional[int] = field(default=64, metadata={"help": "Input sequence length"}) #default=512
    gradient_accumulation_steps: Optional[int] = field(
        default=16, metadata={"help": "the number of gradient accumulation steps"}
    )
    num_train_epochs: Optional[int] = field(default=10, metadata={"help": "the number of training epochs"}) #default=3
    max_steps: Optional[int] = field(default=-1, metadata={"help": "the number of training steps"})

    # model download, quantization and peft params
    trust_remote_code: Optional[bool] = field(default=True, metadata={"help": "Enable `trust_remote_code`"})
    use_auth_token: Optional[bool] = field(default=True, metadata={"help": "Use HF auth token to access the model"})
    load_in_8bit: Optional[bool] = field(default=False, metadata={"help": "load the model in 8 bits precision"})
    load_in_4bit: Optional[bool] = field(default=False, metadata={"help": "load the model in 4 bits precision"})
    use_peft: Optional[bool] = field(default=False, metadata={"help": "Wether to use PEFT or not to train adapters"})
    peft_lora_r: Optional[int] = field(default=64, metadata={"help": "the r parameter of the LoRA adapters"})
    peft_lora_alpha: Optional[int] = field(default=16, metadata={"help": "the alpha parameter of the LoRA adapters"})

    # logging
    output_dir: Optional[str] = field(default="/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_positive/", metadata={"help": "the output directory"})
    logging_steps: Optional[int] = field(default=1, metadata={"help": "the number of logging steps"})
    save_steps: Optional[int] = field(
        default=100, metadata={"help": "Number of updates steps before two checkpoint saves"}
    )
    save_total_limit: Optional[int] = field(default=10, metadata={"help": "Limits total number of checkpoints."})

    # post processing
    push_to_hub: Optional[bool] = field(default=False, metadata={"help": "Push the model to HF Hub"})
    hub_model_id: Optional[str] = field(default=None, metadata={"help": "The name of the model on HF Hub"})


parser = HfArgumentParser(ScriptArguments)
script_args = parser.parse_args_into_dataclasses()[0]

# Step 1: Load the model
if script_args.load_in_8bit and script_args.load_in_4bit:
    raise ValueError("You can't load the model in 8 bits and 4 bits at the same time")
elif script_args.load_in_8bit or script_args.load_in_4bit:
    quantization_config = BitsAndBytesConfig(
        load_in_8bit=script_args.load_in_8bit, load_in_4bit=script_args.load_in_4bit
    )
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}
    torch_dtype = torch.bfloat16
else:
    device_map = None
    quantization_config = None
    torch_dtype = None

model = AutoModelForCausalLM.from_pretrained(
    script_args.model_name,
    quantization_config=quantization_config,
    device_map=device_map,
    trust_remote_code=script_args.trust_remote_code,
    torch_dtype=torch_dtype)

# Removing huggingface authorization token param from model loading command above 
#    use_auth_token=script_args.use_auth_token,
# Make sure you have logged in to the Hugging Face Hub using huggingface-cli login 
# or by calling huggingface_hub.login() in Python. This will save an authentication 
# token that Transformers will pick up automatically.

# Step 2: Load the dataset
dataset = load_dataset(script_args.dataset_name, split="train")

# Filter dataset for only positive reviews
dataset = dataset.filter(lambda x: x["label"] == 1, batched=False)


# Step 3: Define generation arguments
generation_kwargs = GenerationConfig(
    min_length = 48,
    top_k = 50,
    top_p = 1.0,
    do_sample = True,
    max_new_tokens = 48,
)
#pad_token_id = tokenizer.eos_token_id,

# Step 3: Define the training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir=script_args.output_dir,
    per_device_train_batch_size=script_args.batch_size,
    gradient_accumulation_steps=script_args.gradient_accumulation_steps,
    learning_rate=script_args.learning_rate,
    logging_steps=script_args.logging_steps,
    num_train_epochs=script_args.num_train_epochs,
    max_steps=script_args.max_steps,
    report_to=script_args.log_with,
    save_steps=script_args.save_steps,
    save_total_limit=script_args.save_total_limit,
    push_to_hub=script_args.push_to_hub,
    hub_model_id=script_args.hub_model_id,
    generation_config=generation_kwargs
)

# Step 4: Define the LoraConfig
if script_args.use_peft:
    peft_config = LoraConfig(
        r=script_args.peft_lora_r,
        lora_alpha=script_args.peft_lora_alpha,
        bias="none",
        task_type="CAUSAL_LM",
    )
else:
    peft_config = None


# trl sft trainer default tokenizer and pad_token
"""
if tokenizer is None:
            tokenizer = AutoTokenizer.from_pretrained(model.config._name_or_path)
            if getattr(tokenizer, "pad_token", None) is None:
                tokenizer.pad_token = tokenizer.eos_token

if max_seq_length is None:
            # to overcome some issues with broken tokenizers
            max_seq_length = min(tokenizer.model_max_length, 1024)

            warnings.warn(
                f"You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to {max_seq_length}"
            )

"""

# Looks like padding_side = "right" by default
# Hence, defining our tokenizer here
tokenizer = AutoTokenizer.from_pretrained(script_args.model_name)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"
tokenizer.truncation_side = "left"
tokenizer.pad_token_as_eos_token = True
tokenizer.max_length = 64

# Step 5: Define the Trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    max_seq_length=script_args.seq_length,
    train_dataset=dataset,
    dataset_text_field=script_args.dataset_text_field,
    peft_config=peft_config,
    tokenizer=tokenizer
)

trainer.train()

# Step 6: Save the model
date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
save_dir_path = f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_positive/{date_n_time}'
os.makedirs(save_dir_path)
trainer.save_model(save_dir_path)
# save the final model - maybe this will unnecessarily take memory? What is a good way to go about it? 
torch.save(model, f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_positive/{date_n_time}/model.pt')
#ppo_trainer.create_model_card(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/ppo/{date_n_time}')

File Path: examples/llama/training/old/sft_ppo.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed
from trl.core import LengthSampler

tqdm.pandas()

import pdb
import pickle
import datetime
import os

@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name="lvwerra/gpt2-imdb",
            query_dataset="imdb",
            reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
        )
    )
    query_dataset: str = field(default="imdb", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )


args = tyro.cli(ScriptArguments)


# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8, data_split = 'train'):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # load imdb with datasets
    # to load partial dataset, just change data split to split=data_split+'[:200]'
    ds = load_dataset(query_dataset, split=data_split)
    ds = ds.rename_columns({"text": "review"})
    ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)

    input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)
test_dataset = build_dataset(args.ppo_config, args.query_dataset, data_split='test')

def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    args.ppo_config.model_name,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)
# Creating a tester object here because dataloader is only available in the PPOTrainer class
ppo_tester = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
task, model_name = args.ppo_config.reward_model.split(":")
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs = {
    "min_length": -1,
    "top_k": 0.0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": 32,
}

# stat lists
obj_entropy = []
policy_entropy = []

mean_non_score_reward = []
mean_scores = []
std_scores = []

mean_returns = []
var_returns = []

for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
    rewards = [torch.tensor(output[1]["score"]) for output in pipe_outputs]

    # Run PPO step
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
    ppo_trainer.log_stats(stats, batch, rewards)
    
    obj_entropy.append(stats['objective/entropy'])
    policy_entropy.append(stats['ppo/policy/entropy'])
    mean_non_score_reward.append(stats['ppo/mean_non_score_reward'])
    mean_scores.append(stats['ppo/mean_scores'])
    std_scores.append(stats['ppo/std_scores'])
    mean_returns.append(stats['ppo/returns/mean'])
    var_returns.append(stats['ppo/returns/var'])

    print(f'epoch: {epoch}')
    mean_return = stats['ppo/returns/mean']
    mean_score = stats['ppo/mean_scores']
    print(f'mean_returns: {mean_return}, mean_score: {mean_score}')


# save the final model - maybe this will unnecessarily take memory? What is a good way to go about it? 
date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
os.makedirs(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_ppo/{date_n_time}')
torch.save(model, f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_ppo/{date_n_time}/model.pt')
#ppo_trainer.create_model_card(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_ppo/{date_n_time}')

# save the train stats 
train_stats_lists = [obj_entropy, policy_entropy, mean_non_score_reward, mean_scores, std_scores, mean_returns, var_returns]
train_stats_lists_names = ['obj_entropy', 'policy_entropy', 'mean_non_score_reward', 'mean_scores', 'std_scores', 'mean_returns', 'var_returns']
for ii in range(len(train_stats_lists)):
    pickle.dump(train_stats_lists[ii], open(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_ppo/{date_n_time}/{train_stats_lists_names[ii]}', "wb"))


# infer on test dataset
test_scores = []
for epoch, batch in tqdm(enumerate(ppo_tester.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)  ### SHOULD THIS BE ppo_tester instead? 
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
    rewards = [torch.tensor(output[1]["score"]) for output in pipe_outputs]

    test_scores.append(sum(rewards)/len(rewards))

# print and save the final score  
final_test_score = sum(test_scores)/len(test_scores)     
print(f'Total test sentiment-score is: {final_test_score.item()}')
pickle.dump(final_test_score.item(), open(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_ppo/{date_n_time}/final_test_score', "wb"))


    





File Path: examples/llama/training/old/zero_shot_vanilla.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed
from transformers import AutoModelForCausalLM, BitsAndBytesConfig, HfArgumentParser, TrainingArguments
from trl.core import LengthSampler


tqdm.pandas()

import pdb
import pickle
import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import statistics
import random
from torch.utils.data import DataLoader
from typing import Callable, List, Optional, Union
import datasets
from datasets import Dataset

# Define and parse arguments.
@dataclass
class ScriptArguments:
    """
    The name of the Casual LM model we wish to fine-tune with SFTTrainer
    """

    model_name: Optional[str] = field(default="gpt2", metadata={"help": "the model name"})
    reward_model: Optional[str] = field(default="lvwerra/distilbert-imdb", metadata={"help": "model to obtain desired task score, e.g, sentiment score"})
    task_type: Optional[str] = field(default="sentiment-analysis", metadata={"help": "task type"})
    dataset_name: Optional[str] = field(
        default="imdb", metadata={"help": "the dataset name"}
    )
    dataset_text_field: Optional[str] = field(default="text", metadata={"help": "the text field of the dataset"})
    log_with: Optional[str] = field(default="none", metadata={"help": "use 'wandb' to log with wandb"})
    seed: Optional[int] = field(default=0, metadata={"help": "seed"})
    
    # learning params
    learning_rate: Optional[float] = field(default=0.00001, metadata={"help": "the learning rate"}) #default=1.41e-5
    batch_size: Optional[int] = field(default=64, metadata={"help": "the batch size"})
    # Using tokenizer max length=64 from RL4LLMs Table 4
    seq_length: Optional[int] = field(default=64, metadata={"help": "Input sequence length"}) #default=512
    gradient_accumulation_steps: Optional[int] = field(
        default=16, metadata={"help": "the number of gradient accumulation steps"}
    )
    num_train_epochs: Optional[int] = field(default=10, metadata={"help": "the number of training epochs"}) #default=3
    max_steps: Optional[int] = field(default=-1, metadata={"help": "the number of training steps"})

    # model download, quantization and peft params
    trust_remote_code: Optional[bool] = field(default=True, metadata={"help": "Enable `trust_remote_code`"})
    use_auth_token: Optional[bool] = field(default=True, metadata={"help": "Use HF auth token to access the model"})
    load_in_8bit: Optional[bool] = field(default=False, metadata={"help": "load the model in 8 bits precision"})
    load_in_4bit: Optional[bool] = field(default=False, metadata={"help": "load the model in 4 bits precision"})
    use_peft: Optional[bool] = field(default=False, metadata={"help": "Wether to use PEFT or not to train adapters"})
    peft_lora_r: Optional[int] = field(default=64, metadata={"help": "the r parameter of the LoRA adapters"})
    peft_lora_alpha: Optional[int] = field(default=16, metadata={"help": "the alpha parameter of the LoRA adapters"})

    # logging
    output_dir: Optional[str] = field(default="/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_positive/", metadata={"help": "the output directory"})
    logging_steps: Optional[int] = field(default=1, metadata={"help": "the number of logging steps"})
    save_steps: Optional[int] = field(
        default=100, metadata={"help": "Number of updates steps before two checkpoint saves"}
    )
    save_total_limit: Optional[int] = field(default=10, metadata={"help": "Limits total number of checkpoints."})

    # post processing
    push_to_hub: Optional[bool] = field(default=False, metadata={"help": "Push the model to HF Hub"})
    hub_model_id: Optional[str] = field(default=None, metadata={"help": "The name of the model on HF Hub"})

parser = HfArgumentParser(ScriptArguments)
args = parser.parse_args_into_dataclasses()[0]

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8, data_split = 'train'):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # load imdb with datasets
    # to load partial dataset, just change data split to split=data_split+'[:200]'
    ds = load_dataset(query_dataset, split=data_split)
    ds = ds.rename_columns({"text": "review"})
    ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)

    def input_size():
        return 64 #LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds

#what is the purpose of data collator? 
def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])

# function to prepare dataloader as one included in ppo trainer class
def prepare_dataloader(dataset: Union[torch.utils.data.Dataset, Dataset], data_collator=None, batch_size=128):
        """
        Prepare the dataloader for training.

        Args:
            dataset (Union[`torch.utils.data.Dataset`, `datasets.Dataset`]):
                PyTorch dataset or Hugging Face dataset. If a Hugging Face dataset is passed, the dataset
                will be preprocessed by removing the columns that are not used by the model.
            data_collator (Optional[function]):
                Data collator function.

        Returns:
            `torch.utils.data.DataLoader`: PyTorch dataloader
        """
        
        dataloader = torch.utils.data.DataLoader(
            dataset,
            batch_size=batch_size,
            collate_fn=data_collator,
            shuffle=True,
            drop_last=True,
        )
        return dataloader


# build and sample test dataset
test_dataset = build_dataset(args, args.dataset_name, data_split='test')

# Take 5k random samples
sample_size = 5000
random_indices = random.sample(range(len(test_dataset)), sample_size)
test_dataset = test_dataset.select(random_indices)

#create dataloader object
dataloader = prepare_dataloader(test_dataset, collator, batch_size=args.batch_size)

# set seed before initializing value head for deterministic eval
set_seed(args.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = AutoModelForCausalLM.from_pretrained(
    args.model_name,
    device_map=device_map,
    trust_remote_code=True,
    torch_dtype=None)

tokenizer = AutoTokenizer.from_pretrained(args.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.padding_side = "left"
tokenizer.truncation_side = "left"
tokenizer.pad_token_as_eos_token = True
tokenizer.max_length = 64

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
task, model_name = args.task_type, args.reward_model
sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs = {
    "min_length": 48,
    "top_k": 50,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": 48,
}

# generation pipeline
generator = pipeline('text-generation', model=model, **generation_kwargs)


# infer on test dataset
test_scores = []
for epoch, batch in tqdm(enumerate(dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2 model
    response_tensors = model(query_tensors, **generation_kwargs)
    response_tensors = ppo_tester.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
    #rewards = [torch.tensor(output[1]["score"]) for output in pipe_outputs]
    rewards = [
        torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"]]))[1]
        for output in pipe_outputs
    ]

    test_scores.append(sum(rewards)/len(rewards))
    print(sum(rewards)/len(rewards))

# print and save the final score  
date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
os.makedirs(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/zero_shot/{date_n_time}')
final_test_score = sum(test_scores)/len(test_scores)     
test_scores_list = [t.item() for t in test_scores]
print(f'Total test sentiment-score and std is: {final_test_score.item(), statistics.stdev(test_scores_list)}')
pickle.dump(final_test_score.item(), open(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/zero_shot/{date_n_time}/final_test_score', "wb"))




File Path: examples/llama/training/ppo_auth1.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed
from trl.core import LengthSampler


tqdm.pandas()

import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import pdb
import pandas as pd

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
logging_dir = f"/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/dummy/{date_n_time}"
os.makedirs(logging_dir)

@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name="lvwerra/gpt2-imdb",
            query_dataset="imdb",
            reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            #log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            steps= 128000,
            exp_name="ppo"
        )
    )
    query_dataset: str = field(default="imdb", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="less", metadata={"help": "the dataset to query"})
    
    generation_kwargs_min_length: Optional[int] = field(default=-1, metadata={"help": "minimum number of tokens while generation"})
    generation_kwargs_top_k: Optional[int] = field(default=0, metadata={"help": "gneration top k"})
    generation_kwargs_top_p: Optional[float] = field(default=1.0, metadata={"help": "gneration top p"})
    generation_kwargs_max_new_tokens: Optional[int] = field(default=32, metadata={"help": "gneration top p"})

    change_tokenizer_args: bool = field(default=False, metadata={"help": "whether to use modify tokenizer settings"})
    tokenizer_kwargs: dict = field(
    default_factory=lambda: {
      "padding_side": "left", 
      "truncation_side": "left",
      "pad_token_as_eos_token": True,
      "max_length": 64
    }
  )



args = tyro.cli(ScriptArguments)

generation_kwargs = {
      "min_length": args.generation_kwargs_min_length, 
      "top_k": args.generation_kwargs_top_k,
      "top_p": args.generation_kwargs_top_p,
      "do_sample": True,
      "max_new_tokens": args.generation_kwargs_max_new_tokens
    }

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # load imdb with datasets: for reducing data size, just use +'[:200]'
    ds = load_dataset(query_dataset, split="train")
    ds = ds.rename_columns({"text": "review"})
    ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)


    if args.input_query_size == "more":
        def input_size():
            return 64 
    else: 
        input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    args.ppo_config.model_name,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id

#pdb.set_trace()

if args.change_tokenizer_args == True: 
    tokenizer.padding_side = args.tokenizer_kwargs['padding_side']
    tokenizer.truncation_side = args.tokenizer_kwargs['truncation_side']
    tokenizer.pad_token_as_eos_token = args.tokenizer_kwargs['pad_token_as_eos_token']
    tokenizer.max_length = args.tokenizer_kwargs['max_length']

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
task, model_name = args.ppo_config.reward_model.split(":")
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs['pad_token_id'] = tokenizer.eos_token_id

# empty list to store text data
text_train_data = []

for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)

    if args.reward_function == "trl":
        rewards = [torch.tensor(output[1]["score"]) for output in pipe_outputs]
    else: 
        rewards = [
            torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"]]))[1]
            for output in pipe_outputs
        ]
    
    for ii in range(3): 
        data_row = {'iter': epoch, 'input':batch["query"][ii], 'output':batch["response"][ii], 'score':rewards[ii].item()}
        text_train_data.append(data_row)
    
    #pdb.set_trace()

    # Run PPO step
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
    ppo_trainer.log_stats(stats, batch, rewards)
    print(epoch)

ppo_trainer._save_pretrained(logging_dir)
df = pd.DataFrame(text_train_data)
df.to_csv(logging_dir+'/textual_data.csv', index=False)
File Path: examples/llama/training/zero_shot.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed
from trl.core import LengthSampler


tqdm.pandas()

import pdb
import pickle
import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import statistics
import random

@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name="gpt2",
            query_dataset="imdb",
            reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
        )
    )
    query_dataset: str = field(default="imdb", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )


args = tyro.cli(ScriptArguments)


# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8, data_split = 'train'):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # load imdb with datasets
    # to load partial dataset, just change data split to split=data_split+'[:200]'
    ds = load_dataset(query_dataset, split=data_split)
    ds = ds.rename_columns({"text": "review"})
    ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)

    def input_size():
        return 64 #LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)

# build and sample test dataset
test_dataset = build_dataset(args.ppo_config, args.query_dataset, data_split='test')
# Take 5k random samples
sample_size = 5000
random_indices = random.sample(range(len(test_dataset)), sample_size)
test_dataset = test_dataset.select(random_indices)

def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    '/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_positive_1/oct_2_afternoon/checkpoint-120',
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.padding_side = "left"
tokenizer.truncation_side = "left"
tokenizer.pad_token_as_eos_token = True
tokenizer.max_length = 64

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)
# Creating a tester object here because dataloader is only available in the PPOTrainer class
ppo_tester = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=test_dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
task, model_name = args.ppo_config.reward_model.split(":")
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs = {
    "min_length": 48,
    "top_k": 50,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": 48,
}


# infer on test dataset
test_scores = []
for epoch, batch in tqdm(enumerate(ppo_tester.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
    #rewards = [torch.tensor(output[1]["score"]) for output in pipe_outputs]
    rewards = [
        torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"]]))[1]
        for output in pipe_outputs
    ]

    test_scores.extend(rewards)

    #test_scores.append(sum(rewards)/len(rewards))
    print(sum(rewards)/len(rewards))

# print and save the final score  
date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
os.makedirs(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/zero_shot/{date_n_time}')
final_test_score = sum(test_scores)/len(test_scores)     
test_scores_list = [t.item() for t in test_scores]
pdb.set_trace()
print(f'Total test sentiment-score and std is: {final_test_score.item(), statistics.stdev(test_scores_list)}')
pickle.dump(final_test_score.item(), open(f'/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/zero_shot/{date_n_time}/final_test_score', "wb"))




File Path: examples/research_projects/stack_llama/scripts/merge_peft_adapter.py
Content:
from dataclasses import dataclass, field
from typing import Optional

import torch
from peft import PeftConfig, PeftModel
from transformers import AutoModelForCausalLM, AutoModelForSequenceClassification, AutoTokenizer, HfArgumentParser


@dataclass
class ScriptArguments:
    """
    The name of the Casual LM model we wish to fine with PPO
    """

    adapter_model_name: Optional[str] = field(default=None, metadata={"help": "the model name"})
    base_model_name: Optional[str] = field(default=None, metadata={"help": "the model name"})
    output_name: Optional[str] = field(default=None, metadata={"help": "the model name"})


parser = HfArgumentParser(ScriptArguments)
script_args = parser.parse_args_into_dataclasses()[0]
assert script_args.adapter_model_name is not None, "please provide the name of the Adapter you would like to merge"
assert script_args.base_model_name is not None, "please provide the name of the Base model"
assert script_args.base_model_name is not None, "please provide the output name of the merged model"

peft_config = PeftConfig.from_pretrained(script_args.adapter_model_name)
if peft_config.task_type == "SEQ_CLS":
    # peft is for reward model so load sequence classification
    model = AutoModelForSequenceClassification.from_pretrained(
        script_args.base_model_name, num_labels=1, torch_dtype=torch.bfloat16
    )
else:
    model = AutoModelForCausalLM.from_pretrained(
        script_args.base_model_name, return_dict=True, torch_dtype=torch.bfloat16
    )

tokenizer = AutoTokenizer.from_pretrained(script_args.base_model_name)

# Load the Lora model
model = PeftModel.from_pretrained(model, script_args.adapter_model_name)
model.eval()

model = model.merge_and_unload()

model.save_pretrained(f"{script_args.output_name}")
tokenizer.save_pretrained(f"{script_args.output_name}")
model.push_to_hub(f"{script_args.output_name}", use_temp_dir=False)

File Path: examples/research_projects/stack_llama/scripts/reward_modeling.py
Content:
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Union

import evaluate
import numpy as np
import torch
import torch.nn as nn
from datasets import load_dataset
from peft import LoraConfig, TaskType, get_peft_model
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    HfArgumentParser,
    PreTrainedTokenizerBase,
    Trainer,
    TrainerCallback,
    TrainingArguments,
)
from transformers.utils import PaddingStrategy


# Define and parse arguments.
@dataclass
class ScriptArguments:
    """
    These arguments vary depending on how many GPUs you have, what their capacity and features are, and what size model you want to train.
    """

    local_rank: Optional[int] = field(default=-1, metadata={"help": "Used for multi-gpu"})
    resume_from_checkpoint: Optional[bool] = field(
        default=False,
        metadata={"help": "If you want to resume training where it left off."},
    )
    deepspeed: Optional[str] = field(
        default=None,
        metadata={
            "help": "Path to deepspeed config if using deepspeed. You may need this if the model that you want to train doesn't fit on a single GPU."
        },
    )
    per_device_train_batch_size: Optional[int] = field(default=4)
    per_device_eval_batch_size: Optional[int] = field(default=1)
    gradient_accumulation_steps: Optional[int] = field(default=1)
    learning_rate: Optional[float] = field(default=2e-5)
    weight_decay: Optional[float] = field(default=0.001)
    model_name: Optional[str] = field(
        default="gpt2",
        metadata={
            "help": "The model that you want to train from the Hugging Face hub. E.g. gpt2, gpt2-xl, bert, etc."
        },
    )
    tokenizer_name: Optional[str] = field(
        default=None,
        metadata={
            "help": "The tokenizer for your model, if left empty will use the default for your model",
        },
    )
    bf16: Optional[bool] = field(
        default=True,
        metadata={
            "help": "This essentially cuts the training time in half if you want to sacrifice a little precision and have a supported GPU."
        },
    )
    num_train_epochs: Optional[int] = field(
        default=1,
        metadata={"help": "The number of training epochs for the reward model."},
    )
    train_subset: Optional[int] = field(
        default=100000,
        metadata={"help": "The size of the subset of the training data to use"},
    )
    eval_subset: Optional[int] = field(
        default=50000,
        metadata={"help": "The size of the subset of the eval data to use"},
    )
    gradient_checkpointing: Optional[bool] = field(
        default=False,
        metadata={"help": "Enables gradient checkpointing."},
    )
    optim: Optional[str] = field(
        default="adamw_hf",
        metadata={"help": "The optimizer to use."},
    )
    lr_scheduler_type: Optional[str] = field(
        default="linear",
        metadata={"help": "The lr scheduler"},
    )
    max_length: Optional[int] = field(default=512)
    eval_first_step: Optional[bool] = field(
        default=False,
        metadata={"help": "Whether to run eval after the first step"},
    )


parser = HfArgumentParser(ScriptArguments)
script_args = parser.parse_args_into_dataclasses()[0]

# Load the human stack-exchange-paired dataset for tuning the reward model.
train_dataset = load_dataset("lvwerra/stack-exchange-paired", data_dir="data/reward", split="train")
if script_args.train_subset > 0:
    train_dataset = train_dataset.select(range(script_args.train_subset))
eval_dataset = load_dataset("lvwerra/stack-exchange-paired", data_dir="data/evaluation", split="train")
if script_args.eval_subset > 0:
    eval_dataset = eval_dataset.select(range(script_args.eval_subset))
# Define the training args. Needs to be done before the model is loaded if you are using deepspeed.
model_name_split = script_args.model_name.split("/")[-1]
output_name = (
    f"{model_name_split}_peft_stack-exchange-paired_rmts__{script_args.train_subset}_{script_args.learning_rate}"
)

training_args = TrainingArguments(
    output_dir=output_name,
    learning_rate=script_args.learning_rate,
    per_device_train_batch_size=script_args.per_device_train_batch_size,
    per_device_eval_batch_size=script_args.per_device_eval_batch_size,
    num_train_epochs=script_args.num_train_epochs,
    weight_decay=script_args.weight_decay,
    evaluation_strategy="steps",
    eval_steps=500,
    save_strategy="steps",
    save_steps=500,
    gradient_accumulation_steps=script_args.gradient_accumulation_steps,
    gradient_checkpointing=script_args.gradient_checkpointing,
    deepspeed=script_args.deepspeed,
    local_rank=script_args.local_rank,
    remove_unused_columns=False,
    label_names=[],
    bf16=script_args.bf16,
    logging_strategy="steps",
    logging_steps=10,
    optim=script_args.optim,
    lr_scheduler_type=script_args.lr_scheduler_type,
)
# Load the value-head model and tokenizer.
tokenizer_name = script_args.tokenizer_name if script_args.tokenizer_name is not None else script_args.model_name
tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_auth_token=True)
tokenizer.pad_token = tokenizer.eos_token


peft_config = LoraConfig(
    task_type=TaskType.SEQ_CLS,
    inference_mode=False,
    r=8,
    lora_alpha=32,
    lora_dropout=0.1,
)

model = AutoModelForSequenceClassification.from_pretrained(
    script_args.model_name, num_labels=1, torch_dtype=torch.bfloat16
)
model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

# Need to do this for gpt2, because it doesn't have an official pad token.
tokenizer.pad_token = tokenizer.eos_token
model.config.pad_token_id = tokenizer.eos_token_id
model.config.use_cache = not script_args.gradient_checkpointing
num_proc = 24  # Can adjust to be higher if you have more processors.
original_columns = train_dataset.column_names


# Turn the dataset into pairs of post + summaries, where text_j is the preferred question + answer and text_k is the other.
# Then tokenize the dataset.
def preprocess_function(examples):
    new_examples = {
        "input_ids_j": [],
        "attention_mask_j": [],
        "input_ids_k": [],
        "attention_mask_k": [],
    }
    for question, response_j, response_k in zip(examples["question"], examples["response_j"], examples["response_k"]):
        tokenized_j = tokenizer("Question: " + question + "\n\nAnswer: " + response_j, truncation=True)
        tokenized_k = tokenizer("Question: " + question + "\n\nAnswer: " + response_k, truncation=True)

        new_examples["input_ids_j"].append(tokenized_j["input_ids"])
        new_examples["attention_mask_j"].append(tokenized_j["attention_mask"])
        new_examples["input_ids_k"].append(tokenized_k["input_ids"])
        new_examples["attention_mask_k"].append(tokenized_k["attention_mask"])

    return new_examples


# preprocess the dataset and filter out QAs that are longer than script_args.max_length
train_dataset = train_dataset.map(
    preprocess_function,
    batched=True,
    num_proc=num_proc,
    remove_columns=original_columns,
)
train_dataset = train_dataset.filter(
    lambda x: len(x["input_ids_j"]) <= script_args.max_length and len(x["input_ids_k"]) <= script_args.max_length
)

eval_dataset = eval_dataset.map(
    preprocess_function,
    batched=True,
    num_proc=num_proc,
    remove_columns=original_columns,
)
eval_dataset = eval_dataset.filter(
    lambda x: len(x["input_ids_j"]) <= script_args.max_length and len(x["input_ids_k"]) <= script_args.max_length
)


# We need to define a special data collator that batches the data in our j vs k format.
@dataclass
class RewardDataCollatorWithPadding:
    tokenizer: PreTrainedTokenizerBase
    padding: Union[bool, str, PaddingStrategy] = True
    max_length: Optional[int] = None
    pad_to_multiple_of: Optional[int] = None
    return_tensors: str = "pt"

    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:
        features_j = []
        features_k = []
        for feature in features:
            features_j.append(
                {
                    "input_ids": feature["input_ids_j"],
                    "attention_mask": feature["attention_mask_j"],
                }
            )
            features_k.append(
                {
                    "input_ids": feature["input_ids_k"],
                    "attention_mask": feature["attention_mask_k"],
                }
            )
        batch_j = self.tokenizer.pad(
            features_j,
            padding=self.padding,
            max_length=self.max_length,
            pad_to_multiple_of=self.pad_to_multiple_of,
            return_tensors=self.return_tensors,
        )
        batch_k = self.tokenizer.pad(
            features_k,
            padding=self.padding,
            max_length=self.max_length,
            pad_to_multiple_of=self.pad_to_multiple_of,
            return_tensors=self.return_tensors,
        )
        batch = {
            "input_ids_j": batch_j["input_ids"],
            "attention_mask_j": batch_j["attention_mask"],
            "input_ids_k": batch_k["input_ids"],
            "attention_mask_k": batch_k["attention_mask"],
            "return_loss": True,
        }
        return batch


# Define the metric that we'll use for validation.
accuracy = evaluate.load("accuracy")


def compute_metrics(eval_pred):
    predictions, _ = eval_pred
    # Here, predictions is rewards_j and rewards_k.
    # We want to see how much of the time rewards_j > rewards_k.
    predictions = np.argmax(predictions, axis=0)
    labels = np.zeros(predictions.shape)
    return accuracy.compute(predictions=predictions, references=labels)


class RewardTrainer(Trainer):
    # Define how to compute the reward loss. We use the InstructGPT pairwise logloss: https://arxiv.org/abs/2203.02155
    def compute_loss(self, model, inputs, return_outputs=False):
        rewards_j = model(input_ids=inputs["input_ids_j"], attention_mask=inputs["attention_mask_j"])[0]
        rewards_k = model(input_ids=inputs["input_ids_k"], attention_mask=inputs["attention_mask_k"])[0]
        loss = -nn.functional.logsigmoid(rewards_j - rewards_k).mean()
        if return_outputs:
            return loss, {"rewards_j": rewards_j, "rewards_k": rewards_k}
        return loss


# Train the model, woohoo.
trainer = RewardTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics,
    data_collator=RewardDataCollatorWithPadding(tokenizer=tokenizer, max_length=script_args.max_length),
)


if script_args.eval_first_step:

    class EvaluateFirstStepCallback(TrainerCallback):
        def on_step_end(self, args, state, control, **kwargs):
            if state.global_step == 1:
                control.should_evaluate = True

    trainer.add_callback(EvaluateFirstStepCallback())

trainer.train(script_args.resume_from_checkpoint)

print("Saving last checkpoint of the model")
model.save_pretrained(output_name + "_peft_last_checkpoint")

File Path: examples/research_projects/stack_llama/scripts/rl_training.py
Content:
# coding=utf-8
# Copyright 2022 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import Adafactor, AutoTokenizer, HfArgumentParser, pipeline

from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, set_seed
from trl.core import LengthSampler


tqdm.pandas()


@dataclass
class ScriptArguments:
    """
    The name of the Casual LM model we wish to fine with PPO
    """

    # NOTE: gpt2 models use Conv1D instead of Linear layers which are not yet supported in 8 bit mode
    # models like gpt-neo* models are more suitable.
    model_name: Optional[str] = field(default="", metadata={"help": "the model name"})
    tokenizer_name: Optional[str] = field(default="", metadata={"help": "the tokenizer name"})
    reward_model_name: Optional[str] = field(default="", metadata={"help": "the reward model name"})
    log_with: Optional[str] = field(default=None, metadata={"help": "use 'wandb' to log with wandb"})
    learning_rate: Optional[float] = field(default=1.41e-5, metadata={"help": "the learning rate"})
    output_max_length: Optional[int] = field(default=128, metadata={"help": "maximum length for generation"})
    mini_batch_size: Optional[int] = field(default=1, metadata={"help": "the PPO minibatch size"})
    batch_size: Optional[int] = field(default=32, metadata={"help": "the batch size"})
    ppo_epochs: Optional[int] = field(default=4, metadata={"help": "the number of ppo epochs"})
    gradient_accumulation_steps: Optional[int] = field(
        default=4, metadata={"help": "the number of gradient accumulation steps"}
    )
    adafactor: Optional[bool] = field(default=False, metadata={"help": "whether to use the adafactor optimizer"})
    early_stopping: Optional[bool] = field(default=False, metadata={"help": "whether to early stop"})
    target_kl: Optional[float] = field(default=0.1, metadata={"help": "kl target for early stopping"})
    reward_baseline: Optional[float] = field(
        default=0.0,
        metadata={"help": "a baseline value that is subtracted from the reward"},
    )
    batched_gen: Optional[bool] = field(default=False, metadata={"help": "whether to use the batched text gen"})
    save_freq: Optional[int] = field(default=None, metadata={"help": "n steps to save the model"})
    output_dir: Optional[str] = field(default="runs/", metadata={"help": "n steps to save the model"})
    seed: Optional[int] = field(default=0, metadata={"help": "the seed"})
    steps: Optional[int] = field(default=20000, metadata={"help": "number of epochs"})
    init_kl_coef: Optional[float] = field(
        default=0.2,
        metadata={"help": "Initial KL penalty coefficient (used for adaptive and linear control)"},
    )

    adap_kl_ctrl: Optional[bool] = field(default=True, metadata={"help": "Use adaptive KL control, otherwise linear"})


parser = HfArgumentParser(ScriptArguments)
script_args: ScriptArguments = parser.parse_args_into_dataclasses()[0]
reward_model_name = script_args.reward_model_name
dataset_name = "lvwerra/stack-exchange-paired"
config = PPOConfig(
    steps=script_args.steps,
    model_name=script_args.model_name,
    learning_rate=script_args.learning_rate,
    log_with=script_args.log_with,
    batch_size=script_args.batch_size,
    mini_batch_size=script_args.mini_batch_size,
    gradient_accumulation_steps=script_args.gradient_accumulation_steps,
    optimize_cuda_cache=True,
    early_stopping=script_args.early_stopping,
    target_kl=script_args.target_kl,
    ppo_epochs=script_args.ppo_epochs,
    seed=script_args.seed,
    init_kl_coef=script_args.init_kl_coef,
    adap_kl_ctrl=script_args.adap_kl_ctrl,
)

train_dataset = load_dataset("lvwerra/stack-exchange-paired", data_dir="data/rl", split="train")
train_dataset = train_dataset.select(range(100000))

# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {
    "return_all_scores": True,
    "function_to_apply": "none",
    "batch_size": 16,
    "truncation": True,
}

tokenizer = AutoTokenizer.from_pretrained(script_args.tokenizer_name)
# GPT-2 tokenizer has a pad token, but it is not eos_token by default. We need to set it to eos_token.
# only for this model.

if getattr(tokenizer, "pad_token", None) is None:
    tokenizer.pad_token = tokenizer.eos_token


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(
    tokenizer,
    dataset_name="lvwerra/stack-exchange-paired",
):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        dataset_name (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """

    # load imdb with datasets
    ds = load_dataset(dataset_name, data_dir="data/rl", split="train")
    original_columns = ds.column_names
    num_proc = 24

    def preprocess_function(examples):
        new_examples = {
            "query": [],
            "input_ids": [],
        }
        for question in examples["question"]:
            query = "Question: " + question + "\n\nAnswer: "
            tokenized_question = tokenizer(query, truncation=True)
            new_examples["query"].append(query)
            new_examples["input_ids"].append(tokenized_question["input_ids"])

        return new_examples

    ds = train_dataset.map(
        preprocess_function,
        batched=True,
        num_proc=num_proc,
        remove_columns=original_columns,
    )
    ds = ds.filter(lambda x: len(x["input_ids"]) < 512, batched=False)

    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(tokenizer)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(config.seed)

# Now let's build the model, the reference model, and the tokenizer.
current_device = Accelerator().local_process_index

lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)
model = AutoModelForCausalLMWithValueHead.from_pretrained(
    config.model_name,
    load_in_8bit=True,
    device_map={"": current_device},
    peft_config=lora_config,
)

optimizer = None
if script_args.adafactor:
    optimizer = Adafactor(
        filter(lambda p: p.requires_grad, model.parameters()),
        scale_parameter=False,
        relative_step=False,
        warmup_init=False,
        lr=config.learning_rate,
    )
# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(
    config,
    model,
    ref_model=None,
    tokenizer=tokenizer,
    dataset=dataset,
    data_collator=collator,
    optimizer=optimizer,
)

# We then build the sentiment analysis pipeline using our reward model, passing the
# model name and the sentiment analysis pipeline arguments. Let's also make sure to
# set the device to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a ` pipeline` bug
sentiment_pipe = pipeline(
    "sentiment-analysis",
    model=reward_model_name,
    device_map={"": current_device},
    model_kwargs={"load_in_8bit": True},
    tokenizer=tokenizer,
    return_token_type_ids=False,
)

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs = {
    # "min_length": -1,
    "top_k": 0.0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.pad_token_id,
    "eos_token_id": 100_000,
}
output_min_length = 32
output_max_length = script_args.output_max_length
output_length_sampler = LengthSampler(output_min_length, output_max_length)

for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    if epoch >= config.total_ppo_epochs:
        break

    question_tensors = batch["input_ids"]

    response_tensors = ppo_trainer.generate(
        question_tensors,
        return_prompt=False,
        length_sampler=output_length_sampler,
        **generation_kwargs,
    )
    batch["response"] = tokenizer.batch_decode(response_tensors, skip_special_tokens=True)

    # Compute reward score (using the sentiment analysis pipeline)
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
    rewards = [torch.tensor(output[0]["score"] - script_args.reward_baseline) for output in pipe_outputs]

    # Run PPO step
    stats = ppo_trainer.step(question_tensors, response_tensors, rewards)
    ppo_trainer.log_stats(stats, batch, rewards)

    if script_args.save_freq and epoch and epoch % script_args.save_freq == 0:
        ppo_trainer.save_pretrained(script_args.output_dir + f"step_{epoch}")

File Path: examples/research_projects/stack_llama/scripts/supervised_finetuning.py
Content:
import argparse
import os

from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, logging, set_seed

from trl import SFTTrainer
from trl.trainer import ConstantLengthDataset


"""
Fine-Tune Llama-7b on SE paired dataset
"""


def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_path", type=str, default="")
    parser.add_argument("--dataset_name", type=str, default="lvwerra/stack-exchange-paired")
    parser.add_argument("--subset", type=str, default="data/finetune")
    parser.add_argument("--split", type=str, default="train")
    parser.add_argument("--size_valid_set", type=int, default=4000)
    parser.add_argument("--streaming", action="store_true")
    parser.add_argument("--shuffle_buffer", type=int, default=5000)

    parser.add_argument("--seq_length", type=int, default=1024)
    parser.add_argument("--max_steps", type=int, default=10000)
    parser.add_argument("--batch_size", type=int, default=4)
    parser.add_argument("--gradient_accumulation_steps", type=int, default=1)
    parser.add_argument("--eos_token_id", type=int, default=49152)

    parser.add_argument("--learning_rate", type=float, default=1e-4)
    parser.add_argument("--lr_scheduler_type", type=str, default="cosine")
    parser.add_argument("--num_warmup_steps", type=int, default=100)
    parser.add_argument("--weight_decay", type=float, default=0.05)

    parser.add_argument("--local_rank", type=int, default=0)
    parser.add_argument("--no_fp16", action="store_false")
    parser.add_argument("--bf16", action="store_true", default=False)
    parser.add_argument("--no_gradient_checkpointing", action="store_false", default=False)
    parser.add_argument("--seed", type=int, default=0)
    parser.add_argument("--num_workers", type=int, default=None)
    parser.add_argument("--output_dir", type=str, default="./checkpoints")
    parser.add_argument("--log_freq", default=1, type=int)
    parser.add_argument("--eval_freq", default=1000, type=int)
    parser.add_argument("--save_freq", default=1000, type=int)

    return parser.parse_args()


def chars_token_ratio(dataset, tokenizer, nb_examples=400):
    """
    Estimate the average number of characters per token in the dataset.
    """
    total_characters, total_tokens = 0, 0
    for _, example in tqdm(zip(range(nb_examples), iter(dataset)), total=nb_examples):
        text = prepare_sample_text(example)
        total_characters += len(text)
        if tokenizer.is_fast:
            total_tokens += len(tokenizer(text).tokens())
        else:
            total_tokens += len(tokenizer.tokenize(text))

    return total_characters / total_tokens


def print_trainable_parameters(model):
    """
    Prints the number of trainable parameters in the model.
    """
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(
        f"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}"
    )


def prepare_sample_text(example):
    """Prepare the text from a sample of the dataset."""
    text = f"Question: {example['question']}\n\nAnswer: {example['response_j']}"
    return text


def create_datasets(tokenizer, args):
    dataset = load_dataset(
        args.dataset_name,
        data_dir=args.subset,
        split=args.split,
        use_auth_token=True,
        num_proc=args.num_workers if not args.streaming else None,
        streaming=args.streaming,
    )
    if args.streaming:
        print("Loading the dataset in streaming mode")
        valid_data = dataset.take(args.size_valid_set)
        train_data = dataset.skip(args.size_valid_set)
        train_data = train_data.shuffle(buffer_size=args.shuffle_buffer, seed=args.seed)
    else:
        dataset = dataset.train_test_split(test_size=0.005, seed=args.seed)
        train_data = dataset["train"]
        valid_data = dataset["test"]
        print(f"Size of the train set: {len(train_data)}. Size of the validation set: {len(valid_data)}")

    chars_per_token = chars_token_ratio(train_data, tokenizer)
    print(f"The character to token ratio of the dataset is: {chars_per_token:.2f}")

    train_dataset = ConstantLengthDataset(
        tokenizer,
        train_data,
        formatting_func=prepare_sample_text,
        infinite=True,
        seq_length=args.seq_length,
        chars_per_token=chars_per_token,
    )
    valid_dataset = ConstantLengthDataset(
        tokenizer,
        valid_data,
        formatting_func=prepare_sample_text,
        infinite=False,
        seq_length=args.seq_length,
        chars_per_token=chars_per_token,
    )
    return train_dataset, valid_dataset


def run_training(args, train_data, val_data):
    print("Loading the model")

    lora_config = LoraConfig(
        r=16,
        lora_alpha=32,
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
    )

    train_data.start_iteration = 0

    print("Starting main loop")

    training_args = TrainingArguments(
        output_dir=args.output_dir,
        dataloader_drop_last=True,
        evaluation_strategy="steps",
        max_steps=args.max_steps,
        eval_steps=args.eval_freq,
        save_steps=args.save_freq,
        logging_steps=args.log_freq,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=args.batch_size,
        learning_rate=args.learning_rate,
        lr_scheduler_type=args.lr_scheduler_type,
        warmup_steps=args.num_warmup_steps,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        gradient_checkpointing=not args.no_gradient_checkpointing,
        fp16=not args.no_fp16,
        bf16=args.bf16,
        weight_decay=args.weight_decay,
        run_name="llama-7b-finetuned",
        report_to="wandb",
        ddp_find_unused_parameters=False,
    )

    model = AutoModelForCausalLM.from_pretrained(
        args.model_path, load_in_8bit=True, device_map={"": Accelerator().process_index}
    )

    trainer = SFTTrainer(
        model=model,
        args=training_args,
        train_dataset=train_data,
        eval_dataset=val_data,
        peft_config=lora_config,
        packing=True,
    )

    print_trainable_parameters(trainer.model)

    print("Training...")
    trainer.train()

    print("Saving last checkpoint of the model")
    trainer.model.save_pretrained(os.path.join(args.output_dir, "final_checkpoint/"))


def main(args):
    tokenizer = AutoTokenizer.from_pretrained(args.model_path)
    train_dataset, eval_dataset = create_datasets(tokenizer, args)
    run_training(args, train_dataset, eval_dataset)


if __name__ == "__main__":
    args = get_args()
    assert args.model_path != "", "Please provide the llama model path"

    set_seed(args.seed)
    os.makedirs(args.output_dir, exist_ok=True)

    logging.set_verbosity_error()

    main(args)

File Path: examples/research_projects/stack_llama_2/scripts/dpo_llama2.py
Content:
# 0. imports
import os
from dataclasses import dataclass, field
from typing import Dict, Optional

import torch
from datasets import Dataset, load_dataset
from peft import AutoPeftModelForCausalLM, LoraConfig
from transformers import AutoTokenizer, HfArgumentParser, TrainingArguments

from trl import DPOTrainer


# Define and parse arguments.
@dataclass
class ScriptArguments:
    """
    The arguments for the DPO training script.
    """

    # data parameters
    beta: Optional[float] = field(default=0.1, metadata={"help": "the beta parameter for DPO loss"})

    # training parameters
    model_name_or_path: Optional[str] = field(
        default="../sft/results/final_checkpoint",
        metadata={"help": "the location of the SFT model name or path"},
    )
    learning_rate: Optional[float] = field(default=5e-4, metadata={"help": "optimizer learning rate"})
    lr_scheduler_type: Optional[str] = field(default="cosine", metadata={"help": "the lr scheduler type"})
    warmup_steps: Optional[int] = field(default=100, metadata={"help": "the number of warmup steps"})
    weight_decay: Optional[float] = field(default=0.05, metadata={"help": "the weight decay"})
    optimizer_type: Optional[str] = field(default="paged_adamw_32bit", metadata={"help": "the optimizer type"})

    per_device_train_batch_size: Optional[int] = field(default=4, metadata={"help": "train batch size per device"})
    per_device_eval_batch_size: Optional[int] = field(default=1, metadata={"help": "eval batch size per device"})
    gradient_accumulation_steps: Optional[int] = field(
        default=4, metadata={"help": "the number of gradient accumulation steps"}
    )
    gradient_checkpointing: Optional[bool] = field(
        default=True, metadata={"help": "whether to use gradient checkpointing"}
    )

    lora_alpha: Optional[float] = field(default=16, metadata={"help": "the lora alpha parameter"})
    lora_dropout: Optional[float] = field(default=0.05, metadata={"help": "the lora dropout parameter"})
    lora_r: Optional[int] = field(default=8, metadata={"help": "the lora r parameter"})

    max_prompt_length: Optional[int] = field(default=512, metadata={"help": "the maximum prompt length"})
    max_length: Optional[int] = field(default=1024, metadata={"help": "the maximum sequence length"})
    max_steps: Optional[int] = field(default=1000, metadata={"help": "max number of training steps"})
    logging_steps: Optional[int] = field(default=10, metadata={"help": "the logging frequency"})
    save_steps: Optional[int] = field(default=100, metadata={"help": "the saving frequency"})
    eval_steps: Optional[int] = field(default=100, metadata={"help": "the evaluation frequency"})

    output_dir: Optional[str] = field(default="./results", metadata={"help": "the output directory"})
    log_freq: Optional[int] = field(default=1, metadata={"help": "the logging frequency"})

    # instrumentation
    sanity_check: Optional[bool] = field(default=False, metadata={"help": "only train on 1000 samples"})
    report_to: Optional[str] = field(
        default="wandb",
        metadata={
            "help": 'The list of integrations to report the results and logs to. Supported platforms are `"azure_ml"`,'
            '`"comet_ml"`, `"mlflow"`, `"neptune"`, `"tensorboard"`,`"clearml"` and `"wandb"`. '
            'Use `"all"` to report to all integrations installed, `"none"` for no integrations.'
        },
    )
    # debug argument for distributed training
    ignore_bias_buffers: Optional[bool] = field(
        default=False,
        metadata={
            "help": "fix for DDP issues with LM bias/mask buffers - invalid scalar type,`inplace operation. See"
            "https://github.com/huggingface/transformers/issues/22482#issuecomment-1595790992"
        },
    )


def get_stack_exchange_paired(
    data_dir: str = "data/rl",
    sanity_check: bool = False,
    cache_dir: str = None,
    num_proc=24,
) -> Dataset:
    """Load the stack-exchange-paired dataset from Hugging Face and convert it to the necessary format.

    The dataset is converted to a dictionary with the following structure:
    {
        'prompt': List[str],
        'chosen': List[str],
        'rejected': List[str],
    }

    Prompts are structured as follows:
      "Question: " + <prompt> + "\n\nAnswer: "
    """
    dataset = load_dataset(
        "lvwerra/stack-exchange-paired",
        split="train",
        cache_dir=cache_dir,
        data_dir=data_dir,
    )
    original_columns = dataset.column_names

    if sanity_check:
        dataset = dataset.select(range(min(len(dataset), 1000)))

    def return_prompt_and_responses(samples) -> Dict[str, str]:
        return {
            "prompt": ["Question: " + question + "\n\nAnswer: " for question in samples["question"]],
            "chosen": samples["response_j"],
            "rejected": samples["response_k"],
        }

    return dataset.map(
        return_prompt_and_responses,
        batched=True,
        num_proc=num_proc,
        remove_columns=original_columns,
    )


if __name__ == "__main__":
    parser = HfArgumentParser(ScriptArguments)
    script_args = parser.parse_args_into_dataclasses()[0]

    # 1. load a pretrained model
    model = AutoPeftModelForCausalLM.from_pretrained(
        script_args.model_name_or_path,
        low_cpu_mem_usage=True,
        torch_dtype=torch.float16,
        load_in_4bit=True,
    )
    model.config.use_cache = False

    if script_args.ignore_bias_buffers:
        # torch distributed hack
        model._ddp_params_and_buffers_to_ignore = [
            name for name, buffer in model.named_buffers() if buffer.dtype == torch.bool
        ]

    model_ref = AutoPeftModelForCausalLM.from_pretrained(
        script_args.model_name_or_path,
        low_cpu_mem_usage=True,
        torch_dtype=torch.float16,
        load_in_4bit=True,
    )
    tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")
    tokenizer.pad_token = tokenizer.eos_token

    # 2. Load the Stack-exchange paired dataset
    train_dataset = get_stack_exchange_paired(data_dir="data/rl", sanity_check=script_args.sanity_check)
    train_dataset = train_dataset.filter(
        lambda x: len(x["prompt"]) + len(x["chosen"]) <= script_args.max_length
        and len(x["prompt"]) + len(x["rejected"]) <= script_args.max_length
    )

    # 3. Load evaluation dataset
    eval_dataset = get_stack_exchange_paired(data_dir="data/evaluation", sanity_check=True)
    eval_dataset = eval_dataset.filter(
        lambda x: len(x["prompt"]) + len(x["chosen"]) <= script_args.max_length
        and len(x["prompt"]) + len(x["rejected"]) <= script_args.max_length
    )

    # 4. initialize training arguments:
    training_args = TrainingArguments(
        per_device_train_batch_size=script_args.per_device_train_batch_size,
        per_device_eval_batch_size=script_args.per_device_eval_batch_size,
        max_steps=script_args.max_steps,
        logging_steps=script_args.logging_steps,
        save_steps=script_args.save_steps,
        gradient_accumulation_steps=script_args.gradient_accumulation_steps,
        gradient_checkpointing=script_args.gradient_checkpointing,
        learning_rate=script_args.learning_rate,
        evaluation_strategy="steps",
        eval_steps=script_args.eval_steps,
        output_dir=script_args.output_dir,
        report_to=script_args.report_to,
        lr_scheduler_type=script_args.lr_scheduler_type,
        warmup_steps=script_args.warmup_steps,
        optim=script_args.optimizer_type,
        bf16=True,
        remove_unused_columns=False,
        run_name="dpo_llama2",
    )

    peft_config = LoraConfig(
        r=script_args.lora_r,
        lora_alpha=script_args.lora_alpha,
        lora_dropout=script_args.lora_dropout,
        target_modules=[
            "q_proj",
            "v_proj",
            "k_proj",
            "out_proj",
            "fc_in",
            "fc_out",
            "wte",
        ],
        bias="none",
        task_type="CAUSAL_LM",
    )

    # 5. initialize the DPO trainer
    dpo_trainer = DPOTrainer(
        model,
        model_ref,
        args=training_args,
        beta=script_args.beta,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        tokenizer=tokenizer,
        peft_config=peft_config,
        max_prompt_length=script_args.max_prompt_length,
        max_length=script_args.max_length,
    )

    # 6. train
    dpo_trainer.train()
    dpo_trainer.save_model(script_args.output_dir)

    # 7. save
    output_dir = os.path.join(script_args.output_dir, "final_checkpoint")
    dpo_trainer.model.save_pretrained(output_dir)

File Path: examples/research_projects/stack_llama_2/scripts/sft_llama2.py
Content:
# Fine-Tune Llama2-7b on SE paired dataset
import os
from dataclasses import dataclass, field
from typing import Optional

import torch
from accelerate import Accelerator
from datasets import load_dataset
from peft import AutoPeftModelForCausalLM, LoraConfig
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments

from trl import SFTTrainer
from trl.trainer import ConstantLengthDataset


@dataclass
class ScriptArguments:
    model_name: Optional[str] = field(default="meta-llama/Llama-2-7b-hf", metadata={"help": "the model name"})
    log_with: Optional[str] = field(default="none", metadata={"help": "use 'wandb' to log with wandb"})

    dataset_name: Optional[str] = field(default="lvwerra/stack-exchange-paired", metadata={"help": "the dataset name"})
    subset: Optional[str] = field(default="data/finetune", metadata={"help": "the subset to use"})
    split: Optional[str] = field(default="train", metadata={"help": "the split to use"})
    size_valid_set: Optional[int] = field(default=4000, metadata={"help": "the size of the validation set"})
    streaming: Optional[bool] = field(default=True, metadata={"help": "whether to stream the dataset"})
    shuffle_buffer: Optional[int] = field(default=5000, metadata={"help": "the shuffle buffer size"})
    seq_length: Optional[int] = field(default=1024, metadata={"help": "the sequence length"})
    num_workers: Optional[int] = field(default=4, metadata={"help": "the number of workers"})

    max_steps: Optional[int] = field(default=500, metadata={"help": "the maximum number of sgd steps"})
    logging_steps: Optional[int] = field(default=10, metadata={"help": "the logging frequency"})
    save_steps: Optional[int] = field(default=10, metadata={"help": "the saving frequency"})
    per_device_train_batch_size: Optional[int] = field(default=4, metadata={"help": "the per device train batch size"})
    per_device_eval_batch_size: Optional[int] = field(default=1, metadata={"help": "the per device eval batch size"})
    gradient_accumulation_steps: Optional[int] = field(default=2, metadata={"help": "the gradient accumulation steps"})
    gradient_checkpointing: Optional[bool] = field(
        default=True, metadata={"help": "whether to use gradient checkpointing"}
    )
    group_by_length: Optional[bool] = field(default=False, metadata={"help": "whether to group by length"})
    packing: Optional[bool] = field(default=True, metadata={"help": "whether to use packing for SFTTrainer"})

    lora_alpha: Optional[float] = field(default=16, metadata={"help": "the lora alpha parameter"})
    lora_dropout: Optional[float] = field(default=0.05, metadata={"help": "the lora dropout parameter"})
    lora_r: Optional[int] = field(default=8, metadata={"help": "the lora r parameter"})

    learning_rate: Optional[float] = field(default=1e-4, metadata={"help": "the learning rate"})
    lr_scheduler_type: Optional[str] = field(default="cosine", metadata={"help": "the lr scheduler type"})
    num_warmup_steps: Optional[int] = field(default=100, metadata={"help": "the number of warmup steps"})
    weight_decay: Optional[float] = field(default=0.05, metadata={"help": "the weight decay"})
    optimizer_type: Optional[str] = field(default="paged_adamw_32bit", metadata={"help": "the optimizer type"})

    output_dir: Optional[str] = field(default="./results", metadata={"help": "the output directory"})
    log_freq: Optional[int] = field(default=1, metadata={"help": "the logging frequency"})


parser = HfArgumentParser(ScriptArguments)
script_args = parser.parse_args_into_dataclasses()[0]

if script_args.group_by_length and script_args.packing:
    raise ValueError("Cannot use both packing and group by length")


def chars_token_ratio(dataset, tokenizer, nb_examples=400):
    """
    Estimate the average number of characters per token in the dataset.
    """
    total_characters, total_tokens = 0, 0
    for _, example in tqdm(zip(range(nb_examples), iter(dataset)), total=nb_examples):
        text = prepare_sample_text(example)
        total_characters += len(text)
        if tokenizer.is_fast:
            total_tokens += len(tokenizer(text).tokens())
        else:
            total_tokens += len(tokenizer.tokenize(text))

    return total_characters / total_tokens


def print_trainable_parameters(model):
    """
    Prints the number of trainable parameters in the model.
    """
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(
        f"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}"
    )


def prepare_sample_text(example):
    """Prepare the text from a sample of the dataset."""
    text = f"Question: {example['question']}\n\nAnswer: {example['response_j']}"
    return text


def create_datasets(tokenizer, args):
    dataset = load_dataset(
        args.dataset_name,
        data_dir=args.subset,
        split=args.split,
        use_auth_token=True,
        num_proc=args.num_workers if not args.streaming else None,
        streaming=args.streaming,
    )
    if args.streaming:
        print("Loading the dataset in streaming mode")
        valid_data = dataset.take(args.size_valid_set)
        train_data = dataset.skip(args.size_valid_set)
        train_data = train_data.shuffle(buffer_size=args.shuffle_buffer, seed=None)
    else:
        dataset = dataset.train_test_split(test_size=0.005, seed=None)
        train_data = dataset["train"]
        valid_data = dataset["test"]
        print(f"Size of the train set: {len(train_data)}. Size of the validation set: {len(valid_data)}")

    chars_per_token = chars_token_ratio(train_data, tokenizer)
    print(f"The character to token ratio of the dataset is: {chars_per_token:.2f}")

    train_dataset = ConstantLengthDataset(
        tokenizer,
        train_data,
        formatting_func=prepare_sample_text,
        infinite=True,
        seq_length=args.seq_length,
        chars_per_token=chars_per_token,
    )
    valid_dataset = ConstantLengthDataset(
        tokenizer,
        valid_data,
        formatting_func=prepare_sample_text,
        infinite=False,
        seq_length=args.seq_length,
        chars_per_token=chars_per_token,
    )
    return train_dataset, valid_dataset


bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)

base_model = AutoModelForCausalLM.from_pretrained(
    script_args.model_name,
    quantization_config=bnb_config,
    device_map={"": Accelerator().local_process_index},
    trust_remote_code=True,
    use_auth_token=True,
)
base_model.config.use_cache = False

peft_config = LoraConfig(
    r=script_args.lora_r,
    lora_alpha=script_args.lora_alpha,
    lora_dropout=script_args.lora_dropout,
    target_modules=["q_proj", "v_proj"],
    bias="none",
    task_type="CAUSAL_LM",
)

tokenizer = AutoTokenizer.from_pretrained(script_args.model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"  # Fix weird overflow issue with fp16 training


training_args = TrainingArguments(
    output_dir=script_args.output_dir,
    per_device_train_batch_size=script_args.per_device_train_batch_size,
    gradient_accumulation_steps=script_args.gradient_accumulation_steps,
    per_device_eval_batch_size=script_args.per_device_eval_batch_size,
    learning_rate=script_args.learning_rate,
    logging_steps=script_args.logging_steps,
    max_steps=script_args.max_steps,
    report_to=script_args.log_with,
    save_steps=script_args.save_steps,
    group_by_length=script_args.group_by_length,
    lr_scheduler_type=script_args.lr_scheduler_type,
    warmup_steps=script_args.num_warmup_steps,
    optim=script_args.optimizer_type,
    bf16=True,
    remove_unused_columns=False,
    run_name="sft_llama2",
)

train_dataset, eval_dataset = create_datasets(tokenizer, script_args)

trainer = SFTTrainer(
    model=base_model,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    peft_config=peft_config,
    packing=script_args.packing,
    max_seq_length=None,
    tokenizer=tokenizer,
    args=training_args,
)
trainer.train()
trainer.save_model(script_args.output_dir)

output_dir = os.path.join(script_args.output_dir, "final_checkpoint")
trainer.model.save_pretrained(output_dir)

# Free memory for merging weights
del base_model
torch.cuda.empty_cache()

model = AutoPeftModelForCausalLM.from_pretrained(output_dir, device_map="auto", torch_dtype=torch.bfloat16)
model = model.merge_and_unload()

output_merged_dir = os.path.join(script_args.output_dir, "final_merged_checkpoint")
model.save_pretrained(output_merged_dir, safe_serialization=True)

File Path: examples/research_projects/tools/calculator.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import re

import numpy as np
import torch
from transformers import AutoTokenizer, load_tool

from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, TextEnvironment


def generate_data(n):
    """Generate random arithmetic tasks and answers."""
    tasks, answers = [], []
    for _ in range(n):
        a = np.random.randint(0, 50)
        b = np.random.randint(0, 50)
        op = np.random.choice(["-", "+", "*"])
        tasks.append(f"\n\nWhat is {a} {op} {b}?")
        if op == "-":
            answers.append(a - b)
        elif op == "+":
            answers.append(a + b)
        else:
            answers.append(a * b)
    return tasks, answers


def exact_match_reward(responses, answers=None):
    """Reward if generated response contains correct answer."""
    rewards = []
    pattern = r"Result\s*=\s*(-?\d+(?:\.\d+)?)\s*<submit>"  # generated by chatGPT
    for response, answer in zip(responses, answers):
        reward = 0.0
        predicted_number = None
        match_pattern = re.findall(pattern, response)
        if match_pattern:
            predicted_number = float(match_pattern[0])
        if predicted_number is not None:
            if np.abs(predicted_number - answer) < 0.01:
                reward += 1.0
        rewards.append(torch.tensor(reward))
    return rewards


# set up models
model_id = "gpt2"
model = AutoModelForCausalLMWithValueHead.from_pretrained(model_id)
model_ref = AutoModelForCausalLMWithValueHead.from_pretrained(model_id)
tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.pad_token = tokenizer.eos_token

# system prompt
prompt = """\
What is 13-3?

<request><SimpleCalculatorTool>13-3<call>10.0<response>

Result=10<submit>

What is 4*3?

<request><SimpleCalculatorTool>4*3<call>12.0<response>

Result=12<submit>"""

generation_kwargs = {
    "min_length": -1,
    "top_k": 0.0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "eos_token_id": -1,
    "max_new_tokens": 32,
}

# trainer
ppo_config = PPOConfig(
    batch_size=256,
    learning_rate=1.41e-5,
    mini_batch_size=64,
    log_with="wandb",
)
ppo_trainer = PPOTrainer(ppo_config, model, model_ref, tokenizer)

# text env
text_env = TextEnvironment(
    model,
    tokenizer,
    {"SimpleCalculatorTool": load_tool("ybelkada/simple-calculator")},
    exact_match_reward,
    prompt,
    generation_kwargs=generation_kwargs,
)

# main training loop
for step in range(100):
    tasks, answers = generate_data(ppo_config.batch_size)
    queries, responses, masks, rewards, histories = text_env.run(tasks, answers=answers)
    train_stats = ppo_trainer.step(queries, responses, rewards, masks)

    response_texts = [tokenizer.decode(response) for response in responses]
    query_texts = [tokenizer.decode(query) for query in queries]
    texts = {"query": [qt.split("<submit>")[-1].strip() for qt in query_texts], "response": response_texts}
    ppo_trainer.log_stats(train_stats, texts, rewards, columns_to_log=["query", "response", "answer"])
ppo_trainer.save_pretrained(model_id + "-calculator")

File Path: examples/research_projects/tools/python_interpreter.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
import re
from dataclasses import dataclass, field
from typing import Optional

import numpy as np
import torch
from datasets import load_dataset
from peft import LoraConfig
from transformers import AutoTokenizer, HfArgumentParser, load_tool

from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, TextEnvironment


os.environ["HF_ALLOW_CODE_EVAL"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"


@dataclass
class ScriptArguments:
    model_name: Optional[str] = field(default="bigcode/starcoderbase", metadata={"help": "the model name"})
    learning_rate: Optional[float] = field(default=1e-5, metadata={"help": "the learning rate"})
    mini_batch_size: Optional[int] = field(default=1, metadata={"help": "the PPO minibatch size"})
    batch_size: Optional[int] = field(default=32, metadata={"help": "the batch size"})
    gradient_accumulation_steps: Optional[int] = field(
        default=16, metadata={"help": "the number of gradient accumulation steps"}
    )
    max_new_tokens: Optional[int] = field(default=256, metadata={"help": "max number of generated tokens per turn"})
    ppo_epochs: Optional[int] = field(default=1, metadata={"help": "max number of ppo epochs"})
    n_epochs: Optional[int] = field(default=32, metadata={"help": "max number of ppo epochs"})


parser = HfArgumentParser(ScriptArguments)
args = parser.parse_args_into_dataclasses()[0]


def exact_match_reward(responses, answers=None):
    """Reward if generated response contains correct answer."""
    rewards = []
    pattern = r"Result\s*=\s*(-?\d+(?:\.\d+)?)\s*<submit>"  # generated by chatGPT
    for response, answer in zip(responses, answers):
        reward = 0.0
        try:
            predicted_number = None
            match_pattern = re.findall(pattern, response)
            if match_pattern:
                predicted_number = float(match_pattern[0])
            if predicted_number is not None:
                if np.abs((predicted_number - float(answer))) < 0.1:
                    reward += 1.0
        except:  # noqa
            pass
        rewards.append(torch.tensor(reward))
    return rewards


def evaluate(test_dataloader, text_env, ppo_trainer):
    test_rewards = []
    for test_batch in test_dataloader:
        _, _, _, rewards, _ = text_env.run(test_batch["query"], answers=test_batch["answer"])
        test_rewards.extend(rewards)
    test_rewards = ppo_trainer.accelerator.gather_for_metrics(
        torch.stack(test_rewards).to(ppo_trainer.accelerator.device)
    )
    return test_rewards.mean()


lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["c_proj", "c_attn", "q_attn"],
)

# set up models
model = AutoModelForCausalLMWithValueHead.from_pretrained(
    args.model_name,
    use_auth_token=True,
    load_in_4bit=True,
    peft_config=lora_config,
)
tokenizer = AutoTokenizer.from_pretrained(args.model_name, use_auth_token=True)
tokenizer.pad_token = tokenizer.eos_token

ds = load_dataset("gsm8k", "main", split="train")
ds = ds.rename_columns({"question": "query"})
ds = ds.map(lambda x: {"answer": x["answer"].split("#### ")[1]})
ds = ds.select(range(1, len(ds)))  # skip the first sample which is used in prompt

ds_test = load_dataset("gsm8k", "main", split="test")
ds_test = ds_test.rename_columns({"question": "query"})
ds_test = ds_test.map(lambda x: {"answer": x["answer"].split("#### ")[1]})

test_dataloader = torch.utils.data.DataLoader(ds_test, batch_size=args.batch_size)

# prompt
prompt = """\
Example of using a Python API to solve math questions.

Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?

<request><PythonInterpreter>
def solution():
    money_initial = 23
    bagels = 5
    bagel_cost = 3
    money_spent = bagels * bagel_cost
    money_left = money_initial - money_spent
    result = money_left
    return result
print(solution())
<call>72<response>

Result = 72 <submit>

Q: """

generation_kwargs = {
    "min_length": -1,
    "top_k": 0.0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "eos_token_id": -1,
    "max_new_tokens": args.max_new_tokens,
}

# trainer
ppo_config = PPOConfig(
    batch_size=args.batch_size,
    learning_rate=args.learning_rate,
    mini_batch_size=args.mini_batch_size,
    ppo_epochs=args.ppo_epochs,
    gradient_accumulation_steps=args.gradient_accumulation_steps,
    log_with="wandb",
    tracker_project_name="trl-gsm8k",
    remove_unused_columns=False,
    optimize_cuda_cache=True,
)

ppo_trainer = PPOTrainer(config=ppo_config, model=model, tokenizer=tokenizer, dataset=ds)
test_dataloader = ppo_trainer.accelerator.prepare(test_dataloader)

# text env
text_env = TextEnvironment(
    model,
    tokenizer,
    [load_tool("lvwerra/python-interpreter")],
    exact_match_reward,
    prompt,
    max_turns=2,
    generation_kwargs=generation_kwargs,
)

# main training loop
for epoch in range(args.n_epochs):
    for step, batch in enumerate(ppo_trainer.dataloader):
        if (step == 0) and (epoch % 4 == 0):  # evaluate every 4 epochs
            reward_mean_test = evaluate(test_dataloader, text_env, ppo_trainer)
        else:
            reward_mean_test = None

        queries, responses, masks, rewards, histories = text_env.run(batch["query"], answers=batch["answer"])
        train_stats = ppo_trainer.step(queries, responses, rewards, masks)

        # logging
        if reward_mean_test is not None:
            train_stats["env/reward_mean_test"] = reward_mean_test
        texts = {
            "query": batch["query"],
            "response": [tokenizer.decode(response) for response in responses],
            "answer": batch["answer"],
        }
        ppo_trainer.log_stats(train_stats, texts, rewards, columns_to_log=["query", "response", "answer"])

reward_mean_test = evaluate(test_dataloader, text_env, ppo_trainer)
ppo_trainer.save_pretrained(f"model/{args.model_name}-gsm8k")

File Path: examples/research_projects/tools/triviaqa.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
from dataclasses import dataclass, field
from typing import Optional

import torch
from datasets import load_dataset
from peft import LoraConfig
from transformers import AutoTokenizer, HfArgumentParser, load_tool

from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, TextEnvironment


os.environ["HF_ALLOW_CODE_EVAL"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"


@dataclass
class ScriptArguments:
    model_name: Optional[str] = field(default="bigcode/starcoderbase", metadata={"help": "the model name"})
    log_with: Optional[str] = field(default=None, metadata={"help": "use 'wandb' to log with wandb"})
    learning_rate: Optional[float] = field(default=1e-5, metadata={"help": "the learning rate"})
    mini_batch_size: Optional[int] = field(default=1, metadata={"help": "the PPO minibatch size"})
    batch_size: Optional[int] = field(default=32, metadata={"help": "the batch size"})
    gradient_accumulation_steps: Optional[int] = field(
        default=16, metadata={"help": "the number of gradient accumulation steps"}
    )
    max_new_tokens: Optional[int] = field(default=256, metadata={"help": "max number of generated tokens per turn"})
    ppo_epochs: Optional[int] = field(default=1, metadata={"help": "max number of ppo epochs"})
    iterations: Optional[int] = field(default=1000, metadata={"help": "the number of iterations"})
    seed: Optional[int] = field(default=0, metadata={"help": "the random seed"})


parser = HfArgumentParser(ScriptArguments)
args = parser.parse_args_into_dataclasses()[0]

lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["c_proj", "c_attn", "q_attn"],
)

# set up models
model = AutoModelForCausalLMWithValueHead.from_pretrained(
    args.model_name,
    use_auth_token=True,
    trust_remote_code=True,
    load_in_4bit=True,
    peft_config=lora_config,
)
tokenizer = AutoTokenizer.from_pretrained(args.model_name, use_auth_token=True)
tokenizer.pad_token = tokenizer.eos_token

# system prompt
prompt = """\
Answer the following question:

Q: In which branch of the arts is Patricia Neary famous?
A: Ballets
A2: <request><Wiki>Patricia Neary<call>Patricia Neary (born October 27, 1942) is an American ballerina, choreographer and ballet director, who has been particularly active in Switzerland. She has also been a highly successful ambassador for the Balanchine Trust, bringing George Balanchine's ballets to 60 cities around the globe.<response>
Result=Ballets<submit>

Q: Who won Super Bowl XX?
A: Chicago Bears
A2: <request><Wiki>Super Bowl XX<call>Super Bowl XX was an American football game between the National Football Conference (NFC) champion Chicago Bears and the American Football Conference (AFC) champion New England Patriots to decide the National Football League (NFL) champion for the 1985 season. The Bears defeated the Patriots by the score of 46–10, capturing their first NFL championship (and Chicago's first overall sports victory) since 1963, three years prior to the birth of the Super Bowl. Super Bowl XX was played on January 26, 1986 at the Louisiana Superdome in New Orleans.<response>
Result=Chicago Bears<submit>

Q: """

generation_kwargs = {
    "min_length": -1,
    "top_k": 0.0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "eos_token_id": -1,
    "max_new_tokens": args.max_new_tokens,
}

# trainer
config = PPOConfig(
    batch_size=args.batch_size,
    model_name=args.model_name,
    learning_rate=args.learning_rate,
    log_with=args.log_with,
    mini_batch_size=args.mini_batch_size,
    ppo_epochs=args.ppo_epochs,
    gradient_accumulation_steps=args.gradient_accumulation_steps,
    seed=args.seed,
    optimize_cuda_cache=True,
)
ppo_trainer = PPOTrainer(config=config, model=model, tokenizer=tokenizer)
dataset = load_dataset("trivia_qa", "rc", split="train")
local_seed = args.seed + ppo_trainer.accelerator.process_index * 100003  # Prime
dataset = dataset.shuffle(local_seed)


def data_generator():
    for i in range(len(dataset)):
        yield dataset[i]["question"], [item for item in dataset[i]["answer"]["normalized_aliases"]]


gen = data_generator()
gen = iter(gen)


def generate_data(n):
    tasks, answers = [], []
    for i in range(n):
        q, a = next(gen)
        tasks.append(q)
        answers.append(a)
    return tasks, answers


def exact_match_reward(responses, answers=None):
    """Reward if generated response contains correct answer."""
    rewards = []
    for response, answer in zip(responses, answers):
        reward = 0.0
        for a in answer:
            if a.lower() in response.lower():
                reward += 1.0
                break
        rewards.append(torch.tensor(reward))
    return rewards


# text env
tool = load_tool("vwxyzjn/pyserini-wikipedia-kilt-doc")
# limit the amount if tokens
tool_fn = lambda x: tool(x).split("\n")[1][:600]  # noqa
text_env = TextEnvironment(
    model,
    tokenizer,
    {"Wiki": tool_fn},
    exact_match_reward,
    prompt,
    generation_kwargs=generation_kwargs,
    max_tool_reponse=400,
)


def print_trainable_parameters(model):
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(
        f"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}"
    )


print_trainable_parameters(model)
# main training loop
for i in range(args.iterations):
    tasks, answers = generate_data(config.batch_size)
    queries, responses, masks, rewards, histories = text_env.run(tasks, answers=answers)
    train_stats = ppo_trainer.step(queries, responses, rewards, masks)
    response_texts = [tokenizer.decode(response) for response in responses]
    query_texts = [tokenizer.decode(query) for query in queries]
    texts = {
        "query": [qt.split("<submit>")[-1].strip() for qt in query_texts],
        "response": response_texts,
        "answer": [", ".join(item) for item in answers],
    }
    all_rewards = ppo_trainer.accelerator.gather(torch.tensor(rewards, device=ppo_trainer.accelerator.device))
    ppo_trainer.log_stats(
        train_stats, texts, [item for item in all_rewards], columns_to_log=["query", "response", "answer"]
    )
    if i % 100 == 0:
        ppo_trainer.save_pretrained(f"models/{args.model_name}_{args.seed}_{i}_triviaqa")

File Path: examples/research_projects/toxicity/scripts/evaluate-toxicity.py
Content:
import argparse
import csv

import evaluate
import numpy as np
import torch
from datasets import load_dataset
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer


toxicity = evaluate.load("ybelkada/toxicity", "DaNLP/da-electra-hatespeech-detection", module_type="measurement")
ds = load_dataset("OxAISH-AL-LLM/wiki_toxic", split="test")

parser = argparse.ArgumentParser(description="Evaluate de-toxified models")
parser.add_argument("--model_type", default="all", type=str, help="Relative path to the source model folder")
parser.add_argument("--output_file", default="toxicity.csv", type=str, help="Relative path to the source model folder")
parser.add_argument("--batch_size", default=64, type=int, help="Batch size")
parser.add_argument("--num_samples", default=400, type=int, help="Number of samples")
parser.add_argument("--context_length", default=2000, type=int, help="Number of samples")
parser.add_argument("--max_new_tokens", default=30, type=int, help="Max new tokens for generation")
args = parser.parse_args()


if args.model_type == "all":
    MODELS_TO_TEST = [
        "ybelkada/gpt-neo-125m-detox",
        "EleutherAI/gpt-neo-125M",
        "EleutherAI/gpt-neo-2.7B",
        "ybelkada/gpt-neo-2.7B-detox",
        "ybelkada/gpt-j-6b-sharded-bf16",
        "ybelkada/gpt-j-6b-detoxs",
    ]
elif args.model_type == "gpt-neo":
    MODELS_TO_TEST = [
        "ybelkada/gpt-neo-125m-detox",
        "EleutherAI/gpt-neo-125M",
        "EleutherAI/gpt-neo-2.7B",
        "ybelkada/gpt-neo-2.7B-detox",
    ]
elif args.model_type == "gpt-j":
    MODELS_TO_TEST = [
        "ybelkada/gpt-j-6b-sharded-bf16",
        "ybelkada/gpt-j-6b-detox",
    ]
else:
    MODELS_TO_TEST = [args.model_type]
NUM_SAMPLES = args.num_samples
BATCH_SIZE = args.batch_size
output_file = args.output_file
max_new_tokens = args.max_new_tokens
context_length = args.context_length
device = torch.cuda.current_device() if torch.cuda.is_available() else "cpu"

# consider only toxic prompts
ds = ds.filter(lambda x: x["label"] == 1)

toxicities = {}

# open a csv file
file = open(f"{output_file}", "w", newline="")
writer = csv.writer(file)
# add first rows
writer.writerow(["model_id", "mean_toxicity", "std_toxicity"])


for model_id in tqdm(MODELS_TO_TEST):
    model = AutoModelForCausalLM.from_pretrained(model_id, device_map={"": device}, torch_dtype=torch.bfloat16)
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"
    input_texts = []

    for i, example in enumerate(ds):
        # set seed
        torch.manual_seed(42)

        input_text = example["comment_text"]
        input_texts.append(input_text[:2000])

        if i > NUM_SAMPLES:
            break

        if (i + 1) % BATCH_SIZE == 0:
            inputs = tokenizer(input_texts, return_tensors="pt", padding=True).to(device)
            inputs.input_ids = inputs.input_ids[:context_length]
            inputs.attention_mask = inputs.attention_mask[:context_length]
            outputs = model.generate(**inputs, do_sample=True, max_new_tokens=max_new_tokens, use_cache=True)
            generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)
            generated_texts = [
                generated_text.replace(input_texts[i], "") for i, generated_text in enumerate(generated_texts)
            ]
            toxicity_score = toxicity.compute(predictions=generated_texts)
            input_texts = []

            if model_id not in toxicities:
                toxicities[model_id] = []
            toxicities[model_id].extend(toxicity_score["toxicity"])

    # last batch
    inputs = tokenizer(input_texts, return_tensors="pt", padding=True).to(device)
    outputs = model.generate(**inputs, do_sample=True, max_new_tokens=30)
    generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)
    generated_texts = [generated_text.replace(input_texts[i], "") for i, generated_text in enumerate(generated_texts)]
    toxicity_score = toxicity.compute(predictions=generated_texts)
    toxicities[model_id].extend(toxicity_score["toxicity"])

    # compute mean & std using np
    mean = np.mean(toxicities[model_id])
    std = np.std(toxicities[model_id])

    # save to file
    writer.writerow([model_id, mean, std])

    # print
    print(f"Model: {model_id} - Mean: {mean} - Std: {std}")

    model = None
    torch.cuda.empty_cache()

# close file
file.close()

File Path: examples/research_projects/toxicity/scripts/gpt-j-6b-toxicity.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
from datasets import load_dataset
from torch.optim import Adam
from tqdm import tqdm
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    HfArgumentParser,
    RobertaForSequenceClassification,
    RobertaTokenizer,
)

from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, create_reference_model, set_seed
from trl.core import LengthSampler


tqdm.pandas()

########################################################################
# This is a fully working simple example to use trl with accelerate.
#
# This example fine-tunes a GPTJ model to generate less toxic contents
# by using allenai/real-toxicity-prompts dataset. We use PPO
#  (proximal policy optimization) to optimize the model.
# in any of the following settings (with the same script):
#   - single CPU or single GPU
#   - multi GPUS (using PyTorch distributed mode)
#   - multi GPUS (using DeepSpeed ZeRO-Offload stages 1 & 2)
#   - fp16 (mixed-precision) or fp32 (normal precision)
#
# To run it in each of these various modes, first initialize the accelerate
# configuration with `accelerate config`
#
########################################################################


# We first define the configuration of the experiment, defining the model, the dataset,
# the training parameters, and the PPO parameters.
# Check the default arguments in the `PPOConfig` class for more details.
# If you want to log with tensorboard, add the kwarg
# `project_kwargs={"logging_dir": PATH_TO_LOGS}` to the PPOConfig.
@dataclass
class ScriptArguments:
    """
    The name of the Casual LM model we wish to fine with PPO
    """

    # NOTE: gpt2 models use Conv1D instead of Linear layers which are not yet supported in 8 bit mode
    # models like gpt-neo* models are more suitable.
    model_name: Optional[str] = field(default="ybelkada/gpt-j-6b-sharded-bf16", metadata={"help": "the model name"})
    log_with: Optional[str] = field(default=None, metadata={"help": "use 'wandb' to log with wandb"})
    learning_rate: Optional[float] = field(default=(1.47e-5) * 2, metadata={"help": "the learning rate"})
    mini_batch_size: Optional[int] = field(default=4, metadata={"help": "the PPO minibatch size"})
    batch_size: Optional[int] = field(default=16, metadata={"help": "the batch size"})
    gradient_accumulation_steps: Optional[int] = field(
        default=1, metadata={"help": "the number of gradient accumulation steps"}
    )
    model_save_path: Optional[str] = field(
        default="./gpt-j-6B-detoxified-long-context-26-shl-1e4-final",
        metadata={"help": "the path to save the model"},
    )


parser = HfArgumentParser(ScriptArguments)
script_args = parser.parse_args_into_dataclasses()[0]

config = PPOConfig(
    model_name=script_args.model_name,
    learning_rate=script_args.learning_rate,
    log_with=script_args.log_with,
    ppo_epochs=100,
    mini_batch_size=script_args.mini_batch_size,
    batch_size=script_args.batch_size,
    gradient_accumulation_steps=script_args.gradient_accumulation_steps,
)


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(
    config, dataset_name="allenai/real-toxicity-prompts", input_min_text_length=5, input_max_text_length=10
):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        dataset_name (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token

    ds = load_dataset(dataset_name, split="train")

    def filter_fn(sample):
        toxicity = sample["prompt"]["toxicity"]
        return toxicity is not None and toxicity > 0.3

    ds = ds.filter(filter_fn, batched=False)

    input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        prompt = sample["prompt"]["text"]
        continuation = sample["continuation"]["text"]

        sample["input_ids"] = tokenizer.encode(prompt + continuation)[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")

    ds = ds.train_test_split(test_size=0.2, shuffle=False)["train"]

    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
min_input_length = 30
max_input_length = 40
dataset = build_dataset(config, input_min_text_length=min_input_length, input_max_text_length=max_input_length)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(config.seed)

# Now let's build the model, the reference model, and the tokenizer. We first load the model
# in bfloat16 to save memory using `transformers`.
model = AutoModelForCausalLM.from_pretrained(config.model_name, torch_dtype=torch.bfloat16)
# And then we pass the loaded model to `AutoModelForCausalLMWithValueHead`.
model = AutoModelForCausalLMWithValueHead.from_pretrained(model)

# We create a reference model by sharing 20 layers
ref_model = create_reference_model(model, num_shared_layers=20)

# We make sure to use `Adam` optimizer on the model parameters that require gradients.
optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=config.learning_rate)

# GPT-2 / GPT-J tokenizer has a pad token, but it is not eos_token by default. We need to set it to eos_token.
# only for this model.
tokenizer = AutoTokenizer.from_pretrained(config.model_name)
tokenizer.pad_token = tokenizer.eos_token

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(
    config,
    model,
    ref_model=ref_model,
    tokenizer=tokenizer,
    dataset=dataset,
    data_collator=collator,
    optimizer=optimizer,
)

# We then build the reward pipeline, we will use the toxicity model to compute the reward.
# We first load the toxicity model and tokenizer.
toxicity_model_id = "facebook/roberta-hate-speech-dynabench-r4-target"
toxicity_tokenizer = RobertaTokenizer.from_pretrained(toxicity_model_id)
# We load the toxicity model in fp16 to save memory.
toxicity_model = RobertaForSequenceClassification.from_pretrained(toxicity_model_id, torch_dtype=torch.float16).to(
    ppo_trainer.accelerator.device
)


# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs = {
    "min_length": -1,
    "top_k": 0.0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
}
output_min_length = 20
output_max_length = 30
output_length_sampler = LengthSampler(output_min_length, output_max_length)

model_save_path = script_args.model_save_path

for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from the policy model
    response_tensors = []
    for query in query_tensors:
        gen_len = output_length_sampler()
        generation_kwargs["max_new_tokens"] = gen_len
        response = ppo_trainer.generate(query, **generation_kwargs)
        response_tensors.append(response.squeeze()[-gen_len:])
    batch["response"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]

    # Compute sentiment score # noqa
    texts = batch["response"]
    toxicity_inputs = toxicity_tokenizer(texts, padding=True, truncation=True, return_tensors="pt").to(
        ppo_trainer.accelerator.device
    )
    logits = toxicity_model(**toxicity_inputs).logits.float()
    toxicity_labels = (logits[:, 0]).tolist()

    rewards = [torch.tensor(output) for output in toxicity_labels]

    # Run PPO step
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
    ppo_trainer.log_stats(stats, batch, rewards)

    # Save model every 100 epochs
    if epoch % 100 == 0:
        if ppo_trainer.accelerator.is_main_process:
            ppo_trainer.save_pretrained(model_save_path)

File Path: examples/scripts/multi_adapter_rl.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import LlamaTokenizer

from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer
from trl.core import LengthSampler


rm_adapter_id = "trl-lib/llama-7b-hh-rm-adapter"
model_name = "huggyllama/llama-7b"
dataset_name = "Anthropic/hh-rlhf"

input_min_text_length = 6
input_max_text_length = 12


def create_and_prepare_dataset(tokenizer):
    dataset = load_dataset(dataset_name, split="train[:1%]")

    input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(example):
        text_size = input_size()
        example["input_ids"] = tokenizer.encode(example["chosen"])[:text_size]
        example["query"] = tokenizer.decode(example["input_ids"])
        return example

    dataset = dataset.map(tokenize, batched=False)
    dataset.set_format("torch")
    return dataset


lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)
model = AutoModelForCausalLMWithValueHead.from_pretrained(
    model_name,
    load_in_8bit=True,
    device_map={"": 0},
    peft_config=lora_config,
    reward_adapter=rm_adapter_id,
)
tokenizer = LlamaTokenizer.from_pretrained(model_name)

tokenizer.pad_token = tokenizer.eos_token

dataset = create_and_prepare_dataset(tokenizer)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


config = PPOConfig(
    model_name=model_name,
    learning_rate=1e-5,
    batch_size=8,
    mini_batch_size=2,
    gradient_accumulation_steps=2,
    optimize_cuda_cache=True,
)

ppo_trainer = PPOTrainer(
    config,
    model,
    ref_model=None,
    tokenizer=tokenizer,
    dataset=dataset,
    data_collator=collator,
)

generation_kwargs = {
    "top_k": 0.0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.pad_token_id,
}

for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    question_tensors = batch["input_ids"]

    response_tensors = ppo_trainer.generate(
        question_tensors,
        return_prompt=False,
        **generation_kwargs,
    )
    batch["response"] = tokenizer.batch_decode(response_tensors, skip_special_tokens=True)

    # Compute reward score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt").to(ppo_trainer.accelerator.device)
    raw_rewards = ppo_trainer.model.compute_reward_score(**inputs)
    rewards = [raw_rewards[i, -1, 1] for i in range(len(raw_rewards))]  # take last token

    # Run PPO step
    stats = ppo_trainer.step(question_tensors, response_tensors, rewards)
    ppo_trainer.log_stats(stats, batch, rewards)

File Path: examples/scripts/multi_adapter_rl_v2.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import BitsAndBytesConfig, HfArgumentParser, LlamaTokenizer

from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer
from trl.core import LengthSampler


input_min_text_length = 6
input_max_text_length = 12


@dataclass
class ScriptArguments:
    """
    The name of the Casual LM model we wish to fine with PPO
    """

    model_name: Optional[str] = field(default="huggyllama/llama-7b", metadata={"help": "the model name"})
    dataset_name: Optional[str] = field(default="Anthropic/hh-rlhf", metadata={"help": "the dataset name"})
    rm_adapter: Optional[str] = field(
        default="trl-lib/llama-7b-hh-rm-adapter", metadata={"help": "the rm adapter name"}
    )
    log_with: Optional[str] = field(default=None, metadata={"help": "use 'wandb' to log with wandb"})
    use_safetensors: Optional[bool] = field(default=False, metadata={"help": "Use safetensors"})
    seed: Optional[int] = field(default=0, metadata={"help": "the random seed"})
    use_score_scaling: Optional[bool] = field(default=False, metadata={"help": "Use score scaling"})
    use_score_norm: Optional[bool] = field(
        default=False, metadata={"help": "Use score normalization. Only applicable if use_score_scaling is True"}
    )
    score_clip: Optional[float] = field(default=None, metadata={"help": "Score clipping"})


parser = HfArgumentParser(ScriptArguments)
script_args = parser.parse_args_into_dataclasses()[0]


def create_and_prepare_dataset(tokenizer):
    dataset = load_dataset(script_args.dataset_name, split="train[:1%]")

    input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(example):
        text_size = input_size()
        example["input_ids"] = tokenizer.encode(example["chosen"])[:text_size]
        example["query"] = tokenizer.decode(example["input_ids"])
        return example

    dataset = dataset.map(tokenize, batched=False)
    dataset.set_format("torch")
    return dataset


lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)
nf4_config = BitsAndBytesConfig(
    load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.bfloat16
)
model = AutoModelForCausalLMWithValueHead.from_pretrained(
    script_args.model_name,
    device_map={"": 0},
    peft_config=lora_config,
    quantization_config=nf4_config,
    reward_adapter=script_args.rm_adapter,
    use_safetensors=script_args.use_safetensors,
)
tokenizer = LlamaTokenizer.from_pretrained(script_args.model_name)

tokenizer.pad_token = tokenizer.eos_token

dataset = create_and_prepare_dataset(tokenizer)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


config = PPOConfig(
    model_name=script_args.model_name,
    log_with=script_args.log_with,
    learning_rate=1e-5,
    batch_size=8,
    mini_batch_size=2,
    gradient_accumulation_steps=2,
    optimize_cuda_cache=True,
    seed=script_args.seed,
    use_score_scaling=script_args.use_score_scaling,
    use_score_norm=script_args.use_score_norm,
    score_clip=script_args.score_clip,
)

ppo_trainer = PPOTrainer(
    config,
    model,
    ref_model=None,
    tokenizer=tokenizer,
    dataset=dataset,
    data_collator=collator,
)

generation_kwargs = {
    "top_k": 0.0,
    "top_p": 0.9,
    "do_sample": True,
    "pad_token_id": tokenizer.pad_token_id,
}

for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    question_tensors = batch["input_ids"]

    response_tensors = ppo_trainer.generate(
        question_tensors,
        return_prompt=False,
        **generation_kwargs,
    )
    batch["response"] = tokenizer.batch_decode(response_tensors, skip_special_tokens=True)

    # Compute reward score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt").to(ppo_trainer.accelerator.device)
    raw_rewards = ppo_trainer.model.compute_reward_score(**inputs)
    rewards = [raw_rewards[i, -1, 1] for i in range(len(raw_rewards))]  # take last token

    # Run PPO step
    stats = ppo_trainer.step(question_tensors, response_tensors, rewards)
    ppo_trainer.log_stats(stats, batch, rewards)

File Path: examples/scripts/reward_trainer.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoModelForSequenceClassification, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser

from trl import RewardConfig, RewardTrainer


tqdm.pandas()


@dataclass
class ScriptArguments:
    """
    Hyperparameters to fine-tune a reward model on a given dataset with the `RewardTrainer`.
    """

    model_name: Optional[str] = field(default="facebook/opt-350m", metadata={"help": "the model name"})
    dataset_name: Optional[str] = field(default="Anthropic/hh-rlhf", metadata={"help": "the dataset name"})
    dataset_text_field: Optional[str] = field(default="text", metadata={"help": "the text field of the dataset"})
    log_with: Optional[str] = field(default=None, metadata={"help": "use 'wandb' to log with wandb"})
    logging_steps: Optional[int] = field(default=500, metadata={"help": "the number of update steps between two logs"})
    eval_split: Optional[str] = field(
        default="none", metadata={"help": "the dataset split to evaluate on; default to 'none' (no evaluation)"}
    )
    learning_rate: Optional[float] = field(default=1.41e-5, metadata={"help": "the learning rate"})
    batch_size: Optional[int] = field(default=64, metadata={"help": "the batch size"})
    num_train_epochs: Optional[int] = field(default=1, metadata={"help": "the number of training epochs"})
    seq_length: Optional[int] = field(default=512, metadata={"help": "Input sequence length"})
    gradient_accumulation_steps: Optional[int] = field(
        default=16, metadata={"help": "the number of gradient accumulation steps"}
    )
    gradient_checkpointing: Optional[bool] = field(default=True, metadata={"help": "Enable gradient checkpointing"})
    load_in_8bit: Optional[bool] = field(default=False, metadata={"help": "load the model in 8 bits precision"})
    load_in_4bit: Optional[bool] = field(default=False, metadata={"help": "load the model in 4 bits precision"})
    use_peft: Optional[bool] = field(default=False, metadata={"help": "Wether to use PEFT or not to train adapters"})
    trust_remote_code: Optional[bool] = field(default=True, metadata={"help": "Enable `trust_remote_code`"})
    output_dir: Optional[str] = field(default="output", metadata={"help": "the output directory"})


parser = HfArgumentParser(ScriptArguments)
script_args = parser.parse_args_into_dataclasses()[0]

# Step 1: Load the model
if script_args.load_in_8bit and script_args.load_in_4bit:
    raise ValueError("You can't load the model in 8 bits and 4 bits at the same time")
elif script_args.load_in_8bit or script_args.load_in_4bit:
    quantization_config = BitsAndBytesConfig(
        load_in_8bit=script_args.load_in_8bit, load_in_4bit=script_args.load_in_4bit
    )
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}
else:
    device_map = None
    quantization_config = None

model = AutoModelForSequenceClassification.from_pretrained(
    script_args.model_name,
    quantization_config=quantization_config,
    device_map=device_map,
    trust_remote_code=script_args.trust_remote_code,
    num_labels=1,
)

# Step 2: Load the dataset and pre-process it
tokenizer = AutoTokenizer.from_pretrained(script_args.model_name)
train_dataset = load_dataset(script_args.dataset_name, split="train")


# Tokenize chosen/rejected pairs of inputs
# Adapt this section to your needs for custom datasets
def preprocess_function(examples):
    new_examples = {
        "input_ids_chosen": [],
        "attention_mask_chosen": [],
        "input_ids_rejected": [],
        "attention_mask_rejected": [],
    }
    for chosen, rejected in zip(examples["chosen"], examples["rejected"]):
        tokenized_chosen = tokenizer(chosen, truncation=True)
        tokenized_rejected = tokenizer(rejected, truncation=True)

        new_examples["input_ids_chosen"].append(tokenized_chosen["input_ids"])
        new_examples["attention_mask_chosen"].append(tokenized_chosen["attention_mask"])
        new_examples["input_ids_rejected"].append(tokenized_rejected["input_ids"])
        new_examples["attention_mask_rejected"].append(tokenized_rejected["attention_mask"])

    return new_examples


# Preprocess the dataset and filter out examples that are longer than script_args.max_length
train_dataset = train_dataset.map(
    preprocess_function,
    batched=True,
    num_proc=4,
)
train_dataset = train_dataset.filter(
    lambda x: len(x["input_ids_chosen"]) <= script_args.seq_length
    and len(x["input_ids_rejected"]) <= script_args.seq_length
)

if script_args.eval_split == "none":
    eval_dataset = None
else:
    eval_dataset = load_dataset(script_args.dataset_name, split=script_args.eval_split)

    eval_dataset = eval_dataset.map(
        preprocess_function,
        batched=True,
        num_proc=4,
    )
    eval_dataset = eval_dataset.filter(
        lambda x: len(x["input_ids_chosen"]) <= script_args.seq_length
        and len(x["input_ids_rejected"]) <= script_args.seq_length
    )


# Step 3: Define the training arguments
training_args = RewardConfig(
    output_dir=script_args.output_dir,
    per_device_train_batch_size=script_args.batch_size,
    num_train_epochs=script_args.num_train_epochs,
    gradient_accumulation_steps=script_args.gradient_accumulation_steps,
    gradient_checkpointing=script_args.gradient_checkpointing,
    learning_rate=script_args.learning_rate,
    report_to="wandb" if script_args.log_with == "wandb" else "tensorboard",
    remove_unused_columns=False,
    optim="adamw_torch",
    logging_steps=script_args.logging_steps,
    evaluation_strategy="steps" if script_args.eval_split != "none" else "no",
    max_length=script_args.seq_length,
)

# Step 4: Define the LoraConfig
if script_args.use_peft:
    peft_config = LoraConfig(r=16, lora_alpha=16, bias="none", task_type="SEQ_CLS", modules_to_save=["scores"])
else:
    peft_config = None

# Step 5: Define the Trainer
trainer = RewardTrainer(
    model=model,
    tokenizer=tokenizer,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    peft_config=peft_config,
)

trainer.train()

File Path: examples/scripts/sentiment_tuning.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed
from trl.core import LengthSampler


tqdm.pandas()

import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
logging_dir = f"/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_ppo/{date_n_time}"
os.makedirs(logging_dir)

@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name="lvwerra/gpt2-imdb",
            query_dataset="imdb",
            reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            #log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir}
        )
    )
    query_dataset: str = field(default="imdb", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )


args = tyro.cli(ScriptArguments)


# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # load imdb with datasets
    ds = load_dataset(query_dataset, split="train")
    ds = ds.rename_columns({"text": "review"})
    ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)

    input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    args.ppo_config.model_name,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
task, model_name = args.ppo_config.reward_model.split(":")
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
generation_kwargs = {
    "min_length": -1,
    "top_k": 0.0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "max_new_tokens": 32,
}

for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
    rewards = [torch.tensor(output[1]["score"]) for output in pipe_outputs]

    # Run PPO step
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
    ppo_trainer.log_stats(stats, batch, rewards)

File Path: examples/scripts/sentiment_tuning_auth1.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
import tyro
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoTokenizer, pipeline

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed
from trl.core import LengthSampler


tqdm.pandas()

import datetime
import os
from transformers import GPT2Tokenizer, GPT2Model
import pdb
import pandas as pd

date_n_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
logging_dir = f"/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/trl_sft_ppo/{date_n_time}"
os.makedirs(logging_dir)

@dataclass
class ScriptArguments:
    ppo_config: PPOConfig = field(
        default_factory=lambda: PPOConfig(
            model_name="lvwerra/gpt2-imdb",
            query_dataset="imdb",
            reward_model="sentiment-analysis:lvwerra/distilbert-imdb",
            learning_rate=1.41e-5,
            #log_with=None,
            mini_batch_size=128,
            batch_size=128,
            gradient_accumulation_steps=1,
            early_stopping=False,
            target_kl=6.0,
            kl_penalty="kl",
            seed=0,
            use_score_scaling=False,
            use_score_norm=False,
            score_clip=None,
            log_with="tensorboard",
            project_kwargs={"logging_dir": logging_dir},
            steps= 128000,
            exp_name="ppo"
        )
    )
    query_dataset: str = field(default="imdb", metadata={"help": "the dataset to query"})
    use_seq2seq: bool = field(default=False, metadata={"help": "whether to use seq2seq models"})
    use_peft: bool = field(default=False, metadata={"help": "whether to use peft"})
    peft_config: Optional[LoraConfig] = field(
        default_factory=lambda: LoraConfig(
            r=16,
            lora_alpha=16,
            bias="none",
            task_type="CAUSAL_LM",
        ),
    )
    reward_function: str = field(default="trl", metadata={"help": "whether to use trl or rl4lm reward"})
    input_query_size: str = field(default="less", metadata={"help": "the dataset to query"})
    generation_kwargs: dict = field(
    default_factory=lambda: {
      "min_length": -1, 
      "top_k": 0.0,
      "top_p": 1.0,
      "do_sample": True,
      "max_new_tokens": 32
    }
  )
    change_tokenizer_args: bool = field(default=False, metadata={"help": "whether to use modify tokenizer settings"})
    tokenizer_kwargs: dict = field(
    default_factory=lambda: {
      "padding_side": "left", 
      "truncation_side": "left",
      "pad_token_as_eos_token": True,
      "max_length": 64
    }
  )



args = tyro.cli(ScriptArguments)


# We then define the arguments to pass to the sentiment analysis pipeline.
# We set `return_all_scores` to True to get the sentiment score for each token.
sent_kwargs = {"return_all_scores": True, "function_to_apply": "none", "batch_size": 16}

trl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead


# Below is an example function to build the dataset. In our case, we use the IMDB dataset
# from the `datasets` library. One should customize this function to train the model on
# its own dataset.
def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):
    """
    Build dataset for training. This builds the dataset from `load_dataset`, one should
    customize this function to train the model on its own dataset.

    Args:
        query_dataset (`str`):
            The name of the dataset to be loaded.

    Returns:
        dataloader (`torch.utils.data.DataLoader`):
            The dataloader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # load imdb with datasets: for reducing data size, just use +'[:200]'
    ds = load_dataset(query_dataset, split="train")
    ds = ds.rename_columns({"text": "review"})
    ds = ds.filter(lambda x: len(x["review"]) > 200, batched=False)


    if args.input_query_size == "more":
        def input_size():
            return 64 
    else: 
        input_size = LengthSampler(input_min_text_length, input_max_text_length)

    def tokenize(sample):
        sample["input_ids"] = tokenizer.encode(sample["review"])[: input_size()]
        sample["query"] = tokenizer.decode(sample["input_ids"])
        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")
    return ds


# We retrieve the dataloader by calling the `build_dataset` function.
dataset = build_dataset(args.ppo_config, args.query_dataset)


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


# set seed before initializing value head for deterministic eval
set_seed(args.ppo_config.seed)

# Now let's build the model, the reference model, and the tokenizer.
if not args.use_peft:
    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)
    device_map = None
    peft_config = None
else:
    peft_config = args.peft_config
    ref_model = None
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}

model = trl_model_class.from_pretrained(
    args.ppo_config.model_name,
    trust_remote_code=True,
    device_map=device_map,
    peft_config=peft_config,
)


tokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
tokenizer.pad_token_id = tokenizer.eos_token_id

#pdb.set_trace()

if args.change_tokenizer_args == True: 
    tokenizer.padding_side = args.tokenizer_kwargs['padding_side']
    tokenizer.truncation_side = args.tokenizer_kwargs['truncation_side']
    tokenizer.pad_token_as_eos_token = args.tokenizer_kwargs['pad_token_as_eos_token']
    tokenizer.max_length = args.tokenizer_kwargs['max_length']

# We then build the PPOTrainer, passing the model, the reference model, the tokenizer
ppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)

# We then build the sentiment analysis pipeline, passing the model name and the
# sentiment analysis pipeline arguments. Let's also make sure to set the device
# to the same device as the PPOTrainer.
device = ppo_trainer.accelerator.device
if ppo_trainer.accelerator.num_processes == 1:
    device = 0 if torch.cuda.is_available() else "cpu"  # to avoid a `pipeline` bug
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
task, model_name = args.ppo_config.reward_model.split(":")
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline(task, model=model_name, device=device)
else:
    sentiment_pipe = pipeline(task, model=model_name, device=device)

# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.
if sentiment_pipe.tokenizer.pad_token_id is None:
    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id

if sentiment_pipe.model.config.pad_token_id is None:
    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id

# We then define the arguments to pass to the `generate` function. These arguments
# are passed to the `generate` function of the PPOTrainer, which is a wrapper around
# the `generate` function of the trained model.
args.generation_kwargs['pad_token_id'] = tokenizer.eos_token_id

# empty list to store text data
text_train_data = []

for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # Get response from gpt2
    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **args.generation_kwargs)
    batch["response"] = tokenizer.batch_decode(response_tensors)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)

    if args.reward_function == "trl":
        rewards = [torch.tensor(output[1]["score"]) for output in pipe_outputs]
    else: 
        rewards = [
            torch.nn.functional.softmax(torch.tensor([output[0]["score"], output[1]["score"]]))[1]
            for output in pipe_outputs
        ]
    
    for ii in range(3): 
        data_row = {'iter': epoch, 'input':batch["query"][ii], 'output':batch["response"][ii], 'score':rewards[ii].item()}
        text_train_data.append(data_row)
    
    #pdb.set_trace()

    # Run PPO step
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
    ppo_trainer.log_stats(stats, batch, rewards)
    print(epoch)

ppo_trainer._save_pretrained(logging_dir)
df = pd.DataFrame(text_train_data)
df.to_csv(logging_dir+'/textual_data.csv', index=False)
File Path: examples/scripts/sft_trainer.py
Content:
# coding=utf-8
# Copyright 2023 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

import torch
from accelerate import Accelerator
from datasets import load_dataset
from peft import LoraConfig
from tqdm import tqdm
from transformers import AutoModelForCausalLM, BitsAndBytesConfig, HfArgumentParser, TrainingArguments

from trl import SFTTrainer


tqdm.pandas()


# Define and parse arguments.
@dataclass
class ScriptArguments:
    """
    The name of the Casual LM model we wish to fine with SFTTrainer
    """

    model_name: Optional[str] = field(default="facebook/opt-350m", metadata={"help": "the model name"})
    dataset_name: Optional[str] = field(
        default="timdettmers/openassistant-guanaco", metadata={"help": "the dataset name"}
    )
    dataset_text_field: Optional[str] = field(default="text", metadata={"help": "the text field of the dataset"})
    log_with: Optional[str] = field(default="none", metadata={"help": "use 'wandb' to log with wandb"})
    learning_rate: Optional[float] = field(default=1.41e-5, metadata={"help": "the learning rate"})
    batch_size: Optional[int] = field(default=64, metadata={"help": "the batch size"})
    seq_length: Optional[int] = field(default=512, metadata={"help": "Input sequence length"})
    gradient_accumulation_steps: Optional[int] = field(
        default=16, metadata={"help": "the number of gradient accumulation steps"}
    )
    load_in_8bit: Optional[bool] = field(default=False, metadata={"help": "load the model in 8 bits precision"})
    load_in_4bit: Optional[bool] = field(default=False, metadata={"help": "load the model in 4 bits precision"})
    use_peft: Optional[bool] = field(default=False, metadata={"help": "Wether to use PEFT or not to train adapters"})
    trust_remote_code: Optional[bool] = field(default=True, metadata={"help": "Enable `trust_remote_code`"})
    output_dir: Optional[str] = field(default="output", metadata={"help": "the output directory"})
    peft_lora_r: Optional[int] = field(default=64, metadata={"help": "the r parameter of the LoRA adapters"})
    peft_lora_alpha: Optional[int] = field(default=16, metadata={"help": "the alpha parameter of the LoRA adapters"})
    logging_steps: Optional[int] = field(default=1, metadata={"help": "the number of logging steps"})
    use_auth_token: Optional[bool] = field(default=True, metadata={"help": "Use HF auth token to access the model"})
    num_train_epochs: Optional[int] = field(default=3, metadata={"help": "the number of training epochs"})
    max_steps: Optional[int] = field(default=-1, metadata={"help": "the number of training steps"})
    save_steps: Optional[int] = field(
        default=100, metadata={"help": "Number of updates steps before two checkpoint saves"}
    )
    save_total_limit: Optional[int] = field(default=10, metadata={"help": "Limits total number of checkpoints."})
    push_to_hub: Optional[bool] = field(default=False, metadata={"help": "Push the model to HF Hub"})
    hub_model_id: Optional[str] = field(default=None, metadata={"help": "The name of the model on HF Hub"})


parser = HfArgumentParser(ScriptArguments)
script_args = parser.parse_args_into_dataclasses()[0]

# Step 1: Load the model
if script_args.load_in_8bit and script_args.load_in_4bit:
    raise ValueError("You can't load the model in 8 bits and 4 bits at the same time")
elif script_args.load_in_8bit or script_args.load_in_4bit:
    quantization_config = BitsAndBytesConfig(
        load_in_8bit=script_args.load_in_8bit, load_in_4bit=script_args.load_in_4bit
    )
    # Copy the model to each device
    device_map = {"": Accelerator().local_process_index}
    torch_dtype = torch.bfloat16
else:
    device_map = None
    quantization_config = None
    torch_dtype = None

model = AutoModelForCausalLM.from_pretrained(
    script_args.model_name,
    quantization_config=quantization_config,
    device_map=device_map,
    trust_remote_code=script_args.trust_remote_code,
    torch_dtype=torch_dtype,
    use_auth_token=script_args.use_auth_token,
)

# Step 2: Load the dataset
dataset = load_dataset(script_args.dataset_name, split="train")

# Step 3: Define the training arguments
training_args = TrainingArguments(
    output_dir=script_args.output_dir,
    per_device_train_batch_size=script_args.batch_size,
    gradient_accumulation_steps=script_args.gradient_accumulation_steps,
    learning_rate=script_args.learning_rate,
    logging_steps=script_args.logging_steps,
    num_train_epochs=script_args.num_train_epochs,
    max_steps=script_args.max_steps,
    report_to=script_args.log_with,
    save_steps=script_args.save_steps,
    save_total_limit=script_args.save_total_limit,
    push_to_hub=script_args.push_to_hub,
    hub_model_id=script_args.hub_model_id,
)

# Step 4: Define the LoraConfig
if script_args.use_peft:
    peft_config = LoraConfig(
        r=script_args.peft_lora_r,
        lora_alpha=script_args.peft_lora_alpha,
        bias="none",
        task_type="CAUSAL_LM",
    )
else:
    peft_config = None

# Step 5: Define the Trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    max_seq_length=script_args.seq_length,
    train_dataset=dataset,
    dataset_text_field=script_args.dataset_text_field,
    peft_config=peft_config,
)

trainer.train()

# Step 6: Save the model
trainer.save_model(script_args.output_dir)

File Path: examples/scripts/stable_diffusion_tuning.py
Content:
# Copyright 2023 metric-space, The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


import argparse
import os

import numpy as np
import torch
import torch.nn as nn
from huggingface_hub import hf_hub_download
from huggingface_hub.utils import EntryNotFoundError
from transformers import CLIPModel, CLIPProcessor

from trl import DDPOConfig, DDPOTrainer, DefaultDDPOStableDiffusionPipeline


class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(768, 1024),
            nn.Dropout(0.2),
            nn.Linear(1024, 128),
            nn.Dropout(0.2),
            nn.Linear(128, 64),
            nn.Dropout(0.1),
            nn.Linear(64, 16),
            nn.Linear(16, 1),
        )

    @torch.no_grad()
    def forward(self, embed):
        return self.layers(embed)


class AestheticScorer(torch.nn.Module):
    """
    This model attempts to predict the aesthetic score of an image. The aesthetic score
    is a numerical approximation of how much a specific image is liked by humans on average.
    This is from https://github.com/christophschuhmann/improved-aesthetic-predictor
    """

    def __init__(self, *, dtype, model_id, model_filename):
        super().__init__()
        self.clip = CLIPModel.from_pretrained("openai/clip-vit-large-patch14")
        self.processor = CLIPProcessor.from_pretrained("openai/clip-vit-large-patch14")
        self.mlp = MLP()
        try:
            cached_path = hf_hub_download(model_id, model_filename)
        except EntryNotFoundError:
            cached_path = os.path.join(model_id, model_filename)
        state_dict = torch.load(cached_path)
        self.mlp.load_state_dict(state_dict)
        self.dtype = dtype
        self.eval()

    @torch.no_grad()
    def __call__(self, images):
        device = next(self.parameters()).device
        inputs = self.processor(images=images, return_tensors="pt")
        inputs = {k: v.to(self.dtype).to(device) for k, v in inputs.items()}
        embed = self.clip.get_image_features(**inputs)
        # normalize embedding
        embed = embed / torch.linalg.vector_norm(embed, dim=-1, keepdim=True)
        return self.mlp(embed).squeeze(1)


def aesthetic_scorer(hub_model_id, model_filename):
    scorer = AestheticScorer(
        model_id=hub_model_id,
        model_filename=model_filename,
        dtype=torch.float32,
    ).cuda()

    def _fn(images, prompts, metadata):
        images = (images * 255).round().clamp(0, 255).to(torch.uint8)
        scores = scorer(images)
        return scores, {}

    return _fn


# list of example prompts to feed stable diffusion
animals = [
    "cat",
    "dog",
    "horse",
    "monkey",
    "rabbit",
    "zebra",
    "spider",
    "bird",
    "sheep",
    "deer",
    "cow",
    "goat",
    "lion",
    "frog",
    "chicken",
    "duck",
    "goose",
    "bee",
    "pig",
    "turkey",
    "fly",
    "llama",
    "camel",
    "bat",
    "gorilla",
    "hedgehog",
    "kangaroo",
]


def prompt_fn():
    return np.random.choice(animals), {}


def image_outputs_logger(image_data, global_step, accelerate_logger):
    # For the sake of this example, we will only log the last batch of images
    # and associated data
    result = {}
    images, prompts, _, rewards, _ = image_data[-1]

    for i, image in enumerate(images):
        prompt = prompts[i]
        reward = rewards[i].item()
        result[f"{prompt:.25} | {reward:.2f}"] = image.unsqueeze(0)

    accelerate_logger.log_images(
        result,
        step=global_step,
    )


def parse_arguments():
    parser = argparse.ArgumentParser(description="DDPOConfig settings and pretrained model details.")

    # DDPOConfig arguments
    parser.add_argument("--num_epochs", type=int, default=200)
    parser.add_argument("--train_gradient_accumulation_steps", type=int, default=1)
    parser.add_argument("--sample_num_steps", type=int, default=50)
    parser.add_argument("--sample_batch_size", type=int, default=6)
    parser.add_argument("--train_batch_size", type=int, default=3)
    parser.add_argument("--sample_num_batches_per_epoch", type=int, default=4)
    parser.add_argument("--per_prompt_stat_tracking", action="store_true", default=True)
    parser.add_argument("--per_prompt_stat_tracking_buffer_size", type=int, default=32)
    parser.add_argument("--tracker_project_name", default="stable_diffusion_training")
    parser.add_argument("--log_with", default="wandb")

    parser.add_argument("--logging_dir", default="./logs")
    parser.add_argument("--automatic_checkpoint_naming", action="store_true", default=True)
    parser.add_argument("--total_limit", type=int, default=5)
    parser.add_argument("--project_dir", default="./save")

    parser.add_argument("--pretrained_model", default="runwayml/stable-diffusion-v1-5")
    parser.add_argument("--pretrained_revision", default="main")
    parser.add_argument("--hf_user_access_token", required=True)
    parser.add_argument(
        "--hf_hub_model_id",
        help="HuggingFace repo to save model weights to",
        default="ddpo-finetuned-stable-diffusion",
    )

    parser.add_argument(
        "--hf_hub_aesthetic_model_id",
        help="HuggingFace model ID for aesthetic scorer model weights",
        default="trl-lib/ddpo-aesthetic-predictor",
    )

    parser.add_argument(
        "--hf_hub_aesthetic_model_filename",
        default="aesthetic-model.pth",
        help="HuggingFace model filename for aesthetic scorer model weights",
    )

    return parser.parse_args()


if __name__ == "__main__":
    args = parse_arguments()

    project_kwargs = {
        "logging_dir": args.logging_dir,
        "automatic_checkpoint_naming": args.automatic_checkpoint_naming,
        "total_limit": args.total_limit,
        "project_dir": args.project_dir,
    }

    config = DDPOConfig(
        num_epochs=args.num_epochs,
        train_gradient_accumulation_steps=args.train_gradient_accumulation_steps,
        sample_num_steps=args.sample_num_steps,
        sample_batch_size=args.sample_batch_size,
        train_batch_size=args.train_batch_size,
        sample_num_batches_per_epoch=args.sample_num_batches_per_epoch,
        per_prompt_stat_tracking=args.per_prompt_stat_tracking,
        per_prompt_stat_tracking_buffer_size=args.per_prompt_stat_tracking_buffer_size,
        tracker_project_name=args.tracker_project_name,
        log_with=args.log_with,
        project_kwargs=project_kwargs,
    )

    pipeline = DefaultDDPOStableDiffusionPipeline(
        args.pretrained_model, pretrained_model_revision=args.pretrained_revision, use_lora=True
    )

    trainer = DDPOTrainer(
        config,
        aesthetic_scorer(args.hf_hub_aesthetic_model_id, args.hf_hub_aesthetic_model_filename),
        prompt_fn,
        pipeline,
        image_samples_hook=image_outputs_logger,
    )

    trainer.train()

    trainer.push_to_hub(args.hf_hub_model_id, token=args.hf_user_access_token)

File Path: scripts/stale.py
Content:
# Copyright 2023 The HuggingFace Team, the AllenNLP library authors. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
Script to close stale issue. Taken in part from the AllenNLP repository.
https://github.com/allenai/allennlp.
"""
import os
from datetime import datetime as dt

from github import Github


LABELS_TO_EXEMPT = [
    "good first issue",
    "good second issue",
    "feature request",
]


def main():
    g = Github(os.environ["GITHUB_TOKEN"])
    repo = g.get_repo("huggingface/trl")
    open_issues = repo.get_issues(state="open")

    for issue in open_issues:
        comments = sorted([comment for comment in issue.get_comments()], key=lambda i: i.created_at, reverse=True)
        last_comment = comments[0] if len(comments) > 0 else None
        if (
            last_comment is not None
            and last_comment.user.login == "github-actions[bot]"
            and (dt.utcnow() - issue.updated_at).days > 7
            and (dt.utcnow() - issue.created_at).days >= 30
            and not any(label.name.lower() in LABELS_TO_EXEMPT for label in issue.get_labels())
        ):
            issue.edit(state="closed")
        elif (
            (dt.utcnow() - issue.updated_at).days > 23
            and (dt.utcnow() - issue.created_at).days >= 30
            and not any(label.name.lower() in LABELS_TO_EXEMPT for label in issue.get_labels())
        ):
            issue.create_comment(
                "This issue has been automatically marked as stale because it has not had "
                "recent activity. If you think this still needs to be addressed "
                "please comment on this thread.\n\n"
            )


if __name__ == "__main__":
    main()

File Path: setup.py
Content:
""" trl is an open library for RL with transformer models.

Note:

   VERSION needs to be formatted following the MAJOR.MINOR.PATCH convention
   (we need to follow this convention to be able to retrieve versioned scripts)

Simple check list for release from AllenNLP repo: https://github.com/allenai/allennlp/blob/master/setup.py

To create the package for pypi.

0. Prerequisites:
   - Dependencies:
     - twine: "pip install twine"
   - Create an account in (and join the 'trl' project):
     - PyPI: https://pypi.org/
     - Test PyPI: https://test.pypi.org/

1. Change the version in:
   - __init__.py
   - setup.py

2. Commit these changes: "git commit -m 'Release: VERSION'"

3. Add a tag in git to mark the release: "git tag VERSION -m 'Add tag VERSION for pypi'"
   Push the tag to remote: git push --tags origin main

4. Build both the sources and the wheel. Do not change anything in setup.py between
   creating the wheel and the source distribution (obviously).

   First, delete any "build" directory that may exist from previous builds.

   For the wheel, run: "python setup.py bdist_wheel" in the top level directory.
   (this will build a wheel for the python version you use to build it).

   For the sources, run: "python setup.py sdist"
   You should now have a /dist directory with both .whl and .tar.gz source versions.

5. Check that everything looks correct by uploading the package to the pypi test server:

   twine upload dist/* -r pypitest --repository-url=https://test.pypi.org/legacy/

   Check that you can install it in a virtualenv/notebook by running:
   pip install huggingface_hub fsspec aiohttp
   pip install -U tqdm
   pip install -i https://testpypi.python.org/pypi evaluate

6. Upload the final version to actual pypi:
   twine upload dist/* -r pypi

7. Fill release notes in the tag in github once everything is looking hunky-dory.

8. Change the version in __init__.py and setup.py to X.X.X+1.dev0 (e.g. VERSION=1.18.3 -> 1.18.4.dev0).
   Then push the change with a message 'set dev version'
"""

from setuptools import find_packages, setup


__version__ = "0.7.2.dev0"  # expected format is one of x.y.z.dev0, or x.y.z.rc1 or x.y.z (no to dashes, yes to dots)

REQUIRED_PKGS = [
    "torch>=1.4.0",
    "transformers>=4.18.0",
    "numpy>=1.18.2",
    "accelerate",
    "datasets",
    "tyro>=0.5.7",
]
EXTRAS = {
    "test": ["parameterized", "pytest", "pytest-xdist", "accelerate", "peft>=0.4.0", "diffusers>=0.18.0"],
    "peft": ["peft>=0.4.0"],
    "diffusers": ["diffusers>=0.18.0"],
    "deepspeed": ["deepspeed>=0.9.5"],
    "dev": ["parameterized", "pytest", "pytest-xdist", "pre-commit", "peft>=0.4.0", "diffusers>=0.18.0"],
    "benchmark": ["wandb", "ghapi", "openrlbenchmark==0.2.1a5", "requests", "deepspeed"],
}

setup(
    name="trl",
    license="Apache 2.0",
    classifiers=[
        "Development Status :: 2 - Pre-Alpha",
        "Intended Audience :: Developers",
        "Intended Audience :: Science/Research",
        "License :: OSI Approved :: Apache Software License",
        "Natural Language :: English",
        "Operating System :: OS Independent",
        "Programming Language :: Python :: 3",
        "Programming Language :: Python :: 3.7",
        "Programming Language :: Python :: 3.8",
        "Programming Language :: Python :: 3.9",
        "Programming Language :: Python :: 3.10",
    ],
    url="https://github.com/huggingface/trl",
    packages=find_packages(),
    include_package_data=True,
    install_requires=REQUIRED_PKGS,
    extras_require=EXTRAS,
    python_requires=">=3.7",
    long_description=open("README.md", encoding="utf-8").read(),
    long_description_content_type="text/markdown",
    zip_safe=False,
    version=__version__,
    description="A Pytorch implementation of Proximal Policy Optimization for transfomer language models.",
    keywords="ppo, transformers, huggingface, gpt2, language modeling, rlhf",
    author="Leandro von Werra",
    author_email="leandro.vonwerra@gmail.com",
)

File Path: tests/__init__.py
Content:

File Path: tests/test_best_of_n_sampler.py
Content:
import unittest

import torch
from transformers import AutoTokenizer, GenerationConfig

from trl import AutoModelForCausalLMWithValueHead
from trl.core import LengthSampler
from trl.extras import BestOfNSampler


def queries_to_scores(list_of_strings):
    return [torch.rand(1).item() for _ in list_of_strings]


class BestOfNSamplerTester(unittest.TestCase):
    """
    Tests the BestOfNSampler class
    """

    ref_model_name = "trl-internal-testing/dummy-GPT2-correct-vocab"
    output_length_sampler = LengthSampler(2, 6)
    model = AutoModelForCausalLMWithValueHead.from_pretrained(ref_model_name)
    tokenizer = AutoTokenizer.from_pretrained(ref_model_name)
    tokenizer.pad_token = tokenizer.eos_token
    output_length_sampler = LengthSampler(2, 6)

    def test_different_input_types(self):
        r"""
        Tests if the different input types normalizer works
        """

        generation_config = GenerationConfig(
            min_length=-1,
            top_k=0.0,
            top_p=1.0,
            do_sample=True,
            pad_token_id=self.tokenizer.eos_token_id,
        )

        output_length_sampler = LengthSampler(2, 6)

        best_of_n = BestOfNSampler(
            self.model,
            self.tokenizer,
            queries_to_scores,
            length_sampler=output_length_sampler,
            generation_config=generation_config,
        )

        queries = ["hello world", "goodbye world"]
        tokenized_queries = [self.tokenizer.encode(query) for query in queries]

        various_queries_formats = [
            (tokenized_queries[0], 1),
            (tokenized_queries, 2),
            (torch.tensor(tokenized_queries[1]), 1),
            ([torch.tensor(query) for query in tokenized_queries], 2),
        ]

        for q, expected_length in various_queries_formats:
            results = best_of_n.generate(q)
            self.assertIsInstance(results, list)
            assert len(results) == expected_length

    def test_different_sample_sizes_and_n_candidates_values(self):
        r"""
        Tests different sample sizes and n_candidates values
        """
        generation_config = GenerationConfig(
            min_length=-1,
            top_k=0.0,
            top_p=1.0,
            do_sample=True,
            pad_token_id=self.tokenizer.eos_token_id,
        )

        output_length_sampler = LengthSampler(6, 10)

        for sample_value, n_candidates_values, expected in [
            (4, 2, 2),
            (10, 3, 3),
            (6, 4, 4),
        ]:
            best_of_n = BestOfNSampler(
                self.model,
                self.tokenizer,
                queries_to_scores,
                length_sampler=output_length_sampler,
                generation_config=generation_config,
                sample_size=sample_value,
                n_candidates=n_candidates_values,
            )

            queries = ["hello world", "troll the world"]
            tokenized_queries = [self.tokenizer.encode(query) for query in queries]
            results = best_of_n.generate(tokenized_queries)
            for result in results:
                assert len(result) == expected

File Path: tests/test_core.py
Content:
# Copyright 2022 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import unittest

import torch

from trl.core import masked_mean, masked_var, masked_whiten, whiten


class CoreTester(unittest.TestCase):
    """
    A wrapper class for testing core utils functions
    """

    @classmethod
    def setUpClass(cls):
        cls.test_input = torch.Tensor([1, 2, 3, 4])
        cls.test_mask = torch.Tensor([0, 1, 1, 0])
        cls.test_input_unmasked = cls.test_input[1:3]

    def test_masked_mean(self):
        self.assertEqual(torch.mean(self.test_input_unmasked), masked_mean(self.test_input, self.test_mask))

    def test_masked_var(self):
        self.assertEqual(torch.var(self.test_input_unmasked), masked_var(self.test_input, self.test_mask))

    def test_masked_whiten(self):
        whiten_unmasked = whiten(self.test_input_unmasked)
        whiten_masked = masked_whiten(self.test_input, self.test_mask)[1:3]
        diffs = (whiten_unmasked - whiten_masked).sum()
        self.assertAlmostEqual(diffs, 0)

File Path: tests/test_data_collator_completion_only.py
Content:
# Copyright 2023 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import unittest

import torch
from transformers import AutoTokenizer

from trl import DataCollatorForCompletionOnlyLM


class DataCollatorForCompletionOnlyLMTester(unittest.TestCase):
    def test_data_collator_finds_response_template_llama2_tokenizer(self):
        self.tokenizer = AutoTokenizer.from_pretrained("trl-internal-testing/dummy-GPT2-correct-vocab")
        self.instruction = """### System: You are a helpful assistant.

### User: How much is 2+2?

### Assistant: 2+2 equals 4"""
        self.response_template = "\n### Assistant:"

        # GPT2Tokenizer: [198, 21017, 15286, 25] -> [15286, 25]
        # Llama2Tokenizer: [29871, 13, 2277, 29937, 4007, 22137, 29901] -> [2277, 29937, 4007, 22137, 29901]
        self.tokenized_response_w_context = self.tokenizer.encode(self.response_template, add_special_tokens=False)[2:]

        # Plain check on string
        self.assertIn(self.response_template, self.instruction)
        self.tokenized_instruction = self.tokenizer.encode(self.instruction, add_special_tokens=False)

        # Test the fix for #598
        # Pass already tokenized (w context) and truncated response_template so token_ids are like in the instruction + response
        self.collator = DataCollatorForCompletionOnlyLM(self.tokenized_response_w_context, tokenizer=self.tokenizer)
        self.collator.torch_call([self.tokenized_instruction])

    def test_data_collator_handling_of_long_sequences(self):
        self.tokenizer = AutoTokenizer.from_pretrained("trl-internal-testing/dummy-GPT2-correct-vocab")
        self.instruction = """### System: You are a helpful assistant.

### User: How much is 2+2? I'm asking because I'm not sure. And I'm not sure because I'm not good at math.
"""
        self.response_template = "\n### Assistant:"
        # check DataCollatorForCompletionOnlyLM using response template only
        self.tokenized_instruction = self.tokenizer.encode(self.instruction, add_special_tokens=False)
        self.collator = DataCollatorForCompletionOnlyLM(self.response_template, tokenizer=self.tokenizer)
        encoded_instance = self.collator.torch_call([self.tokenized_instruction])
        result = torch.all(encoded_instance["labels"] == -100)
        self.assertTrue(result, "Not all values in the tensor are -100.")

        # check DataCollatorForCompletionOnlyLM using response template and instruction template
        self.instruction_template = "\n### User:"
        self.collator = DataCollatorForCompletionOnlyLM(
            self.response_template, self.instruction_template, tokenizer=self.tokenizer
        )
        encoded_instance = self.collator.torch_call([self.tokenized_instruction])
        result = torch.all(encoded_instance["labels"] == -100)
        self.assertTrue(result, "Not all values in the tensor are -100.")

File Path: tests/test_ddpo_trainer.py
Content:
# Copyright 2023 metric-space, The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import gc
import unittest

import torch

from trl import DDPOConfig, DDPOTrainer, DefaultDDPOStableDiffusionPipeline


def scorer_function(images, prompts, metadata):
    return torch.randn(1) * 3.0, {}


def prompt_function():
    return ("cabbages", {})


class DDPOTrainerTester(unittest.TestCase):
    """
    Test the DDPOTrainer class.
    """

    def setUp(self):
        self.ddpo_config = DDPOConfig(
            num_epochs=2,
            train_gradient_accumulation_steps=1,
            per_prompt_stat_tracking_buffer_size=32,
            sample_num_batches_per_epoch=2,
            sample_batch_size=2,
            mixed_precision=None,
            save_freq=1000000,
        )
        pretrained_model = "hf-internal-testing/tiny-stable-diffusion-torch"
        pretrained_revision = "main"

        pipeline = DefaultDDPOStableDiffusionPipeline(
            pretrained_model, pretrained_model_revision=pretrained_revision, use_lora=False
        )

        self.trainer = DDPOTrainer(self.ddpo_config, scorer_function, prompt_function, pipeline)

        return super().setUp()

    def tearDown(self) -> None:
        gc.collect()

    def test_loss(self):
        advantage = torch.tensor([-1.0])
        clip_range = 0.0001
        ratio = torch.tensor([1.0])
        loss = self.trainer.loss(advantage, clip_range, ratio)
        self.assertEqual(loss.item(), 1.0)

    def test_generate_samples(self):
        samples, output_pairs = self.trainer._generate_samples(1, 2)
        self.assertEqual(len(samples), 1)
        self.assertEqual(len(output_pairs), 1)
        self.assertEqual(len(output_pairs[0][0]), 2)

    def test_calculate_loss(self):
        samples, _ = self.trainer._generate_samples(1, 2)
        sample = samples[0]

        latents = sample["latents"][0, 0].unsqueeze(0)
        next_latents = sample["next_latents"][0, 0].unsqueeze(0)
        log_probs = sample["log_probs"][0, 0].unsqueeze(0)
        timesteps = sample["timesteps"][0, 0].unsqueeze(0)
        prompt_embeds = sample["prompt_embeds"]
        advantage = torch.tensor([1.0], device=prompt_embeds.device)

        self.assertEqual(latents.shape, (1, 4, 64, 64))
        self.assertEqual(next_latents.shape, (1, 4, 64, 64))
        self.assertEqual(log_probs.shape, (1,))
        self.assertEqual(timesteps.shape, (1,))
        self.assertEqual(prompt_embeds.shape, (2, 77, 32))
        loss, approx_kl, clipfrac = self.trainer.calculate_loss(
            latents, timesteps, next_latents, log_probs, advantage, prompt_embeds
        )

        self.assertTrue(torch.isfinite(loss.cpu()))

File Path: tests/test_dpo_trainer.py
Content:
# Copyright 2023 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import tempfile
import unittest

import torch
from datasets import Dataset
from parameterized import parameterized
from pytest import mark
from transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoTokenizer, TrainingArguments

from trl import DPOTrainer

from .testing_utils import require_peft


class DPOTrainerTester(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        cls.model_id = "trl-internal-testing/dummy-GPT2-correct-vocab"
        cls.model = AutoModelForCausalLM.from_pretrained(cls.model_id)
        cls.ref_model = AutoModelForCausalLM.from_pretrained(cls.model_id)
        cls.tokenizer = AutoTokenizer.from_pretrained(cls.model_id)
        cls.tokenizer.pad_token = cls.tokenizer.eos_token

        # get t5 as seq2seq example:
        model_id = "trl-internal-testing/tiny-T5ForConditionalGeneration-correct-vocab"
        cls.t5_model = AutoModelForSeq2SeqLM.from_pretrained(model_id)
        cls.t5_ref_model = AutoModelForSeq2SeqLM.from_pretrained(model_id)
        cls.t5_tokenizer = AutoTokenizer.from_pretrained(model_id)

    def _init_dummy_dataset(self):
        # fmt: off
        dummy_dataset_dict = {
            "prompt": [
                "hello",
                "how are you",
                "What is your name?",
                "What is your name?",
                "Which is the best programming language?",
                "Which is the best programming language?",
                "Which is the best programming language?",
            ],
            "chosen": [
                "hi nice to meet you",
                "I am fine",
                "My name is Mary",
                "My name is Mary",
                "Python",
                "Python",
                "Python",
            ],
            "rejected": [
                "leave me alone",
                "I am not fine",
                "Whats it to you?",
                "I dont have a name",
                "Javascript",
                "C++",
                "Java",
            ],
        }
        # fmt: on
        return Dataset.from_dict(dummy_dataset_dict)

    @parameterized.expand(
        [
            ["gpt2"],
            ["t5"],
        ]
    )
    def test_dpo_trainer(self, name):
        with tempfile.TemporaryDirectory() as tmp_dir:
            training_args = TrainingArguments(
                output_dir=tmp_dir,
                per_device_train_batch_size=2,
                max_steps=3,
                remove_unused_columns=False,
                gradient_accumulation_steps=1,
                learning_rate=9e-1,
                evaluation_strategy="steps",
            )

            dummy_dataset = self._init_dummy_dataset()

            if name == "gpt2":
                model = self.model
                ref_model = self.ref_model
                tokenizer = self.tokenizer
            elif name == "t5":
                model = self.t5_model
                ref_model = self.t5_ref_model
                tokenizer = self.t5_tokenizer

            trainer = DPOTrainer(
                model=model,
                ref_model=ref_model,
                beta=0.1,
                args=training_args,
                tokenizer=tokenizer,
                train_dataset=dummy_dataset,
                eval_dataset=dummy_dataset,
            )

            previous_trainable_params = {n: param.clone() for n, param in trainer.model.named_parameters()}

            trainer.train()

            self.assertIsNotNone(trainer.state.log_history[-1]["train_loss"])

            # check the params have changed
            for n, param in previous_trainable_params.items():
                new_param = trainer.model.get_parameter(n)
                # check the params have changed - ignore 0 biases
                if param.sum() != 0:
                    self.assertFalse(torch.equal(param, new_param))

    def test_dpo_trainer_without_providing_ref_model(self):
        with tempfile.TemporaryDirectory() as tmp_dir:
            training_args = TrainingArguments(
                output_dir=tmp_dir,
                per_device_train_batch_size=2,
                max_steps=3,
                remove_unused_columns=False,
                gradient_accumulation_steps=4,
                learning_rate=9e-1,
                evaluation_strategy="steps",
            )

            dummy_dataset = self._init_dummy_dataset()

            trainer = DPOTrainer(
                model=self.model,
                ref_model=None,
                beta=0.1,
                args=training_args,
                tokenizer=self.tokenizer,
                train_dataset=dummy_dataset,
                eval_dataset=dummy_dataset,
            )

            previous_trainable_params = {n: param.clone() for n, param in trainer.model.named_parameters()}

            trainer.train()

            self.assertIsNotNone(trainer.state.log_history[-1]["train_loss"])

            # check the params have changed
            for n, param in previous_trainable_params.items():
                new_param = trainer.model.get_parameter(n)
                # check the params have changed - ignore 0 biases
                if param.sum() != 0:
                    self.assertFalse(torch.equal(param, new_param))

    @require_peft
    @mark.peft_test
    def test_dpo_trainer_without_providing_ref_model_with_lora(self):
        from peft import LoraConfig

        lora_config = LoraConfig(
            r=16,
            lora_alpha=32,
            lora_dropout=0.05,
            bias="none",
            task_type="CAUSAL_LM",
        )

        with tempfile.TemporaryDirectory() as tmp_dir:
            training_args = TrainingArguments(
                output_dir=tmp_dir,
                per_device_train_batch_size=2,
                max_steps=3,
                remove_unused_columns=False,
                gradient_accumulation_steps=4,
                learning_rate=9e-1,
                evaluation_strategy="steps",
            )

            dummy_dataset = self._init_dummy_dataset()

            trainer = DPOTrainer(
                model=self.model,
                ref_model=None,
                beta=0.1,
                args=training_args,
                tokenizer=self.tokenizer,
                train_dataset=dummy_dataset,
                eval_dataset=dummy_dataset,
                peft_config=lora_config,
            )

            previous_trainable_params = {n: param.clone() for n, param in trainer.model.named_parameters()}

            trainer.train()

            self.assertIsNotNone(trainer.state.log_history[-1]["train_loss"])

            # check the params have changed
            for n, param in previous_trainable_params.items():
                if "lora" in n:
                    new_param = trainer.model.get_parameter(n)
                    # check the params have changed - ignore 0 biases
                    if param.sum() != 0:
                        self.assertFalse(torch.equal(param, new_param))

File Path: tests/test_e2e.py
Content:
import subprocess


def test_hello_world():
    subprocess.run(
        "python examples/hello_world.py",
        shell=True,
        check=True,
    )

File Path: tests/test_environments.py
Content:
# Copyright 2023 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import unittest
from unittest.mock import patch

import torch
from transformers import AutoTokenizer

from trl import AutoModelForCausalLMWithValueHead, TextEnvironment, TextHistory


class DummyTool:
    def __call__(self, text):
        return text


def dummy_generate(histories):
    for i in range(len(histories)):
        histories[i].append_segment("<request><DummyTool>test<call>", torch.tensor([1, 2, 3]), system=False)
    return histories


class TextHistoryTest(unittest.TestCase):
    def test_text_history_init(self):
        text = "Hello there!"
        tokens = torch.tensor([1, 2, 3])

        history = TextHistory(text, tokens)
        self.assertEqual(history.text, text)
        self.assertTrue(torch.equal(history.tokens, tokens))
        self.assertTrue(torch.equal(history.token_masks, torch.zeros_like(tokens)))

        history = TextHistory(text, tokens, system=False)
        self.assertTrue(torch.equal(history.token_masks, torch.ones_like(tokens)))

    def test_text_history_append_segment(self):
        text = "Hello there!"
        tokens = torch.tensor([1, 2, 3])

        history = TextHistory(text, tokens)
        history.append_segment("General Kenobi!", torch.tensor([4, 5, 6]), system=False)
        self.assertEqual(history.text, text + "General Kenobi!")
        self.assertTrue(torch.equal(history.tokens, torch.tensor([1, 2, 3, 4, 5, 6])))
        self.assertTrue(torch.equal(history.token_masks, torch.tensor([0, 0, 0, 1, 1, 1])))

        history.append_segment("You are a bold one!", torch.tensor([7, 8, 9]))
        self.assertEqual(history.text, text + "General Kenobi!" + "You are a bold one!")
        self.assertTrue(torch.equal(history.tokens, torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])))
        self.assertTrue(torch.equal(history.token_masks, torch.tensor([0, 0, 0, 1, 1, 1, 0, 0, 0])))

    def test_text_history_complete(self):
        text = "Hello there!"
        tokens = torch.tensor([1, 2, 3])
        history = TextHistory(text, tokens)
        history.complete()
        self.assertTrue(history.completed)
        self.assertFalse(history.truncated)

        history.complete(truncated=True)
        self.assertTrue(history.completed)
        self.assertTrue(history.truncated)

    def test_text_history_last_segment(self):
        text = "Hello there!"
        tokens = torch.tensor([1, 2, 3])
        history = TextHistory(text, tokens)
        history.append_segment("General Kenobi!", torch.tensor([4, 5, 6]))
        history.append_segment("You are a bold one!", torch.tensor([7, 8, 9]))
        self.assertEqual(history.last_text_segment, "You are a bold one!")

    def test_text_history_split_query_response(self):
        text = "Hello there!"
        tokens = torch.tensor([1, 2, 3])
        history = TextHistory(text, tokens)
        history.append_segment("General Kenobi!", torch.tensor([4, 5, 6]), system=False)
        history.append_segment("You are a bold one!", torch.tensor([7, 8, 9]), system=True)
        query, response, mask = history.split_query_response_tokens()

        self.assertTrue(torch.equal(query, torch.tensor([1, 2, 3])))
        self.assertTrue(torch.equal(response, torch.tensor([4, 5, 6, 7, 8, 9])))
        self.assertTrue(torch.equal(mask, torch.tensor([1, 1, 1, 0, 0, 0])))


class TextEnvironmentTester(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        # model_id
        cls.model_id = "trl-internal-testing/dummy-GPT2-correct-vocab"

        # get models and tokenizer
        cls.gpt2_model = AutoModelForCausalLMWithValueHead.from_pretrained(cls.model_id)
        cls.gpt2_tokenizer = AutoTokenizer.from_pretrained(cls.model_id)
        cls.gpt2_tokenizer.pad_token = cls.gpt2_tokenizer.eos_token

    def test_text_environment_setup(self):
        env = TextEnvironment(
            self.gpt2_model,
            self.gpt2_tokenizer,
            tools=[DummyTool()],
            reward_fn=lambda x: torch.tensor(1),
            prompt="I am a prompt!\n",
        )
        self.assertEqual(env.prompt, "I am a prompt!\n")
        self.assertEqual(list(env.tools.keys()), ["DummyTool"])
        self.assertTrue(isinstance(env.tools["DummyTool"], DummyTool))
        self.assertEqual(env.reward_fn("Hello there!"), 1)

    def test_text_environment_generate(self):
        generation_kwargs = {"do_sample": False, "max_new_tokens": 4, "pad_token_id": self.gpt2_tokenizer.eos_token_id}
        env = TextEnvironment(
            self.gpt2_model,
            self.gpt2_tokenizer,
            tools=[DummyTool()],
            reward_fn=lambda x: torch.tensor(1),
            prompt="I am a prompt!\n",
            generation_kwargs=generation_kwargs,
        )

        input_texts = ["this is a test", "this is another, longer test"]

        model_inputs = [self.gpt2_tokenizer(txt, return_tensors="pt").input_ids.squeeze() for txt in input_texts]

        generations_batched = env._generate_batched(model_inputs, batch_size=2)
        generations_batched = self.gpt2_tokenizer.batch_decode(generations_batched)

        generations_single = [env._generate_batched([inputs], batch_size=1)[0] for inputs in model_inputs]
        generations_single = self.gpt2_tokenizer.batch_decode(generations_single)

        self.assertEqual(generations_single, generations_batched)

    def test_text_environment_tool_call_parsing(self):
        string_valid = "Something something <request><Tool1>Hello there!<call>"
        string_invalid_request = "Something something <Tool1>Hello there!<call>"
        string_invalid_call = "Something something <request><Tool1>Hello there!"
        string_invalid_tool = "Something something <request>|Tool2|Hello there!<call>"
        string_invalid_random = "<>abcdefghijklm<>nopqrstuvwxyz<>"

        env = TextEnvironment(
            self.gpt2_model,
            self.gpt2_tokenizer,
            tools=[DummyTool()],
            reward_fn=lambda x: torch.tensor(1),
            prompt="I am a prompt!\n",
        )
        tool, response = env.parse_tool_call(string_valid)
        self.assertEqual(tool, "Tool1")
        self.assertEqual(response, "Hello there!")

        tool, response = env.parse_tool_call(string_invalid_request)
        self.assertEqual(tool, None)
        self.assertEqual(response, None)

        tool, response = env.parse_tool_call(string_invalid_call)
        self.assertEqual(tool, None)
        self.assertEqual(response, None)

        tool, response = env.parse_tool_call(string_invalid_tool)
        self.assertEqual(tool, None)
        self.assertEqual(response, None)

        tool, response = env.parse_tool_call(string_invalid_random)
        self.assertEqual(tool, None)
        self.assertEqual(response, None)

    def test_text_environment_tool_truncation(self):
        env = TextEnvironment(
            self.gpt2_model,
            self.gpt2_tokenizer,
            tools={"dummy": lambda x: "a" * 1000},
            reward_fn=lambda x: torch.tensor(1),
            prompt="I am a prompt!\n",
        )

        env.max_tool_response = 100
        history = env.step(TextHistory("<request><dummy>Hello there!<call>", torch.tensor([1, 2, 3])))
        self.assertEqual(len(history.last_text_segment) - len(env.response_token), 100)

        env.max_tool_response = 500
        history = env.step(TextHistory("<request><dummy>Hello there!<call>", torch.tensor([1, 2, 3])))
        self.assertEqual(len(history.last_text_segment) - len(env.response_token), 500)

        env.max_tool_response = 1001
        history = env.step(TextHistory("<request><dummy>Hello there!<call>", torch.tensor([1, 2, 3])))
        self.assertEqual(len(history.last_text_segment) - len(env.response_token), 1000)

        env.max_tool_response = 2000
        history = env.step(TextHistory("<request><dummy>Hello there!<call>", torch.tensor([1, 2, 3])))
        self.assertEqual(len(history.last_text_segment) - len(env.response_token), 1000)

    @patch.object(TextEnvironment, "generate", side_effect=dummy_generate)
    def test_text_environment_max_calls(self, mock_generate):
        env = TextEnvironment(
            self.gpt2_model,
            self.gpt2_tokenizer,
            tools={"DummyTool": DummyTool()},
            reward_fn=lambda x: [torch.tensor(1) for _ in x],
            prompt="I am a prompt!\n",
        )

        env.max_turns = 1
        _, _, _, _, histories = env.run(["test"])
        self.assertEqual(
            histories[0].text, "I am a prompt!\n" + "test" + 1 * "<request><DummyTool>test<call>test<response>"
        )

        env.max_turns = 2
        _, _, _, _, histories = env.run(["test"])
        self.assertEqual(
            histories[0].text, "I am a prompt!\n" + "test" + 2 * "<request><DummyTool>test<call>test<response>"
        )

        env.max_turns = 4
        _, _, _, _, histories = env.run(["test"])
        self.assertEqual(
            histories[0].text, "I am a prompt!\n" + "test" + 4 * "<request><DummyTool>test<call>test<response>"
        )

    def test_text_environment_compute_rewards(self):
        env = TextEnvironment(
            self.gpt2_model,
            self.gpt2_tokenizer,
            tools={"DummyTool": DummyTool()},
            reward_fn=lambda x: [torch.tensor(i) for i, _ in enumerate(x)],
            prompt="I am a prompt!\n",
        )

        histories = [TextHistory("<request><DummyTool>test<call>", torch.tensor([1, 2, 3])) for _ in range(8)]
        histories = env.compute_reward(histories)

        for i in range(8):
            self.assertEqual(histories[i].reward, i)

    @patch.object(TextEnvironment, "generate", side_effect=dummy_generate)
    def test_text_environment_run(self, mock_generate):
        env = TextEnvironment(
            self.gpt2_model,
            self.gpt2_tokenizer,
            tools={"DummyTool": DummyTool()},
            reward_fn=lambda x: [torch.tensor(i) for i, _ in enumerate(x)],
            prompt="I am a prompt!\n",
            max_turns=2,
        )
        task_1 = "Hello there!"
        task_2 = "Hello there! General Kenobi!"

        query, response, response_mask, reward, histories = env.run([task_1, task_2])
        self.assertEqual(len(query[0]), 9)
        self.assertEqual(len(query[1]), 12)
        self.assertEqual(len(response[0]), 14)
        self.assertEqual(len(response[1]), 14)
        self.assertEqual(response_mask[0].sum(), 2 * 3)  # mocked generate always adds 3 toknes
        self.assertEqual(response_mask[1].sum(), 2 * 3)  # mocked generate always adds 3 toknes
        self.assertEqual(reward[0], 0)
        self.assertEqual(reward[1], 1)
        self.assertEqual(
            histories[0].text, "I am a prompt!\n" + "Hello there!" + 2 * "<request><DummyTool>test<call>test<response>"
        )
        self.assertEqual(
            histories[1].text,
            "I am a prompt!\n" + "Hello there! General Kenobi!" + 2 * "<request><DummyTool>test<call>test<response>",
        )

File Path: tests/test_modeling_value_head.py
Content:
# Copyright 2022 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import gc
import tempfile
import unittest

import torch
from transformers import AutoModel, AutoModelForCausalLM, AutoModelForSeq2SeqLM

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, create_reference_model


ALL_CAUSAL_LM_MODELS = [
    "trl-internal-testing/tiny-random-CodeGenForCausalLM",
    "trl-internal-testing/tiny-random-GPTJForCausalLM",
    "trl-internal-testing/tiny-random-GPTNeoForCausalLM",
    "trl-internal-testing/tiny-random-GPTNeoXForCausalLM",
    "trl-internal-testing/tiny-random-OPTForCausalLM",
    "trl-internal-testing/tiny-random-BloomForCausalLM",
    "trl-internal-testing/tiny-random-GPT2LMHeadModel",
    "trl-internal-testing/tiny-random-CodeGenForCausalLM-sharded",
    # "trl-internal-testing/tiny-random-LlamaForCausalLM", uncomment on the next transformers release
]

ALL_SEQ2SEQ_MODELS = [
    "trl-internal-testing/tiny-random-BartForConditionalGeneration",
    "trl-internal-testing/tiny-random-BigBirdPegasusForConditionalGeneration",
    "trl-internal-testing/tiny-random-BlenderbotForConditionalGeneration",
    "trl-internal-testing/tiny-random-BlenderbotSmallForConditionalGeneration",
    "trl-internal-testing/tiny-random-FSMTForConditionalGeneration",
    "trl-internal-testing/tiny-random-LEDForConditionalGeneration",
    "trl-internal-testing/tiny-random-LongT5ForConditionalGeneration",
    "trl-internal-testing/tiny-random-M2M100ForConditionalGeneration",
    "trl-internal-testing/tiny-random-MarianMTModel",
    "trl-internal-testing/tiny-random-MBartForConditionalGeneration",
    "trl-internal-testing/tiny-random-MT5ForConditionalGeneration",
    "trl-internal-testing/tiny-random-MvpForConditionalGeneration",
    "trl-internal-testing/tiny-random-PegasusForConditionalGeneration",
    "trl-internal-testing/tiny-random-PegasusXForConditionalGeneration",
    "trl-internal-testing/tiny-random-PLBartForConditionalGeneration",
    "trl-internal-testing/tiny-random-ProphetNetForConditionalGeneration",
    "trl-internal-testing/tiny-random-SwitchTransformersForConditionalGeneration",
    "trl-internal-testing/tiny-random-T5ForConditionalGeneration",
]


class VHeadModelTester:
    all_model_names = None
    trl_model_class = None
    transformers_model_class = None

    def test_value_head(self):
        r"""
        Test if the v-head is added to the model successfully
        """
        for model_name in self.all_model_names:
            model = self.trl_model_class.from_pretrained(model_name)
            self.assertTrue(hasattr(model, "v_head"))

    def test_value_head_shape(self):
        r"""
        Test if the v-head has the correct shape
        """
        for model_name in self.all_model_names:
            model = self.trl_model_class.from_pretrained(model_name)
            self.assertTrue(model.v_head.summary.weight.shape[0] == 1)

    def test_value_head_init_random(self):
        r"""
        Test if the v-head has been randomly initialized.
        We can check that by making sure the bias is different
        than zeros by default.
        """
        for model_name in self.all_model_names:
            model = self.trl_model_class.from_pretrained(model_name)
            self.assertFalse(torch.allclose(model.v_head.summary.bias, torch.zeros_like(model.v_head.summary.bias)))

    def test_value_head_not_str(self):
        r"""
        Test if the v-head is added to the model successfully, by passing a non `PretrainedModel`
        as an argument to `from_pretrained`.
        """
        for model_name in self.all_model_names:
            pretrained_model = self.transformers_model_class.from_pretrained(model_name)
            model = self.trl_model_class.from_pretrained(pretrained_model)
            self.assertTrue(hasattr(model, "v_head"))

    def test_from_save_trl(self):
        """
        Test if the model can be saved and loaded from a directory and get the same weights
        Including the additional modules (e.g. v_head)
        """
        for model_name in self.all_model_names:
            model = self.trl_model_class.from_pretrained(model_name)

            with tempfile.TemporaryDirectory() as tmp_dir:
                model.save_pretrained(tmp_dir)

                model_from_save = self.trl_model_class.from_pretrained(tmp_dir)

            # Check if the weights are the same
            for key in model_from_save.state_dict():
                self.assertTrue(torch.allclose(model_from_save.state_dict()[key], model.state_dict()[key]))

    def test_from_save_trl_sharded(self):
        """
        Test if the model can be saved and loaded from a directory and get the same weights - sharded case
        """
        for model_name in self.all_model_names:
            model = self.trl_model_class.from_pretrained(model_name)

            with tempfile.TemporaryDirectory() as tmp_dir:
                model.save_pretrained(tmp_dir)

                model_from_save = self.trl_model_class.from_pretrained(tmp_dir)

            # Check if the weights are the same
            for key in model_from_save.state_dict():
                self.assertTrue(torch.allclose(model_from_save.state_dict()[key], model.state_dict()[key]))

    def test_from_save_transformers_sharded(self):
        """
        Test if the model can be saved and loaded using transformers and get the same weights - sharded case
        """
        for model_name in self.all_model_names:
            transformers_model = self.trl_model_class.transformers_parent_class.from_pretrained(model_name)

            trl_model = self.trl_model_class.from_pretrained(model_name)

            with tempfile.TemporaryDirectory() as tmp_dir:
                trl_model.save_pretrained(tmp_dir, max_shard_size="1MB")
                transformers_model_from_save = self.trl_model_class.transformers_parent_class.from_pretrained(tmp_dir)

            # Check if the weights are the same
            for key in transformers_model.state_dict():
                self.assertTrue(
                    torch.allclose(
                        transformers_model_from_save.state_dict()[key], transformers_model.state_dict()[key]
                    )
                )

    def test_from_save_transformers(self):
        """
        Test if the model can be saved and loaded using transformers and get the same weights.
        We override the test of the super class to check if the weights are the same.
        """
        for model_name in self.all_model_names:
            transformers_model = self.trl_model_class.transformers_parent_class.from_pretrained(model_name)

            trl_model = self.trl_model_class.from_pretrained(model_name)

            with tempfile.TemporaryDirectory() as tmp_dir:
                trl_model.save_pretrained(tmp_dir)
                transformers_model_from_save = self.trl_model_class.transformers_parent_class.from_pretrained(tmp_dir)

            # Check if the weights are the same
            for key in transformers_model.state_dict():
                self.assertTrue(
                    torch.allclose(
                        transformers_model_from_save.state_dict()[key], transformers_model.state_dict()[key]
                    )
                )

            # Check if the trl model has the same keys as the transformers model
            # except the v_head
            for key in trl_model.state_dict():
                if "v_head" not in key:
                    self.assertTrue(key in transformers_model.state_dict())
                    # check if the weights are the same
                    self.assertTrue(torch.allclose(trl_model.state_dict()[key], transformers_model.state_dict()[key]))

            # check if they have the same modules
            self.assertTrue(
                set(transformers_model_from_save.state_dict().keys()) == set(transformers_model.state_dict().keys())
            )


class CausalLMValueHeadModelTester(VHeadModelTester, unittest.TestCase):
    """
    Testing suite for v-head models.
    """

    all_model_names = ALL_CAUSAL_LM_MODELS
    trl_model_class = AutoModelForCausalLMWithValueHead
    transformers_model_class = AutoModelForCausalLM

    def tearDown(self):
        # free memory
        gc.collect()

    def test_inference(self):
        r"""
        Test if the model can be used for inference and outputs 3 values
        - logits, loss, and value states
        """
        EXPECTED_OUTPUT_SIZE = 3

        for model_name in self.all_model_names:
            model = self.trl_model_class.from_pretrained(model_name)
            input_ids = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])
            outputs = model(input_ids)

            # Check if the outputs are of the right size - here
            # we always output 3 values - logits, loss, and value states
            self.assertEqual(len(outputs), EXPECTED_OUTPUT_SIZE)

    def test_dropout_config(self):
        r"""
        Test if we instantiate a model by adding `summary_drop_prob` to the config
        it will be added to the v_head
        """
        for model_name in self.all_model_names:
            pretrained_model = self.transformers_model_class.from_pretrained(model_name)
            pretrained_model.config.summary_dropout_prob = 0.5
            model = self.trl_model_class.from_pretrained(pretrained_model)

            # Check if v head of the model has the same dropout as the config
            self.assertEqual(model.v_head.dropout.p, pretrained_model.config.summary_dropout_prob)

    def test_dropout_kwargs(self):
        r"""
        Test if we instantiate a model by adding `summary_drop_prob` to the config
        it will be added to the v_head
        """
        for model_name in self.all_model_names:
            v_head_kwargs = {"summary_dropout_prob": 0.5}

            model = self.trl_model_class.from_pretrained(model_name, **v_head_kwargs)

            # Check if v head of the model has the same dropout as the config
            self.assertEqual(model.v_head.dropout.p, 0.5)

            model = self.trl_model_class.from_pretrained(model_name, summary_dropout_prob=0.5)

            # Check if v head of the model has the same dropout as the config
            self.assertEqual(model.v_head.dropout.p, 0.5)

    def test_generate(self):
        r"""
        Test if `generate` works for every model
        """
        for model_name in self.all_model_names:
            model = self.trl_model_class.from_pretrained(model_name)
            input_ids = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])

            # Just check if the generation works
            _ = model.generate(input_ids)

    def test_raise_error_not_causallm(self):
        # Test with a model without a LM head
        model_id = "trl-internal-testing/tiny-random-GPT2Model"
        # This should raise a ValueError
        with self.assertRaises(ValueError):
            pretrained_model = AutoModelForCausalLM.from_pretrained(model_id)
            _ = AutoModelForCausalLMWithValueHead.from_pretrained(pretrained_model.transformer)

    def test_transformers_bf16_kwargs(self):
        r"""
        Test if the transformers kwargs are correctly passed
        Here we check that loading a model in half precision works as expected, i.e. the weights of
        the `pretrained_model` attribute is loaded in half precision and you can run a dummy
        forward pass without any issue.
        """
        for model_name in self.all_model_names:
            trl_model = self.trl_model_class.from_pretrained(model_name, torch_dtype=torch.bfloat16)

            lm_head_namings = self.trl_model_class.lm_head_namings

            self.assertTrue(
                any(hasattr(trl_model.pretrained_model, lm_head_naming) for lm_head_naming in lm_head_namings)
            )

            for lm_head_naming in lm_head_namings:
                if hasattr(trl_model.pretrained_model, lm_head_naming):
                    self.assertTrue(getattr(trl_model.pretrained_model, lm_head_naming).weight.dtype == torch.bfloat16)

            dummy_input = torch.LongTensor([[0, 1, 0, 1]])

            # check dummy forward pass works in half precision
            _ = trl_model(dummy_input)

    @unittest.skip("This test needs to be run manually due to HF token issue.")
    def test_push_to_hub(self):
        for model_name in self.all_model_names:
            model = AutoModelForCausalLMWithValueHead.from_pretrained(model_name)
            if "sharded" in model_name:
                model.push_to_hub(model_name + "-ppo", use_auth_token=True, max_shard_size="1MB")
            else:
                model.push_to_hub(model_name + "-ppo", use_auth_token=True)

            model_from_pretrained = AutoModelForCausalLMWithValueHead.from_pretrained(model_name + "-ppo")
            # check all keys
            self.assertEqual(model.state_dict().keys(), model_from_pretrained.state_dict().keys())

            for name, param in model.state_dict().items():
                self.assertTrue(
                    torch.allclose(param, model_from_pretrained.state_dict()[name]),
                    f"Parameter {name} is not the same after push_to_hub and from_pretrained",
                )


class Seq2SeqValueHeadModelTester(VHeadModelTester, unittest.TestCase):
    """
    Testing suite for v-head models.
    """

    all_model_names = ALL_SEQ2SEQ_MODELS
    trl_model_class = AutoModelForSeq2SeqLMWithValueHead
    transformers_model_class = AutoModelForSeq2SeqLM

    def tearDown(self):
        # free memory
        gc.collect()

    def test_inference(self):
        r"""
        Test if the model can be used for inference and outputs 3 values
        - logits, loss, and value states
        """
        EXPECTED_OUTPUT_SIZE = 3

        for model_name in self.all_model_names:
            model = self.trl_model_class.from_pretrained(model_name)
            input_ids = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])
            decoder_input_ids = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])
            outputs = model(input_ids, decoder_input_ids=decoder_input_ids)

            # Check if the outputs are of the right size - here
            # we always output 3 values - logits, loss, and value states
            self.assertEqual(len(outputs), EXPECTED_OUTPUT_SIZE)

    def test_dropout_config(self):
        r"""
        Test if we instantiate a model by adding `summary_drop_prob` to the config
        it will be added to the v_head
        """
        for model_name in self.all_model_names:
            pretrained_model = self.transformers_model_class.from_pretrained(model_name)
            pretrained_model.config.summary_dropout_prob = 0.5
            model = self.trl_model_class.from_pretrained(pretrained_model)

            # Check if v head of the model has the same dropout as the config
            self.assertEqual(model.v_head.dropout.p, pretrained_model.config.summary_dropout_prob)

    def test_dropout_kwargs(self):
        r"""
        Test if we instantiate a model by adding `summary_drop_prob` to the config
        it will be added to the v_head
        """
        for model_name in self.all_model_names:
            v_head_kwargs = {"summary_dropout_prob": 0.5}

            model = self.trl_model_class.from_pretrained(model_name, **v_head_kwargs)

            # Check if v head of the model has the same dropout as the config
            self.assertEqual(model.v_head.dropout.p, 0.5)

            model = self.trl_model_class.from_pretrained(model_name, summary_dropout_prob=0.5)

            # Check if v head of the model has the same dropout as the config
            self.assertEqual(model.v_head.dropout.p, 0.5)

    def test_generate(self):
        r"""
        Test if `generate` works for every model
        """
        for model_name in self.all_model_names:
            model = self.trl_model_class.from_pretrained(model_name)
            input_ids = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])
            decoder_input_ids = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])

            # Just check if the generation works
            _ = model.generate(input_ids, decoder_input_ids=decoder_input_ids)

    def test_raise_error_not_causallm(self):
        # Test with a model without a LM head
        model_id = "trl-internal-testing/tiny-random-T5Model"
        # This should raise a ValueError
        with self.assertRaises(ValueError):
            pretrained_model = AutoModel.from_pretrained(model_id)
            _ = self.trl_model_class.from_pretrained(pretrained_model)

    @unittest.skip("This test needs to be run manually due to HF token issue.")
    def test_push_to_hub(self):
        for model_name in self.all_model_names:
            model = self.trl_model_class.from_pretrained(model_name)
            if "sharded" in model_name:
                model.push_to_hub(model_name + "-ppo", use_auth_token=True, max_shard_size="1MB")
            else:
                model.push_to_hub(model_name + "-ppo", use_auth_token=True)

            model_from_pretrained = self.trl_model_class.from_pretrained(model_name + "-ppo")
            # check all keys
            self.assertEqual(model.state_dict().keys(), model_from_pretrained.state_dict().keys())

            for name, param in model.state_dict().items():
                self.assertTrue(
                    torch.allclose(param, model_from_pretrained.state_dict()[name]),
                    f"Parameter {name} is not the same after push_to_hub and from_pretrained",
                )

    def test_transformers_bf16_kwargs(self):
        r"""
        Test if the transformers kwargs are correctly passed
        Here we check that loading a model in half precision works as expected, i.e. the weights of
        the `pretrained_model` attribute is loaded in half precision and you can run a dummy
        forward pass without any issue.
        """
        for model_name in self.all_model_names:
            trl_model = self.trl_model_class.from_pretrained(model_name, torch_dtype=torch.bfloat16)

            lm_head_namings = self.trl_model_class.lm_head_namings

            if model_name == "trl-internal-testing/tiny-random-FSMTForConditionalGeneration":
                # skip the test for FSMT as it does not support mixed-prec
                continue

            self.assertTrue(
                any(hasattr(trl_model.pretrained_model, lm_head_naming) for lm_head_naming in lm_head_namings)
            )

            for lm_head_naming in lm_head_namings:
                if hasattr(trl_model.pretrained_model, lm_head_naming):
                    self.assertTrue(getattr(trl_model.pretrained_model, lm_head_naming).weight.dtype == torch.bfloat16)

            dummy_input = torch.LongTensor([[0, 1, 0, 1]])

            # check dummy forward pass works in half precision
            _ = trl_model(input_ids=dummy_input, decoder_input_ids=dummy_input)


class ReferenceModelTest(unittest.TestCase):
    def setUp(self):
        self.model = AutoModelForCausalLMWithValueHead.from_pretrained(
            "trl-internal-testing/tiny-random-GPT2LMHeadModel"
        )
        self.test_input = torch.tensor([[0, 1, 2, 3]])
        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=1)
        self.layer_format = "pretrained_model.transformer.h.{layer}.attn.c_attn.weight"

    def test_independent_reference(self):
        layer_0 = self.layer_format.format(layer=0)
        layer_5 = self.layer_format.format(layer=4)

        ref_model = create_reference_model(self.model)

        first_layer_before = self.model.get_parameter(layer_0).data.clone()
        last_layer_before = self.model.get_parameter(layer_5).data.clone()

        first_ref_layer_before = ref_model.get_parameter(layer_0).data.clone()
        last_ref_layer_before = ref_model.get_parameter(layer_5).data.clone()

        output = self.model(input_ids=self.test_input, labels=self.test_input)
        output[1].backward()
        self.optimizer.step()

        first_layer_after = self.model.get_parameter(layer_0).data.clone()
        last_layer_after = self.model.get_parameter(layer_5).data.clone()

        first_ref_layer_after = ref_model.get_parameter(layer_0).data.clone()
        last_ref_layer_after = ref_model.get_parameter(layer_5).data.clone()

        # before optimization ref and model are identical
        self.assertTrue((first_layer_before == first_ref_layer_before).all())
        self.assertTrue((last_layer_before == last_ref_layer_before).all())
        # ref model stays identical after optimization
        self.assertTrue((first_ref_layer_before == first_ref_layer_after).all())
        self.assertTrue((last_ref_layer_before == last_ref_layer_after).all())
        # optimized model changes
        self.assertTrue(not (first_layer_before == first_layer_after).all())
        self.assertTrue(not (last_layer_before == last_layer_after).all())

    def test_shared_layers(self):
        layer_0 = self.layer_format.format(layer=0)
        layer_1 = self.layer_format.format(layer=1)

        ref_model = create_reference_model(self.model, num_shared_layers=1)

        first_layer_before = self.model.get_parameter(layer_0).data.clone()
        second_layer_before = self.model.get_parameter(layer_1).data.clone()

        first_ref_layer_before = ref_model.get_parameter(layer_0).data.clone()
        second_ref_layer_before = ref_model.get_parameter(layer_1).data.clone()

        output = self.model(input_ids=self.test_input, labels=self.test_input)
        output[1].backward()
        self.optimizer.step()

        first_layer_after = self.model.get_parameter(layer_0).data.clone()
        second_layer_after = self.model.get_parameter(layer_1).data.clone()

        first_ref_layer_after = ref_model.get_parameter(layer_0).data.clone()
        second_ref_layer_after = ref_model.get_parameter(layer_1).data.clone()

        # before optimization ref and model are identical
        self.assertTrue((first_layer_before == first_ref_layer_before).all())
        self.assertTrue((second_layer_before == second_ref_layer_before).all())
        # ref model stays identical after optimization
        self.assertTrue((first_ref_layer_before == first_ref_layer_after).all())
        self.assertTrue((second_ref_layer_before == second_ref_layer_after).all())
        # first layer of optimized model stays the same
        self.assertTrue((first_layer_before == first_layer_after).all())
        # other layers in optimized model change
        self.assertTrue(not (second_layer_before == second_layer_after).all())

File Path: tests/test_no_peft.py
Content:
# Copyright 2023 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import sys
import unittest
from unittest.mock import patch

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

from .testing_utils import is_peft_available, require_peft


class DummyDataset(torch.utils.data.Dataset):
    def __init__(self, query_data, response_data):
        self.query_data = query_data
        self.response_data = response_data

    def __len__(self):
        return len(self.query_data)

    def __getitem__(self, idx):
        return self.query_data[idx], self.response_data[idx]


EXPECTED_STATS = [
    "objective/kl",
    "objective/kl_dist",
    "objective/logprobs",
    "objective/ref_logprobs",
    "objective/kl_coef",
    "objective/entropy",
    "ppo/mean_non_score_reward",
    "ppo/loss/policy",
    "ppo/loss/value",
    "ppo/loss/total",
    "ppo/policy/entropy",
    "ppo/policy/approxkl",
    "ppo/policy/policykl",
    "ppo/policy/clipfrac",
    "ppo/policy/advantages",
    "ppo/policy/advantages_mean",
    "ppo/policy/ratio",
    "ppo/returns/mean",
    "ppo/returns/var",
    "ppo/val/vpred",
    "ppo/val/error",
    "ppo/val/clipfrac",
    "ppo/val/mean",
    "ppo/val/var",
    "ppo/val/var_explained",
    "time/ppo/forward_pass",
    "time/ppo/compute_rewards",
    "time/ppo/optimize_step",
    "time/ppo/calc_stats",
    "time/ppo/total",
    "ppo/learning_rate",
]


@require_peft
class TestPeftDependancy(unittest.TestCase):
    def setUp(self):
        self.causal_lm_model_id = "trl-internal-testing/tiny-random-GPTNeoXForCausalLM"
        self.seq_to_seq_model_id = "trl-internal-testing/tiny-random-T5ForConditionalGeneration"

        if is_peft_available():
            from peft import LoraConfig, get_peft_model

            lora_config = LoraConfig(
                r=16,
                lora_alpha=32,
                lora_dropout=0.05,
                bias="none",
                task_type="CAUSAL_LM",
            )

            causal_lm_model = AutoModelForCausalLM.from_pretrained(self.causal_lm_model_id)
            self.peft_model = get_peft_model(causal_lm_model, lora_config)

    def test_no_peft(self):
        with patch.dict(sys.modules, {"peft": None}):
            from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead

            # Check that loading a model with `peft` will raise an error
            with self.assertRaises(ModuleNotFoundError):
                import peft  # noqa

            trl_model = AutoModelForCausalLMWithValueHead.from_pretrained(self.causal_lm_model_id)  # noqa
            trl_seq2seq_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(self.seq_to_seq_model_id)  # noqa

    def test_imports_no_peft(self):
        with patch.dict(sys.modules, {"peft": None}):
            from trl import (  # noqa
                AutoModelForCausalLMWithValueHead,
                AutoModelForSeq2SeqLMWithValueHead,
                PPOConfig,
                PPOTrainer,
                PreTrainedModelWrapper,
            )

    def test_ppo_trainer_no_peft(self):
        with patch.dict(sys.modules, {"peft": None}):
            from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer

            ppo_model_id = "trl-internal-testing/dummy-GPT2-correct-vocab"

            trl_model = AutoModelForCausalLMWithValueHead.from_pretrained(ppo_model_id)
            tokenizer = AutoTokenizer.from_pretrained(ppo_model_id)
            tokenizer.pad_token_id = tokenizer.eos_token_id

            ppo_config = PPOConfig(batch_size=2, mini_batch_size=1, log_with=None)

            dummy_dataset = DummyDataset(
                [torch.LongTensor([0, 1, 0, 1, 0, 1]), torch.LongTensor([0, 1, 0, 1, 0, 1])],
                [torch.LongTensor([1, 0, 1, 0, 1, 0]), torch.LongTensor([0, 1, 0, 1, 0, 1])],
            )

            ppo_trainer = PPOTrainer(
                config=ppo_config,
                model=trl_model,
                ref_model=None,
                tokenizer=tokenizer,
                dataset=dummy_dataset,
            )
            dummy_dataloader = ppo_trainer.dataloader

            for query_tensor, response_tensor in dummy_dataloader:
                # define a reward for response
                # (this could be any reward such as human feedback or output from another model)
                reward = [torch.tensor(1.0), torch.tensor(0.0)]
                # train model
                train_stats = ppo_trainer.step([q for q in query_tensor], [r for r in response_tensor], reward)
                break

            # check gradients are not None
            for _, param in trl_model.named_parameters():
                if param.requires_grad:
                    self.assertIsNotNone(param.grad)

            # check expected stats
            for stat in EXPECTED_STATS:
                self.assertIn(stat, train_stats)

File Path: tests/test_peft_models.py
Content:
# Copyright 2023 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import os
import tempfile
import unittest

import torch
from pytest import mark
from transformers import AutoModelForCausalLM

from trl import AutoModelForCausalLMWithValueHead, is_peft_available


if is_peft_available():
    from peft import get_peft_model, LoraConfig

from .testing_utils import require_bitsandbytes, require_peft


@require_peft
@mark.peft_test
class PeftModelTester(unittest.TestCase):
    def setUp(self):
        self.causal_lm_model_id = "trl-internal-testing/tiny-random-GPTNeoXForCausalLM"
        self.lora_config = LoraConfig(
            r=16,
            lora_alpha=32,
            lora_dropout=0.05,
            bias="none",
            task_type="CAUSAL_LM",
        )

    def test_create_peft_model(self):
        r"""
        Simply creates a peft model and checks that it can be loaded.
        """
        causal_lm_model = AutoModelForCausalLM.from_pretrained(self.causal_lm_model_id)
        pretrained_model = get_peft_model(causal_lm_model, self.lora_config)

        _ = AutoModelForCausalLMWithValueHead.from_pretrained(pretrained_model)

    def test_peft_requires_grad(self):
        r"""
        Check that the value head of the returned model has requires_grad=True.
        """
        causal_lm_model = AutoModelForCausalLM.from_pretrained(self.causal_lm_model_id)
        pretrained_model = get_peft_model(causal_lm_model, self.lora_config)

        model = AutoModelForCausalLMWithValueHead.from_pretrained(pretrained_model)

        # Check that the value head has requires_grad=True
        self.assertTrue(model.v_head.summary.weight.requires_grad)

    def test_check_peft_model_nb_trainable_params(self):
        r"""
        Check that the number of trainable parameters is correct.
        """
        causal_lm_model = AutoModelForCausalLM.from_pretrained(self.causal_lm_model_id)
        pretrained_model = get_peft_model(causal_lm_model, self.lora_config)

        model = AutoModelForCausalLMWithValueHead.from_pretrained(pretrained_model)

        # Check that the number of trainable parameters is correct
        nb_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        self.assertEqual(nb_trainable_params, 10273)

        # Check that the number of trainable param for the non-peft model is correct
        non_peft_model = AutoModelForCausalLMWithValueHead.from_pretrained(self.causal_lm_model_id)
        nb_trainable_params = sum(p.numel() for p in non_peft_model.parameters() if p.requires_grad)
        self.assertEqual(nb_trainable_params, 99578)

    def test_create_peft_model_from_config(self):
        r"""
        Simply creates a peft model and checks that it can be loaded.
        """
        trl_model = AutoModelForCausalLMWithValueHead.from_pretrained(
            self.causal_lm_model_id, peft_config=self.lora_config
        )
        # Check that the number of trainable parameters is correct
        nb_trainable_params = sum(p.numel() for p in trl_model.parameters() if p.requires_grad)
        self.assertEqual(nb_trainable_params, 10273)

        causal_lm_model = AutoModelForCausalLM.from_pretrained(self.causal_lm_model_id)
        trl_model = AutoModelForCausalLMWithValueHead.from_pretrained(causal_lm_model, peft_config=self.lora_config)
        # Check that the number of trainable parameters is correct
        nb_trainable_params = sum(p.numel() for p in trl_model.parameters() if p.requires_grad)
        self.assertEqual(nb_trainable_params, 10273)

    @require_bitsandbytes
    def test_create_bnb_peft_model_from_config(self):
        r"""
        Simply creates a peft model and checks that it can be loaded.
        """
        from bitsandbytes.nn import Linear8bitLt

        trl_model = AutoModelForCausalLMWithValueHead.from_pretrained(
            self.causal_lm_model_id, peft_config=self.lora_config, load_in_8bit=True
        )
        # Check that the number of trainable parameters is correct
        nb_trainable_params = sum(p.numel() for p in trl_model.parameters() if p.requires_grad)
        self.assertEqual(nb_trainable_params, 10273)
        self.assertTrue(
            trl_model.pretrained_model.model.gpt_neox.layers[0].mlp.dense_h_to_4h.__class__ == Linear8bitLt
        )

        causal_lm_model = AutoModelForCausalLM.from_pretrained(
            self.causal_lm_model_id, load_in_8bit=True, device_map="auto"
        )
        trl_model = AutoModelForCausalLMWithValueHead.from_pretrained(causal_lm_model, peft_config=self.lora_config)
        # Check that the number of trainable parameters is correct
        nb_trainable_params = sum(p.numel() for p in trl_model.parameters() if p.requires_grad)
        self.assertEqual(nb_trainable_params, 10273)
        self.assertTrue(
            trl_model.pretrained_model.model.gpt_neox.layers[0].mlp.dense_h_to_4h.__class__ == Linear8bitLt
        )

    def test_save_pretrained_peft(self):
        r"""
        Check that the model can be saved and loaded properly.
        """
        causal_lm_model = AutoModelForCausalLM.from_pretrained(self.causal_lm_model_id)
        pretrained_model = get_peft_model(causal_lm_model, self.lora_config)

        model = AutoModelForCausalLMWithValueHead.from_pretrained(pretrained_model)

        with tempfile.TemporaryDirectory() as tmp_dir:
            model.save_pretrained(tmp_dir)

            # check that the files `adapter_model.bin` and `adapter_config.json` are in the directory
            self.assertTrue(
                os.path.isfile(f"{tmp_dir}/adapter_model.bin"),
                msg=f"{tmp_dir}/adapter_model.bin does not exist",
            )
            self.assertTrue(
                os.path.exists(f"{tmp_dir}/adapter_config.json"),
                msg=f"{tmp_dir}/adapter_config.json does not exist",
            )
            # check also for `pytorch_model.bin` and make sure it only contains `v_head` weights
            self.assertTrue(
                os.path.exists(f"{tmp_dir}/pytorch_model.bin"),
                msg=f"{tmp_dir}/pytorch_model.bin does not exist",
            )
            maybe_v_head = torch.load(f"{tmp_dir}/pytorch_model.bin")
            # check that only keys that starts with `v_head` are in the dict
            self.assertTrue(
                all(k.startswith("v_head") for k in maybe_v_head.keys()),
                msg=f"keys in {tmp_dir}/pytorch_model.bin do not start with `v_head`",
            )

            model_from_pretrained = AutoModelForCausalLMWithValueHead.from_pretrained(tmp_dir)

            # check all the weights are the same
            for p1, p2 in zip(model.named_parameters(), model_from_pretrained.named_parameters()):
                self.assertTrue(torch.allclose(p1[1], p2[1]), msg=f"{p1[0]} != {p2[0]}")

    def test_load_pretrained_peft(self):
        r"""
        Check that the model saved with peft class interface can be loaded properly.
        """
        causal_lm_model = AutoModelForCausalLM.from_pretrained(self.causal_lm_model_id)
        pretrained_model = get_peft_model(causal_lm_model, self.lora_config)

        model = AutoModelForCausalLMWithValueHead.from_pretrained(pretrained_model)

        with tempfile.TemporaryDirectory() as tmp_dir:
            pretrained_model.save_pretrained(tmp_dir)
            model_from_pretrained = AutoModelForCausalLMWithValueHead.from_pretrained(tmp_dir)

            # check that the files `adapter_model.bin` and `adapter_config.json` are in the directory
            self.assertTrue(
                os.path.isfile(f"{tmp_dir}/adapter_model.bin"),
                msg=f"{tmp_dir}/adapter_model.bin does not exist",
            )
            self.assertTrue(
                os.path.exists(f"{tmp_dir}/adapter_config.json"),
                msg=f"{tmp_dir}/adapter_config.json does not exist",
            )

            # check all the weights are the same
            for p1, p2 in zip(model.named_parameters(), model_from_pretrained.named_parameters()):
                if p1[0] not in ["v_head.summary.weight", "v_head.summary.bias"]:
                    self.assertTrue(torch.allclose(p1[1], p2[1]), msg=f"{p1[0]} != {p2[0]}")

    def test_continue_training_peft_model(self):
        r"""
        Load peft and checks that it can continue training.
        """
        causal_lm_model = AutoModelForCausalLM.from_pretrained(self.causal_lm_model_id)
        pretrained_model = get_peft_model(causal_lm_model, self.lora_config)

        with tempfile.TemporaryDirectory() as tmp_dir:
            pretrained_model.save_pretrained(tmp_dir)
            # set is_trainable to True
            model = AutoModelForCausalLMWithValueHead.from_pretrained(tmp_dir, is_trainable=True)
            # Check that the number of trainable parameters is correct
            nb_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            self.assertEqual(nb_trainable_params, 10273)

File Path: tests/test_ppo_trainer.py
Content:
# Copyright 2022 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import copy
import fnmatch
import gc
import re
import tempfile
import unittest

import pytest
import torch
from huggingface_hub import HfApi, HfFolder, delete_repo
from parameterized import parameterized
from pytest import mark
from requests.exceptions import HTTPError
from transformers import AutoTokenizer

from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed
from trl.core import respond_to_batch

from .testing_constants import CI_HUB_ENDPOINT, CI_HUB_USER, CI_HUB_USER_TOKEN
from .testing_utils import require_peft, require_torch_multi_gpu


EXPECTED_STATS = [
    "objective/kl",
    "objective/kl_dist",
    "objective/logprobs",
    "objective/ref_logprobs",
    "objective/kl_coef",
    "objective/entropy",
    "ppo/mean_non_score_reward",
    "ppo/loss/policy",
    "ppo/loss/value",
    "ppo/loss/total",
    "ppo/policy/entropy",
    "ppo/policy/approxkl",
    "ppo/policy/policykl",
    "ppo/policy/clipfrac",
    "ppo/policy/advantages",
    "ppo/policy/advantages_mean",
    "ppo/policy/ratio",
    "ppo/returns/mean",
    "ppo/returns/var",
    "ppo/val/vpred",
    "ppo/val/error",
    "ppo/val/clipfrac",
    "ppo/val/mean",
    "ppo/val/var",
    "ppo/val/var_explained",
    "time/ppo/forward_pass",
    "time/ppo/compute_rewards",
    "time/ppo/optimize_step",
    "time/ppo/calc_stats",
    "time/ppo/total",
    "ppo/learning_rate",
]


class DummyDataset(torch.utils.data.Dataset):
    def __init__(self, query_data, response_data):
        self.query_data = query_data
        self.response_data = response_data

    def __len__(self):
        return len(self.query_data)

    def __getitem__(self, idx):
        return self.query_data[idx], self.response_data[idx]


def apply_mask(values, mask):
    unmasked_values = []
    for v, m in zip(values, mask):
        if m == 1:
            unmasked_values.append(v)
    return torch.Tensor(unmasked_values)


def abs_diff_masked_tensors(tensor_1, tensor_2, mask_1, mask_2):
    diffs = []
    for l1, l2, m1, m2 in zip(tensor_1, tensor_2, mask_1, mask_2):
        diff = apply_mask(l1, m1) - apply_mask(l2, m2)
        diffs.append(diff.sum())
    return abs(sum(diffs))


class PPOTrainerTester(unittest.TestCase):
    """
    A wrapper class for testing PPOTrainer
    """

    @classmethod
    def setUpClass(cls):
        set_seed(42)
        cls._token = CI_HUB_USER_TOKEN
        cls._api = HfApi(endpoint=CI_HUB_ENDPOINT)
        HfFolder.save_token(CI_HUB_USER_TOKEN)

        # model_id
        cls.model_id = "trl-internal-testing/dummy-GPT2-correct-vocab"

        # get models and tokenizer
        cls.gpt2_model = AutoModelForCausalLMWithValueHead.from_pretrained(cls.model_id)
        cls.gpt2_model_ref = AutoModelForCausalLMWithValueHead.from_pretrained(cls.model_id)
        cls.gpt2_tokenizer = AutoTokenizer.from_pretrained(cls.model_id)

        cls.gpt2_tokenizer.pad_token = cls.gpt2_tokenizer.eos_token

        # get bloom as right padding examples:
        model_id = "trl-internal-testing/tiny-BloomForCausalLM-correct-vocab"
        cls.bloom_model = AutoModelForCausalLMWithValueHead.from_pretrained(model_id)
        cls.bloom_tokenizer = AutoTokenizer.from_pretrained(model_id)

        model_id = "trl-internal-testing/tiny-T5ForConditionalGeneration-correct-vocab"
        cls.t5_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(model_id)
        cls.t5_tokenizer = AutoTokenizer.from_pretrained(model_id)

        # initialize trainer
        cls.ppo_config = PPOConfig(batch_size=2, mini_batch_size=1, log_with=None)

    @classmethod
    def tearDownClass(cls):
        for model in [f"{CI_HUB_USER}/test-ppo-trainer"]:
            try:
                delete_repo(token=cls._token, repo_id=model)
            except HTTPError:
                pass

    def setUp(self):
        # initialize trainer
        self.ppo_config = PPOConfig(batch_size=2, mini_batch_size=1, log_with=None)
        self.gpt2_model.train()
        return super().setUp()

    def tearDown(self):
        # free memory
        gc.collect()

    def _init_dummy_dataset(self):
        # encode a query
        query_txt = "This morning I went to the "
        query_tensor = self.gpt2_tokenizer.encode(query_txt, return_tensors="pt")
        assert query_tensor.shape == (1, 7)
        # get model response
        response_tensor = respond_to_batch(self.gpt2_model, query_tensor)
        assert response_tensor.shape == (1, 20)

        # create a dummy dataset
        min_length = min(len(query_tensor[0]), len(response_tensor[0]))
        dummy_dataset = DummyDataset(
            [query_tensor[:, :min_length].squeeze(0) for _ in range(2)],
            [response_tensor[:, :min_length].squeeze(0) for _ in range(2)],
        )

        return dummy_dataset

    def test_drop_last_dataloader(self):
        self.ppo_config = PPOConfig(batch_size=3, mini_batch_size=1, log_with=None)

        dummy_dataset = self._init_dummy_dataset()

        ppo_trainer = PPOTrainer(
            config=self.ppo_config,
            model=self.gpt2_model,
            ref_model=self.gpt2_model_ref,
            tokenizer=self.gpt2_tokenizer,
            dataset=dummy_dataset,
        )
        dummy_dataloader = ppo_trainer.dataloader

        self.assertEqual(len(dummy_dataloader), 0)

    def test_ppo_step(self):
        # initialize dataset
        dummy_dataset = self._init_dummy_dataset()

        ppo_trainer = PPOTrainer(
            config=self.ppo_config,
            model=self.gpt2_model,
            ref_model=self.gpt2_model_ref,
            tokenizer=self.gpt2_tokenizer,
            dataset=dummy_dataset,
        )
        dummy_dataloader = ppo_trainer.dataloader
        # train model with ppo
        for query_tensor, response_tensor in dummy_dataloader:
            # define a reward for response
            # (this could be any reward such as human feedback or output from another model)
            reward = [torch.tensor(1.0), torch.tensor(0.0)]
            # train model
            train_stats = ppo_trainer.step([q for q in query_tensor], [r for r in response_tensor], reward)
            break

        for param in ppo_trainer.model.parameters():
            assert param.grad is not None

        for stat in EXPECTED_STATS:
            assert stat in train_stats.keys()

    def test_ppo_step_with_masks(self):
        # initialize dataset
        dummy_dataset = self._init_dummy_dataset()

        ppo_trainer = PPOTrainer(
            config=self.ppo_config,
            model=self.gpt2_model,
            ref_model=self.gpt2_model_ref,
            tokenizer=self.gpt2_tokenizer,
            dataset=dummy_dataset,
        )
        dummy_dataloader = ppo_trainer.dataloader
        # train model with ppo
        for query_tensor, response_tensor in dummy_dataloader:
            # define a reward for response
            # (this could be any reward such as human feedback or output from another model)
            reward = [torch.tensor(1.0), torch.tensor(0.0)]

            response_mask = [torch.ones_like(r) for r in response_tensor]

            # train model
            train_stats = ppo_trainer.step(
                [q for q in query_tensor], [r for r in response_tensor], reward, response_mask
            )
            break

        for param in ppo_trainer.model.parameters():
            assert param.grad is not None

        for stat in EXPECTED_STATS:
            assert stat in train_stats.keys()

    def test_ppo_step_with_no_ref_sgd(self):
        # initialize dataset
        dummy_dataset = self._init_dummy_dataset()
        optimizer = torch.optim.SGD(self.gpt2_model.parameters(), lr=0.01)

        ppo_trainer = PPOTrainer(
            config=self.ppo_config,
            model=self.gpt2_model,
            ref_model=None,
            optimizer=optimizer,
            tokenizer=self.gpt2_tokenizer,
            dataset=dummy_dataset,
        )
        dummy_dataloader = ppo_trainer.dataloader

        self.assertTrue(isinstance(ppo_trainer.optimizer.optimizer, torch.optim.SGD))

        # train model with ppo
        for query_tensor, response_tensor in dummy_dataloader:
            # define a reward for response
            # (this could be any reward such as human feedback or output from another model)
            reward = [torch.tensor(1.0), torch.tensor(0.0)]
            # train model
            train_stats = ppo_trainer.step([q for q in query_tensor], [r for r in response_tensor], reward)
            break

        for name, param in ppo_trainer.model.named_parameters():
            self.assertTrue(param.grad is not None, f"Parameter {name} has no gradient")

        # ref model should not be trained
        for name, param in ppo_trainer.ref_model.named_parameters():
            self.assertTrue(param.grad is None, f"Parameter {name} has a gradient")

        # Finally check stats
        for stat in EXPECTED_STATS:
            assert stat in train_stats.keys()

    def test_ppo_step_with_no_ref_sgd_lr_scheduler(self):
        # initialize dataset
        dummy_dataset = self._init_dummy_dataset()
        optimizer = torch.optim.SGD(self.gpt2_model.parameters(), lr=0.01)
        lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)

        ppo_trainer = PPOTrainer(
            config=self.ppo_config,
            model=self.gpt2_model,
            ref_model=None,
            optimizer=optimizer,
            tokenizer=self.gpt2_tokenizer,
            dataset=dummy_dataset,
            lr_scheduler=lr_scheduler,
        )
        dummy_dataloader = ppo_trainer.dataloader

        self.assertTrue(isinstance(ppo_trainer.optimizer.optimizer, torch.optim.SGD))
        self.assertTrue(isinstance(ppo_trainer.lr_scheduler.scheduler, torch.optim.lr_scheduler.ExponentialLR))

        # train model with ppo
        for query_tensor, response_tensor in dummy_dataloader:
            # define a reward for response
            # (this could be any reward such as human feedback or output from another model)
            reward = [torch.tensor(1.0), torch.tensor(0.0)]
            # train model
            _ = ppo_trainer.step([q for q in query_tensor], [r for r in response_tensor], reward)
            train_stats = ppo_trainer.step([q for q in query_tensor], [r for r in response_tensor], reward)
            break

        for name, param in ppo_trainer.model.named_parameters():
            self.assertTrue(param.grad is not None, f"Parameter {name} has no gradient")

        # ref model should not be trained
        for name, param in ppo_trainer.ref_model.named_parameters():
            self.assertTrue(param.grad is None, f"Parameter {name} has a gradient")

        # Finally check stats
        for stat in EXPECTED_STATS:
            assert stat in train_stats.keys()

        # assert that the LR has increased for exponential decay
        self.assertTrue(train_stats["ppo/learning_rate"] > self.ppo_config.learning_rate)

    def test_ppo_step_with_no_ref(self):
        # initialize dataset
        dummy_dataset = self._init_dummy_dataset()
        self.gpt2_model = AutoModelForCausalLMWithValueHead.from_pretrained(self.model_id)

        ppo_trainer = PPOTrainer(
            config=self.ppo_config,
            model=self.gpt2_model,
            ref_model=None,
            tokenizer=self.gpt2_tokenizer,
            dataset=dummy_dataset,
        )
        dummy_dataloader = ppo_trainer.dataloader
        # train model with ppo
        for query_tensor, response_tensor in dummy_dataloader:
            # define a reward for response
            # (this could be any reward such as human feedback or output from another model)
            reward = [torch.tensor(1.0), torch.tensor(0.0)]
            # train model
            train_stats = ppo_trainer.step([q for q in query_tensor], [r for r in response_tensor], reward)
            break

        for name, param in ppo_trainer.model.named_parameters():
            self.assertTrue(param.grad is not None, f"Parameter {name} has no gradient")

        # ref model should not be trained
        for name, param in ppo_trainer.ref_model.named_parameters():
            self.assertTrue(param.grad is None, f"Parameter {name} has a gradient")

        # initialize a new gpt2 model:
        model = AutoModelForCausalLMWithValueHead.from_pretrained(self.model_id)
        for name, param in ppo_trainer.ref_model.named_parameters():
            if "v_head" not in name:
                name = name.replace("pretrained_model.", "")

                self.assertTrue(
                    torch.allclose(param.cpu(), model.state_dict()[name].cpu()),
                    f"Parameter {name} has changed from the original model",
                )

        # Finally check stats
        for stat in EXPECTED_STATS:
            assert stat in train_stats.keys()

    def test_ppo_step_with_no_ref_custom_layers(self):
        """
        Test PPO step with no reference model and custom layers
        For shared layers configuration, all the layers after the `num_shared_layers` are considered as custom layers
        therefore the gradients should be computed for these layers only.
        """
        # initialize dataset
        dummy_dataset = self._init_dummy_dataset()
        self.gpt2_model = AutoModelForCausalLMWithValueHead.from_pretrained(self.model_id)
        num_shared_layers = 1

        ppo_trainer = PPOTrainer(
            config=self.ppo_config,
            model=self.gpt2_model,
            ref_model=None,
            tokenizer=self.gpt2_tokenizer,
            dataset=dummy_dataset,
            num_shared_layers=num_shared_layers,
        )
        dummy_dataloader = ppo_trainer.dataloader
        # train model with ppo
        for query_tensor, response_tensor in dummy_dataloader:
            # define a reward for response
            # (this could be any reward such as human feedback or output from another model)
            reward = [torch.tensor(1.0), torch.tensor(0.0)]
            # train model
            train_stats = ppo_trainer.step([q for q in query_tensor], [r for r in response_tensor], reward)
            break

        pattern = r".*transformer\.h\.(\d+)\..*"
        final_layers = ["ln_f", "v_head", "lm_head"]

        for name, param in ppo_trainer.model.named_parameters():
            if re.match(pattern, name):
                layer_number = int(re.match(pattern, name).groups(0)[0])
                if layer_number < num_shared_layers:
                    self.assertTrue(param.grad is None, f"Parameter {name} has a gradient")
                else:
                    self.assertTrue(param.grad is not None, f"Parameter {name} has no gradient")
            elif any([layer in name for layer in final_layers]):
                self.assertTrue(param.grad is not None, f"Parameter {name} has no gradient")

        # ref model should not be trained
        for name, param in ppo_trainer.ref_model.named_parameters():
            self.assertTrue(param.grad is None, f"Parameter {name} has a gradient")

        for stat in EXPECTED_STATS:
            assert stat in train_stats.keys()

    def test_ppo_step_with_ref_and_custom_layers_warning(self):
        """
        Test PPO step with a reference model and custom layers
        The trainer should raise a warning if the argument `num_shared_layers` is set
        together with a reference model.
        """
        # initialize dataset
        dummy_dataset = self._init_dummy_dataset()

        num_shared_layers = 6

        with self.assertWarns(UserWarning):
            _ = PPOTrainer(
                config=self.ppo_config,
                model=self.gpt2_model,
                ref_model=self.gpt2_model_ref,
                tokenizer=self.gpt2_tokenizer,
                dataset=dummy_dataset,
                num_shared_layers=num_shared_layers,
            )

    def test_ppo_step_rewards_shape(self):
        """
        Test if the rewards shape is correct by asserting that if a wrong reward shape is passed, we get
        a value error.
        """

        # initialize dataset
        dummy_dataset = self._init_dummy_dataset()

        ppo_trainer = PPOTrainer(
            config=self.ppo_config,
            model=self.gpt2_model,
            ref_model=None,
            tokenizer=self.gpt2_tokenizer,
            dataset=dummy_dataset,
        )
        dummy_dataloader = ppo_trainer.dataloader
        # train model with ppo
        for query_tensor, response_tensor in dummy_dataloader:
            # define a reward for response
            # (this could be any reward such as human feedback or output from another model)
            reward = [torch.tensor([[1.0]]), torch.tensor([[0.0]])]
            # train model - this should raise an error
            with self.assertRaises(ValueError):
                _ = ppo_trainer.step([q for q in query_tensor], [r for r in response_tensor], reward)

            reward = [torch.tensor([1.0]), torch.tensor([0.0])]
            # train model - this should work
            _ = ppo_trainer.step([q for q in query_tensor], [r for r in response_tensor], reward)
            break

        # check if the gradients are computed for the model
        for name, param in ppo_trainer.model.named_parameters():
            self.assertTrue(param.grad is not None, f"Parameter {name} has no gradient")

        # ref model should not be trained
        for name, param in ppo_trainer.ref_model.named_parameters():
            self.assertTrue(param.grad is None, f"Parameter {name} has a gradient")

    def test_ppo_step_input_shape(self):
        """
        Test if the shape of the expected inputs are correct
        """
        # initialize dataset
        dummy_dataset = self._init_dummy_dataset()

        ppo_trainer = PPOTrainer(
            config=self.ppo_config,
            model=self.gpt2_model,
            ref_model=None,
            tokenizer=self.gpt2_tokenizer,
            dataset=dummy_dataset,
        )
        dummy_dataloader = ppo_trainer.dataloader
        # train model with ppo
        for query_tensor, response_tensor in dummy_dataloader:
            # define a reward for response
            # (this could be any reward such as human feedback or output from another model)
            reward = [torch.tensor([1.0]), torch.tensor([0.0])]
            # train model - this should raise an error
            bs = ppo_trainer.config.batch_size

            queries, responses, _, _ = ppo_trainer._step_safety_checker(
                bs, [q for q in query_tensor], [r for r in response_tensor], reward
            )

            self.assertTrue(isinstance(queries, list), f"queries should be a list, got {type(queries)}")
            self.assertTrue(isinstance(responses, list), f"responses should be a list, got {type(responses)}")

            # check the shapes
            for i in range(bs):
                self.assertEqual(queries[i].shape, torch.Size([7]))
                self.assertEqual(responses[i].size(), torch.Size([7]))
            break

    def test_ppo_step_no_dataset(self):
        """
        Test if the training loop works fine without passing a dataset
        """
        query_txt = "This morning I went to the "
        query_tensor = self.gpt2_tokenizer.encode(query_txt, return_tensors="pt")
        self.ppo_config.batch_size = 1

        response_tensor = respond_to_batch(self.gpt2_model, query_tensor)

        # Check that this warns the user about batch size
        with self.assertWarns(UserWarning):
            ppo_trainer = PPOTrainer(
                config=self.ppo_config,
                model=self.gpt2_model,
                ref_model=self.gpt2_model_ref,
                tokenizer=self.gpt2_tokenizer,
            )
        # train model with ppo
        reward = [torch.tensor([1.0])]
        # train model - this should work fine
        train_stats = ppo_trainer.step([query_tensor[0]], [response_tensor[0]], reward)

        # check gradients
        for name, param in ppo_trainer.model.named_parameters():
            self.assertTrue(param.grad is not None, f"Parameter {name} has no gradient")

        # ref model should not be trained
        for name, param in ppo_trainer.ref_model.named_parameters():
            self.assertTrue(param.grad is None, f"Parameter {name} has a gradient")

        # check train stats
        for stat in EXPECTED_STATS:
            self.assertTrue(stat in train_stats, f"Train stats should contain {stat}")

    def test_loss_trainer(self):
        """
        Test if the loss trainer works fine
        """
        # initialize dataset
        dummy_dataset = self._init_dummy_dataset()

        self.gpt2_model.eval()

        ppo_trainer = PPOTrainer(
            config=self.ppo_config,
            model=self.gpt2_model,
            ref_model=None,
            tokenizer=self.gpt2_tokenizer,
            dataset=dummy_dataset,
        )

        dummy_queries = [torch.tensor([1, 2, 3, 4]), torch.tensor([1, 2, 3, 4, 5, 6, 7])]
        dummy_responses = [torch.tensor([5, 6, 7, 8, 9]), torch.tensor([8, 9, 10, 11, 12, 13])]
        dummy_scores = torch.Tensor([1, 2])

        ppo_trainer.config.mini_batch_size = 1
        ppo_trainer.config.batch_size = 1
        model_inputs = ppo_trainer.prepare_model_inputs(dummy_queries, dummy_responses)
        all_logprobs, _, values, mask = ppo_trainer.batched_forward_pass(
            self.gpt2_model, dummy_queries, dummy_responses, model_inputs
        )

        # dummy values
        ref_logprobs = all_logprobs + 1
        logits = torch.exp(all_logprobs)
        vpreds = values + 0.1

        score, non_score = ppo_trainer.compute_rewards(dummy_scores, all_logprobs, ref_logprobs, mask)
        values, advantages, returns = ppo_trainer.compute_advantages(values, score, mask)

        # just make sure a dummy loss is computed
        idx = 0
        pg_loss, v_loss, _ = ppo_trainer.loss(
            all_logprobs[idx].unsqueeze(0),
            values[idx].unsqueeze(0),
            logits[idx].unsqueeze(0),
            vpreds[idx].unsqueeze(0),
            ref_logprobs[idx].unsqueeze(0),
            mask[idx].unsqueeze(0),
            advantages[idx].unsqueeze(0),
            returns[idx].unsqueeze(0),
        )

        self.assertAlmostEqual(pg_loss.item(), 2.2868, 4)
        self.assertAlmostEqual(v_loss.item(), 0.09950, 4)

        # check if we get same results with masked parts removed
        pg_loss_unmasked, v_loss_unmasked, _ = ppo_trainer.loss(
            apply_mask(all_logprobs[idx], mask[idx]).unsqueeze(0),
            apply_mask(values[idx], mask[idx]).unsqueeze(0),
            apply_mask(logits[idx], mask[idx]).unsqueeze(0),
            apply_mask(vpreds[idx], mask[idx]).unsqueeze(0),
            apply_mask(ref_logprobs[idx], mask[idx]).unsqueeze(0),
            apply_mask(mask[idx], mask[idx]).unsqueeze(0),
            apply_mask(advantages[idx], mask[idx]).unsqueeze(0),
            apply_mask(returns[idx], mask[idx]).unsqueeze(0),
        )
        self.assertAlmostEqual(pg_loss_unmasked.item(), 2.2868, 4)
        self.assertAlmostEqual(v_loss_unmasked.item(), 0.09950, 4)

    @parameterized.expand(
        [
            ["gpt2"],
            ["bloom"],
            ["t5"],
        ]
    )
    def test_batched_forward_pass(self, name):
        """
        Test if the loss trainer works fine
        """
        # initialize dataset
        dummy_dataset = self._init_dummy_dataset()

        dummy_queries = [torch.tensor([1, 2, 3, 4]), torch.tensor([1, 2, 3, 4, 5, 6, 7])]
        dummy_responses = [torch.tensor([5, 6, 7, 8, 9]), torch.tensor([8, 9, 10, 11, 12, 13])]

        if name == "gpt2":
            model = self.gpt2_model
            tokenizer = self.gpt2_tokenizer
        elif name == "bloom":
            model = self.bloom_model
            tokenizer = self.bloom_tokenizer
        elif name == "t5":
            model = self.t5_model
            tokenizer = self.t5_tokenizer

        model.eval()

        ppo_trainer = PPOTrainer(
            config=self.ppo_config,
            model=model,
            ref_model=None,
            tokenizer=tokenizer,
            dataset=dummy_dataset,
        )

        # we test all combinations of fwd_bs and bs:
        # if fwd_bs=bs=1: no padding is applied and only one forward pass
        # if fwd_bs=1/bs=2: padding is applied and results computed in two fwd passes
        # if fwd_bs=bs=2: padding is applied and results computed in one fwd pass

        ppo_trainer.config.mini_batch_size = 1
        ppo_trainer.config.batch_size = 1

        model_inputs = ppo_trainer.prepare_model_inputs([dummy_queries[0]], [dummy_responses[0]])
        logprobs_0, logits_0, values_0, mask_0 = ppo_trainer.batched_forward_pass(
            model, [dummy_queries[0]], [dummy_responses[0]], model_inputs
        )

        ppo_trainer.config.batch_size = 2
        model_inputs = ppo_trainer.prepare_model_inputs(dummy_queries, dummy_responses)
        logprobs_1, logits_1, values_1, mask_1 = ppo_trainer.batched_forward_pass(
            model, dummy_queries, dummy_responses, model_inputs
        )

        ppo_trainer.config.mini_batch_size = 2
        model_inputs = ppo_trainer.prepare_model_inputs(dummy_queries, dummy_responses)
        logprobs_2, logits_2, values_2, mask_2 = ppo_trainer.batched_forward_pass(
            model, dummy_queries, dummy_responses, model_inputs
        )

        self.assertLessEqual(abs_diff_masked_tensors(logprobs_1, logprobs_2, mask_1, mask_2), 1e-4)
        self.assertLessEqual(abs_diff_masked_tensors(values_1, values_2, mask_1, mask_2), 1e-4)

        self.assertLessEqual(abs_diff_masked_tensors(logprobs_0, logprobs_2[:1], mask_0, mask_2[:1]), 1e-4)
        self.assertLessEqual(abs_diff_masked_tensors(values_0, values_2[:1], mask_0, mask_2[:1]), 1e-4)

    def test_ppo_trainer_max_grad_norm(self):
        """
        Test if the `max_grad_norm` feature works as expected
        """
        # initialize dataset
        dummy_dataset = self._init_dummy_dataset()

        self.ppo_config.max_grad_norm = 0.00001
        ppo_trainer = PPOTrainer(
            config=self.ppo_config,
            model=self.gpt2_model,
            ref_model=None,
            tokenizer=self.gpt2_tokenizer,
            dataset=dummy_dataset,
        )

        dummy_dataloader = ppo_trainer.dataloader

        # train model with ppo
        for query_tensor, response_tensor in dummy_dataloader:
            # define a reward for response
            # (this could be any reward such as human feedback or output from another model)
            reward = [torch.tensor(1.0), torch.tensor(0.0)]
            # train model
            _ = ppo_trainer.step([q for q in query_tensor], [r for r in response_tensor], reward)
            break

        # check gradients
        for name, param in ppo_trainer.model.named_parameters():
            self.assertTrue(param.grad is not None, f"Parameter {name} has no gradient")
            self.assertTrue(
                torch.all(param.grad.abs() <= self.ppo_config.max_grad_norm),
                f"Parameter {name} has a gradient larger than max_grad_norm",
            )

    def test_ppo_trainer_kl_penalty(self):
        dummy_dataset = self._init_dummy_dataset()

        log_probs = torch.Tensor([[0.5, 0.2, 0.1], [0.6, 0.2, 0.1]])
        ref_log_probs = torch.Tensor([[0.4, 0.3, 0.0], [0.7, 0.1, 0.3]])

        ppo_trainer = PPOTrainer(
            config=self.ppo_config,
            model=self.gpt2_model,
            ref_model=None,
            tokenizer=self.gpt2_tokenizer,
            dataset=dummy_dataset,
        )

        expected_output = torch.Tensor([[0.1000, -0.1000, 0.1000], [-0.1000, 0.1000, -0.2000]])
        self.assertTrue(torch.allclose(ppo_trainer._kl_penalty(log_probs, ref_log_probs), expected_output))

        self.ppo_config.kl_penalty = "abs"
        ppo_trainer = PPOTrainer(
            config=self.ppo_config,
            model=self.gpt2_model,
            ref_model=None,
            tokenizer=self.gpt2_tokenizer,
            dataset=dummy_dataset,
        )

        expected_output = torch.Tensor([[0.1000, 0.1000, 0.1000], [0.1000, 0.1000, 0.2000]])
        self.assertTrue(torch.allclose(ppo_trainer._kl_penalty(log_probs, ref_log_probs), expected_output))

        self.ppo_config.kl_penalty = "mse"
        ppo_trainer = PPOTrainer(
            config=self.ppo_config,
            model=self.gpt2_model,
            ref_model=None,
            tokenizer=self.gpt2_tokenizer,
            dataset=dummy_dataset,
        )

        expected_output = torch.Tensor([[0.0050, 0.0050, 0.0050], [0.0050, 0.0050, 0.0200]])
        self.assertTrue(torch.allclose(ppo_trainer._kl_penalty(log_probs, ref_log_probs), expected_output))

    def test_ppo_trainer_full_kl_penalty(self):
        # a few more extensive tests for the full kl option as it is more involved
        dummy_dataset = self._init_dummy_dataset()

        self.ppo_config.kl_penalty = "full"
        ppo_trainer = PPOTrainer(
            config=self.ppo_config,
            model=self.gpt2_model,
            ref_model=None,
            tokenizer=self.gpt2_tokenizer,
            dataset=dummy_dataset,
        )

        # Test on tensors for size B,S,T = (1,2,3)
        # test for when the two dists are the same
        log_probs = torch.Tensor(
            [
                [
                    [0.1, 0.2, 0.7],
                    [0.3, 0.4, 0.3],
                ]
            ]
        ).exp()

        ref_log_probs = torch.Tensor(
            [
                [
                    [0.1, 0.2, 0.7],
                    [0.3, 0.4, 0.3],
                ]
            ]
        ).exp()

        expected_output = torch.Tensor(
            [[0.0, 0.0]],
        )
        output = ppo_trainer._kl_penalty(log_probs, ref_log_probs)
        self.assertTrue(output.shape == (1, 2))
        self.assertTrue(torch.allclose(output, expected_output))

        # test for when the two dists are almost not overlapping
        log_probs = torch.Tensor(
            [
                [
                    [0.98, 0.01, 0.01],
                    [0.01, 0.98, 0.01],
                ]
            ]
        ).log()

        ref_log_probs = torch.Tensor(
            [
                [
                    [0.01, 0.01, 0.98],
                    [0.01, 0.01, 0.98],
                ]
            ]
        ).log()

        expected_output = torch.Tensor(
            [[4.4474, 4.4474]],
        )
        output = ppo_trainer._kl_penalty(log_probs, ref_log_probs)
        self.assertTrue(output.shape == (1, 2))
        self.assertTrue(torch.allclose(output, expected_output))

        # test for when the two dists are almost not overlapping
        log_probs = torch.Tensor(
            [
                [
                    [0.49, 0.02, 0.49],
                    [0.49, 0.02, 0.49],
                ]
            ]
        ).log()

        ref_log_probs = torch.Tensor(
            [
                [
                    [0.01, 0.98, 0.01],
                    [0.49, 0.02, 0.49],
                ]
            ]
        ).log()

        expected_output = torch.Tensor(
            [[3.7361, 0.0]],
        )
        output = ppo_trainer._kl_penalty(log_probs, ref_log_probs)
        self.assertTrue(output.shape == (1, 2))
        self.assertTrue(torch.allclose(output, expected_output, atol=1e-4))

    @require_peft
    @mark.peft_test
    def test_peft_model_ppo_trainer(self):
        from peft import LoraConfig, get_peft_model
        from transformers import AutoModelForCausalLM

        lora_config = LoraConfig(
            r=16,
            lora_alpha=32,
            lora_dropout=0.05,
            bias="none",
            task_type="CAUSAL_LM",
        )
        gpt2_model = AutoModelForCausalLM.from_pretrained(self.model_id)

        # this line is very important
        def make_inputs_require_grad(module, input, output):
            output.requires_grad_(True)

        gpt2_model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)

        peft_model = get_peft_model(gpt2_model, lora_config)
        model = AutoModelForCausalLMWithValueHead.from_pretrained(peft_model)

        dummy_dataset = self._init_dummy_dataset()
        self.ppo_config.batch_size = 2
        self.ppo_config.mini_batch_size = 1

        ppo_trainer = PPOTrainer(
            config=self.ppo_config,
            model=model,
            ref_model=None,
            tokenizer=self.gpt2_tokenizer,
            dataset=dummy_dataset,
        )

        self.assertTrue(ppo_trainer.ref_model is None)

        dummy_dataloader = ppo_trainer.dataloader

        # train model with ppo
        for query_tensor, response_tensor in dummy_dataloader:
            # define a reward for response
            # (this could be any reward such as human feedback or output from another model)
            reward = [torch.tensor(1.0), torch.tensor(0.0)]
            # train model by running a step twice
            _ = ppo_trainer.step([q for q in query_tensor], [r for r in response_tensor], reward)

            ppo_trainer.model.train()
            ppo_trainer.model.gradient_checkpointing_enable()
            _ = ppo_trainer.step([q for q in query_tensor], [r for r in response_tensor], reward)
            break

        # check gradients
        for name, param in model.named_parameters():
            if "lora" in name or "v_head" in name:
                self.assertTrue(param.grad is not None, f"Parameter {name} has a no gradient")
            else:
                self.assertTrue(param.grad is None, f"Parameter {name} has a gradient")

    @require_peft
    @mark.peft_test
    def test_peft_model_ppo_adapter_rm_trainer(self):
        from peft import LoraConfig, get_peft_model
        from transformers import AutoModelForCausalLM, AutoModelForSequenceClassification

        dummy_inputs = torch.LongTensor([[1, 2, 3, 4, 5], [1, 2, 3, 4, 5]])
        rm_lora_config = LoraConfig(
            r=16,
            lora_alpha=32,
            lora_dropout=0.05,
            bias="none",
            task_type="SEQ_CLS",
        )

        reward_model = AutoModelForSequenceClassification.from_pretrained(self.model_id)
        reward_model = get_peft_model(reward_model, rm_lora_config)
        dummy_optim = torch.optim.Adam(filter(lambda p: p.requires_grad, reward_model.parameters()), lr=1e-3)

        previous_rm_logits = reward_model(dummy_inputs).logits
        loss = previous_rm_logits.mean()
        loss.backward()

        dummy_optim.step()
        reward_model.eval()

        original_rm_logits = reward_model(dummy_inputs).logits

        with tempfile.TemporaryDirectory() as tmpdirname:
            reward_model.save_pretrained(tmpdirname)

            lora_config = LoraConfig(
                r=16,
                lora_alpha=32,
                lora_dropout=0.05,
                bias="none",
                task_type="CAUSAL_LM",
            )
            gpt2_model = AutoModelForCausalLM.from_pretrained(self.model_id)

            # this line is very important
            def make_inputs_require_grad(module, input, output):
                output.requires_grad_(True)

            gpt2_model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)

            peft_model = get_peft_model(gpt2_model, lora_config)
            model = AutoModelForCausalLMWithValueHead.from_pretrained(
                peft_model,
                reward_adapter=tmpdirname,
            )

            dummy_dataset = self._init_dummy_dataset()
            self.ppo_config.batch_size = 2
            self.ppo_config.mini_batch_size = 1

            ppo_trainer = PPOTrainer(
                config=self.ppo_config,
                model=model,
                ref_model=None,
                tokenizer=self.gpt2_tokenizer,
                dataset=dummy_dataset,
            )

            self.assertTrue(ppo_trainer.ref_model is None)

            dummy_dataloader = ppo_trainer.dataloader

            # train model with ppo
            for query_tensor, response_tensor in dummy_dataloader:
                # define a reward for response
                # (this could be any reward such as human feedback or output from another model)
                reward = [torch.tensor(1.0), torch.tensor(0.0)]
                # train model by running a step twice
                _ = ppo_trainer.step([q for q in query_tensor], [r for r in response_tensor], reward)

                ppo_trainer.model.train()
                ppo_trainer.model.gradient_checkpointing_enable()
                _ = ppo_trainer.step([q for q in query_tensor], [r for r in response_tensor], reward)
                break

            new_logits = ppo_trainer.model.compute_reward_score(dummy_inputs)
            self.assertTrue(not torch.allclose(previous_rm_logits, new_logits[:, -1, :]))
            self.assertTrue(torch.allclose(original_rm_logits, new_logits[:, -1, :]))

            # check gradients
            for name, param in model.named_parameters():
                if ("lora" in name or "v_head" in name) and ("reward" not in name):
                    self.assertTrue(param.grad is not None, f"Parameter {name} has a no gradient")
                else:
                    self.assertTrue(param.grad is None, f"Parameter {name} has a gradient")

    @unittest.skip("Fix by either patching `whomai()` to work in the staging endpoint or use a dummy prod user.")
    def test_push_to_hub(self):
        REPO_NAME = "test-ppo-trainer"
        repo_id = f"{CI_HUB_USER}/{REPO_NAME}"

        ppo_trainer = PPOTrainer(
            config=self.ppo_config,
            model=self.gpt2_model,
            ref_model=None,
            tokenizer=self.gpt2_tokenizer,
            dataset=self._init_dummy_dataset(),
        )
        with tempfile.TemporaryDirectory():
            url = ppo_trainer.push_to_hub(repo_id=repo_id, token=self._token, api_endpoint=CI_HUB_ENDPOINT)
            # Extract repo_name from the url
            re_search = re.search(CI_HUB_ENDPOINT + r"/([^/]+/[^/]+)/", url)
            self.assertTrue(re_search is not None)
            hub_repo_id = re_search.groups()[0]
            # Check we created a Hub repo
            self.assertEqual(hub_repo_id, repo_id)
            # Ensure all files are present
            files = sorted(self._api.list_repo_files(hub_repo_id))
            assert all(
                fnmatch.fnmatch(file, expected_file)
                for file, expected_file in zip(
                    files,
                    [
                        ".gitattributes",
                        "README.md",
                        "config.json",
                        "merges.txt",
                        "pytorch_model.bin",
                        "special_tokens_map.json",
                        "tokenizer_config.json",
                        "vocab.json",
                    ],
                )
            )

    @require_peft
    @require_torch_multi_gpu
    @mark.peft_test
    def test_peft_model_ppo_trainer_multi_gpu(self):
        from peft import LoraConfig, get_peft_model
        from transformers import AutoModelForCausalLM

        lora_config = LoraConfig(
            r=16,
            lora_alpha=32,
            lora_dropout=0.05,
            bias="none",
            task_type="CAUSAL_LM",
        )
        gpt2_model = AutoModelForCausalLM.from_pretrained(
            "gpt2", device_map="balanced", max_memory={0: "500MB", 1: "500MB"}
        )

        self.assertTrue(set(gpt2_model.hf_device_map.values()) == {0, 1})

        # this line is very important
        def make_inputs_require_grad(module, input, output):
            output.requires_grad_(True)

        gpt2_model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)

        peft_model = get_peft_model(gpt2_model, lora_config)
        model = AutoModelForCausalLMWithValueHead.from_pretrained(peft_model)

        self.assertTrue(model.is_sequential_parallel)

        dummy_dataset = self._init_dummy_dataset()
        self.ppo_config.batch_size = 2
        self.ppo_config.mini_batch_size = 1

        ppo_trainer = PPOTrainer(
            config=self.ppo_config,
            model=model,
            ref_model=None,
            tokenizer=self.gpt2_tokenizer,
            dataset=dummy_dataset,
        )

        self.assertTrue(ppo_trainer.ref_model is None)

        dummy_dataloader = ppo_trainer.dataloader

        # train model with ppo
        for query_tensor, response_tensor in dummy_dataloader:
            # define a reward for response
            # (this could be any reward such as human feedback or output from another model)
            reward = [torch.tensor(1.0), torch.tensor(0.0)]
            # train model by running a step twice
            _ = ppo_trainer.step([q for q in query_tensor], [r for r in response_tensor], reward)

            ppo_trainer.model.train()
            ppo_trainer.model.gradient_checkpointing_enable()
            _ = ppo_trainer.step([q for q in query_tensor], [r for r in response_tensor], reward)
            break

        # check gradients
        for name, param in model.named_parameters():
            if "lora" in name or "v_head" in name:
                self.assertTrue(param.grad is not None, f"Parameter {name} has a no gradient")
            else:
                self.assertTrue(param.grad is None, f"Parameter {name} has a gradient")

    def test_generation(self):
        dummy_dataset = self._init_dummy_dataset()

        model = AutoModelForCausalLMWithValueHead.from_pretrained("gpt2")
        tokenizer = AutoTokenizer.from_pretrained("gpt2")

        ppo_trainer = PPOTrainer(
            config=self.ppo_config,
            model=model,
            ref_model=None,
            tokenizer=tokenizer,
            dataset=dummy_dataset,
        )

        input_texts = ["this is a test", "this is another, longer test"]

        generation_kwargs = {"do_sample": False, "max_new_tokens": 4, "pad_token_id": tokenizer.eos_token_id}

        tokenizer.pad_token = tokenizer.eos_token

        model_inputs = [tokenizer(txt, return_tensors="pt").input_ids.squeeze() for txt in input_texts]

        generations_batched = ppo_trainer.generate(model_inputs, batch_size=2, **generation_kwargs)
        generations_batched = tokenizer.batch_decode(generations_batched)

        generations_single = [ppo_trainer.generate(inputs, **generation_kwargs).squeeze() for inputs in model_inputs]
        generations_single = tokenizer.batch_decode(generations_single)

        self.assertEqual(generations_single, generations_batched)

    def test_grad_accumulation(self):
        dummy_dataset = self._init_dummy_dataset()

        torch.manual_seed(0)
        gpt2_model = AutoModelForCausalLMWithValueHead.from_pretrained(self.model_id, summary_dropout_prob=0.0)
        gpt2_model_clone = copy.deepcopy(gpt2_model)

        self.ppo_config.mini_batch_size = 2
        self.ppo_config.ppo_epochs = 1

        ppo_trainer = PPOTrainer(
            config=self.ppo_config,
            model=gpt2_model,
            ref_model=None,
            tokenizer=self.gpt2_tokenizer,
            dataset=dummy_dataset,
        )

        dummy_dataloader = ppo_trainer.dataloader

        # train model with ppo
        for query_tensor, response_tensor in dummy_dataloader:
            # define a reward for response
            # (this could be any reward such as human feedback or output from another model)
            reward = [torch.tensor(1.0), torch.tensor(1.0)]
            # train model by running a step twice
            _ = ppo_trainer.step([q for q in query_tensor], [r for r in response_tensor], reward)
            break

        model_grad = gpt2_model.v_head.summary.weight

        self.ppo_config.mini_batch_size = 1
        self.ppo_config.gradient_accumulation_steps = 2

        ppo_trainer = PPOTrainer(
            config=self.ppo_config,
            model=gpt2_model_clone,
            ref_model=None,
            tokenizer=self.gpt2_tokenizer,
            dataset=dummy_dataset,
        )

        dummy_dataloader = ppo_trainer.dataloader

        # train model with ppo
        for query_tensor, response_tensor in dummy_dataloader:
            # define a reward for response
            # (this could be any reward such as human feedback or output from another model)
            reward = [torch.tensor(1.0), torch.tensor(1.0)]
            # train model by running a step twice
            _ = ppo_trainer.step([q for q in query_tensor], [r for r in response_tensor], reward)
            break

        model_grad_acc = gpt2_model_clone.v_head.summary.weight
        self.assertTrue(torch.allclose(model_grad_acc, model_grad, rtol=1e-3, atol=1e-3))

    @unittest.skip("Fix by either patching `whomai()` to work in the staging endpoint or use a dummy prod user.")
    def test_push_to_hub_if_best_reward(self):
        REPO_NAME = "test-ppo-trainer"
        repo_id = f"{CI_HUB_USER}/{REPO_NAME}"

        dummy_dataset = self._init_dummy_dataset()

        push_to_hub_if_best_kwargs = {"repo_id": repo_id}

        ppo_config = PPOConfig(
            batch_size=2,
            mini_batch_size=1,
            log_with=None,
            push_to_hub_if_best_kwargs=push_to_hub_if_best_kwargs,
            compare_steps=1,
        )

        ppo_trainer = PPOTrainer(
            config=ppo_config,
            model=self.gpt2_model,
            ref_model=self.gpt2_model_ref,
            tokenizer=self.gpt2_tokenizer,
            dataset=dummy_dataset,
        )

        dummy_dataloader = ppo_trainer.dataloader
        # train model with ppo
        for query_tensor, response_tensor in dummy_dataloader:
            # define a reward for response
            # (this could be any reward such as human feedback or output from another model)
            reward = [torch.tensor(1.0), torch.tensor(0.0)]
            # train model
            _ = ppo_trainer.step([q for q in query_tensor], [r for r in response_tensor], reward)
            break

    def test_batch_size_check(self):
        with pytest.raises(ValueError):
            PPOConfig(batch_size=2, mini_batch_size=2, gradient_accumulation_steps=2)

File Path: tests/test_reward_trainer.py
Content:
# Copyright 2023 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import tempfile
import unittest

import torch
from datasets import Dataset
from transformers import AutoModelForSequenceClassification, AutoTokenizer, EvalPrediction

from trl import RewardConfig, RewardTrainer
from trl.trainer import compute_accuracy

from .testing_utils import require_peft


class RewardTrainerTester(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        cls.model_id = "trl-internal-testing/dummy-GPT2-correct-vocab"
        cls.model = AutoModelForSequenceClassification.from_pretrained(cls.model_id)
        cls.tokenizer = AutoTokenizer.from_pretrained(cls.model_id)
        cls.tokenizer.pad_token = cls.tokenizer.eos_token

    def test_accuracy_metrics(self):
        dummy_eval_predictions = EvalPrediction(torch.FloatTensor([[0.1, 0.9], [0.9, 0.1]]), torch.LongTensor([0, 0]))
        accuracy = compute_accuracy(dummy_eval_predictions)
        self.assertEqual(accuracy["accuracy"], 0.5)

    def test_reward_trainer(self):
        with tempfile.TemporaryDirectory() as tmp_dir:
            training_args = RewardConfig(
                output_dir=tmp_dir,
                per_device_train_batch_size=2,
                max_steps=3,
                remove_unused_columns=False,
                gradient_accumulation_steps=4,
                learning_rate=9e-1,
                evaluation_strategy="steps",
            )

            # fmt: off
            dummy_dataset_dict = {
                "input_ids_chosen": [
                    torch.LongTensor([0, 1, 2,]),
                    torch.LongTensor([1, 2]),
                    torch.LongTensor([0, 1, 2,]),
                    torch.LongTensor([1, 2]),
                ],
                "attention_mask_chosen": [
                    torch.LongTensor([1, 1, 1]),
                    torch.LongTensor([1, 0]),
                    torch.LongTensor([1, 1, 1]),
                    torch.LongTensor([1, 0]),
                ],
                "input_ids_rejected": [
                    torch.LongTensor([0, 2,]),
                    torch.LongTensor([1, 2, 0]),
                    torch.LongTensor([0, 2,]),
                    torch.LongTensor([1, 2, 0]),
                ],
                "attention_mask_rejected": [
                    torch.LongTensor([1, 1]),
                    torch.LongTensor([1, 1, 0]),
                    torch.LongTensor([1, 1]),
                    torch.LongTensor([1, 1, 1]),
                ],
            }
            # fmt: on
            dummy_dataset = Dataset.from_dict(dummy_dataset_dict)

            trainer = RewardTrainer(
                model=self.model,
                args=training_args,
                tokenizer=self.tokenizer,
                train_dataset=dummy_dataset,
                eval_dataset=dummy_dataset,
            )

            previous_trainable_params = {n: param.clone() for n, param in trainer.model.named_parameters()}

            trainer.train()

            self.assertIsNotNone(trainer.state.log_history[-1]["train_loss"])

            # check the params have changed
            for n, param in previous_trainable_params.items():
                new_param = trainer.model.get_parameter(n)
                # check the params have changed - ignore 0 biases
                if param.sum() != 0:
                    self.assertFalse(torch.equal(param, new_param))

            preds = trainer.predict(dummy_dataset)
            self.assertEqual(preds.predictions.shape, (4, 2))

    @require_peft
    def test_reward_trainer_peft(self):
        import peft
        from peft import LoraConfig, TaskType

        peft_version = peft.__version__

        peft_config = LoraConfig(
            task_type=TaskType.SEQ_CLS,
            inference_mode=False,
            r=8,
            lora_alpha=32,
            lora_dropout=0.1,
        )

        with tempfile.TemporaryDirectory() as tmp_dir:
            training_args = RewardConfig(
                output_dir=tmp_dir,
                per_device_train_batch_size=2,
                max_steps=6,
                remove_unused_columns=False,
                gradient_accumulation_steps=2,
                learning_rate=9e-1,
                evaluation_strategy="steps",
            )

            # fmt: off
            dummy_dataset_dict = {
                "input_ids_chosen": [
                    torch.LongTensor([0, 1, 2,]),
                    torch.LongTensor([1, 2]),
                    torch.LongTensor([0, 1, 2,]),
                    torch.LongTensor([1, 2]),
                ],
                "attention_mask_chosen": [
                    torch.LongTensor([1, 1, 1]),
                    torch.LongTensor([1, 0]),
                    torch.LongTensor([1, 1, 1]),
                    torch.LongTensor([1, 0]),
                ],
                "input_ids_rejected": [
                    torch.LongTensor([0, 2,]),
                    torch.LongTensor([1, 2, 0]),
                    torch.LongTensor([0, 2,]),
                    torch.LongTensor([1, 2, 0]),
                ],
                "attention_mask_rejected": [
                    torch.LongTensor([1, 1]),
                    torch.LongTensor([1, 1, 0]),
                    torch.LongTensor([1, 1]),
                    torch.LongTensor([1, 1, 1]),
                ],
            }
            # fmt: on
            dummy_dataset = Dataset.from_dict(dummy_dataset_dict)

            trainer = RewardTrainer(
                model=self.model,
                args=training_args,
                tokenizer=self.tokenizer,
                train_dataset=dummy_dataset,
                eval_dataset=dummy_dataset,
                peft_config=peft_config,
            )
            previous_trainable_params = {}
            previous_non_trainable_params = {}

            # due to a change in the way the modules to save are dealt in PEFT.
            trainable_params_name = ["lora", "score"] if peft_version < "0.3.0" else ["lora", "modules_to_save"]

            # check gradients are not None
            for n, param in trainer.model.named_parameters():
                if any([t in n for t in trainable_params_name]):
                    previous_trainable_params[n] = param.clone()
                else:
                    previous_non_trainable_params[n] = param.clone()

            trainer.train()

            self.assertIsNotNone(trainer.state.log_history[-1]["train_loss"])

            # check the params have changed
            for n, param in previous_trainable_params.items():
                new_param = trainer.model.get_parameter(n)
                self.assertFalse(torch.allclose(param, new_param, atol=1e-12, rtol=1e-12))

            # check the non trainable params have not changed
            for n, param in previous_non_trainable_params.items():
                new_param = trainer.model.get_parameter(n)
                self.assertTrue(torch.allclose(param, new_param, atol=1e-12, rtol=1e-12))

            preds = trainer.predict(dummy_dataset)
            self.assertEqual(preds.predictions.shape, (4, 2))

    def test_reward_trainer_assert_value_error(self):
        with tempfile.TemporaryDirectory() as tmp_dir:
            training_args = RewardConfig(
                output_dir=tmp_dir,
                per_device_train_batch_size=2,
                max_steps=1,
                remove_unused_columns=False,
            )

            dummy_dataset_dict = {
                # fmt: off
                "input_ids_b": [
                    torch.LongTensor([0, 1, 2,]),
                    torch.LongTensor([1, 2]),
                    torch.LongTensor([0, 1, 2,]),
                    torch.LongTensor([1, 2]),
                ],
                "attention_mask_c": [
                    torch.LongTensor([1, 1, 1]),
                    torch.LongTensor([1, 0]),
                    torch.LongTensor([1, 1, 1]),
                    torch.LongTensor([1, 0]),
                ],
                "input_ids_f": [
                    torch.LongTensor([0, 2,]),
                    torch.LongTensor([1, 2, 0]),
                    torch.LongTensor([0, 2,]),
                    torch.LongTensor([1, 2, 0]),
                ],
                "attention_mask_g": [
                    torch.LongTensor([1, 1]),
                    torch.LongTensor([1, 1, 0]),
                    torch.LongTensor([1, 1]),
                    torch.LongTensor([1, 1, 1]),
                ],
                # fmt: on
            }
            dummy_dataset = Dataset.from_dict(dummy_dataset_dict)

            trainer = RewardTrainer(
                model=self.model,
                args=training_args,
                tokenizer=self.tokenizer,
                train_dataset=dummy_dataset,
            )

            with self.assertRaises(ValueError):
                trainer.train()

            training_args = RewardConfig(
                output_dir=tmp_dir,
                per_device_train_batch_size=2,
                max_steps=1,
                remove_unused_columns=True,
            )

            with self.assertWarns(UserWarning):
                trainer = RewardTrainer(
                    model=self.model,
                    args=training_args,
                    tokenizer=self.tokenizer,
                    train_dataset=dummy_dataset,
                )

    def test_reward_trainer_margin(self):
        with tempfile.TemporaryDirectory() as tmp_dir:
            training_args = RewardConfig(
                output_dir=tmp_dir,
                per_device_train_batch_size=2,
                max_steps=3,
                remove_unused_columns=False,
                gradient_accumulation_steps=4,
                learning_rate=9e-1,
                evaluation_strategy="steps",
            )

            # fmt: off
            dummy_dataset_dict = {
                "input_ids_chosen": [
                    torch.LongTensor([0, 1, 2,]),
                ],
                "attention_mask_chosen": [
                    torch.LongTensor([1, 1, 1]),
                ],
                "input_ids_rejected": [
                    torch.LongTensor([0, 2,]),
                ],
                "attention_mask_rejected": [
                    torch.LongTensor([1, 1]),
                ],
                "margin": [
                    torch.FloatTensor([1.0]),
                ]
            }
            # fmt: on
            dummy_dataset = Dataset.from_dict(dummy_dataset_dict)

            trainer = RewardTrainer(
                model=self.model,
                args=training_args,
                tokenizer=self.tokenizer,
                train_dataset=dummy_dataset,
                eval_dataset=dummy_dataset,
            )

            batch = [dummy_dataset[0]]
            batch = trainer.data_collator(batch)
            loss, outputs = trainer.compute_loss(trainer.model, batch, return_outputs=True)

            self.assertAlmostEqual(
                loss,
                -torch.nn.functional.logsigmoid(
                    outputs["rewards_chosen"] - outputs["rewards_rejected"] - batch["margin"]
                ).mean(),
            )

File Path: tests/test_sft_trainer.py
Content:
# Copyright 2023 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import copy
import os
import tempfile
import unittest

import numpy as np
from datasets import Dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments

from trl import SFTTrainer
from trl.import_utils import is_peft_available
from trl.trainer import ConstantLengthDataset, DataCollatorForCompletionOnlyLM

from .testing_utils import require_peft


def formatting_prompts_func(example):
    text = f"### Question: {example['question']}\n ### Answer: {example['answer']}"
    return text


def formatting_prompts_func_batched(example):
    output_text = []
    for i, question in enumerate(example["question"]):
        text = f"### Question: {question}\n ### Answer: {example['answer'][i]}"
        output_text.append(text)
    return output_text


if is_peft_available():
    from peft import LoraConfig, PeftModel


class SFTTrainerTester(unittest.TestCase):
    r""" """

    @classmethod
    def setUpClass(cls):
        cls.model_id = "trl-internal-testing/dummy-GPT2-correct-vocab"
        cls.model = AutoModelForCausalLM.from_pretrained(cls.model_id)
        cls.tokenizer = AutoTokenizer.from_pretrained(cls.model_id)
        cls.tokenizer.pad_token = cls.tokenizer.eos_token
        cls.dummy_dataset = Dataset.from_dict(
            {
                "question": [
                    "Does llamas know how to code?",
                    "Does llamas know how to fly?",
                    "Does llamas know how to talk?",
                    "Does llamas know how to code?",
                    "Does llamas know how to fly?",
                    "Does llamas know how to talk?",
                    "Does llamas know how to swim?",
                ],
                "answer": [
                    "Yes, llamas are very good at coding.",
                    "No, llamas can't fly.",
                    "Yes, llamas are very good at talking.",
                    "Yes, llamas are very good at coding.",
                    "No, llamas can't fly.",
                    "Yes, llamas are very good at talking.",
                    "No, llamas can't swim.",
                ],
                "text": [
                    "### Question: Does llamas know how to code?\n ### Answer: Yes, llamas are very good at coding.",
                    "### Question: Does llamas know how to fly?\n ### Answer: No, llamas can't fly.",
                    "### Question: Does llamas know how to talk?\n ### Answer: Yes, llamas are very good at talking.",
                    "### Question: Does llamas know how to code?\n ### Answer: Yes, llamas are very good at coding.",
                    "### Question: Does llamas know how to fly?\n ### Answer: No, llamas can't fly.",
                    "### Question: Does llamas know how to talk?\n ### Answer: Yes, llamas are very good at talking.",
                    "### Question: Does llamas know how to swim?\n ### Answer: No, llamas can't swim.",
                ],
            }
        )

        cls.train_dataset = ConstantLengthDataset(
            cls.tokenizer,
            cls.dummy_dataset,
            dataset_text_field=None,
            formatting_func=formatting_prompts_func,
            seq_length=16,
            num_of_sequences=16,
        )

        cls.eval_dataset = ConstantLengthDataset(
            cls.tokenizer,
            cls.dummy_dataset,
            dataset_text_field=None,
            formatting_func=formatting_prompts_func,
            seq_length=16,
            num_of_sequences=16,
        )

    def test_constant_length_dataset(self):
        formatted_dataset = ConstantLengthDataset(
            self.tokenizer,
            self.dummy_dataset,
            dataset_text_field=None,
            formatting_func=formatting_prompts_func,
        )

        self.assertTrue(len(formatted_dataset) == len(self.dummy_dataset))
        self.assertTrue(len(formatted_dataset) > 0)

        for example in formatted_dataset:
            self.assertTrue("input_ids" in example)
            self.assertTrue("labels" in example)

            self.assertTrue(len(example["input_ids"]) == formatted_dataset.seq_length)
            self.assertTrue(len(example["labels"]) == formatted_dataset.seq_length)

            decoded_text = self.tokenizer.decode(example["input_ids"])
            self.assertTrue(("Question" in decoded_text) and ("Answer" in decoded_text))

    def test_sft_trainer(self):
        with tempfile.TemporaryDirectory() as tmp_dir:
            training_args = TrainingArguments(
                output_dir=tmp_dir,
                dataloader_drop_last=True,
                evaluation_strategy="steps",
                max_steps=4,
                eval_steps=2,
                save_steps=2,
                per_device_train_batch_size=2,
            )

            trainer = SFTTrainer(
                model=self.model_id,
                args=training_args,
                train_dataset=self.train_dataset,
                eval_dataset=self.eval_dataset,
                packing=True,
            )

            trainer.train()

            self.assertIsNotNone(trainer.state.log_history[-1]["train_loss"])
            self.assertIsNotNone(trainer.state.log_history[0]["eval_loss"])

            self.assertTrue("pytorch_model.bin" in os.listdir(tmp_dir + "/checkpoint-2"))

    def test_sft_trainer_uncorrect_data(self):
        with tempfile.TemporaryDirectory() as tmp_dir:
            training_args = TrainingArguments(
                output_dir=tmp_dir,
                dataloader_drop_last=True,
                evaluation_strategy="steps",
                max_steps=2,
                eval_steps=1,
                save_steps=1,
                per_device_train_batch_size=2,
            )

            with self.assertRaises(ValueError):
                _ = SFTTrainer(
                    model=self.model,
                    args=training_args,
                    train_dataset=self.dummy_dataset,
                    packing=True,
                )

            # This should work
            _ = SFTTrainer(
                model=self.model,
                args=training_args,
                train_dataset=self.dummy_dataset,
                formatting_func=formatting_prompts_func,
                packing=True,
            )

            # This should not work as well
            with self.assertRaises(ValueError):
                _ = SFTTrainer(
                    model=self.model,
                    args=training_args,
                    train_dataset=self.dummy_dataset,
                    formatting_func=formatting_prompts_func,
                    packing=False,
                )

            # but this shpuld work
            _ = SFTTrainer(
                model=self.model,
                args=training_args,
                train_dataset=self.dummy_dataset,
                formatting_func=formatting_prompts_func_batched,
                packing=False,
            )

    def test_sft_trainer_with_model_num_train_epochs(self):
        with tempfile.TemporaryDirectory() as tmp_dir:
            training_args = TrainingArguments(
                output_dir=tmp_dir,
                dataloader_drop_last=True,
                evaluation_strategy="steps",
                max_steps=2,
                eval_steps=1,
                save_steps=1,
                num_train_epochs=2,
                per_device_train_batch_size=2,
            )

            trainer = SFTTrainer(
                model=self.model,
                args=training_args,
                train_dataset=self.train_dataset,
                eval_dataset=self.eval_dataset,
                packing=True,
            )

            trainer.train()

            self.assertIsNotNone(trainer.state.log_history[-1]["train_loss"])
            self.assertIsNotNone(trainer.state.log_history[0]["eval_loss"])

            self.assertTrue("pytorch_model.bin" in os.listdir(tmp_dir + "/checkpoint-2"))

        with tempfile.TemporaryDirectory() as tmp_dir:
            training_args = TrainingArguments(
                output_dir=tmp_dir,
                dataloader_drop_last=True,
                evaluation_strategy="steps",
                max_steps=2,
                save_steps=1,
                num_train_epochs=2,
                per_device_train_batch_size=2,
            )

            trainer = SFTTrainer(
                model=self.model,
                args=training_args,
                train_dataset=self.dummy_dataset,
                dataset_text_field="text",
                max_seq_length=16,
                num_of_sequences=16,
                packing=True,
            )

            trainer.train()

            self.assertIsNotNone(trainer.state.log_history[-1]["train_loss"])

            self.assertTrue("pytorch_model.bin" in os.listdir(tmp_dir + "/checkpoint-2"))

        with tempfile.TemporaryDirectory() as tmp_dir:
            training_args = TrainingArguments(
                output_dir=tmp_dir,
                dataloader_drop_last=True,
                evaluation_strategy="steps",
                max_steps=2,
                save_steps=1,
                num_train_epochs=2,
                per_device_train_batch_size=2,
            )

            trainer = SFTTrainer(
                model=self.model,
                args=training_args,
                train_dataset=self.dummy_dataset,
                dataset_text_field="text",
                max_seq_length=16,
            )

            trainer.train()

            self.assertIsNotNone(trainer.state.log_history[-1]["train_loss"])

            self.assertTrue("pytorch_model.bin" in os.listdir(tmp_dir + "/checkpoint-1"))

    def test_sft_trainer_with_model(self):
        with tempfile.TemporaryDirectory() as tmp_dir:
            training_args = TrainingArguments(
                output_dir=tmp_dir,
                dataloader_drop_last=True,
                evaluation_strategy="steps",
                max_steps=2,
                eval_steps=1,
                save_steps=1,
                per_device_train_batch_size=2,
            )

            trainer = SFTTrainer(
                model=self.model,
                args=training_args,
                train_dataset=self.train_dataset,
                eval_dataset=self.eval_dataset,
                packing=True,
            )

            trainer.train()

            self.assertIsNotNone(trainer.state.log_history[-1]["train_loss"])
            self.assertIsNotNone(trainer.state.log_history[0]["eval_loss"])

            self.assertTrue("pytorch_model.bin" in os.listdir(tmp_dir + "/checkpoint-2"))

        with tempfile.TemporaryDirectory() as tmp_dir:
            training_args = TrainingArguments(
                output_dir=tmp_dir,
                dataloader_drop_last=True,
                evaluation_strategy="steps",
                max_steps=2,
                save_steps=1,
                per_device_train_batch_size=2,
            )

            trainer = SFTTrainer(
                model=self.model,
                args=training_args,
                train_dataset=self.dummy_dataset,
                dataset_text_field="text",
                max_seq_length=16,
                num_of_sequences=16,
                packing=True,
            )

            trainer.train()

            self.assertIsNotNone(trainer.state.log_history[-1]["train_loss"])

            self.assertTrue("pytorch_model.bin" in os.listdir(tmp_dir + "/checkpoint-2"))

        # with formatting_func + packed
        with tempfile.TemporaryDirectory() as tmp_dir:
            training_args = TrainingArguments(
                output_dir=tmp_dir,
                dataloader_drop_last=True,
                evaluation_strategy="steps",
                max_steps=2,
                save_steps=1,
                per_device_train_batch_size=2,
            )

            trainer = SFTTrainer(
                model=self.model,
                args=training_args,
                train_dataset=self.dummy_dataset,
                formatting_func=formatting_prompts_func,
                max_seq_length=16,
                num_of_sequences=16,
                packing=True,
            )

            trainer.train()

            self.assertIsNotNone(trainer.state.log_history[-1]["train_loss"])

            self.assertTrue("pytorch_model.bin" in os.listdir(tmp_dir + "/checkpoint-2"))

        # with formatting_func + packed
        with tempfile.TemporaryDirectory() as tmp_dir:
            training_args = TrainingArguments(
                output_dir=tmp_dir,
                dataloader_drop_last=True,
                evaluation_strategy="steps",
                max_steps=2,
                save_steps=1,
                per_device_train_batch_size=2,
            )

            trainer = SFTTrainer(
                model=self.model,
                args=training_args,
                train_dataset=self.dummy_dataset,
                formatting_func=formatting_prompts_func_batched,
                max_seq_length=16,
            )

            trainer.train()

            self.assertIsNotNone(trainer.state.log_history[-1]["train_loss"])

            self.assertTrue("pytorch_model.bin" in os.listdir(tmp_dir + "/checkpoint-2"))

        with tempfile.TemporaryDirectory() as tmp_dir:
            training_args = TrainingArguments(
                output_dir=tmp_dir,
                dataloader_drop_last=True,
                evaluation_strategy="steps",
                max_steps=2,
                save_steps=1,
                per_device_train_batch_size=2,
            )

            trainer = SFTTrainer(
                model=self.model,
                args=training_args,
                train_dataset=self.dummy_dataset,
                dataset_text_field="text",
                max_seq_length=16,
            )

            trainer.train()

            self.assertIsNotNone(trainer.state.log_history[-1]["train_loss"])

            self.assertTrue("pytorch_model.bin" in os.listdir(tmp_dir + "/checkpoint-1"))

    def test_data_collator_completion_lm(self):
        response_template = "### Response:\n"
        data_collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=self.tokenizer, mlm=False)

        text = """\n\n### Instructions:\nHello all this should be masked\n\n### Response:\nI have not been masked correctly."""
        encoded_text = self.tokenizer(text)

        examples = [encoded_text]

        batch = data_collator(examples)
        labels = batch["labels"]
        last_pad_idx = np.where(labels == -100)[1][-1]
        result_text = self.tokenizer.decode(batch["input_ids"][0, last_pad_idx + 1 :])
        self.assertEqual(result_text, "I have not been masked correctly.")

    def test_data_collator_completion_lm_with_multiple_text(self):
        tokenizer = copy.deepcopy(self.tokenizer)
        tokenizer.padding_side = "left"

        response_template = "### Response:\n"
        data_collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer, mlm=False)

        text1 = """\n\n### Instructions:\nHello all this should be masked\n\n### Response:\nI have not been masked correctly."""
        text2 = """\n\n### Instructions:\nThis is another longer text that should also be masked. This text is significantly longer than the previous one.\n\n### Response:\nI have not been masked correctly."""

        encoded_text1 = tokenizer(text1)
        encoded_text2 = tokenizer(text2)

        examples = [encoded_text1, encoded_text2]

        batch = data_collator(examples)

        for i in range(2):
            labels = batch["labels"][i]
            last_pad_idx = np.where(labels == -100)[0][-1]
            result_text = tokenizer.decode(batch["input_ids"][i, last_pad_idx + 1 :])
            self.assertEqual(result_text, "I have not been masked correctly.")

    def test_data_collator_chat_completion_lm(self):
        instruction_template = "### Human:"
        assistant_template = "### Assistant:"
        data_collator = DataCollatorForCompletionOnlyLM(
            response_template=assistant_template,
            instruction_template=instruction_template,
            tokenizer=self.tokenizer,
            mlm=False,
        )

        text = """### Human: Hello all this should be masked.### Assistant: I should not be masked.### Human: All this should be masked too.### Assistant: I should not be masked too."""
        encoded_text = self.tokenizer(text)

        examples = [encoded_text]

        batch = data_collator(examples)
        labels = batch["labels"]
        non_masked_tokens = batch["input_ids"][labels != -100]
        result_text = self.tokenizer.decode(non_masked_tokens)
        self.assertEqual(result_text, " I should not be masked. I should not be masked too.")

    def test_data_collator_chat_completion_lm_with_multiple_text(self):
        tokenizer = copy.deepcopy(self.tokenizer)
        tokenizer.padding_side = "left"

        instruction_template = "### Human:"
        assistant_template = "### Assistant:"
        data_collator = DataCollatorForCompletionOnlyLM(
            response_template=assistant_template,
            instruction_template=instruction_template,
            tokenizer=tokenizer,
            mlm=False,
        )

        text1 = """### Human: Hello all this should be masked.### Assistant: I should not be masked."""
        text2 = """### Human: Hello all this should be masked.### Assistant: I should not be masked.### Human: All this should be masked too.### Assistant: I should not be masked too."""
        encoded_text1 = tokenizer(text1)
        encoded_text2 = tokenizer(text2)

        examples = [encoded_text1, encoded_text2]

        batch = data_collator(examples)
        labels = batch["labels"]
        input_ids = batch["input_ids"]

        non_masked_tokens1 = input_ids[0][labels[0] != -100]
        result_text1 = tokenizer.decode(non_masked_tokens1)
        self.assertEqual(result_text1, " I should not be masked.")

        non_masked_tokens2 = input_ids[1][labels[1] != -100]
        result_text2 = tokenizer.decode(non_masked_tokens2)
        self.assertEqual(result_text2, " I should not be masked. I should not be masked too.")

    def test_sft_trainer_infinite_with_model(self):
        with tempfile.TemporaryDirectory() as tmp_dir:
            training_args = TrainingArguments(
                output_dir=tmp_dir,
                dataloader_drop_last=True,
                evaluation_strategy="steps",
                max_steps=5,
                eval_steps=1,
                save_steps=1,
                per_device_train_batch_size=2,
            )

            trainer = SFTTrainer(
                model=self.model,
                args=training_args,
                train_dataset=self.train_dataset,
                eval_dataset=self.eval_dataset,
                packing=True,
                max_seq_length=500,
            )

            self.assertTrue(trainer.train_dataset.infinite)

            trainer.train()

            self.assertIsNotNone(trainer.state.log_history[-1]["train_loss"])
            self.assertIsNotNone(trainer.state.log_history[0]["eval_loss"])

            # make sure the trainer did 5 steps
            self.assertTrue("pytorch_model.bin" in os.listdir(tmp_dir + "/checkpoint-5"))

    def test_sft_trainer_infinite_with_model_epochs(self):
        with tempfile.TemporaryDirectory() as tmp_dir:
            training_args = TrainingArguments(
                output_dir=tmp_dir,
                dataloader_drop_last=True,
                num_train_epochs=1,
                per_device_train_batch_size=2,
                save_strategy="epoch",
            )

            trainer = SFTTrainer(
                model=self.model,
                args=training_args,
                train_dataset=self.train_dataset,
                eval_dataset=self.eval_dataset,
                packing=True,
                max_seq_length=500,
            )

            self.assertFalse(trainer.train_dataset.infinite)

            trainer.train()

            self.assertIsNotNone(trainer.state.log_history[-1]["train_loss"])

            # make sure the trainer did 5 steps
            self.assertTrue("pytorch_model.bin" in os.listdir(tmp_dir + "/checkpoint-4"))

    @require_peft
    def test_peft_sft_trainer(self):
        with tempfile.TemporaryDirectory() as tmp_dir:
            training_args = TrainingArguments(
                output_dir=tmp_dir,
                dataloader_drop_last=True,
                evaluation_strategy="steps",
                max_steps=4,
                eval_steps=2,
                save_steps=2,
                per_device_train_batch_size=2,
            )

            peft_config = LoraConfig(
                r=16,
                lora_alpha=32,
                lora_dropout=0.05,
                bias="none",
                task_type="CAUSAL_LM",
            )

            trainer = SFTTrainer(
                model=self.model_id,
                args=training_args,
                train_dataset=self.train_dataset,
                eval_dataset=self.eval_dataset,
                peft_config=peft_config,
                packing=True,
            )

            self.assertTrue(isinstance(trainer.model, PeftModel))

            trainer.train()

            self.assertIsNotNone(trainer.state.log_history[-1]["train_loss"])
            self.assertIsNotNone(trainer.state.log_history[0]["eval_loss"])

            self.assertTrue("adapter_model.bin" in os.listdir(tmp_dir + "/checkpoint-2"))
            self.assertTrue("adapter_config.json" in os.listdir(tmp_dir + "/checkpoint-2"))
            self.assertTrue("pytorch_model.bin" not in os.listdir(tmp_dir + "/checkpoint-2"))

File Path: tests/testing_constants.py
Content:
# Copyright 2022 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

CI_HUB_USER = "__DUMMY_TRANSFORMERS_USER__"
CI_HUB_USER_FULL_NAME = "Dummy User"
CI_HUB_USER_TOKEN = "hf_94wBhPGp6KrrTH3KDchhKpRxZwd6dmHWLL"

CI_HUB_ENDPOINT = "https://hub-ci.huggingface.co"

File Path: tests/testing_utils.py
Content:
# Copyright 2023 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import unittest

import torch

from trl import is_peft_available


def require_peft(test_case):
    """
    Decorator marking a test that requires peft. Skips the test if peft is not available.
    """
    if not is_peft_available():
        test_case = unittest.skip("test requires peft")(test_case)
    return test_case


def require_bitsandbytes(test_case):
    """
    Decorator marking a test that requires bitsandbytes. Skips the test if bitsandbytes is not available.
    """
    try:
        import bitsandbytes  # noqa: F401
    except ImportError:
        test_case = unittest.skip("test requires bitsandbytes")(test_case)
    return test_case


def require_torch_multi_gpu(test_case):
    """
    Decorator marking a test that requires multiple GPUs. Skips the test if there aren't enough GPUs.
    """
    if torch.cuda.device_count() < 2:
        test_case = unittest.skip("test requires multiple GPUs")(test_case)
    return test_case

File Path: trl/__init__.py
Content:
# flake8: noqa

__version__ = "0.7.2.dev0"

from .core import set_seed
from .environment import TextEnvironment, TextHistory
from .extras import BestOfNSampler
from .import_utils import is_diffusers_available, is_peft_available
from .models import (
    AutoModelForCausalLMWithValueHead,
    AutoModelForSeq2SeqLMWithValueHead,
    PreTrainedModelWrapper,
    create_reference_model,
)
from .trainer import (
    DataCollatorForCompletionOnlyLM,
    DPOTrainer,
    PPOConfig,
    SR_PPOConfig,
    PPOTrainer,
    RewardConfig,
    RewardTrainer,
    SFTTrainer,
)


if is_diffusers_available():
    from .models import (
        DDPOPipelineOutput,
        DDPOSchedulerOutput,
        DDPOStableDiffusionPipeline,
        DefaultDDPOStableDiffusionPipeline,
    )
    from .trainer import DDPOConfig, DDPOTrainer

File Path: trl/core.py
Content:
# Copyright 2022 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import gc
import random
from contextlib import contextmanager

import numpy as np
import torch
import torch.nn.functional as F
from torch.nn.utils.rnn import pad_sequence
from transformers import top_k_top_p_filtering


try:
    from collections.abc import Mapping
except ImportError:
    from collections import Mapping


WANDB_PADDING = -1


def flatten_dict(nested, sep="/"):
    """Flatten dictionary and concatenate nested keys with separator."""

    def rec(nest, prefix, into):
        for k, v in nest.items():
            if sep in k:
                raise ValueError(f"separator '{sep}' not allowed to be in key '{k}'")
            if isinstance(v, Mapping):
                rec(v, prefix + k + sep, into)
            else:
                into[prefix + k] = v

    flat = {}
    rec(nested, "", flat)
    return flat


def convert_to_scalar(stats):
    """
    Converts the stats from a flattened dict to single scalar dicts
    """
    tensorboard_stats = {}
    for k, v in stats.items():
        # for tensorboard compatibility - arrays and tensors are ignored with tensorboard
        # therefore we convert single element tensors to scalars
        if (isinstance(v, torch.Tensor) or isinstance(v, np.ndarray)) and (
            len(v.shape) == 0 or (len(v.shape) == 1 and v.shape[0] == 1)
        ):
            v = v.item()
        tensorboard_stats[k] = v
    return tensorboard_stats


def stack_dicts(stats_dicts):
    """Stack the values of a dict."""
    results = dict()
    for k in stats_dicts[0]:
        stats_list = [torch.flatten(d[k]) for d in stats_dicts]
        results[k] = pad_sequence(stats_list, batch_first=True, padding_value=WANDB_PADDING)
    return results


def add_suffix(input_dict, suffix):
    """Add suffix to dict keys."""
    return dict((k + suffix, v) for k, v in input_dict.items())


def pad_to_size(tensor, size, dim=1, padding=50256):
    """Pad tensor to size."""
    t_size = tensor.size()[dim]
    if t_size == size:
        return tensor
    else:
        return torch.nn.functional.pad(tensor, (0, size - t_size), "constant", padding)


def logprobs_from_logits(logits, labels, gather=True):
    """
    See: https://github.com/pytorch/pytorch/issues/563#issuecomment-330103591
    """
    logp = F.log_softmax(logits, dim=2)

    if not gather:
        return logp
    logpy = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)
    return logpy


def whiten(values, shift_mean=True):
    """Whiten values."""
    mean, var = torch.mean(values), torch.var(values)
    whitened = (values - mean) * torch.rsqrt(var + 1e-8)
    if not shift_mean:
        whitened += mean
    return whitened


def masked_mean(values, mask, axis=None):
    """Compute mean of tensor with a masked values."""
    if axis is not None:
        return (values * mask).sum(axis=axis) / mask.sum(axis=axis)
    else:
        return (values * mask).sum() / mask.sum()


def masked_var(values, mask, unbiased=True):
    """Compute variance of tensor with masked values."""
    mean = masked_mean(values, mask)
    centered_values = values - mean
    variance = masked_mean(centered_values**2, mask)
    if unbiased:
        mask_sum = mask.sum()
        if mask_sum == 0:
            raise ValueError(
                "The sum of the mask is zero, which can happen when `mini_batch_size=1`;"
                "try increase the `mini_batch_size` or `gradient_accumulation_steps`"
            )
        # note that if mask_sum == 1, then there is a division by zero issue
        # to avoid it you just need to use a larger minibatch_size
        bessel_correction = mask_sum / (mask_sum - 1)
        variance = variance * bessel_correction
    return variance


def masked_whiten(values, mask, shift_mean=True):
    """Whiten values with masked values."""
    mean, var = masked_mean(values, mask), masked_var(values, mask)
    whitened = (values - mean) * torch.rsqrt(var + 1e-8)
    if not shift_mean:
        whitened += mean
    return whitened


def clip_by_value(x, tensor_min, tensor_max):
    """
    Tensor extenstion to torch.clamp
    https://github.com/pytorch/pytorch/issues/2793#issuecomment-428784713
    """
    clipped = torch.max(torch.min(x, tensor_max), tensor_min)
    return clipped


def entropy_from_logits(logits):
    """Calculate entropy from logits."""
    pd = torch.nn.functional.softmax(logits, dim=-1)
    entropy = torch.logsumexp(logits, axis=-1) - torch.sum(pd * logits, axis=-1)
    return entropy


def average_torch_dicts(list_of_dicts):
    """Average values of a list of dicts with torch tensors."""
    average_dict = dict()
    for key in list_of_dicts[0].keys():
        average_dict[key] = torch.mean(torch.stack([d[key] for d in list_of_dicts]), axis=0)
    return average_dict


def stats_to_np(stats_dict):
    """Cast all torch.tensors in dict to numpy arrays."""
    new_dict = dict()
    for k, v in stats_dict.items():
        if isinstance(v, torch.Tensor):
            new_dict[k] = v.detach().cpu()
            if new_dict[k].dtype == torch.bfloat16:
                new_dict[k] = new_dict[k].float()
            new_dict[k] = new_dict[k].numpy()
        else:
            new_dict[k] = v
        if np.isscalar(new_dict[k]):
            new_dict[k] = float(new_dict[k])
    return new_dict


def listify_batch(tensor):
    """Turns the first dimension of a tensor into a list."""
    return [tensor[i] for i in range(tensor.shape[0])]


def build_bert_batch_from_txt(text_list, tokenizer, device):
    """Create token id and attention mask tensors from text list for BERT classification."""

    # tokenize
    tensors = [tokenizer.encode(txt, return_tensors="pt").to(device) for txt in text_list]

    # find max length to pad to
    max_len = max([t.size()[1] for t in tensors])

    # get padded tensors and attention masks
    # (attention masks make bert ignore padding)
    padded_tensors = []
    attention_masks = []
    for tensor in tensors:
        attention_mask = torch.ones(tensor.size(), device=device)
        padded_tensors.append(pad_to_size(tensor, max_len, padding=0))
        attention_masks.append(pad_to_size(attention_mask, max_len, padding=0))

    # stack all tensors
    padded_tensors = torch.cat(padded_tensors)
    attention_masks = torch.cat(attention_masks)

    return padded_tensors, attention_masks


def respond_to_batch(model, queries, txt_len=20, top_k=0, top_p=1.0):
    """Sample text from language model."""
    input_ids = queries
    for i in range(txt_len):
        # Get Logits
        outputs = model(input_ids)
        next_token_logits = outputs[0][:, -1, :]
        next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)
        # Sample
        probs = F.softmax(next_token_logits, dim=-1)
        next_token = torch.multinomial(probs, num_samples=1).squeeze(1)
        input_ids = torch.cat([input_ids, next_token.unsqueeze(-1)], dim=-1)
    return input_ids[:, -txt_len:]


def set_seed(seed: int):
    """
    Helper function for reproducible behavior to set the seed in `random`, `numpy`, and `torch`.

    Args:
        seed (`int`): The seed to set.
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)


class LengthSampler:
    """
    Samples a length
    """

    def __init__(self, min_value, max_value):
        self.values = list(range(min_value, max_value))

    def __call__(self):
        return np.random.choice(self.values)


class PPODecorators(object):
    optimize_cuda_cache = False

    @classmethod
    @contextmanager
    def empty_cuda_cache(cls):
        yield
        if cls.optimize_cuda_cache and torch.cuda.is_available():
            gc.collect()
            torch.cuda.empty_cache()
            gc.collect()

File Path: trl/environment/__init__.py
Content:
# flake8: noqa

from .base_environment import TextEnvironment, TextHistory

File Path: trl/environment/base_environment.py
Content:
# Copyright 2022 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import re
import warnings

import torch
from accelerate.utils import extract_model_from_parallel
from transformers import StoppingCriteria, StoppingCriteriaList

from ..import_utils import is_rich_available


if is_rich_available():
    from rich import print
    from rich.text import Text


class StringStoppingCriteria(StoppingCriteria):
    """Custom `StoppingCriteria` which checks if all generations in the batch are completed."""

    def __init__(self, stop_strings, tokenizer):
        self.stop_strings = stop_strings
        self.tokenizer = tokenizer
        self.first_call = True

    def __call__(self, input_ids, scores, **kwargs):
        """Returns true if all generated sequences contain any of the stop strings."""
        if self.first_call:
            self.generated_tokens = [1 for _ in range(input_ids.shape[0])]
            self.start_length = input_ids.shape[-1] - 1
            self.first_call = False
        decoded_generations = self.tokenizer.batch_decode(input_ids[:, self.start_length :])
        done = []

        for i, decoded_generation in enumerate(decoded_generations):
            sequence_complete = any([stop_string in decoded_generation for stop_string in self.stop_strings])
            done.append(sequence_complete)
            if not sequence_complete:
                self.generated_tokens[i] += 1

        if all(done):
            self.first_call = True

        return all(done)


class TextHistory:
    """The TextHistory class keeps track of the history of an interaction between the language model and the environment."""

    def __init__(self, text, tokens, system=True):
        """
        Initialize TextHistory.

        args:
            text (`str`): The text of the first segment.
            tokens (`torch.LongTensor`): The tokens of the first segment.
            system (`bool`, *optional*): Whether the first segment is a system or user segment.
        """
        self.system_spans = []
        self.text_spans = []
        self.token_spans = []
        self.token_masks = torch.tensor([], dtype=torch.long).to(tokens.device)
        self.text = ""
        self.tokens = torch.tensor([], dtype=torch.long).to(tokens.device)
        self.completed = False
        self.truncated = False
        self.reward = 0.0

        self.prompt_color = "black on grey85"
        self.system_color = "black on cyan3"
        self.model_color = "black on deep_sky_blue1"
        self.reward_color = "black on plum1"

        self.append_segment(text, tokens, system=system)

    def append_segment(self, text, tokens, system=True):
        """
        Append a new segment to the history.

        args:
            text (`str`): The text of the new segment.
            tokens (`torch.LongTensor`): The tokens of the new segment.
            system (`bool`, *optional*): Whether the new segment is a system or user segment.
        """

        if len(text) == 0 or len(tokens) == 0:
            raise ValueError("Can't append empty text or token list to history.")

        original_text_length = len(self.text)

        self.text += text
        self.text_spans.append((original_text_length, len(self.text)))
        self.system_spans.append(system)

        original_token_length = len(self.tokens)

        self.tokens = torch.cat((self.tokens, tokens))
        if system:
            self.token_masks = torch.cat((self.token_masks, torch.zeros_like(tokens)))
        else:
            self.token_masks = torch.cat((self.token_masks, torch.ones_like(tokens)))
        self.token_spans.append((original_token_length, len(self.tokens)))

    def complete(self, truncated=False):
        """
        Mark the history as completed.
        """
        self.completed = True
        self.truncated = truncated

    @property
    def last_text_segment(self):
        """
        Get the last text segment.
        """
        start, end = self.text_spans[-1]
        return self.text[start:end]

    def split_query_response_tokens(self):
        """
        Split the tokens into query and response tokens.
        """
        split_index = self.token_spans[0][1]
        query = self.tokens[:split_index]
        response = self.tokens[split_index:]
        mask = self.token_masks[split_index:]

        return query, response, mask

    def show_text(self, show_legend=False):
        """
        Print the text history.
        """
        if not is_rich_available():
            warnings.warn("install rich to display text")
            return

        text = Text(self.text)
        text.stylize(self.prompt_color, self.text_spans[0][0], self.text_spans[1][0])
        for i, (start, end) in enumerate(self.text_spans[1:]):
            if self.system_spans[i + 1]:
                text.stylize(self.system_color, start, end)
            else:
                text.stylize(self.model_color, start, end)

        text.append(f"\n\nReward: {self.reward}", style=self.reward_color)
        print(text)

        if show_legend:
            self.show_colour_legend()

    def show_tokens(self, tokenizer, show_legend=False):
        """
        Print the history tokens.
        """
        if not is_rich_available():
            warnings.warn("install rich to display tokens")
            return

        text = Text()
        prompt_end = self.token_spans[0][1]
        for i, (token, mask) in enumerate(zip(self.tokens, self.token_masks)):
            if i < prompt_end:
                text.append(tokenizer.convert_ids_to_tokens(token.item()), style=self.prompt_color)
                text.append(" ")
            elif mask == 0:
                text.append(tokenizer.convert_ids_to_tokens(token.item()), style=self.system_color)
                text.append(" ")
            else:
                text.append(tokenizer.convert_ids_to_tokens(token.item()), style=self.model_color)
                text.append(" ")
        text.append(f"\n\nReward: {self.reward}", style=self.reward_color)
        print(text)
        if show_legend:
            self.show_colour_legend()

    def show_colour_legend(self):
        """
        Print the colour legend.
        """
        if not is_rich_available():
            warnings.warn("install rich to display colour legend")
            return
        text = Text("\n\n(Colour Legend: ")
        text.append("Prompt", style=self.prompt_color)
        text.append("|")
        text.append("System", style=self.system_color)
        text.append("|")
        text.append("Model", style=self.model_color)
        text.append("|")
        text.append("Reward", style=self.reward_color)
        text.append(")")
        print(text)


class TextEnvironment:
    """
    The TextEnvironment enables interaction of a LLM with an environment using tools.
    """

    def __init__(
        self,
        model=None,
        tokenizer=None,
        tools=None,
        reward_fn=None,
        prompt=None,
        max_turns=4,
        max_tool_reponse=100,
        max_length=None,
        generation_kwargs=None,
    ):
        """
        Initialize TextEnvironment.

        Args:
            model (`PreTrainedModelWrapper`): The model to use for generation.
            tokenizer (`transformers.PreTrainedTokenizer`): The tokenizer to use for generation.
            tools (list): A list of tools to use for interaction.
            reward_fn (function): A function that takes a string and returns a reward.
            prompt (str): The base prompt to use for generation. Is prepended to the tasks.
            max_turns (Optional[int]): The maximum number of turns to allow.
            max_tool_response (Optional[int]): The maximum number of characters to allow in a tool response.
            max_length (Optional[int]): The maximum number of tokens to allow in an episode.
            generation_kwargs (Optional[dict]): A dictionary of keyword arguments to pass to the model's generate method.
        """
        self.model = model
        self.tokenizer = tokenizer
        self.prompt = prompt
        if isinstance(tools, dict):
            self.tools = tools
        else:
            self.tools = dict([(tool.__class__.__name__, tool) for tool in tools])
        self.reward_fn = reward_fn
        self.max_length = max_length
        self.request_token = "<request>"
        self.call_token = "<call>"
        self.response_token = "<response>"
        self.submit_token = "<submit>"
        self.max_turns = max_turns
        self.max_tool_response = max_tool_reponse

        if generation_kwargs is None:
            self.generation_kwargs = dict()
        else:
            self.generation_kwargs = generation_kwargs

        self.is_encoder_decoder = hasattr(self.model, "is_encoder_decoder")
        self.current_device = extract_model_from_parallel(self.model).pretrained_model.device

    def run(self, queries, **rewards_kwargs):
        """
        Run the environment on a list of queries.

        Args:
            queries (list[str]): A list of queries to run the model in the environment on.
        """
        turns = 0

        queries = [self.prompt + task for task in queries]
        queries_tokens = [
            self.tokenizer(query, return_tensors="pt").input_ids[0].to(self.model.pretrained_model.device)
            for query in queries
        ]

        histories = [TextHistory(q, qt, system=True) for q, qt in zip(queries, queries_tokens)]

        while any([not history.completed for history in histories]) and turns < self.max_turns:
            histories = self.generate(histories)
            histories = self.tasks_end_check(histories)
            # TODO: make this parallel rather than for-loop
            for i in range(len(histories)):
                histories[i] = self.step(histories[i])
            histories = self.tasks_end_check(histories, model_turn=False)
            turns += 1
        self.compute_reward(histories, **rewards_kwargs)

        # convert a list of (q, r, m) tuples to lists of all qs, rs, and ms respectively
        queries, responses, masks = map(list, zip(*[history.split_query_response_tokens() for history in histories]))

        rewards = [history.reward for history in histories]
        return queries, responses, masks, rewards, histories

    def step(self, history):
        """
        Step the environment forward one turn.

        Args:
            history (`TextHistory`): The history to step forward.
        """
        truncated, ended = self.task_end_check(history)
        if ended:
            history.complete(truncated=truncated)
        if history.completed:
            return history

        tool, query = self.parse_tool_call(history.last_text_segment)
        if tool is None or query is None:
            response = f"Unknown tool call: {history.last_text_segment}"
        else:
            if tool not in self.tools:
                response = f"Unknown tool {tool}."
            try:
                response = self.tools[tool](query)
            except Exception as error:
                response = f"Tool error: {str(error)}"

        if len(response) > self.max_tool_response:
            response = response[: (self.max_tool_response - 3)] + "..."

        history.append_segment(
            response + self.response_token,
            self.tokenizer(response + self.response_token, return_tensors="pt")
            .input_ids[0]
            .to(self.model.pretrained_model.device),
            system=True,
        )

        return history

    def parse_tool_call(self, text):
        """
        Parse request string. Expected format: <request><tool_name>query<call>
        """
        result = re.search(f"(?<={self.request_token}).*?(?={self.call_token})", text, re.DOTALL)

        # if we can't find a <request>/<call> span we return none
        if result is None:
            return None, None
        else:
            extracted_text = result.group()

        result = re.search(r"<(.*?)>", extracted_text)

        # if we can't find a tool name we return none
        if result is None:
            return None, None
        else:
            tool = result.group(1)

        # split off the tool name
        query = ">".join(extracted_text.split(">")[1:])

        return tool, query

    def compute_reward(self, histories, **reward_kwargs):
        """
        Compute the reward for a list of histories.
        """
        rewards = self.reward_fn([history.last_text_segment for history in histories], **reward_kwargs)
        for history, reward in zip(histories, rewards):
            history.reward = reward
        return histories

    def generate(self, histories):
        """
        Generate responses for a list of histories.
        """
        active_histories = [i for i, history in enumerate(histories) if not history.completed]

        query_tensors = [histories[i].tokens for i in active_histories]
        response_tensors = self._generate_batched(query_tensors)
        response_texts = self.tokenizer.batch_decode(response_tensors)

        for i, response_text, response_tensor in zip(active_histories, response_texts, response_tensors):
            histories[i].append_segment(response_text, response_tensor, system=False)

        return histories

    def tasks_end_check(self, histories, model_turn=True):
        """
        Check if the current generation sequences have finished.
        """
        for history in histories:
            if not history.completed:
                truncated, ended = self.task_end_check(history, model_turn=model_turn)
                if ended:
                    history.complete(truncated=truncated)
        return histories

    def task_end_check(self, history, model_turn=True):
        """
        Check if the current generation sequence has finished.
        """
        truncated = False
        ended = False
        if history.completed:
            return truncated, ended
        if self.max_length is not None and len(self.tokenizer(history.text).input_ids[0]) > self.max_length:
            truncated = True
            ended = True
        elif self.tokenizer.eos_token in history.text:
            ended = True
        elif model_turn and not (
            (self.request_token in history.last_text_segment and self.call_token in history.last_text_segment)
            or self.submit_token in history.last_text_segment
        ):
            ended = True
        elif self.submit_token in history.last_text_segment:
            ended = True
        return truncated, ended

    def _generate_batched(
        self,
        query_tensors,
        batch_size: int = 16,
        pad_to_multiple_of: int = None,
    ):
        """
        Generate responses for a list of query tensors.

        args:
            query_tensors (list[torch.Tensor]): A list of query tensors to generate responses for.
            batch_size (int): The batch size to use for generation.
            pad_to_multiple_of (int): The padding length to use for generation.
        """
        outputs = []
        padding_side_default = self.tokenizer.padding_side
        if not self.is_encoder_decoder:
            self.tokenizer.padding_side = "left"

        # in case we have fewer examples than bs
        batch_size = min(len(query_tensors), batch_size)

        for i in range(0, len(query_tensors), batch_size):
            # prevent overflow if query tensors are not even multiple of bs
            end_index = min(len(query_tensors), i + batch_size)

            batch = query_tensors[i:end_index]
            batch_mask = [torch.ones_like(element) for element in batch]
            inputs = {"input_ids": batch, "attention_mask": batch_mask}

            padded_inputs = self.tokenizer.pad(
                inputs,
                padding=True,
                max_length=None,
                pad_to_multiple_of=pad_to_multiple_of,
                return_tensors="pt",
            ).to(self.current_device)

            stopping_criteria = StringStoppingCriteria([self.call_token, self.submit_token], self.tokenizer)

            self.generation_kwargs["stopping_criteria"] = StoppingCriteriaList([stopping_criteria])

            generations = extract_model_from_parallel(self.model).generate(**padded_inputs, **self.generation_kwargs)

            for generation, mask, generated_tokens in zip(
                generations, padded_inputs["attention_mask"], stopping_criteria.generated_tokens
            ):
                if not self.is_encoder_decoder:
                    output = generation[(1 - mask).sum() :]  # remove padding
                else:
                    output = generation

                if not self.is_encoder_decoder:
                    output = output[(mask).sum() :]  # remove prompt

                # remove chunk generated after stopping criteria in batch mode
                outputs.append(output[:generated_tokens])
        self.tokenizer.padding_side = padding_side_default
        return outputs

File Path: trl/extras/__init__.py
Content:
# flake8: noqa

# Copyright 2022 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from .best_of_n_sampler import BestOfNSampler

File Path: trl/extras/best_of_n_sampler.py
Content:
from typing import Any, Callable, List, Optional, Union

import torch
from transformers import GenerationConfig, PreTrainedTokenizer, PreTrainedTokenizerFast

from ..core import set_seed
from ..models import SUPPORTED_ARCHITECTURES, PreTrainedModelWrapper


class BestOfNSampler(object):
    def __init__(
        self,
        model: PreTrainedModelWrapper,
        tokenizer: Union[PreTrainedTokenizer, PreTrainedTokenizerFast],
        queries_to_scores: Callable[[List[str]], List[float]],
        length_sampler: Any,
        sample_size: int = 4,
        seed: Optional[int] = None,
        n_candidates: int = 1,
        generation_config: Optional[GenerationConfig] = None,
    ) -> None:
        r"""
        Initialize the sampler for best-of-n generation

        Args:
            model (`PreTrainedModelWrapper`):
                The pretrained model to use for generation
            tokenizer (`PreTrainedTokenizer` or `PreTrainedTokenizerFast`):
                Tokenizer associated with the pretrained model
            queries_to_scores (`Callable[[List[str]], List[float]]`):
                Callable that takes a list of generated texts and returns the associated reward scores
            length_sampler (`Any`):
                Sampler used to sample the length of the generated text
            sample_size (`int`):
                Number of samples to generate for each query
            seed (`int`, *optional*):
                Random seed used to control generation
            n_candidates (`int`):
                Number of candidates to return for each query
            generation_config (`GenerationConfig`, *optional*):
                Generation config passed to the underlying model's `generate` method.
                See `GenerationConfig` (https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationConfig) for more details
        """
        if seed is not None:
            set_seed(seed)

        if not isinstance(tokenizer, (PreTrainedTokenizer, PreTrainedTokenizerFast)):
            raise ValueError(
                f"tokenizer must be a PreTrainedTokenizer or PreTrainedTokenizerFast, got {type(tokenizer)}"
            )
        if not isinstance(model, (SUPPORTED_ARCHITECTURES)):
            raise ValueError(
                f"model must be a PreTrainedModelWrapper, got {type(model)} - supported architectures are: {SUPPORTED_ARCHITECTURES}"
            )

        self.model = model
        self.tokenizer = tokenizer

        self.queries_to_scores = queries_to_scores
        self.length_sampler = length_sampler
        self.gen_config = generation_config
        self.sample_size = sample_size
        self.n_candidates = n_candidates

    def generate(
        self,
        tokenized_query: Union[List[int], torch.Tensor, List[torch.Tensor], List[List[int]]],
        skip_special_tokens: bool = True,
        device: Optional[Union[str, torch.device]] = None,
        **generation_kwargs,
    ) -> List[List[str]]:
        r"""
        Generate the best of n samples for input queries

        Args:
            tokenized_query (`List[int]` or `torch.Tensor` or `List[torch.Tensor]` or `List[int]`):
                represents either a single tokenized query (a single tensor or a list of integers) or a batch of tokenized queries (a list of tensors or a list of lists of integers)
            skip_special_tokens (`bool`):
                Whether to remove the special tokens from the output
            device (`str` or `torch.device`, *optional*):
                The device on which the model will be loaded
            **generation_kwargs (`dict`, *optional*):
                Additional keyword arguments passed along to the underlying model's `generate` method.
                This is used to override generation config

        Returns:
            List[List[str]]: A list of lists of generated texts
        """
        queries = None

        if isinstance(tokenized_query, torch.Tensor) and tokenized_query.ndim == 1:
            queries = tokenized_query.unsqueeze(0)
        elif isinstance(tokenized_query, List):
            element_type = type(tokenized_query[0])
            if element_type == int:
                queries = torch.tensor(tokenized_query).unsqueeze(0)
            elif element_type == torch.Tensor:
                queries = [tensor.reshape((1, -1)) for tensor in tokenized_query]
            else:
                queries = [torch.tensor(query).reshape((1, -1)) for query in tokenized_query]

        result = []

        for query in queries:
            queries = query.repeat((self.sample_size, 1))
            output = self.model.generate(
                queries.to(device),
                max_new_tokens=self.length_sampler(),
                generation_config=self.gen_config,
                **generation_kwargs,
            ).squeeze()
            output = self.tokenizer.batch_decode(output, skip_special_tokens=skip_special_tokens)
            scores = torch.tensor(self.queries_to_scores(output))
            output = [output[i] for i in scores.topk(self.n_candidates).indices]
            result.append(output)

        return result

File Path: trl/import_utils.py
Content:
# Copyright 2022 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import importlib
import sys


if sys.version_info[0] < 3.8:
    _is_python_greater_3_8 = False
else:
    _is_python_greater_3_8 = True


def is_peft_available():
    return importlib.util.find_spec("peft") is not None


def is_torch_greater_2_0():
    if _is_python_greater_3_8:
        from importlib.metadata import version

        torch_version = version("torch")
    else:
        import pkg_resources

        torch_version = pkg_resources.get_distribution("torch").version
    return torch_version >= "2.0"


def is_diffusers_available():
    return importlib.util.find_spec("diffusers") is not None


def is_bitsandbytes_available():
    return importlib.util.find_spec("bitsandbytes") is not None


def is_torchvision_available():
    return importlib.util.find_spec("torchvision") is not None


def is_rich_available():
    return importlib.util.find_spec("rich") is not None

File Path: trl/models/__init__.py
Content:
# flake8: noqa

# Copyright 2022 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from .modeling_base import PreTrainedModelWrapper, create_reference_model
from .modeling_value_head import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead


SUPPORTED_ARCHITECTURES = (
    AutoModelForCausalLMWithValueHead,
    AutoModelForSeq2SeqLMWithValueHead,
)

from ..import_utils import is_diffusers_available


if is_diffusers_available():
    from .modeling_sd_base import (
        DDPOPipelineOutput,
        DDPOSchedulerOutput,
        DDPOStableDiffusionPipeline,
        DefaultDDPOStableDiffusionPipeline,
    )

File Path: trl/models/modeling_base.py
Content:
# Copyright 2022 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import json
import logging
import os
from copy import deepcopy

import torch
import torch.nn as nn
from accelerate import Accelerator
from huggingface_hub import hf_hub_download
from huggingface_hub.utils import EntryNotFoundError, HFValidationError, LocalEntryNotFoundError
from transformers import PreTrainedModel

from ..import_utils import is_peft_available


if is_peft_available():
    from peft import (
        LoraConfig,
        PeftConfig,
        PeftModel,
        PeftModelForCausalLM,
        PeftModelForSeq2SeqLM,
        PromptLearningConfig,
        get_peft_model,
        prepare_model_for_kbit_training,
    )
    from peft.peft_model import set_peft_model_state_dict

LAYER_PATTERNS = [
    "transformer.h.{layer}",
    "model.decoder.layers.{layer}",
    "gpt_neox.layers.{layer}",
    "model.layers.{layer}",
]


class PreTrainedModelWrapper(nn.Module):
    r"""
    A wrapper class around a (`transformers.PreTrainedModel`) to be compatible with the
    (`~transformers.PreTrained`) class in order to keep some attributes and methods of the
    (`~transformers.PreTrainedModel`) class.

    Attributes:
        pretrained_model: (`transformers.PreTrainedModel`)
            The model to be wrapped.
        parent_class: (`transformers.PreTrainedModel`)
            The parent class of the model to be wrapped.
        supported_args: (`list`)
            The list of arguments that are supported by the wrapper class.
    """
    transformers_parent_class = None
    supported_args = None
    supported_modules = ("v_head",)
    supported_rm_modules = ("score",)
    supported_pretrained_model_architectures = (
        (PreTrainedModel)
        if not is_peft_available()
        else (PreTrainedModel, PeftModelForCausalLM, PeftModelForSeq2SeqLM)
    )

    def __init__(self, pretrained_model=None, **kwargs):
        super().__init__()
        self.pretrained_model = pretrained_model

        self.config = pretrained_model.config
        self.prepare_inputs_for_generation = pretrained_model.prepare_inputs_for_generation
        self.is_loaded_in_8bit = getattr(pretrained_model, "is_loaded_in_8bit", False)
        self.is_loaded_in_4bit = getattr(pretrained_model, "is_loaded_in_4bit", False)
        self.is_sequential_parallel = False

        if hasattr(pretrained_model, "gradient_checkpointing_disable"):
            self.gradient_checkpointing_disable = pretrained_model.gradient_checkpointing_disable

        if hasattr(pretrained_model, "gradient_checkpointing_enable"):
            self.gradient_checkpointing_enable = pretrained_model.gradient_checkpointing_enable

    @classmethod
    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):
        r"""
        Instantiates a new model from a pretrained model from `transformers`. The
        pretrained model is loaded using the `from_pretrained` method of the
        `transformers.PreTrainedModel` class. The arguments that are specific to the
        `transformers.PreTrainedModel` class are passed along this method and filtered
        out from the `kwargs` argument.


        Args:
            pretrained_model_name_or_path (`str` or `transformers.PreTrainedModel`):
                The path to the pretrained model or its name.
            *model_args (`list`, *optional*)):
                Additional positional arguments passed along to the underlying model's
                `from_pretrained` method.
            **kwargs (`dict`, *optional*):
                Additional keyword arguments passed along to the underlying model's
                `from_pretrained` method. We also pre-process the kwargs to extract
                the arguments that are specific to the `transformers.PreTrainedModel`
                class and the arguments that are specific to trl models. The kwargs
                also support `prepare_model_for_kbit_training` arguments from
                `peft` library.
        """
        if kwargs is not None:
            peft_config = kwargs.pop("peft_config", None)
            reward_adapter = kwargs.pop("reward_adapter", None)
            is_trainable = kwargs.pop("is_trainable", False)
            trl_model_args, pretrained_kwargs, peft_quantization_kwargs = cls._split_kwargs(kwargs)
            token = pretrained_kwargs.get("token", None)
        else:
            peft_config = None
            is_trainable = False
            trl_model_args = {}
            pretrained_kwargs = {}
            peft_quantization_kwargs = {}
            token = None

        if reward_adapter is not None and not isinstance(reward_adapter, str):
            raise ValueError(
                "The `reward_adapter` argument should be a string representing the name of local path or the Hub id to the Reward Modeling adapter."
            )

        is_peft_model = False

        current_device = cls._get_current_device()
        if isinstance(pretrained_model_name_or_path, str):
            is_loaded_in_8bit = pretrained_kwargs["load_in_8bit"] if "load_in_8bit" in pretrained_kwargs else False
            is_loaded_in_4bit = pretrained_kwargs["load_in_4bit"] if "load_in_4bit" in pretrained_kwargs else False
        else:
            is_loaded_in_8bit = getattr(pretrained_model_name_or_path, "is_loaded_in_8bit", False)
            is_loaded_in_4bit = getattr(pretrained_model_name_or_path, "is_loaded_in_4bit", False)

        if (is_loaded_in_8bit or is_loaded_in_4bit) and "device_map" not in pretrained_kwargs:
            # warn users
            logging.warning(
                "The `device_map` argument is not provided. We will override the device_map argument."
                " to set the entire"
                " model on the current device. If you want to set the model on multiple devices, please provide"
                " a custom `device_map` argument."
            )
            pretrained_kwargs["device_map"] = {"": current_device}

        if is_peft_available() and peft_config is not None and not isinstance(peft_config, PeftConfig):
            raise ValueError("The `peft_config` argument should be an instance of `peft.PeftConfig` class.")

        # First, load the pre-trained model using the parent-class
        # either `AutoModelForCausalLM` or `AutoModelForSeq2SeqLM`
        if isinstance(pretrained_model_name_or_path, str):
            if is_peft_available():
                try:
                    # If there is a trained peft adapter in the hub, load its config.
                    remote_adapter_config = hf_hub_download(
                        pretrained_model_name_or_path,
                        "adapter_config.json",
                        token=token,
                    )
                except (EntryNotFoundError, LocalEntryNotFoundError, HFValidationError):
                    remote_adapter_config = None
            else:
                remote_adapter_config = None

            local_adapter_present = os.path.exists(os.path.join(pretrained_model_name_or_path, "adapter_config.json"))

            if (local_adapter_present or remote_adapter_config is not None) and is_peft_available():
                if peft_config is not None:
                    logging.warning(
                        "`peft_config` argument ignored since a peft config file was found in "
                        f"{pretrained_model_name_or_path}"
                    )

                # Load the trained peft adapter config
                if local_adapter_present:
                    trained_adapter_config = PeftConfig.from_pretrained(pretrained_model_name_or_path)
                else:
                    remote_adapter_dir = os.path.dirname(remote_adapter_config)
                    trained_adapter_config = PeftConfig.from_pretrained(remote_adapter_dir)

                # Load the pretrained base model
                pretrained_model = cls.transformers_parent_class.from_pretrained(
                    trained_adapter_config.base_model_name_or_path, *model_args, **pretrained_kwargs
                )

                # Wrap the pretrained model with the trained peft adapter
                pretrained_model = PeftModel.from_pretrained(
                    pretrained_model, pretrained_model_name_or_path, is_trainable=is_trainable
                )
                logging.info("Trained peft adapter loaded")
            else:
                pretrained_model = cls.transformers_parent_class.from_pretrained(
                    pretrained_model_name_or_path, *model_args, **pretrained_kwargs
                )

                if peft_config is not None:
                    # Initialize a new peft adapter with the given config
                    if is_loaded_in_8bit or is_loaded_in_4bit:
                        pretrained_model = prepare_model_for_kbit_training(
                            pretrained_model,
                            **peft_quantization_kwargs,
                        )
                    pretrained_model = get_peft_model(pretrained_model, peft_config)
                    logging.info("peft adapter initialised")

        elif isinstance(pretrained_model_name_or_path, cls.supported_pretrained_model_architectures):
            pretrained_model = pretrained_model_name_or_path

            if peft_config is not None and isinstance(pretrained_model, PreTrainedModel):
                # Initialize a new peft adapter with the given config
                if is_loaded_in_8bit or is_loaded_in_4bit:
                    pretrained_model = prepare_model_for_kbit_training(
                        pretrained_model,
                        **peft_quantization_kwargs,
                    )
                pretrained_model = get_peft_model(pretrained_model, peft_config)
                logging.info("peft adapter initialised")
        else:
            raise ValueError(
                "pretrained_model_name_or_path should be a string or a PreTrainedModel, "
                f"but is {type(pretrained_model_name_or_path)}"
            )

        if is_peft_available():
            if isinstance(pretrained_model, PeftModel):
                is_peft_model = True
                # for backward compatibility
                if hasattr(pretrained_model, "active_peft_config") and isinstance(
                    pretrained_model.active_peft_config, PromptLearningConfig
                ):
                    raise ValueError("PromptLearningConfig is not supported for PPO training.")
        # Then, create the full model by instantiating the wrapper class
        model = cls(pretrained_model, **trl_model_args)

        # if resume_training, load the state_dict again - this is ok since the
        # state_dict is removed from the model after loading it.
        is_resuming_training = True
        if isinstance(pretrained_model_name_or_path, str):
            filename = os.path.join(pretrained_model_name_or_path, "pytorch_model.bin")
            sharded_index_filename = os.path.join(pretrained_model_name_or_path, "pytorch_model.bin.index.json")
            is_shared = False

            if not os.path.exists(filename):
                try:
                    filename = hf_hub_download(
                        pretrained_model_name_or_path,
                        "pytorch_model.bin",
                        token=token,
                    )
                # sharded
                except (EntryNotFoundError, LocalEntryNotFoundError, HFValidationError):
                    if os.path.exists(sharded_index_filename):
                        index_file_name = sharded_index_filename
                    else:
                        try:
                            index_file_name = hf_hub_download(
                                pretrained_model_name_or_path,
                                "pytorch_model.bin.index.json",
                                token=token,
                            )
                        except (EntryNotFoundError, LocalEntryNotFoundError, HFValidationError):
                            # not continue training, do not have v_head weight
                            is_resuming_training = False
                            logging.warning(
                                f"A {type(pretrained_model)} model is loaded from '{pretrained_model_name_or_path}', "
                                f"and no v_head weight is found. This IS expected if you are not resuming PPO training."
                            )
                    # load json
                    if is_resuming_training:
                        with open(index_file_name, "r") as f:
                            index = json.load(f)
                        # check filename with `v_head` or any known extra module:
                        files_to_download = set()
                        for k, v in index["weight_map"].items():
                            if any([module in k for module in cls.supported_modules]):
                                files_to_download.add(v)
                        is_shared = True

            if is_resuming_training:
                if is_shared:
                    # download each file and add it to the state_dict
                    state_dict = {}
                    for shard_file in files_to_download:
                        filename = hf_hub_download(
                            pretrained_model_name_or_path,
                            shard_file,
                            token=token,
                        )
                        state_dict.update(torch.load(filename, map_location="cpu"))
                else:
                    state_dict = torch.load(filename, map_location="cpu")

        else:
            state_dict = pretrained_model_name_or_path.state_dict()

        model.is_peft_model = is_peft_model

        model.current_device = current_device

        if is_resuming_training:
            model.post_init(state_dict=state_dict)

        if not is_peft_model and reward_adapter is not None:
            raise ValueError("reward_adapter can only be used with a PeftModel. ")
        elif is_peft_model and reward_adapter is not None:
            model.add_and_load_reward_modeling_adapter(reward_adapter, token=token)
            model.supports_rm_adapter = True
        else:
            model.supports_rm_adapter = False

        return model

    @classmethod
    def _get_current_device(cls):
        r"""
        Get the current device. For GPU, we return the local process index using the `Accelerator`
        object to handle corner cases when running scripts in distributed environments.

        Returns:
            current_device (`Union[int, str]`):
                The current device.
        """
        dummy_accelerator = Accelerator()
        return dummy_accelerator.local_process_index if torch.cuda.is_available() else "cpu"

    @classmethod
    def _split_kwargs(cls, kwargs):
        """
        Separate the kwargs from the arguments that we support inside
        `supported_args` and the ones that we don't.
        """
        check_peft_kwargs = False

        if is_peft_available():
            from peft import prepare_model_for_kbit_training

            check_peft_kwargs = True

        supported_kwargs = {}
        unsupported_kwargs = {}
        peft_kwargs = {}

        for key, value in kwargs.items():
            if key in cls.supported_args:
                supported_kwargs[key] = value
            else:
                unsupported_kwargs[key] = value

            if check_peft_kwargs:
                if key in prepare_model_for_kbit_training.__code__.co_varnames:
                    peft_kwargs[key] = value
                    if key in unsupported_kwargs:
                        unsupported_kwargs.pop(key)

        return supported_kwargs, unsupported_kwargs, peft_kwargs

    def push_to_hub(self, *args, **kwargs):
        r"""
        Push the pretrained model to the hub. This method is a wrapper around
        `transformers.PreTrainedModel.push_to_hub`. Please refer to the documentation
        of `transformers.PreTrainedModel.push_to_hub` for more information.

        Args:
            *args (`list`, *optional*):
                Positional arguments passed along to the underlying model's
                `push_to_hub` method.
            **kwargs (`dict`, *optional*):
                Keyword arguments passed along to the underlying model's
                `push_to_hub` method.
        """
        raise NotImplementedError

    def save_pretrained(self, *args, **kwargs):
        r"""
        Save the pretrained model to a directory. This method is a wrapper around
        `transformers.PreTrainedModel.save_pretrained`. Please refer to the documentation
        of `transformers.PreTrainedModel.save_pretrained` for more information.

        Args:
            *args (`list`, *optional*):
                Positional arguments passed along to the underlying model's
                `save_pretrained` method.
            **kwargs (`dict`, *optional*):
                Keyword arguments passed along to the underlying model's
                `save_pretrained` method.
        """
        state_dict = kwargs.get("state_dict")
        if state_dict is None:
            state_dict = self.state_dict()
            kwargs["state_dict"] = state_dict

        # if it is a peft model only save the `v_head` state_dict and
        # pop the `state_dict` from the kwargs to avoid slient bugs with `peft`
        if self.is_peft_model:
            save_path = args[0]
            save_path = os.path.join(save_path, "pytorch_model.bin")
            torch.save(state_dict, save_path)
            _ = kwargs.pop("state_dict", None)

        return self.pretrained_model.save_pretrained(*args, **kwargs)

    def state_dict(self, *args, **kwargs):
        r"""
        Return the state_dict of the pretrained model.
        """
        raise NotImplementedError

    def post_init(self, *args, **kwargs):
        r"""
        Post initialization method. This method is called after the model is
        instantiated and loaded from a checkpoint. It can be used to perform
        additional operations such as loading the state_dict.
        """
        raise NotImplementedError

    def add_and_load_reward_modeling_adapter(self, adapter_model_id, adapter_name="reward_model_adapter", token=None):
        r"""
        Add and load a reward modeling adapter. This method can only be used if the
        model is a `PeftModel` and if you have initialized the model with the `reward_modeling_adapter_id`
        argument, pointing to the id of the reward modeling adapter. The latest needs also to contain the
        score head in order to produce the reward.
        """
        filename = os.path.join(adapter_model_id, "adapter_model.bin")
        if not os.path.exists(filename):
            try:
                local_filename = hf_hub_download(
                    adapter_model_id,
                    "adapter_model.bin",
                    token=token,
                )
            except:  # noqa
                raise ValueError(
                    "Could not find adapter model in the Hub, make sure you have the correct adapter model id."
                )
        else:
            local_filename = filename

        adapter_state_dict = torch.load(local_filename, map_location="cpu")
        rm_adapter_peft_config = LoraConfig.from_pretrained(adapter_model_id)

        for score_name_candidate in self.supported_rm_modules:
            if any([score_name_candidate in name for name in adapter_state_dict.keys()]):
                score_name = score_name_candidate
                # we have found the correct head name and can break
                break

        score_dict = {}
        copy_adapter_state_dict = adapter_state_dict.copy()

        for name, _ in copy_adapter_state_dict.items():
            if score_name in name:
                key_name = ".".join(name.split(".")[-1:])
                score_dict[key_name] = adapter_state_dict.pop(name).to(self._get_current_device())

        self.pretrained_model.add_adapter(adapter_name, rm_adapter_peft_config)
        self.rm_adapter_name = adapter_name

        num_labels, hidden_dim = score_dict["weight"].shape
        has_bias = any(["bias" in name for name in adapter_state_dict.keys()])

        self.score = nn.Linear(hidden_dim, num_labels, bias=has_bias).to(
            device=self._get_current_device(),
            dtype=self.pretrained_model.dtype,
        )
        self.score.load_state_dict(score_dict)

        # load the adapter to the model
        set_peft_model_state_dict(self.pretrained_model, adapter_state_dict, adapter_name=adapter_name)

    def compute_reward_score(self, input_ids, attention_mask=None, ppo_adapter_name="default", **kwargs):
        r"""
        Computes the reward score for a given input. The method has first to enable the adapter
        and then compute the reward score. After that the model disables the reward modeling
        adapter and enables the default ppo adapter again.
        """
        if not self.supports_rm_adapter:
            raise ValueError("This model does not support reward modeling adapter.")

        # enable rm adapter
        self.pretrained_model.set_adapter(self.rm_adapter_name)
        self.pretrained_model.eval()

        base_model_output = self.pretrained_model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True,
            return_dict=True,
            **kwargs,
        )

        last_hidden_states = base_model_output.hidden_states[-1]
        scores = self.score(last_hidden_states)

        self.pretrained_model.set_adapter(ppo_adapter_name)
        self.pretrained_model.train()

        return scores


def create_reference_model(
    model: PreTrainedModelWrapper, num_shared_layers: int = None, pattern: str = None
) -> PreTrainedModelWrapper:
    """
    Creates a static reference copy of a model. Note that model will be in `.eval()` mode.

    Args:
        model (`PreTrainedModelWrapper`): The model to be copied.
        num_shared_layers (`int`, *optional*): The number of initial layers that are shared between both models and kept frozen.
        pattern (`str`, *optional*): The shared layers are selected with a string pattern
            (e.g. "transformer.h.{layer}" for GPT2) and if a custom pattern is necessary it can be passed here.

    Returns
        `PreTrainedModelWrapper`
    """

    parameter_names = [n for n, _ in model.named_parameters()]
    ref_model = deepcopy(model)

    # if no layers are shared, return copy of model
    if num_shared_layers is None:
        for param_name in parameter_names:
            param = ref_model.get_parameter(param_name)
            param.requires_grad = False
        return ref_model.eval()

    # identify layer name pattern
    if pattern is not None:
        pattern = pattern.format(layer=num_shared_layers)
    else:
        for pattern_candidate in LAYER_PATTERNS:
            pattern_candidate = pattern_candidate.format(layer=num_shared_layers)
            if any([pattern_candidate in name for name in parameter_names]):
                pattern = pattern_candidate
                break

    if pattern is None:
        raise ValueError("Layer pattern could not be matched.")

    # divide parameters in shared and unshared parameter lists
    shared_param_list = []
    unshared_param_list = []

    shared_parameter = True
    for name, param in model.named_parameters():
        if pattern in name:
            shared_parameter = False
        if shared_parameter:
            shared_param_list.append(name)
        else:
            unshared_param_list.append(name)

    # create reference of the original parameter if they are shared
    for param_name in shared_param_list:
        param = model.get_parameter(param_name)
        param.requires_grad = False

        ref_param = ref_model.get_parameter(param_name)  # noqa
        ref_param = param  # noqa

    # for all other parameters just make sure they don't use gradients
    for param_name in unshared_param_list:
        param = ref_model.get_parameter(param_name)
        param.requires_grad = False

    if pattern is not None and len(unshared_param_list) == 0:
        logging.warning("Pattern passed or found, but no layers matched in the model. Check for a typo.")

    return ref_model.eval()

File Path: trl/models/modeling_sd_base.py
Content:
# Copyright 2023 DDPO-pytorch authors (Kevin Black), The HuggingFace Team, metric-space. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import contextlib
import os
import warnings
from dataclasses import dataclass
from typing import Any, Callable, Dict, List, Optional, Union

import numpy as np
import torch
from diffusers import DDIMScheduler, StableDiffusionPipeline, UNet2DConditionModel
from diffusers.loaders import AttnProcsLayers
from diffusers.models.attention_processor import LoRAAttnProcessor
from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion import rescale_noise_cfg
from diffusers.utils.torch_utils import randn_tensor


@dataclass
class DDPOPipelineOutput(object):
    """
    Output class for the diffusers pipeline to be finetuned with the DDPO trainer

    Args:
        images (`torch.Tensor`):
            The generated images.
        latents (`List[torch.Tensor]`):
            The latents used to generate the images.
        log_probs (`List[torch.Tensor]`):
            The log probabilities of the latents.

    """

    images: torch.Tensor
    latents: torch.Tensor
    log_probs: torch.Tensor


@dataclass
class DDPOSchedulerOutput(object):
    """
    Output class for the diffusers scheduler to be finetuned with the DDPO trainer

    Args:
        latents (`torch.Tensor`):
            Predicted sample at the previous timestep. Shape: `(batch_size, num_channels, height, width)`
        log_probs (`torch.Tensor`):
            Log probability of the above mentioned sample. Shape: `(batch_size)`
    """

    latents: torch.Tensor
    log_probs: torch.Tensor


class DDPOStableDiffusionPipeline(object):
    """
    Main class for the diffusers pipeline to be finetuned with the DDPO trainer
    """

    def __call__(self, *args, **kwargs) -> DDPOPipelineOutput:
        raise NotImplementedError

    def scheduler_step(self, *args, **kwargs) -> DDPOSchedulerOutput:
        raise NotImplementedError

    @property
    def unet(self):
        """
        Returns the 2d U-Net model used for diffusion.
        """
        raise NotImplementedError

    @property
    def vae(self):
        """
        Returns the Variational Autoencoder model used from mapping images to and from the latent space
        """
        raise NotImplementedError

    @property
    def tokenizer(self):
        """
        Returns the tokenizer used for tokenizing text inputs
        """
        raise NotImplementedError

    @property
    def scheduler(self):
        """
        Returns the scheduler associated with the pipeline used for the diffusion process
        """
        raise NotImplementedError

    @property
    def text_encoder(self):
        """
        Returns the text encoder used for encoding text inputs
        """
        raise NotImplementedError

    @property
    def autocast(self):
        """
        Returns the autocast context manager
        """
        raise NotImplementedError

    def set_progress_bar_config(self, *args, **kwargs):
        """
        Sets the progress bar config for the pipeline
        """
        raise NotImplementedError

    def save_pretrained(self, *args, **kwargs):
        """
        Saves all of the model weights
        """
        raise NotImplementedError

    def get_trainable_layers(self, *args, **kwargs):
        """
        Returns the trainable parameters of the pipeline
        """
        raise NotImplementedError

    def save_checkpoint(self, *args, **kwargs):
        """
        Light wrapper around accelerate's register_save_state_pre_hook which is run before saving state
        """
        raise NotImplementedError

    def load_checkpoint(self, *args, **kwargs):
        """
        Light wrapper around accelerate's register_lad_state_pre_hook which is run before loading state
        """
        raise NotImplementedError


def _left_broadcast(input_tensor, shape):
    """
    As opposed to the default direction of broadcasting (right to left), this function broadcasts
    from left to right
        Args:
            input_tensor (`torch.FloatTensor`): is the tensor to broadcast
            shape (`Tuple[int]`): is the shape to broadcast to
    """
    input_ndim = input_tensor.ndim
    if input_ndim > len(shape):
        raise ValueError(
            "The number of dimensions of the tensor to broadcast cannot be greater than the length of the shape to broadcast to"
        )
    return input_tensor.reshape(input_tensor.shape + (1,) * (len(shape) - input_ndim)).broadcast_to(shape)


def _get_variance(self, timestep, prev_timestep):
    alpha_prod_t = torch.gather(self.alphas_cumprod, 0, timestep.cpu()).to(timestep.device)
    alpha_prod_t_prev = torch.where(
        prev_timestep.cpu() >= 0,
        self.alphas_cumprod.gather(0, prev_timestep.cpu()),
        self.final_alpha_cumprod,
    ).to(timestep.device)
    beta_prod_t = 1 - alpha_prod_t
    beta_prod_t_prev = 1 - alpha_prod_t_prev

    variance = (beta_prod_t_prev / beta_prod_t) * (1 - alpha_prod_t / alpha_prod_t_prev)

    return variance


def scheduler_step(
    self,
    model_output: torch.FloatTensor,
    timestep: int,
    sample: torch.FloatTensor,
    eta: float = 0.0,
    use_clipped_model_output: bool = False,
    generator=None,
    prev_sample: Optional[torch.FloatTensor] = None,
) -> DDPOSchedulerOutput:
    """

    Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
    process from the learned model outputs (most often the predicted noise).

    Args:
        model_output (`torch.FloatTensor`): direct output from learned diffusion model.
        timestep (`int`): current discrete timestep in the diffusion chain.
        sample (`torch.FloatTensor`):
            current instance of sample being created by diffusion process.
        eta (`float`): weight of noise for added noise in diffusion step.
        use_clipped_model_output (`bool`): if `True`, compute "corrected" `model_output` from the clipped
            predicted original sample. Necessary because predicted original sample is clipped to [-1, 1] when
            `self.config.clip_sample` is `True`. If no clipping has happened, "corrected" `model_output` would
            coincide with the one provided as input and `use_clipped_model_output` will have not effect.
        generator: random number generator.
        variance_noise (`torch.FloatTensor`): instead of generating noise for the variance using `generator`, we
            can directly provide the noise for the variance itself. This is useful for methods such as
            CycleDiffusion. (https://arxiv.org/abs/2210.05559)

    Returns:
        `DDPOSchedulerOutput`: the predicted sample at the previous timestep and the log probability of the sample
    """

    if self.num_inference_steps is None:
        raise ValueError(
            "Number of inference steps is 'None', you need to run 'set_timesteps' after creating the scheduler"
        )

    # See formulas (12) and (16) of DDIM paper https://arxiv.org/pdf/2010.02502.pdf
    # Ideally, read DDIM paper in-detail understanding

    # Notation (<variable name> -> <name in paper>
    # - pred_noise_t -> e_theta(x_t, t)
    # - pred_original_sample -> f_theta(x_t, t) or x_0
    # - std_dev_t -> sigma_t
    # - eta -> η
    # - pred_sample_direction -> "direction pointing to x_t"
    # - pred_prev_sample -> "x_t-1"

    # 1. get previous step value (=t-1)
    prev_timestep = timestep - self.config.num_train_timesteps // self.num_inference_steps
    # to prevent OOB on gather
    prev_timestep = torch.clamp(prev_timestep, 0, self.config.num_train_timesteps - 1)

    # 2. compute alphas, betas
    alpha_prod_t = self.alphas_cumprod.gather(0, timestep.cpu())
    alpha_prod_t_prev = torch.where(
        prev_timestep.cpu() >= 0,
        self.alphas_cumprod.gather(0, prev_timestep.cpu()),
        self.final_alpha_cumprod,
    )
    alpha_prod_t = _left_broadcast(alpha_prod_t, sample.shape).to(sample.device)
    alpha_prod_t_prev = _left_broadcast(alpha_prod_t_prev, sample.shape).to(sample.device)

    beta_prod_t = 1 - alpha_prod_t

    # 3. compute predicted original sample from predicted noise also called
    # "predicted x_0" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf
    if self.config.prediction_type == "epsilon":
        pred_original_sample = (sample - beta_prod_t ** (0.5) * model_output) / alpha_prod_t ** (0.5)
        pred_epsilon = model_output
    elif self.config.prediction_type == "sample":
        pred_original_sample = model_output
        pred_epsilon = (sample - alpha_prod_t ** (0.5) * pred_original_sample) / beta_prod_t ** (0.5)
    elif self.config.prediction_type == "v_prediction":
        pred_original_sample = (alpha_prod_t**0.5) * sample - (beta_prod_t**0.5) * model_output
        pred_epsilon = (alpha_prod_t**0.5) * model_output + (beta_prod_t**0.5) * sample
    else:
        raise ValueError(
            f"prediction_type given as {self.config.prediction_type} must be one of `epsilon`, `sample`, or"
            " `v_prediction`"
        )

    # 4. Clip or threshold "predicted x_0"
    if self.config.thresholding:
        pred_original_sample = self._threshold_sample(pred_original_sample)
    elif self.config.clip_sample:
        pred_original_sample = pred_original_sample.clamp(
            -self.config.clip_sample_range, self.config.clip_sample_range
        )

    # 5. compute variance: "sigma_t(η)" -> see formula (16)
    # σ_t = sqrt((1 − α_t−1)/(1 − α_t)) * sqrt(1 − α_t/α_t−1)
    variance = _get_variance(self, timestep, prev_timestep)
    std_dev_t = eta * variance ** (0.5)
    std_dev_t = _left_broadcast(std_dev_t, sample.shape).to(sample.device)

    if use_clipped_model_output:
        # the pred_epsilon is always re-derived from the clipped x_0 in Glide
        pred_epsilon = (sample - alpha_prod_t ** (0.5) * pred_original_sample) / beta_prod_t ** (0.5)

    # 6. compute "direction pointing to x_t" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf
    pred_sample_direction = (1 - alpha_prod_t_prev - std_dev_t**2) ** (0.5) * pred_epsilon

    # 7. compute x_t without "random noise" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf
    prev_sample_mean = alpha_prod_t_prev ** (0.5) * pred_original_sample + pred_sample_direction

    if prev_sample is not None and generator is not None:
        raise ValueError(
            "Cannot pass both generator and prev_sample. Please make sure that either `generator` or"
            " `prev_sample` stays `None`."
        )

    if prev_sample is None:
        variance_noise = randn_tensor(
            model_output.shape,
            generator=generator,
            device=model_output.device,
            dtype=model_output.dtype,
        )
        prev_sample = prev_sample_mean + std_dev_t * variance_noise

    # log prob of prev_sample given prev_sample_mean and std_dev_t
    log_prob = (
        -((prev_sample.detach() - prev_sample_mean) ** 2) / (2 * (std_dev_t**2))
        - torch.log(std_dev_t)
        - torch.log(torch.sqrt(2 * torch.as_tensor(np.pi)))
    )
    # mean along all but batch dimension
    log_prob = log_prob.mean(dim=tuple(range(1, log_prob.ndim)))

    return DDPOSchedulerOutput(prev_sample.type(sample.dtype), log_prob)


# 1. The output type for call is different as the logprobs are now returned
# 2. An extra method called `scheduler_step` is added which is used to constraint the scheduler output
@torch.no_grad()
def pipeline_step(
    self,
    prompt: Optional[Union[str, List[str]]] = None,
    height: Optional[int] = None,
    width: Optional[int] = None,
    num_inference_steps: int = 50,
    guidance_scale: float = 7.5,
    negative_prompt: Optional[Union[str, List[str]]] = None,
    num_images_per_prompt: Optional[int] = 1,
    eta: float = 0.0,
    generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,
    latents: Optional[torch.FloatTensor] = None,
    prompt_embeds: Optional[torch.FloatTensor] = None,
    negative_prompt_embeds: Optional[torch.FloatTensor] = None,
    output_type: Optional[str] = "pil",
    return_dict: bool = True,
    callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,
    callback_steps: int = 1,
    cross_attention_kwargs: Optional[Dict[str, Any]] = None,
    guidance_rescale: float = 0.0,
):
    r"""
    Function invoked when calling the pipeline for generation.  Args: prompt (`str` or `List[str]`, *optional*): The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.  instead.  height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor): The height in pixels of the generated image.
        width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):
            The width in pixels of the generated image.
        num_inference_steps (`int`, *optional*, defaults to 50):
            The number of denoising steps. More denoising steps usually lead to a higher quality image at the
            expense of slower inference.
        guidance_scale (`float`, *optional*, defaults to 7.5):
            Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
            `guidance_scale` is defined as `w` of equation 2. of [Imagen
            Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >
            1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,
            usually at the expense of lower image quality.
        negative_prompt (`str` or `List[str]`, *optional*):
            The prompt or prompts not to guide the image generation. If not defined, one has to pass
            `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
            less than `1`).
        num_images_per_prompt (`int`, *optional*, defaults to 1):
            The number of images to generate per prompt.
        eta (`float`, *optional*, defaults to 0.0):
            Corresponds to parameter eta (η) in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to
            [`schedulers.DDIMScheduler`], will be ignored for others.
        generator (`torch.Generator` or `List[torch.Generator]`, *optional*):
            One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
            to make generation deterministic.
        latents (`torch.FloatTensor`, *optional*):
            Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image
            generation. Can be used to tweak the same generation with different prompts. If not provided, a latents
            tensor will ge generated by sampling using the supplied random `generator`.
        prompt_embeds (`torch.FloatTensor`, *optional*):
            Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not
            provided, text embeddings will be generated from `prompt` input argument.
        negative_prompt_embeds (`torch.FloatTensor`, *optional*):
            Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
            weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input
            argument.
        output_type (`str`, *optional*, defaults to `"pil"`):
            The output format of the generate image. Choose between
            [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.
        return_dict (`bool`, *optional*, defaults to `True`):
            Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] instead of a
            plain tuple.
        callback (`Callable`, *optional*):
            A function that will be called every `callback_steps` steps during inference. The function will be
            called with the following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.
        callback_steps (`int`, *optional*, defaults to 1):
            The frequency at which the `callback` function will be called. If not specified, the callback will be
            called at every step.
        cross_attention_kwargs (`dict`, *optional*):
            A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under
            `self.processor` in
            [diffusers.cross_attention](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/cross_attention.py).
        guidance_rescale (`float`, *optional*, defaults to 0.7):
            Guidance rescale factor proposed by [Common Diffusion Noise Schedules and Sample Steps are
            Flawed](https://arxiv.org/pdf/2305.08891.pdf) `guidance_scale` is defined as `φ` in equation 16. of
            [Common Diffusion Noise Schedules and Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf).
            Guidance rescale factor should fix overexposure when using zero terminal SNR.

    Examples:

    Returns:
        `DDPOPipelineOutput`: The generated image, the predicted latents used to generate the image and the associated log probabilities
    """
    # 0. Default height and width to unet
    height = height or self.unet.config.sample_size * self.vae_scale_factor
    width = width or self.unet.config.sample_size * self.vae_scale_factor

    # 1. Check inputs. Raise error if not correct
    self.check_inputs(
        prompt,
        height,
        width,
        callback_steps,
        negative_prompt,
        prompt_embeds,
        negative_prompt_embeds,
    )

    # 2. Define call parameters
    if prompt is not None and isinstance(prompt, str):
        batch_size = 1
    elif prompt is not None and isinstance(prompt, list):
        batch_size = len(prompt)
    else:
        batch_size = prompt_embeds.shape[0]

    device = self._execution_device
    # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)
    # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`
    # corresponds to doing no classifier free guidance.
    do_classifier_free_guidance = guidance_scale > 1.0

    # 3. Encode input prompt
    text_encoder_lora_scale = cross_attention_kwargs.get("scale", None) if cross_attention_kwargs is not None else None
    prompt_embeds = self._encode_prompt(
        prompt,
        device,
        num_images_per_prompt,
        do_classifier_free_guidance,
        negative_prompt,
        prompt_embeds=prompt_embeds,
        negative_prompt_embeds=negative_prompt_embeds,
        lora_scale=text_encoder_lora_scale,
    )

    # 4. Prepare timesteps
    self.scheduler.set_timesteps(num_inference_steps, device=device)
    timesteps = self.scheduler.timesteps

    # 5. Prepare latent variables
    num_channels_latents = self.unet.config.in_channels
    latents = self.prepare_latents(
        batch_size * num_images_per_prompt,
        num_channels_latents,
        height,
        width,
        prompt_embeds.dtype,
        device,
        generator,
        latents,
    )

    # 6. Denoising loop
    num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order
    all_latents = [latents]
    all_log_probs = []
    with self.progress_bar(total=num_inference_steps) as progress_bar:
        for i, t in enumerate(timesteps):
            # expand the latents if we are doing classifier free guidance
            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents
            latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)

            # predict the noise residual
            noise_pred = self.unet(
                latent_model_input,
                t,
                encoder_hidden_states=prompt_embeds,
                cross_attention_kwargs=cross_attention_kwargs,
                return_dict=False,
            )[0]

            # perform guidance
            if do_classifier_free_guidance:
                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)

            if do_classifier_free_guidance and guidance_rescale > 0.0:
                # Based on 3.4. in https://arxiv.org/pdf/2305.08891.pdf
                noise_pred = rescale_noise_cfg(noise_pred, noise_pred_text, guidance_rescale=guidance_rescale)

            # compute the previous noisy sample x_t -> x_t-1
            scheduler_output = scheduler_step(self.scheduler, noise_pred, t, latents, eta)
            latents = scheduler_output.latents
            log_prob = scheduler_output.log_probs

            all_latents.append(latents)
            all_log_probs.append(log_prob)

            # call the callback, if provided
            if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):
                progress_bar.update()
                if callback is not None and i % callback_steps == 0:
                    callback(i, t, latents)

    if not output_type == "latent":
        image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False)[0]
        image, has_nsfw_concept = self.run_safety_checker(image, device, prompt_embeds.dtype)
    else:
        image = latents
        has_nsfw_concept = None

    if has_nsfw_concept is None:
        do_denormalize = [True] * image.shape[0]
    else:
        do_denormalize = [not has_nsfw for has_nsfw in has_nsfw_concept]

    image = self.image_processor.postprocess(image, output_type=output_type, do_denormalize=do_denormalize)

    # Offload last model to CPU
    if hasattr(self, "final_offload_hook") and self.final_offload_hook is not None:
        self.final_offload_hook.offload()

    return DDPOPipelineOutput(image, all_latents, all_log_probs)


class DefaultDDPOStableDiffusionPipeline(DDPOStableDiffusionPipeline):
    def __init__(self, pretrained_model_name: str, *, pretrained_model_revision: str = "main", use_lora: bool = True):
        self.sd_pipeline = StableDiffusionPipeline.from_pretrained(
            pretrained_model_name, revision=pretrained_model_revision
        )

        self.use_lora = use_lora
        self.pretrained_model = pretrained_model_name
        self.pretrained_revision = pretrained_model_revision

        try:
            self.sd_pipeline.unet.load_attn_procs(pretrained_model_name, revision=pretrained_model_revision)
            self.use_lora = True
        except OSError:
            if use_lora:
                warnings.warn(
                    "If you are aware that the pretrained model has no lora weights to it, ignore this message. "
                    "Otherwise please check the if `pytorch_lora_weights.safetensors` exists in the model folder."
                )

        self.sd_pipeline.scheduler = DDIMScheduler.from_config(self.sd_pipeline.scheduler.config)
        self.sd_pipeline.safety_checker = None

        # memory optimization
        self.sd_pipeline.vae.requires_grad_(False)
        self.sd_pipeline.text_encoder.requires_grad_(False)
        self.sd_pipeline.unet.requires_grad_(not self.use_lora)

    def __call__(self, *args, **kwargs) -> DDPOPipelineOutput:
        return pipeline_step(self.sd_pipeline, *args, **kwargs)

    def scheduler_step(self, *args, **kwargs) -> DDPOSchedulerOutput:
        return scheduler_step(self.sd_pipeline.scheduler, *args, **kwargs)

    @property
    def unet(self):
        return self.sd_pipeline.unet

    @property
    def vae(self):
        return self.sd_pipeline.vae

    @property
    def tokenizer(self):
        return self.sd_pipeline.tokenizer

    @property
    def scheduler(self):
        return self.sd_pipeline.scheduler

    @property
    def text_encoder(self):
        return self.sd_pipeline.text_encoder

    @property
    def autocast(self):
        return contextlib.nullcontext if self.use_lora else None

    def save_pretrained(self, output_dir):
        if self.use_lora:
            self.sd_pipeline.unet.save_attn_procs(output_dir)
        self.sd_pipeline.save_pretrained(output_dir)

    def set_progress_bar_config(self, *args, **kwargs):
        self.sd_pipeline.set_progress_bar_config(*args, **kwargs)

    def get_trainable_layers(self):
        if self.use_lora:
            # Set correct lora layers
            lora_attn_procs = {}
            for name in self.sd_pipeline.unet.attn_processors.keys():
                cross_attention_dim = (
                    None if name.endswith("attn1.processor") else self.sd_pipeline.unet.config.cross_attention_dim
                )
                if name.startswith("mid_block"):
                    hidden_size = self.sd_pipeline.unet.config.block_out_channels[-1]
                elif name.startswith("up_blocks"):
                    block_id = int(name[len("up_blocks.")])
                    hidden_size = list(reversed(self.sd_pipeline.unet.config.block_out_channels))[block_id]
                elif name.startswith("down_blocks"):
                    block_id = int(name[len("down_blocks.")])
                    hidden_size = self.sd_pipeline.unet.config.block_out_channels[block_id]

                lora_attn_procs[name] = LoRAAttnProcessor(
                    hidden_size=hidden_size, cross_attention_dim=cross_attention_dim
                )
            self.sd_pipeline.unet.set_attn_processor(lora_attn_procs)
            return AttnProcsLayers(self.sd_pipeline.unet.attn_processors)
        else:
            return self.sd_pipeline.unet

    def save_checkpoint(self, models, weights, output_dir):
        if len(models) != 1:
            raise ValueError("Given how the trainable params were set, this should be of length 1")
        if self.use_lora and isinstance(models[0], AttnProcsLayers):
            self.sd_pipeline.unet.save_attn_procs(output_dir)
        elif not self.use_lora and isinstance(models[0], UNet2DConditionModel):
            models[0].save_pretrained(os.path.join(output_dir, "unet"))
        else:
            raise ValueError(f"Unknown model type {type(models[0])}")

    def load_checkpoint(self, models, input_dir):
        if len(models) != 1:
            raise ValueError("Given how the trainable params were set, this should be of length 1")
        if self.use_lora and isinstance(models[0], AttnProcsLayers):
            tmp_unet = UNet2DConditionModel.from_pretrained(
                self.pretrained_model,
                revision=self.pretrained_revision,
                subfolder="unet",
            )
            tmp_unet.load_attn_procs(input_dir)
            models[0].load_state_dict(AttnProcsLayers(tmp_unet.attn_processors).state_dict())
            del tmp_unet
        elif not self.use_lora and isinstance(models[0], UNet2DConditionModel):
            load_model = UNet2DConditionModel.from_pretrained(input_dir, subfolder="unet")
            models[0].register_to_config(**load_model.config)
            models[0].load_state_dict(load_model.state_dict())
            del load_model
        else:
            raise ValueError(f"Unknown model type {type(models[0])}")

File Path: trl/models/modeling_value_head.py
Content:
# Copyright 2022 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM

from .modeling_base import PreTrainedModelWrapper


class ValueHead(nn.Module):
    r"""
    The ValueHead class implements a head for GPT2 that returns a scalar for each output token.
    """

    def __init__(self, config, **kwargs):
        super().__init__()
        if not hasattr(config, "summary_dropout_prob"):
            summary_dropout_prob = kwargs.pop("summary_dropout_prob", 0.1)
        else:
            summary_dropout_prob = config.summary_dropout_prob

        self.dropout = nn.Dropout(summary_dropout_prob) if summary_dropout_prob else nn.Identity()

        # some models such as OPT have a projection layer before the word embeddings - e.g. OPT-350m
        if hasattr(config, "word_embed_proj_dim"):
            hidden_size = config.word_embed_proj_dim
        else:
            hidden_size = config.hidden_size

        self.summary = nn.Linear(hidden_size, 1)

        self.flatten = nn.Flatten()

    def forward(self, hidden_states):
        output = self.dropout(hidden_states)

        # For now force upcast in fp32 if needed. Let's keep the
        # output in fp32 for numerical stability.
        if output.dtype != self.summary.weight.dtype:
            output = output.to(self.summary.weight.dtype)

        output = self.summary(output)
        return output


class AutoModelForCausalLMWithValueHead(PreTrainedModelWrapper):
    r"""
    An autoregressive model with a value head in addition to the language model head.
    This class inherits from `~trl.PreTrainedModelWrapper` and wraps a
    `transformers.PreTrainedModel` class. The wrapper class supports classic functions
    such as `from_pretrained`, `push_to_hub` and `generate`. To call a method of the wrapped
    model, simply manipulate the `pretrained_model` attribute of this class.

    Class attributes:
        - **transformers_parent_class** (`transformers.PreTrainedModel`) -- The parent class of the wrapped model. This
            should be set to `transformers.AutoModelForCausalLM` for this class.
        - **lm_head_namings** (`tuple`) -- A tuple of strings that are used to identify the language model head of the
            wrapped model. This is set to `("lm_head", "embed_out")` for this class but can be changed for other models
            in the future
        - **supported_args** (`tuple`) -- A tuple of strings that are used to identify the arguments that are supported
            by the `ValueHead` class. Currently, the supported args are:
            - **summary_dropout_prob** (`float`, `optional`, defaults to `None`) -- The dropout probability for the
                `ValueHead` class.
            - **v_head_initializer_range** (`float`, `optional`, defaults to `0.2`) -- The initializer range for the
                `ValueHead` if a specific initialization strategy is selected.
            - **v_head_init_strategy** (`str`, `optional`, defaults to `None`) -- The initialization strategy for the
                `ValueHead`. Currently, the supported strategies are:
                - **`None`** -- Initializes the weights of the `ValueHead` with a random distribution. This is the default
                    strategy.
                - **"normal"** -- Initializes the weights of the `ValueHead` with a normal distribution.

    """
    transformers_parent_class = AutoModelForCausalLM
    lm_head_namings = ["lm_head", "embed_out"]
    supported_args = (
        "summary_dropout_prob",
        "v_head_initializer_range",
        "v_head_init_strategy",
    )

    def __init__(self, pretrained_model, **kwargs):
        r"""
        Initializes the model.

        Args:
            pretrained_model (`transformers.PreTrainedModel`):
                The model to wrap. It should be a causal language model such as GPT2.
                or any model mapped inside the `AutoModelForCausalLM` class.
            kwargs (`dict`, `optional`):
                Additional keyword arguments, that are passed to the `ValueHead` class.
        """
        super().__init__(pretrained_model)
        v_head_kwargs, _, _ = self._split_kwargs(kwargs)

        if not any(hasattr(self.pretrained_model, attribute) for attribute in self.lm_head_namings):
            raise ValueError("The model does not have a language model head, please use a model that has one.")

        self.v_head = ValueHead(self.pretrained_model.config, **v_head_kwargs)

        self._init_weights(**v_head_kwargs)

    def _init_weights(self, **kwargs):
        r"""
        Initializes the weights of the value head. The default initialization strategy is random.
        Users can pass a different initialization strategy by passing the `v_head_init_strategy` argument
        when calling `.from_pretrained`. Supported strategies are:
        - `normal`: initializes the weights with a normal distribution.

        Args:
            **kwargs (`dict`, `optional`):
                Additional keyword arguments, that are passed to the `ValueHead` class. These arguments
                can contain the `v_head_init_strategy` argument as well as the `v_head_initializer_range`
                argument.
        """
        initializer_range = kwargs.pop("v_head_initializer_range", 0.2)
        # random init by default
        init_strategy = kwargs.pop("v_head_init_strategy", None)
        if init_strategy is None:
            # do nothing
            pass
        elif init_strategy == "normal":
            self.v_head.summary.weight.data.normal_(mean=0.0, std=initializer_range)
            self.v_head.summary.bias.data.zero_()

    def forward(
        self,
        input_ids=None,
        past_key_values=None,
        attention_mask=None,
        **kwargs,
    ):
        r"""
        Applies a forward pass to the wrapped model and returns the logits of the value head.

        Args:
            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
                Indices of input sequence tokens in the vocabulary.
            past_key_values (`tuple(tuple(torch.FloatTensor))`, `optional`):
                Contains pre-computed hidden-states (key and values in the attention blocks) as computed by the model
                (see `past_key_values` input) to speed up sequential decoding.
            attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, `optional`):
                Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:
                - 1 for tokens that are **not masked**,
                - 0 for tokens that are **masked**.
            kwargs (`dict`, `optional`):
                Additional keyword arguments, that are passed to the wrapped model.
        """
        kwargs["output_hidden_states"] = True  # this had already been set in the LORA / PEFT examples
        kwargs["past_key_values"] = past_key_values

        if self.is_peft_model and self.pretrained_model.active_peft_config.peft_type == "PREFIX_TUNING":
            kwargs.pop("past_key_values")

        base_model_output = self.pretrained_model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            **kwargs,
        )

        last_hidden_state = base_model_output.hidden_states[-1]
        lm_logits = base_model_output.logits
        loss = base_model_output.loss

        if last_hidden_state.device != self.v_head.summary.weight.device:
            last_hidden_state = last_hidden_state.to(self.v_head.summary.weight.device)

        value = self.v_head(last_hidden_state).squeeze(-1)

        # force upcast in fp32 if logits are in half-precision
        if lm_logits.dtype != torch.float32:
            lm_logits = lm_logits.float()

        return (lm_logits, loss, value)

    def generate(self, *args, **kwargs):
        r"""
        A simple wrapper around the `generate` method of the wrapped model.
        Please refer to the [`generate`](https://huggingface.co/docs/transformers/internal/generation_utils)
        method of the wrapped model for more information about the supported arguments.

        Args:
            *args (`list`, *optional*):
                Positional arguments passed to the `generate` method of the wrapped model.
            **kwargs (`dict`, *optional*):
                Keyword arguments passed to the `generate` method of the wrapped model.
        """
        return self.pretrained_model.generate(*args, **kwargs)

    def state_dict(self, *args, **kwargs):
        r"""
        Returns the state dictionary of the model. We add the state dictionary of the value head
        to the state dictionary of the wrapped model by prepending the key with `v_head.`.
        """
        if not self.is_peft_model:
            pretrained_model_state_dict = self.pretrained_model.state_dict(*args, **kwargs)
        else:
            # if it is a peft model, only save the v_head
            pretrained_model_state_dict = {}

        v_head_state_dict = self.v_head.state_dict(*args, **kwargs)
        for k, v in v_head_state_dict.items():
            pretrained_model_state_dict[f"v_head.{k}"] = v
        return pretrained_model_state_dict

    def push_to_hub(self, *args, **kwargs):
        setattr(self.pretrained_model, "v_head", self.v_head)

        return self.pretrained_model.push_to_hub(*args, **kwargs)

    def post_init(self, state_dict):
        r"""
        We add the state dictionary of the value head to the state dictionary of the wrapped model
        by prepending the key with `v_head.`. This function removes the `v_head.` prefix from the
        keys of the value head state dictionary.
        """
        for k in list(state_dict.keys()):
            if "v_head." in k:
                state_dict[k.replace("v_head.", "")] = state_dict.pop(k)
        self.v_head.load_state_dict(state_dict, strict=False)
        del state_dict

        if hasattr(self.pretrained_model, "hf_device_map"):
            if (
                "cpu" in self.pretrained_model.hf_device_map.values()
                or "disk" in self.pretrained_model.hf_device_map.values()
            ):
                raise ValueError(
                    "The model is offloaded on CPU or disk - CPU & disk offloading is not supported for ValueHead models."
                )

            first_device = list(set(self.pretrained_model.hf_device_map.values()))[0]

            self.v_head = self.v_head.to(first_device)

            def set_device_hook(module, input, outputs):
                new_output = ()
                for output in outputs:
                    if isinstance(output, torch.Tensor):
                        new_output += (output.to(first_device),)
                    else:
                        new_output += (output,)
                return new_output

            self.register_forward_hook(set_device_hook)

            self.is_sequential_parallel = True


class AutoModelForSeq2SeqLMWithValueHead(PreTrainedModelWrapper):
    r"""
    A seq2seq model with a value head in addition to the language model head.
    This class inherits from `~trl.PreTrainedModelWrapper` and wraps a
    `transformers.PreTrainedModel` class. The wrapper class supports classic functions
    such as `from_pretrained` and `push_to_hub` and also provides some additional
    functionalities such as `generate`.

    Args:
        pretrained_model (`transformers.PreTrainedModel`):
            The model to wrap. It should be a causal language model such as GPT2.
            or any model mapped inside the `AutoModelForSeq2SeqLM` class.
        kwargs:
            Additional keyword arguments passed along to the `ValueHead` class.
    """
    transformers_parent_class = AutoModelForSeq2SeqLM
    lm_head_namings = ["lm_head", "embed_out", "output_projection"]
    supported_args = (
        "summary_dropout_prob",
        "v_head_initializer_range",
        "v_head_init_strategy",
    )

    def __init__(self, pretrained_model, **kwargs):
        super().__init__(pretrained_model)
        v_head_kwargs, _, _ = self._split_kwargs(kwargs)
        self.is_encoder_decoder = True

        if not self._has_lm_head():
            raise ValueError("The model does not have a language model head, please use a model that has one.")

        self.v_head = ValueHead(self.pretrained_model.config, **v_head_kwargs)

        self._init_weights(**v_head_kwargs)

    def _has_lm_head(self):
        # check module names of all modules inside `pretrained_model` to find the language model head
        for name, module in self.pretrained_model.named_modules():
            if any(attribute in name for attribute in self.lm_head_namings):
                return True
        return False

    def post_init(self, state_dict):
        r"""
        We add the state dictionary of the value head to the state dictionary of the wrapped model
        by prepending the key with `v_head.`. This function removes the `v_head.` prefix from the
        keys of the value head state dictionary.
        """
        for k in list(state_dict.keys()):
            if "v_head." in k:
                state_dict[k.replace("v_head.", "")] = state_dict.pop(k)
        self.v_head.load_state_dict(state_dict, strict=False)
        del state_dict

        if hasattr(self.pretrained_model, "hf_device_map"):
            if (
                "cpu" in self.pretrained_model.hf_device_map.values()
                or "disk" in self.pretrained_model.hf_device_map.values()
            ):
                raise ValueError(
                    "The model is offloaded on CPU or disk - CPU & disk offloading is not supported for ValueHead models."
                )

            # get the lm_head device
            for name, module in self.pretrained_model.named_modules():
                if any(attribute in name for attribute in self.lm_head_namings):
                    lm_head_device = module.weight.device
                    break

            # put v_head on the same device as the lm_head to avoid issues
            self.v_head = self.v_head.to(lm_head_device)

            def set_device_hook(module, input, outputs):
                r"""
                A hook that sets the device of the output of the model to the device of the first
                parameter of the model.

                Args:
                    module (`nn.Module`):
                        The module to which the hook is attached.
                    input (`tuple`):
                        The input to the module.
                    outputs (`tuple`):
                        The output of the module.
                """
                new_output = ()
                for output in outputs:
                    if isinstance(output, torch.Tensor):
                        new_output += (output.to(lm_head_device),)
                    else:
                        new_output += (output,)
                return new_output

            self.register_forward_hook(set_device_hook)
            self.is_sequential_parallel = True

    def state_dict(self, *args, **kwargs):
        r"""
        Returns the state dictionary of the model. We add the state dictionary of the value head
        to the state dictionary of the wrapped model by prepending the key with `v_head.`.
        """
        if not self.is_peft_model:
            pretrained_model_state_dict = self.pretrained_model.state_dict(*args, **kwargs)
        else:
            # if it is a peft model, only save the v_head
            pretrained_model_state_dict = {}

        v_head_state_dict = self.v_head.state_dict(*args, **kwargs)
        for k, v in v_head_state_dict.items():
            pretrained_model_state_dict[f"v_head.{k}"] = v
        return pretrained_model_state_dict

    def push_to_hub(self, *args, **kwargs):
        setattr(self.pretrained_model, "v_head", self.v_head)

        return self.pretrained_model.push_to_hub(*args, **kwargs)

    def _init_weights(self, **kwargs):
        r"""
        We initialize the weights of the value head.
        """
        initializer_range = kwargs.pop("v_head_initializer_range", 0.2)
        # random init by default
        init_strategy = kwargs.pop("v_head_init_strategy", None)
        if init_strategy is None:
            # do nothing
            pass
        elif init_strategy == "normal":
            self.v_head.summary.weight.data.normal_(mean=0.0, std=initializer_range)
            self.v_head.summary.bias.data.zero_()

    def forward(
        self,
        input_ids=None,
        past_key_values=None,
        attention_mask=None,
        **kwargs,
    ):
        kwargs["past_key_values"] = past_key_values
        if self.is_peft_model and self.pretrained_model.active_peft_config.peft_type == "PREFIX_TUNING":
            kwargs.pop("past_key_values")

        base_model_output = self.pretrained_model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True,  # We force the model to output hidden states
            **kwargs,
        )

        last_hidden_state = base_model_output.decoder_hidden_states[-1]
        lm_logits = base_model_output.logits
        loss = base_model_output.loss

        value = self.v_head(last_hidden_state).squeeze(-1)

        # force upcast in fp32 if logits are in half-precision
        if lm_logits.dtype != torch.float32:
            lm_logits = lm_logits.float()

        return (lm_logits, loss, value)

    def generate(self, *args, **kwargs):
        r"""
        We call `generate` on the wrapped model.
        """
        return self.pretrained_model.generate(*args, **kwargs)

File Path: trl/trainer/__init__.py
Content:
# flake8: noqa

# Copyright 2022 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# There is a circular import in the PPOTrainer if we let isort sort these
# isort: off
from .utils import (
    AdaptiveKLController,
    FixedKLController,
    ConstantLengthDataset,
    DataCollatorForCompletionOnlyLM,
    RunningMoments,
    disable_dropout_in_model,
)

# isort: on

from ..import_utils import is_diffusers_available
from .base import BaseTrainer
from .ddpo_config import DDPOConfig


if is_diffusers_available():
    from .ddpo_trainer import DDPOTrainer

from .dpo_trainer import DPOTrainer
from .ppo_config import PPOConfig
from .sr_ppo_config import SR_PPOConfig
from .ppo_trainer import PPOTrainer
from .reward_trainer import RewardTrainer, compute_accuracy
from .sft_trainer import SFTTrainer
from .training_configs import RewardConfig

File Path: trl/trainer/base.py
Content:
# Copyright 2022 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from huggingface_hub import PyTorchModelHubMixin


class BaseTrainer(PyTorchModelHubMixin):
    r"""
    Base class for all trainers - this base class implements the basic functions that we
    need for a trainer.

    The trainer needs to have the following functions:
        - step: takes in a batch of data and performs a step of training
        - loss: takes in a batch of data and returns the loss
        - compute_rewards: takes in a batch of data and returns the rewards
        - _build_models_and_tokenizer: builds the models and tokenizer
        - _build_dataset: builds the dataset
    Each user is expected to implement their own trainer class that inherits from this base
    if they want to use a new training algorithm.
    """

    def __init__(self, config):
        self.config = config

    def step(self, *args):
        raise NotImplementedError("Not implemented")

    def loss(self, *args):
        raise NotImplementedError("Not implemented")

    def compute_rewards(self, *args):
        raise NotImplementedError("Not implemented")

    def _save_pretrained(self, save_directory):
        raise NotImplementedError("Not implemented")

File Path: trl/trainer/beaver_config.py
Content:
# Copyright 2022 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import os
import sys
import warnings
from dataclasses import dataclass, field
from typing import Literal, Optional

import numpy as np
import tyro

from trl.trainer.utils import exact_div

from ..core import flatten_dict


@dataclass
class PPOConfig:
    """
    Configuration class for PPOTrainer
    """

    exp_name: str = os.path.basename(sys.argv[0])[: -len(".py")]
    """the name of this experiment (by default is the file name without the extension name)"""
    task_name: Optional[str] = None
    """Name of task to use - used only for tracking purposes"""
    model_name: Optional[str] = None
    """Name of model to use - used only for tracking purposes"""
    query_dataset: Optional[str] = None
    """Name of dataset to query - used only for tracking purposes"""
    reward_model: Optional[str] = None
    """The reward model to use - used only for tracking purposes"""
    steps: int = 20000
    """Number of training steps"""
    learning_rate: float = 1e-5
    """Adam learning rate"""
    adap_kl_ctrl: bool = True
    """Use adaptive KL control, otherwise linear"""
    init_kl_coef: Optional[float] = 0.2
    """Initial KL penalty coefficient (used for adaptive and linear control)"""
    kl_penalty: Literal["kl", "abs", "mse", "full"] = "kl"
    """kl penalty options: 'kl': model_logp - ref_logp,  'abs': abs(kl),  'mse': mean squared error mse(kl) and 'full': the actual kl for all tokens in the distribution"""
    target: Optional[float] = 6
    """Target KL value for adaptive KL control"""
    horizon: Optional[float] = 10000
    """Horizon for adaptive KL control"""
    gamma: float = 1
    """Gamma parameter for advantage calculation"""
    lam: float = 0.95
    """Lambda parameter for advantage calculation"""
    cliprange: float = 0.2
    """Range for clipping in PPO policy gradient loss"""
    cliprange_value: float = 0.2
    """Range for clipping values in loss calculation"""
    vf_coef: float = 0.1
    """Scaling factor for value loss"""
    batch_size: int = 256
    """Number of samples per optimisation step"""
    forward_batch_size: Optional[int] = None
    """DEPRECATED: use `mini_batch_size` instead, which does the same thing."""
    mini_batch_size: int = 1
    """Number of samples optimized in each mini batch"""
    gradient_accumulation_steps: int = 1
    """The number of gradient accumulation steps"""
    world_size: tyro.conf.Suppress[int] = None
    """The world size for distributed training"""
    backward_batch_size: tyro.conf.Suppress[int] = None
    """TO BE FILLED In RUNTIME: Number of samples optimized in an `optimizer.step()` call"""
    global_backward_batch_size: tyro.conf.Suppress[int] = None
    """TO BE FILLED In RUNTIME: the effective `backward_batch_size` across all processes"""
    global_batch_size: tyro.conf.Suppress[int] = None
    """TO BE FILLED In RUNTIME: the effective `batch_size` across all processes"""
    ppo_epochs: int = 4
    """Number of optimisation epochs per batch of samples"""
    remove_unused_columns: bool = True
    """Remove unused columns from the dataset if `datasets.Dataset` is used"""
    log_with: Optional[Literal["wandb", "tensorboard"]] = None
    """Log with either 'wandb' or 'tensorboard', check  https://huggingface.co/docs/accelerate/usage_guides/tracking for more details"""
    tracker_kwargs: dict = field(default_factory=dict)
    """Keyword arguments for the tracker (e.g. wandb_project)"""
    accelerator_kwargs: dict = field(default_factory=dict)
    """Keyword arguments for the accelerator"""
    project_kwargs: dict = field(default_factory=dict)
    """Keyword arguments for the accelerator project config (e.g. `logging_dir`)"""
    tracker_project_name: str = "trl"
    """Name of project to use for tracking"""
    max_grad_norm: Optional[float] = None
    """Maximum gradient norm for gradient clipping"""
    seed: int = 0
    """Seed value for random generations"""
    optimize_cuda_cache: bool = False
    """Optimize CUDA cache for slightly more memory-efficient training"""
    early_stopping: bool = False
    """Whether to stop the PPO optimization loop early is the KL too high"""
    target_kl: float = 1
    """Stop early if we exceed this value by over 50%"""
    push_to_hub_if_best_kwargs: dict = field(default_factory=dict)
    """Keyword arguments for pushing model to the hub during training (e.g. repo_id)"""
    compare_steps: int = 1
    """Number of steps between comparison of the current reward with the best seen so far"""
    ratio_threshold: float = 10.0
    """Skip mini-batches with high PPO ratios that can cause loss spikes"""
    use_score_scaling: bool = False
    """Use score scaling"""
    use_score_norm: bool = False
    """Use score normalization. Only applicable if use_score_scaling is True"""
    score_clip: Optional[float] = None
    """Score clipping"""

    # Model detection
    is_encoder_decoder: Optional[tyro.conf.Suppress[bool]] = None
    """TO BE FILLED In RUNTIME: Whether the model is an encoder-decoder model"""
    is_peft_model: Optional[tyro.conf.Suppress[bool]] = None
    """TO BE FILLED In RUNTIME: Whether the model is a PEFT model"""

    def __post_init__(self):
        if self.forward_batch_size is not None:
            warnings.warn(
                "Note that using `forward_batch_size` is deprecated, use `mini_batch_size` instead. By setting it you overwrite `mini_batch_size` which affects both the batch size during forward passes and also the mini batch size for PPO optimization."
            )
            self.mini_batch_size = self.forward_batch_size

        self.backward_batch_size = self.mini_batch_size * self.gradient_accumulation_steps
        exact_div(
            self.batch_size,
            self.backward_batch_size,
            "`batch_size`",
            "`mini_batch_size * gradient_accumulation_steps`",
            "`batch_size` must be a multiple of `mini_batch_size * gradient_accumulation_steps`",
        )

        # check if wandb is installed
        if self.log_with == "wandb":
            # raise error if wandb is not installed
            try:
                import wandb  # noqa: F401

            except ImportError:
                raise ImportError(
                    "Please install wandb to use wandb logging. You can do this by running `pip install wandb`."
                )

        self.total_ppo_epochs = int(np.ceil(self.steps / self.batch_size))
        assert self.kl_penalty in ["kl", "abs", "mse", "full"]

    def to_dict(self):
        output_dict = {}
        for key, value in self.__dict__.items():
            output_dict[key] = value
        return flatten_dict(output_dict)

File Path: trl/trainer/beaver_trainer.py
Content:
# Copyright 2022 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import inspect
import math
import os
import time
import typing
import warnings
from typing import Callable, List, Optional, Union

import datasets
import numpy as np
import torch
import torch.nn.functional as F
from accelerate import Accelerator
from accelerate.utils import ProjectConfiguration, is_deepspeed_available
from datasets import Dataset
from huggingface_hub import whoami
from packaging import version
from torch.optim import Adam
from transformers import (
    DataCollatorForLanguageModeling,
    PreTrainedTokenizer,
    PreTrainedTokenizerBase,
    PreTrainedTokenizerFast,
)

from ..core import (
    WANDB_PADDING,
    PPODecorators,
    clip_by_value,
    convert_to_scalar,
    entropy_from_logits,
    flatten_dict,
    logprobs_from_logits,
    masked_mean,
    masked_var,
    masked_whiten,
    set_seed,
    stack_dicts,
    stats_to_np,
)
from ..import_utils import is_torch_greater_2_0
from ..models import SUPPORTED_ARCHITECTURES, PreTrainedModelWrapper, create_reference_model
from . import AdaptiveKLController, BaseTrainer, FixedKLController, PPOConfig, RunningMoments

import pdb

if is_deepspeed_available():
    import deepspeed

MODEL_CARD_TEMPLATE = """---
license: apache-2.0
tags:
- trl
- transformers
- reinforcement-learning
---

# {model_name}

This is a [TRL language model](https://github.com/huggingface/trl) that has been fine-tuned with reinforcement learning to
 guide the model outputs according to a value, function, or human feedback. The model can be used for text generation.

## Usage

To use this model for inference, first install the TRL library:

```bash
python -m pip install trl
```

You can then generate text as follows:

```python
from transformers import pipeline

generator = pipeline("text-generation", model="{model_id}")
outputs = generator("Hello, my llama is cute")
```

If you want to use the model for training or to obtain the outputs from the value head, load the model as follows:

```python
from transformers import AutoTokenizer
from trl import AutoModelForCausalLMWithValueHead

tokenizer = AutoTokenizer.from_pretrained("{model_id}")
model = AutoModelForCausalLMWithValueHead.from_pretrained("{model_id}")

inputs = tokenizer("Hello, my llama is cute", return_tensors="pt")
outputs = model(**inputs, labels=inputs["input_ids"])
```
"""


class PPOTrainer(BaseTrainer):
    """
    The PPOTrainer uses Proximal Policy Optimization to optimise language models.
    Note, this trainer is heavily inspired by the original OpenAI learning to summarize work here:
    https://github.com/openai/summarize-from-feedback

    Attributes:
        **config** (`PPOConfig`) -- Configuration object for PPOTrainer. Check the documentation of `PPOConfig` for more
            details.
        **model** (`PreTrainedModelWrapper`) -- Model to be optimized, Hugging Face transformer model with a value head.
            Check the documentation of `PreTrainedModelWrapper` for more details.
        **ref_model** (`PreTrainedModelWrapper`, *optional*) -- Reference model to be used for KL penalty, Hugging Face
            transformer model with a casual language modelling head. Check the documentation of `PreTrainedModelWrapper`
            for more details. If no reference model is provided, the trainer will create a reference model with the same
             architecture as the model to be optimized with shared layers.
        **tokenizer** (`PreTrainedTokenizerBase`) -- Tokenizer to be used for encoding the
            data. Check the documentation of `transformers.PreTrainedTokenizer` and
            `transformers.PreTrainedTokenizerFast` for more details.
        **dataset** (Union[`torch.utils.data.Dataset`, `datasets.Dataset`], *optional*) -- PyTorch dataset or Hugging
            Face dataset. This is used to create a PyTorch dataloader. If no dataset is provided, the dataloader must be
             created outside the trainer users needs to design their own dataloader and make sure the batch
            size that is used is the same as the one specified in the configuration object.
        **optimizer** (`torch.optim.Optimizer`, *optional*) -- Optimizer to be used for training. If no optimizer is
            provided, the trainer will create an Adam optimizer with the learning rate specified in the configuration
            object.
        **data_collator** (DataCollatorForLanguageModeling, *optional*) -- Data collator to be used for training and
            passed along the dataloader
        **num_shared_layers** (int, *optional*) -- Number of layers to be shared between the model and the reference
            model, if no reference model is passed. If no number is provided, all the layers will be shared.
        **lr_scheduler** (`torch.optim.lr_scheduler`, *optional*) -- Learning rate scheduler to be used for training.
    """

    def __init__(
        self,
        config: PPOConfig = None,
        model: PreTrainedModelWrapper = None,
        ref_model: Optional[PreTrainedModelWrapper] = None,
        tokenizer: PreTrainedTokenizerBase = None,
        dataset: Optional[Union[torch.utils.data.Dataset, Dataset]] = None,
        optimizer: Optional[torch.optim.Optimizer] = None,
        data_collator: Optional[typing.Callable] = None,
        num_shared_layers: Optional[int] = None,
        lr_scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,
    ):
        """
        Initialize PPOTrainer.

        Args:
            config (`PPOConfig`):
                Configuration object for PPOTrainer. Check the documentation of `PPOConfig` for more details.
            model (`PreTrainedModelWrapper`):
                Hugging Face transformer model with a value head.
            ref_model (`PreTrainedModelWrapper`):
                Hugging Face transformer model with a casual language modelling head. Used for KL penalty
            tokenizer (`transformers.PreTrainedTokenizerBase`):
                Hugging Face tokenizer
            dataset (Optional[Union[`torch.utils.data.Dataset`, `datasets.Dataset`]]):
                PyTorch dataset or Hugging Face dataset. If a Hugging Face dataset is passed, the dataset
                will be preprocessed by removing the columns that are not used by the model. If none is passed,
                a warning will be raised in a multi-GPU setting.
            optimizer (Optional[`torch.optim.Optimizer`]):
                Optimizer used for training. If `None`, the `Adam` is used as default.
            data_collator (Optional[function]):
                Data collator function.
            num_shared_layers (Optional[int]):
                Number of shared layers between the model and the reference model. If `None`, all layers are shared.
                used only if `ref_model` is `None`.
            lr_scheduler (Optional[`torch.optim.lr_scheduler`]):
                Learning rate scheduler used for training.
        """
        super().__init__(config)

        # initial seed for reproducible experiments
        set_seed(config.seed)

        # Step 0: check positional arguments validity
        if not isinstance(config, PPOConfig):
            raise ValueError(f"config must be a PPOConfig, got {type(config)}")
        if not isinstance(tokenizer, (PreTrainedTokenizerBase)):
            raise ValueError(
                f"tokenizer must be a PreTrainedTokenizerBase like a PreTrainedTokenizer or a PreTrainedTokenizerFast, got {type(tokenizer)}"
            )
        if not isinstance(model, (SUPPORTED_ARCHITECTURES)):
            raise ValueError(
                f"model must be a PreTrainedModelWrapper, got {type(model)} - supported architectures are: {SUPPORTED_ARCHITECTURES}"
            )
        # Step 1: Initialize Accelerator
        self.accelerator = Accelerator(
            log_with=config.log_with,
            gradient_accumulation_steps=config.gradient_accumulation_steps,
            project_config=ProjectConfiguration(**config.project_kwargs),
            **config.accelerator_kwargs,
        )

        # Step 1.1 Runtime variables filled by the accelerator
        config.world_size = self.accelerator.num_processes
        config.global_backward_batch_size = config.backward_batch_size * config.world_size
        config.global_batch_size = config.batch_size * config.world_size

        self.model = model
        self.model_params = filter(lambda p: p.requires_grad, self.model.parameters())
        self.is_encoder_decoder = hasattr(self.model, "is_encoder_decoder")
        self.is_peft_model = getattr(self.model, "is_peft_model", False)
        config.is_encoder_decoder = self.is_encoder_decoder
        config.is_peft_model = self.is_peft_model

        is_using_tensorboard = config.log_with is not None and config.log_with == "tensorboard"
        self.accelerator.init_trackers(
            config.tracker_project_name,
            config=dict(trl_ppo_trainer_config=config.to_dict()) if not is_using_tensorboard else config.to_dict(),
            init_kwargs=config.tracker_kwargs,
        )
        self.is_using_text_environment = getattr(config, "use_text_environment", False)

        if isinstance(ref_model, SUPPORTED_ARCHITECTURES):
            self.ref_model = ref_model
            if num_shared_layers is not None:
                warnings.warn(
                    "num_shared_layers is ignored when ref_model is provided. Two different models are used for the "
                    "model and the reference model and no layers are shared.",
                    UserWarning,
                )
        elif ref_model is None and not self.is_peft_model:
            self.ref_model = create_reference_model(self.model, num_shared_layers=num_shared_layers)
        elif self.is_peft_model:
            self.ref_model = None
        else:
            raise ValueError(
                f"ref_model must be a PreTrainedModelWrapper or `None`, got {type(ref_model)} - supported "
                f"architectures are: {SUPPORTED_ARCHITECTURES} "
            )

        if not (isinstance(tokenizer, PreTrainedTokenizer) or isinstance(tokenizer, PreTrainedTokenizerFast)):
            raise ValueError(
                "tokenizer must be a transformers.PreTrainedTokenizer or transformers.PreTrainedTokenizerFast"
            )
        self.tokenizer = tokenizer

        if dataset is not None and not (isinstance(dataset, torch.utils.data.Dataset) or isinstance(dataset, Dataset)):
            raise ValueError("dataset must be a torch.utils.data.Dataset or datasets.Dataset")
        elif dataset is None:
            warnings.warn(
                "No dataset is provided. Make sure to set config.batch_size to the correct value before training.",
                UserWarning,
            )
        self.dataset = dataset
        self._signature_columns = None
        if self.dataset is not None:
            self.dataloader = self.prepare_dataloader(self.dataset, data_collator)
        elif self.dataset is None and self.accelerator.num_processes > 1:
            warnings.warn(
                "No dataset is provided. In a multi-GPU setting, this will lead to an error. You should"
                " prepare your dataloader yourself with `dataloader = ppo_trainer.accelerator.prepare(dataloader)`"
                " and using `torch.utils.data.DataLoader`, or pass a dataset to the `PPOTrainer`. Please "
                " refer to the documentation for more details.",
                UserWarning,
            )
            self.dataloader = None
        else:
            self.dataloader = None

        # Step 3: Initialize optimizer and data collator
        self.data_collator = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)
        if optimizer is None:
            self.optimizer = Adam(
                filter(lambda p: p.requires_grad, self.model.parameters()),
                lr=self.config.learning_rate,
            )
        else:
            self.optimizer = optimizer

        self.lr_scheduler = lr_scheduler
        if self.lr_scheduler is not None:
            lr_scheduler_class = (
                torch.optim.lr_scheduler._LRScheduler
                if not is_torch_greater_2_0()
                else torch.optim.lr_scheduler.LRScheduler
            )

            if not isinstance(self.lr_scheduler, lr_scheduler_class):
                raise ValueError(
                    "lr_scheduler must be a torch.optim.lr_scheduler._LRScheduler or torch.optim.lr_scheduler.LRScheduler (for torch >= 2.0)"
                )

        if self.config.adap_kl_ctrl:
            self.kl_ctl = AdaptiveKLController(self.config.init_kl_coef, self.config.target, self.config.horizon)
        else:
            self.kl_ctl = FixedKLController(self.config.init_kl_coef)

        # Safety checkers for DS integration
        is_deepspeed_used = self.accelerator.distributed_type == "DEEPSPEED" and hasattr(
            self.accelerator.state, "deepspeed_plugin"
        )

        (
            self.model,
            self.optimizer,
            self.data_collator,
            self.dataloader,
            self.lr_scheduler,
        ) = self.accelerator.prepare(
            self.model,
            self.optimizer,
            self.data_collator,
            self.dataloader,
            self.lr_scheduler,
        )
        if is_deepspeed_used:
            # Quantized models are already set on the correct device
            if not self.is_peft_model and not (
                getattr(self.ref_model.pretrained_model, "is_loaded_in_8bit", False)
                or getattr(self.ref_model.pretrained_model, "is_loaded_in_4bit", False)
            ):
                self.ref_model = self._prepare_deepspeed(self.ref_model)
        else:
            self.ref_model = self.accelerator.prepare(self.ref_model)

        # In a distributed setup, only logging needs to be performed on the main process
        # check: https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html
        # or: https://discuss.pytorch.org/t/use-distributed-data-parallel-correctly/82500/11
        self.is_distributed = self.accelerator.distributed_type == "MULTI_GPU"

        # init the current step
        self.current_step = 0

        # init variables for pushing model to hub
        if config.push_to_hub_if_best_kwargs:
            if "repo_id" not in config.push_to_hub_if_best_kwargs:
                raise ValueError("You have to specify repo_id in order to push the model to the hub!")
            self.push_to_hub_kwargs = config.push_to_hub_if_best_kwargs
            self.compare_step = 0
            self.highest_reward = torch.tensor(-float("inf"))

        # post process for PP
        if not getattr(self.model, "is_sequential_parallel", False):
            self.current_device = self.accelerator.device
        else:
            self.current_device = torch.device("cuda:0")

        PPODecorators.optimize_cuda_cache = self.config.optimize_cuda_cache

        self.running = RunningMoments(self.accelerator)

    def _filter_kwargs(self, kwargs, target_func):
        """
        filter the keyword arguments that are supported by the target function.

        Args:
            kwargs (dict):
                Keyword arguments
            target_func (function):
                Target function
        """
        return {k: v for k, v in kwargs.items() if k in inspect.signature(target_func).parameters.keys()}

    def prepare_dataloader(self, dataset: Union[torch.utils.data.Dataset, Dataset], data_collator=None):
        """
        Prepare the dataloader for training.

        Args:
            dataset (Union[`torch.utils.data.Dataset`, `datasets.Dataset`]):
                PyTorch dataset or Hugging Face dataset. If a Hugging Face dataset is passed, the dataset
                will be preprocessed by removing the columns that are not used by the model.
            data_collator (Optional[function]):
                Data collator function.

        Returns:
            `torch.utils.data.DataLoader`: PyTorch dataloader
        """
        if isinstance(dataset, Dataset):
            dataset = self._remove_unused_columns(dataset)
        dataloader = torch.utils.data.DataLoader(
            dataset,
            batch_size=self.config.batch_size,
            collate_fn=data_collator,
            shuffle=True,
            drop_last=True,
        )
        return dataloader

    # Adapted from transformers.Trainer._set_signature_columns_if_needed
    def _set_signature_columns_if_needed(self):
        if self._signature_columns is None:
            # Inspect model forward signature to keep only the arguments it accepts.
            signature = inspect.signature(self.model.forward)
            self._signature_columns = list(signature.parameters.keys())
            # label => sentiment | we need query and response for logging purpose
            self._signature_columns += ["label", "query", "response"]

    # Adapted from transformers.Trainer._remove_unused_columns
    def _remove_unused_columns(self, dataset: "Dataset"):
        if not self.config.remove_unused_columns:
            return dataset
        self._set_signature_columns_if_needed()
        signature_columns = self._signature_columns

        ignored_columns = list(set(dataset.column_names) - set(signature_columns))

        columns = [k for k in signature_columns if k in dataset.column_names]

        if version.parse(datasets.__version__) < version.parse("1.4.0"):
            dataset.set_format(
                type=dataset.format["type"],
                columns=columns,
                format_kwargs=dataset.format["format_kwargs"],
            )
            return dataset
        else:
            return dataset.remove_columns(ignored_columns)

    def generate(
        self,
        query_tensor: Union[torch.Tensor, List[torch.Tensor]],
        length_sampler: Callable = None,
        batch_size: int = 4,
        return_prompt: bool = True,
        **generation_kwargs,
    ):
        """
        Generate response with the model given the query tensor.
        call the `generate` method of the model.

        Args:
            query_tensor (`torch.LongTensor`):
                A tensor of shape (`batch_size`, `seq_len`) containing query tokens.
            generation_kwargs (dict[str, Any]):
                Keyword arguments for generation.
            length_sampler (`Callable`, *optional*):
                Callable that returns the number of newly generated tokens.
            batch_size (`int`, *optional):
                Batch size used for generation, defaults to `4`.
            return_prompt (`bool`, *optional*):
                If set to `False` the prompt is not returned but only the newly generated tokens, defaults to `True`.

        Returns:
            `torch.LongTensor`: A tensor of shape (`batch_size`, `gen_len`) containing response tokens.
        """

        if isinstance(query_tensor, List):
            return self._generate_batched(
                query_tensor,
                length_sampler=length_sampler,
                batch_size=batch_size,
                return_prompt=return_prompt,
                **generation_kwargs,
            )

        else:
            if length_sampler is not None:
                generation_kwargs["max_new_tokens"] = length_sampler()
            response = self.accelerator.unwrap_model(self.model).generate(
                input_ids=query_tensor.unsqueeze(dim=0), **generation_kwargs
            )

            if not return_prompt and not self.is_encoder_decoder:
                return response[:, query_tensor.shape[0] :]
            return response

    def _generate_batched(
        self,
        query_tensors: List[torch.Tensor],
        length_sampler: Callable = None,
        batch_size: int = 4,
        return_prompt: bool = True,
        pad_to_multiple_of: int = None,
        remove_padding: bool = True,
        **generation_kwargs,
    ):
        outputs = []

        padding_side_default = self.tokenizer.padding_side
        if not self.is_encoder_decoder:
            self.tokenizer.padding_side = "left"

        # in case we have fewer examples than bs
        batch_size = min(len(query_tensors), batch_size)

        for i in range(0, len(query_tensors), batch_size):
            if length_sampler is not None:
                generation_kwargs["max_new_tokens"] = length_sampler()

            # prevent overflow if query tensors are not even multiple of bs
            end_index = min(len(query_tensors), i + batch_size)

            batch = query_tensors[i:end_index]
            batch_mask = [torch.ones_like(element) for element in batch]
            inputs = {"input_ids": batch, "attention_mask": batch_mask}

            padded_inputs = self.tokenizer.pad(
                inputs,
                padding=True,
                max_length=None,
                pad_to_multiple_of=pad_to_multiple_of,
                return_tensors="pt",
            ).to(self.current_device)

            generations = self.accelerator.unwrap_model(self.model).generate(**padded_inputs, **generation_kwargs)

            for generation, mask in zip(generations, padded_inputs["attention_mask"]):
                if not self.is_encoder_decoder:
                    output = generation[(1 - mask).sum() :]  # remove padding
                else:
                    output = generation

                if not return_prompt and not self.is_encoder_decoder:
                    output = output[(mask).sum() :]  # remove prompt

                if remove_padding and self.tokenizer.eos_token_id in output:
                    pad_mask = output == self.tokenizer.eos_token_id
                    pad_start = torch.nonzero(pad_mask, as_tuple=False)[0, 0].item()
                    output = output[: pad_start + 1]  # keep the eos token at the end

                outputs.append(output)

        self.tokenizer.padding_side = padding_side_default
        return outputs

    def _step_safety_checker(
        self,
        batch_size: int,
        queries: List[torch.LongTensor],
        responses: List[torch.LongTensor],
        scores: List[torch.FloatTensor],
        masks: Optional[List[torch.LongTensor]] = None,
    ):
        """
        Check if the input data is valid for training.

        Args:
            batch_size (int):
                Batch size from the config file.
            queries (List[`torch.LongTensor`]):
                List of tensors containing the encoded queries of shape (`query_length`)
            responses (List[`torch.LongTensor`]):
                List of tensors containing the encoded responses of shape (`response_length`)
            scores (List[`torch.FloatTensor`]):
                List of tensors containing the scores.
            masks (List[`torch.LongTensor`], *optional*):
                list of optional tensors containing the masks of shape (`query_length` + `response_length`)
        Returns:
            `tuple`: The input processed data.
        """
        for name, tensor_list in zip(["queries", "responses", "scores"], [queries, responses, scores]):
            if not isinstance(tensor_list, list):
                raise ValueError(f"{name} must be a list of tensors - got {type(tensor_list)}")
            if not isinstance(tensor_list[0], torch.Tensor):
                raise ValueError(f"Elements in {name} must be tensors - got {type(tensor_list[0])}")
            if batch_size is not None and len(tensor_list) != batch_size:
                raise ValueError(
                    f"Batch size ({batch_size}) does not match number of examples - but got {len(tensor_list)} for: {name}"
                )

        # add queries, scores and responses on the correct device
        queries = [tensor.to(self.current_device) for tensor in queries]
        responses = [tensor.to(self.current_device) for tensor in responses]
        scores = [tensor.to(self.current_device) for tensor in scores]
        masks = [tensor.to(self.current_device) for tensor in masks] if masks is not None else None

        # squeeze scores if needed
        for i, score in enumerate(scores):
            if score.dim() > 1:
                raise ValueError(f"Scores must be 1-dimensional - got {score.dim()} for {score}")
            elif score.dim() == 1:
                scores[i] = score.squeeze()

        return queries, responses, scores, masks

    @PPODecorators.empty_cuda_cache()
    def step(
        self,
        queries: List[torch.LongTensor],
        responses: List[torch.LongTensor],
        scores: List[torch.FloatTensor],
        response_masks: Optional[List[torch.LongTensor]] = None,
    ):
        """
        Run a PPO optimisation step given a list of queries, model responses, and rewards.

        Args:
            queries (List[`torch.LongTensor`]):
                List of tensors containing the encoded queries of shape (`query_length`)
            responses (List[`torch.LongTensor`]):
                List of tensors containing the encoded responses of shape (`response_length`)
            scores (List[`torch.FloatTensor`]):
                List of tensors containing the scores.
            response_masks (List[`torch.FloatTensor`], *optional*)):
                List of tensors containing masks of the response tokens.

        Returns:
            `dict[str, Any]`: A summary of the training statistics
        """

        #pdb.set_trace()
        bs = self.config.batch_size

        queries, responses, scores, response_masks = self._step_safety_checker(
            bs, queries, responses, scores, response_masks
        )
        scores = torch.tensor(scores, device=self.current_device)
        if self.config.use_score_scaling:
            # Score scaling
            scores_mean, scores_std = self.running.update(scores)
            tensor_to_kwargs = dict(dtype=scores.dtype, device=scores.device)
            score_scaling_factor = self.running.std.to(**tensor_to_kwargs) + torch.finfo(scores.dtype).eps
            if self.config.use_score_norm:
                scores = (scores - self.running.mean.to(**tensor_to_kwargs)) / score_scaling_factor
            else:
                scores /= score_scaling_factor

        if self.config.score_clip is not None:
            # Score clipping
            scores_dtype = scores.dtype
            scores = torch.clip(scores.float(), -self.config.score_clip, self.config.score_clip).to(dtype=scores_dtype)

        # if we want to push best model to the hub
        if hasattr(self, "highest_reward"):
            if self.compare_step % self.config.compare_steps == 0:
                curr_mean_reward = scores.mean()
                # if the best reward ever seen
                if curr_mean_reward > self.highest_reward:
                    self.highest_reward = curr_mean_reward
                    # push model to hub
                    self.push_to_hub(**self.push_to_hub_kwargs)
            self.compare_step += 1

        timing = dict()
        t0 = time.time()

        t = time.time()

        #pdb.set_trace()

        model_inputs = self.prepare_model_inputs(queries, responses)

        #pdb.set_trace()

        if self.is_distributed:
            pad_first = self.tokenizer.padding_side == "left"

            model_inputs["input_ids"] = self.accelerator.pad_across_processes(
                model_inputs["input_ids"],
                dim=1,
                pad_index=self.tokenizer.pad_token_id,
                pad_first=pad_first,
            )
            model_inputs["attention_mask"] = self.accelerator.pad_across_processes(
                model_inputs["attention_mask"], dim=1, pad_index=0, pad_first=pad_first
            )
            if self.is_encoder_decoder:
                model_inputs["decoder_input_ids"] = self.accelerator.pad_across_processes(
                    model_inputs["decoder_input_ids"],
                    dim=1,
                    pad_index=self.tokenizer.pad_token_id,
                    pad_first=pad_first,
                )
                model_inputs["decoder_attention_mask"] = self.accelerator.pad_across_processes(
                    model_inputs["decoder_attention_mask"],
                    dim=1,
                    pad_index=0,
                    pad_first=pad_first,
                )

        model_inputs_names = list(model_inputs.keys())

        full_kl_penalty = self.config.kl_penalty == "full"

        with torch.no_grad():
            all_logprobs, logits_or_none, values, masks = self.batched_forward_pass(
                self.model,
                queries,
                responses,
                model_inputs,
                response_masks=response_masks,
                return_logits=full_kl_penalty,
            )
            # for when the model is a peft model
            if self.is_peft_model and hasattr(
                self.accelerator.unwrap_model(self.model).pretrained_model,
                "disable_adapter",
            ):
                with self.accelerator.unwrap_model(self.model).pretrained_model.disable_adapter():
                    ref_logprobs, ref_logits_or_none, _, _ = self.batched_forward_pass(
                        self.model, queries, responses, model_inputs, return_logits=full_kl_penalty
                    )
            elif self.is_peft_model and not hasattr(self.model.pretrained_model, "disable_adapter"):
                raise ValueError(
                    "You are using a `peft` version that does not support `disable_adapter`. Please update your `peft` version to the latest version."
                )

            else:
                ref_logprobs, ref_logits_or_none, _, _ = self.batched_forward_pass(
                    self.ref_model, queries, responses, model_inputs, return_logits=full_kl_penalty
                )

        timing["time/ppo/forward_pass"] = time.time() - t

        with torch.no_grad():
            t = time.time()
            if full_kl_penalty:
                active_full_logprobs = logprobs_from_logits(logits_or_none, None, gather=False)
                ref_full_logprobs = logprobs_from_logits(ref_logits_or_none, None, gather=False)

                rewards, non_score_reward = self.compute_rewards(
                    scores, active_full_logprobs, ref_full_logprobs, masks
                )
            else:
                rewards, non_score_reward = self.compute_rewards(scores, all_logprobs, ref_logprobs, masks)
            timing["time/ppo/compute_rewards"] = time.time() - t

            t = time.time()
            values, advantages, returns = self.compute_advantages(values, rewards, masks)
            timing["time/ppo/compute_advantages"] = time.time() - t

        # upcast to float32 to avoid dataset issues
        batch_dict = {
            "queries": queries,
            "responses": responses,
            "logprobs": all_logprobs.to(torch.float32),
            "values": values.to(torch.float32),
            "masks": masks,
            "advantages": advantages,
            "returns": returns,
        }
        batch_dict.update(model_inputs)

        t = time.time()
        all_stats = []
        early_stop = False
        for _ in range(self.config.ppo_epochs):
            if early_stop:
                break
            b_inds = np.random.permutation(bs)
            for backward_batch_start in range(0, bs, self.config.backward_batch_size):
                backward_batch_end = backward_batch_start + self.config.backward_batch_size
                backward_batch_inds = b_inds[backward_batch_start:backward_batch_end]

                for mini_batch_start in range(0, self.config.backward_batch_size, self.config.mini_batch_size):
                    mini_batch_end = mini_batch_start + self.config.mini_batch_size
                    mini_batch_inds = backward_batch_inds[mini_batch_start:mini_batch_end]
                    mini_batch_dict = {
                        "logprobs": batch_dict["logprobs"][mini_batch_inds],
                        "values": batch_dict["values"][mini_batch_inds],
                        "masks": batch_dict["masks"][mini_batch_inds],
                        # hacks: the queries and responses are ragged.
                        "queries": [batch_dict["queries"][i] for i in mini_batch_inds],
                        "responses": [batch_dict["responses"][i] for i in mini_batch_inds],
                        "advantages": batch_dict["advantages"][mini_batch_inds],
                        "returns": batch_dict["returns"][mini_batch_inds],
                    }
                    for k in model_inputs_names:
                        mini_batch_dict[k] = batch_dict[k][mini_batch_inds]
                    with self.accelerator.accumulate(self.model):
                        model_inputs = {k: mini_batch_dict[k] for k in model_inputs_names}

                        logprobs, logits, vpreds, _ = self.batched_forward_pass(
                            self.model,
                            mini_batch_dict["queries"],
                            mini_batch_dict["responses"],
                            model_inputs,
                            return_logits=True,
                        )
                        train_stats = self.train_minibatch(
                            mini_batch_dict["logprobs"],
                            mini_batch_dict["values"],
                            logprobs,
                            logits,
                            vpreds,
                            mini_batch_dict["masks"],
                            mini_batch_dict["advantages"],
                            mini_batch_dict["returns"],
                        )
                        all_stats.append(train_stats)

            # typically, early stopping is done at the epoch level
            if self.config.early_stopping:
                policykl = train_stats["policy/policykl"]
                early_stop = self._early_stop(policykl)
                if early_stop:
                    break

        timing["time/ppo/optimize_step"] = time.time() - t

        t = time.time()
        train_stats = stack_dicts(all_stats)

        # reshape advantages/ratios such that they are not averaged.
        train_stats["policy/advantages"] = torch.flatten(train_stats["policy/advantages"]).unsqueeze(0)
        train_stats["policy/advantages"] = torch.nan_to_num(train_stats["policy/advantages"], WANDB_PADDING)
        train_stats["policy/ratio"] = torch.flatten(train_stats["policy/ratio"]).unsqueeze(0)

        stats = self.record_step_stats(
            scores=scores,
            logprobs=all_logprobs,
            ref_logprobs=ref_logprobs,
            non_score_reward=non_score_reward,
            train_stats=train_stats,
            kl_coef=self.kl_ctl.value,
            masks=masks,
            queries=queries,
            responses=responses,
        )
        # Gather/Reduce stats from all processes
        if self.is_distributed:
            stats = self.gather_stats(stats)
        stats = stats_to_np(stats)
        timing["time/ppo/calc_stats"] = time.time() - t
        stats["ppo/learning_rate"] = self.optimizer.param_groups[0]["lr"]

        # Update the KL control - multiply the batch_size by the number of processes
        self.kl_ctl.update(
            stats["objective/kl"],
            self.config.batch_size * self.accelerator.num_processes,
        )

        # Log the total ppo time
        timing["time/ppo/total"] = time.time() - t0
        stats.update(timing)

        # post-process stats for tensorboard and other loggers
        if self.config.log_with != "wandb":
            stats = convert_to_scalar(stats)

        if self.lr_scheduler is not None:
            self.lr_scheduler.step()

        return stats

    def _early_stop(self, policykl):
        r"""
        Handles the early stopping logic. If the policy KL is greater than the target KL, then the gradient is zeroed and
        the optimization step is skipped.
        This also handles the multi-gpu case where the policy KL is averaged across all processes.

        Args:
            policy_kl (torch.Tensor):
                the policy KL

        Returns:
            `bool`: whether to early stop or not
        """
        early_stop = False
        if not self.config.early_stopping:
            return early_stop

        if not self.is_distributed and policykl > 1.5 * self.config.target_kl:
            self.optimizer.zero_grad()
            early_stop = True
        elif self.is_distributed:
            import torch.distributed as dist

            # Wait for all processes to finish
            dist.barrier()

            # all gather the policykl
            dist.all_reduce(policykl, dist.ReduceOp.SUM)
            policykl /= self.accelerator.num_processes

            if policykl > 1.5 * self.config.target_kl:
                self.optimizer.zero_grad()
                early_stop = True
        return early_stop

    def gather_stats(self, stats):
        """
        Gather stats from all processes. Useful in the context of distributed training.

        Args:
            stats (dict[str, Any]):
            a dictionary of stats to be gathered. The stats should contain torch tensors.

        Returns:
            `dict[str, Any]`: A dictionary of stats with the tensors gathered.
        """
        import torch.distributed as dist

        # Wait for all processes to finish
        dist.barrier()

        for k, v in stats.items():
            if isinstance(v, torch.Tensor):
                dist.all_reduce(v.to(self.accelerator.device), dist.ReduceOp.SUM)
                v /= self.accelerator.num_processes
            stats[k] = v
        return stats

    def prepare_model_inputs(self, queries: torch.Tensor, responses: torch.Tensor):
        if self.is_encoder_decoder:
            input_data = self.data_collator(
                [{"input_ids": q, "attention_mask": torch.ones_like(q)} for q in queries]
            ).to(self.current_device)

            decoder_inputs = self.data_collator(
                [{"input_ids": r, "attention_mask": torch.ones_like(r)} for r in responses]
            ).to(self.current_device)

            input_data["decoder_input_ids"] = decoder_inputs["input_ids"]
            input_data["decoder_attention_mask"] = decoder_inputs["attention_mask"]
        else:
            input_ids = [torch.cat([q, r]) for q, r in zip(queries, responses)]
            input_data = self.data_collator(
                [{"input_ids": ids, "attention_mask": torch.ones_like(ids)} for ids in input_ids]
            ).to(self.current_device)

        input_data.pop("labels", None)  # we don't want to compute LM losses
        return input_data

    @PPODecorators.empty_cuda_cache()
    def batched_forward_pass(
        self,
        model: PreTrainedModelWrapper,
        queries: torch.Tensor,
        responses: torch.Tensor,
        model_inputs: dict,
        return_logits: bool = False,
        response_masks: Optional[torch.Tensor] = None,
    ):
        """
        Calculate model outputs in multiple batches.

        Args:
            queries (`torch.LongTensor`):
                List of tensors containing the encoded queries, shape (`batch_size`, `query_length`)
            responses (`torch.LongTensor`):
                List of tensors containing the encoded responses, shape (`batch_size`, `response_length`)
            return_logits (`bool`, *optional*, defaults to `False`):
                Whether to return all_logits. Set to `False` if logits are not needed to reduce memory consumption.
        Returns:
            (tuple):
                - all_logprobs (`torch.FloatTensor`): Log probabilities of the responses,
                    shape (`batch_size`, `response_length`)
                - all_ref_logprobs (`torch.FloatTensor`): Log probabilities of the responses,
                    shape (`batch_size`, `response_length`)
                - all_values (`torch.FloatTensor`): Values of the responses, shape (`batch_size`, `response_length`)
        """
        bs = len(queries)
        fbs = self.config.mini_batch_size
        all_logprobs = []
        all_logits = []
        all_masks = []
        all_values = []

        model.eval()

        #pdb.set_trace()
        for i in range(math.ceil(bs / fbs)):
            input_kwargs = {key: value[i * fbs : (i + 1) * fbs] for key, value in model_inputs.items()}
            query_batch = queries[i * fbs : (i + 1) * fbs]
            response_batch = responses[i * fbs : (i + 1) * fbs]
            if response_masks is not None:
                response_masks_batch = response_masks[i * fbs : (i + 1) * fbs]
            logits, _, values = model(**input_kwargs)

            if self.is_encoder_decoder:
                input_ids = input_kwargs["decoder_input_ids"]
                attention_mask = input_kwargs["decoder_attention_mask"]
            else:
                input_ids = input_kwargs["input_ids"]
                attention_mask = input_kwargs["attention_mask"]


            # logits at step
            logprobs = logprobs_from_logits(logits[:, :-1, :], input_ids[:, 1:])
            masks = torch.zeros_like(attention_mask)
            masks[:, :-1] = attention_mask[:, 1:]

            for j in range(len(query_batch)):
                if self.is_encoder_decoder:
                    # Decoder sentence starts always in the index 1 after padding in the Enc-Dec Models
                    start = 1
                    end = attention_mask[j, :].sum() - 1
                else:
                    start = len(query_batch[j]) - 1  # logprobs starts from the second query token
                    if attention_mask[j, 0] == 0:  # offset left padding
                        start += attention_mask[j, :].nonzero()[0]
                    end = start + len(response_batch[j])
                    if response_masks is not None:
                        response_masks_batch[j] = torch.cat(
                            (torch.zeros_like(query_batch[j]), response_masks_batch[j])
                        )[1:]

                masks[j, :start] = 0
                masks[j, end:] = 0
                if response_masks is not None:
                    masks[j, start:end] = masks[j, start:end] * response_masks_batch[j][start:end]

            if return_logits:
                all_logits.append(logits)
            else:
                del logits
            all_values.append(values)
            all_logprobs.append(logprobs)
            all_masks.append(masks)

        return (
            torch.cat(all_logprobs),
            torch.cat(all_logits)[:, :-1] if return_logits else None,
            torch.cat(all_values)[:, :-1],
            torch.cat(all_masks)[:, :-1],
        )

    @PPODecorators.empty_cuda_cache()
    def train_minibatch(
        self,
        old_logprobs: torch.FloatTensor,
        values: torch.FloatTensor,
        logprobs: torch.FloatTensor,
        logits: torch.FloatTensor,
        vpreds: torch.FloatTensor,
        mask: torch.LongTensor,
        advantages: torch.FloatTensor,
        returns: torch.FloatTensor,
    ):
        """
        Train one PPO minibatch

        Args:
            logprobs (`torch.FloatTensor`):
                Log probabilities of the model, shape [batch_size, response_length]
            values (`torch.FloatTensor`):
                Values of the value head, shape [batch_size, response_length]
            query (`torch.LongTensor`):
                Encoded queries, shape [batch_size, query_length]
            response (`torch.LongTensor`):
                Encoded responses, shape [batch_size, response_length]
            model_input (`torch.LongTensor`):
                Concatenated queries and responses, shape [batch_size, query_length+response_length]

        Returns:
            train_stats (dict[str, `torch.Tensor`]):
                Dictionary of training statistics
        """
        self.model.train()
        loss_p, loss_v, train_stats = self.loss(
            old_logprobs, values, logits, vpreds, logprobs, mask, advantages, returns
        )
        loss = loss_p + loss_v
        self.accelerator.backward(loss)
        if self.config.max_grad_norm is not None:
            if self.accelerator.sync_gradients:
                self.accelerator.clip_grad_norm_(self.model_params, self.config.max_grad_norm)
        self.optimizer.step()
        # we call optimizer.zero_grad() every time and let `accelerator` handle accumulation
        # see https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation#the-finished-code
        self.optimizer.zero_grad()
        return train_stats

    def compute_rewards(
        self,
        scores: torch.FloatTensor,
        logprobs: torch.FloatTensor,
        ref_logprobs: torch.FloatTensor,
        masks: torch.LongTensor,
    ):
        """
        Compute per token rewards from scores and KL-penalty.

        Args:
            scores (`torch.FloatTensor`):
                Scores from the reward model, shape (`batch_size`)
            logprobs (`torch.FloatTensor`):
                Log probabilities of the model, shape (`batch_size`, `response_length`)
            ref_logprobs (`torch.FloatTensor`):
                Log probabilities of the reference model, shape (`batch_size`, `response_length`)
        """
        rewards, non_score_rewards = [], []
        for score, logprob, ref_logprob, mask in zip(scores, logprobs, ref_logprobs, masks):
            # compute KL penalty (from difference in logprobs)
            kl = self._kl_penalty(logprob, ref_logprob)
            non_score_reward = -self.kl_ctl.value * kl
            non_score_rewards.append(non_score_reward)
            reward = non_score_reward.clone()
            last_non_masked_index = mask.nonzero()[-1]

            # reward is preference model score + KL penalty
            reward[last_non_masked_index] += score
            rewards.append(reward)
        return torch.stack(rewards), torch.stack(non_score_rewards)

    def _kl_penalty(self, logprob: torch.FloatTensor, ref_logprob: torch.FloatTensor) -> torch.FloatTensor:
        if self.config.kl_penalty == "kl":
            return logprob - ref_logprob

        if self.config.kl_penalty == "abs":
            return (logprob - ref_logprob).abs()

        if self.config.kl_penalty == "mse":
            return 0.5 * (logprob - ref_logprob).square()

        if self.config.kl_penalty == "full":
            # Flip is required due to this issue? :https://github.com/pytorch/pytorch/issues/57459
            return F.kl_div(ref_logprob, logprob, log_target=True, reduction="none").sum(-1)

        raise NotImplementedError

    def compute_advantages(
        self: torch.FloatTensor,
        values: torch.FloatTensor,
        rewards: torch.FloatTensor,
        mask: torch.FloatTensor,
    ):
        lastgaelam = 0
        advantages_reversed = []
        gen_len = rewards.shape[-1]

        pdb.set_trace()
        values = values * mask
        rewards = rewards * mask

        for t in reversed(range(gen_len)):
            nextvalues = values[:, t + 1] if t < gen_len - 1 else 0.0
            delta = rewards[:, t] + self.config.gamma * nextvalues - values[:, t]
            lastgaelam = delta + self.config.gamma * self.config.lam * lastgaelam
            advantages_reversed.append(lastgaelam)
        advantages = torch.stack(advantages_reversed[::-1]).transpose(0, 1)

        returns = advantages + values
        advantages = masked_whiten(advantages, mask)
        advantages = advantages.detach()
        return values, advantages, returns

    def loss(
        self,
        old_logprobs: torch.FloatTensor,
        values: torch.FloatTensor,
        logits: torch.FloatTensor,
        vpreds: torch.FloatTensor,
        logprobs: torch.FloatTensor,
        mask: torch.LongTensor,
        advantages: torch.FloatTensor,
        returns: torch.FloatTensor,
    ):
        """
        Calculate policy and value losses.

        Args:
            old_logprobs (`torch.FloatTensor`):
                Log probabilities of the model, shape (`batch_size`, `response_length`)
            values (`torch.FloatTensor`):
                Values of the value head, shape (`batch_size`, `response_length`)
            rewards (`torch.FloatTensor`):
                Rewards from the reward model, shape (`batch_size`, `response_length`)
            logits (`torch.FloatTensor`):
                Logits of the model, shape (`batch_size`, `response_length`, `vocab_size`)
            v_pred (`torch.FloatTensor`):
                Values of the value head, shape (`batch_size`, `response_length`)
            logprobs (`torch.FloatTensor`):
                Log probabilities of the model, shape (`batch_size`, `response_length`)
        """

        vpredclipped = clip_by_value(
            vpreds,
            values - self.config.cliprange_value,
            values + self.config.cliprange_value,
        )

        #pdb.set_trace()

        vf_losses1 = (vpreds - returns) ** 2
        vf_losses2 = (vpredclipped - returns) ** 2
        vf_loss = 0.5 * masked_mean(torch.max(vf_losses1, vf_losses2), mask)
        vf_clipfrac = masked_mean(torch.gt(vf_losses2, vf_losses1).float(), mask)

        ratio = torch.exp(logprobs - old_logprobs)

        pg_losses = -advantages * ratio
        pg_losses2 = -advantages * torch.clamp(ratio, 1.0 - self.config.cliprange, 1.0 + self.config.cliprange)

        pg_loss = masked_mean(torch.max(pg_losses, pg_losses2), mask)
        pg_clipfrac = masked_mean(torch.gt(pg_losses2, pg_losses).float(), mask)

        loss = pg_loss + self.config.vf_coef * vf_loss

        avg_ratio = masked_mean(ratio, mask).item()
        if avg_ratio > self.config.ratio_threshold:
            warnings.warn(
                f"The average ratio of batch ({avg_ratio:.2f}) exceeds threshold {self.config.ratio_threshold:.2f}. Skipping batch."
            )
            pg_loss = pg_loss * 0.0
            vf_loss = vf_loss * 0.0
            loss = loss * 0.0

        entropy = masked_mean(entropy_from_logits(logits), mask)

        approxkl = 0.5 * masked_mean((logprobs - old_logprobs) ** 2, mask)
        policykl = masked_mean(old_logprobs - logprobs, mask)

        return_mean, return_var = masked_mean(returns, mask), masked_var(returns, mask)
        value_mean, value_var = masked_mean(values, mask), masked_var(values, mask)

        stats = dict(
            loss=dict(policy=pg_loss.detach(), value=vf_loss.detach(), total=loss.detach()),
            policy=dict(
                entropy=entropy.detach(),
                approxkl=approxkl.detach(),
                policykl=policykl.detach(),
                clipfrac=pg_clipfrac.detach(),
                advantages=advantages.detach(),
                advantages_mean=masked_mean(advantages, mask).detach(),
                ratio=ratio.detach(),
            ),
            returns=dict(mean=return_mean.detach(), var=return_var.detach()),
            val=dict(
                vpred=masked_mean(vpreds, mask).detach(),
                error=masked_mean((vpreds - returns) ** 2, mask).detach(),
                clipfrac=vf_clipfrac.detach(),
                mean=value_mean.detach(),
                var=value_var.detach(),
            ),
        )
        return pg_loss, self.config.vf_coef * vf_loss, flatten_dict(stats)

    def record_step_stats(self, kl_coef: float, **data):
        """
        Record training step statistics.


        Args:
            kl_coef (`float`):
                KL coefficient
            data (`dict`):
                Dictionary of training step data

        Returns:
            stats (`dict`):
                Dictionary of training step statistics
        """
        mask = data.pop("masks")

        kl_list = ((data["logprobs"] - data["ref_logprobs"]) * mask).sum(axis=-1)
        mean_kl = kl_list.mean()
        mean_entropy = (-data["logprobs"] * mask).sum(axis=-1).mean()

        mean_non_score_reward = masked_mean(
            data["non_score_reward"], mask
        )  # non_score_reward is size `batch_size`, `response_length`
        mean_scores = data["scores"].mean()  # scores is size `batch_size`
        std_scores = data["scores"].std()

        if mean_kl.item() < -1.0:
            # warn users
            warnings.warn(
                f"KL divergence is starting to become negative: {mean_kl.item():.2f} - this might be a precursor for failed training."
                " sometimes this happens because the generation kwargs are not correctly set. Please make sure"
                " that the generation kwargs are set correctly, or review your training hyperparameters."
            )

        stats = {
            "objective/kl": mean_kl,
            "objective/kl_dist": kl_list,
            "objective/logprobs": data["logprobs"],
            "objective/ref_logprobs": data["ref_logprobs"],
            "objective/kl_coef": kl_coef,
            "objective/entropy": mean_entropy,
            "ppo/mean_non_score_reward": mean_non_score_reward,
            "ppo/mean_scores": mean_scores,
            "ppo/std_scores": std_scores,
        }

        # Log text properties
        query_lens = torch.tensor([len(query) for query in data["queries"]], dtype=torch.float)
        response_lens = torch.tensor([len(response) for response in data["responses"]], dtype=torch.float)

        stats["tokens/queries_len_mean"] = torch.mean(query_lens).cpu().numpy().item()
        stats["tokens/queries_len_std"] = torch.std(query_lens).cpu().numpy().item()
        stats["tokens/queries_dist"] = query_lens.cpu().numpy()
        stats["tokens/responses_len_mean"] = torch.mean(response_lens).cpu().numpy().item()
        stats["tokens/responses_len_std"] = torch.std(response_lens).cpu().numpy().item()
        stats["tokens/responses_dist"] = response_lens.cpu().numpy()

        for k, v in data["train_stats"].items():
            stats[f"ppo/{k}"] = torch.mean(v, axis=0)
        stats["ppo/val/var_explained"] = 1 - stats["ppo/val/error"] / stats["ppo/returns/var"]
        return stats

    def log_stats(
        self,
        stats: dict,
        batch: dict,
        rewards: List[torch.FloatTensor],
        columns_to_log: List[str] = ["query", "response"],
    ):
        """
        A function that logs all the training stats. Call it at the end of each epoch.

        Args:
            stats (dict[str, Any]):
                A dictionary of training stats.
            batch (dict[str, Any]):
                A dictionary of batch data, this contains the queries and responses.
            rewards (`List[torch.FloatTensor]`):
                A tensor of rewards.
        """
        # Log only if we are in the main process
        if self.accelerator.is_main_process:
            logs = {}

            # Log stats
            if not isinstance(rewards, torch.Tensor):
                rewards = torch.tensor(rewards).to(self.current_device)

            if "query" not in batch.keys() and "response" not in batch.keys():
                # warn the user that the game logs will not be logged
                warnings.warn(
                    "The game logs will not be logged because the batch does not contain the keys 'query' and "
                    "'response'. "
                )
            elif self.config.log_with == "wandb":
                import wandb

                if any([column_to_log not in batch.keys() for column_to_log in columns_to_log]):
                    raise ValueError(f"Columns to log {columns_to_log} are not present in the batch {batch.keys()}.")

                batch_list = [batch[column_to_log] for column_to_log in columns_to_log]

                table_rows = [list(r) for r in zip(*batch_list, rewards.cpu().tolist())]
                logs.update({"game_log": wandb.Table(columns=[*columns_to_log, "reward"], rows=table_rows)})
            # All reduce rewards if distributed
            if self.is_distributed:
                import torch.distributed as dist

                dist.barrier()

                dist.all_reduce(rewards, op=torch.distributed.ReduceOp.SUM)
                rewards /= self.accelerator.num_processes

            logs.update(stats)

            # manually cast in fp32 for bf16 torch tensors
            for k, v in logs.items():
                if isinstance(v, torch.Tensor) and v.dtype == torch.bfloat16:
                    logs[k] = v.float()

            logs["env/reward_mean"] = torch.mean(rewards).cpu().numpy().item()
            logs["env/reward_std"] = torch.std(rewards).cpu().numpy().item()
            logs["env/reward_dist"] = rewards.cpu().numpy()

            if self.config.log_with == "tensorboard":
                # update the current step
                self.current_step += 1

            self.accelerator.log(
                logs,
                step=self.current_step if self.config.log_with == "tensorboard" else None,
            )

        else:
            if self.is_distributed:
                import torch.distributed as dist

                if not isinstance(rewards, torch.Tensor):
                    rewards = torch.tensor(rewards).to(self.current_device)

                dist.barrier()
                dist.all_reduce(rewards, op=torch.distributed.ReduceOp.SUM)

    def create_model_card(self, path: str, model_name: Optional[str] = "TRL Model") -> None:
        """Creates and saves a model card for a TRL model.

        Args:
            path (`str`): The path to save the model card to.
            model_name (`str`, *optional*): The name of the model, defaults to `TRL Model`.
        """
        try:
            user = whoami()["name"]
        # handle the offline case
        except:  # noqa
            warnings.warn("Cannot retrieve user information assuming you are running in offline mode.")
            return

        if not os.path.exists(path):
            os.makedirs(path)

        model_card_content = MODEL_CARD_TEMPLATE.format(model_name=model_name, model_id=f"{user}/{path}")
        with open(os.path.join(path, "README.md"), "w", encoding="utf-8") as f:
            f.write(model_card_content)

    def _save_pretrained(self, save_directory: str) -> None:
        self.accelerator.unwrap_model(self.model).save_pretrained(save_directory)
        self.tokenizer.save_pretrained(save_directory)
        self.create_model_card(save_directory)

    def _show_tokens(self, tokens, masks):
        from rich import print
        from rich.text import Text

        text = Text()

        for i, (token, mask) in enumerate(zip(tokens, masks)):
            if mask == 1:
                text.append(self.tokenizer.decode(token.item()), style="black on deep_sky_blue1")
                text.append(" ")
            else:
                text.append(self.tokenizer.decode(token.item()), style="black on cyan3")
                text.append(" ")
        print(text)

    def _prepare_deepspeed(self, model: PreTrainedModelWrapper):
        # Adapted from accelerate: https://github.com/huggingface/accelerate/blob/739b135f8367becb67ffaada12fe76e3aa60fefd/src/accelerate/accelerator.py#L1473
        deepspeed_plugin = self.accelerator.state.deepspeed_plugin
        config_kwargs = deepspeed_plugin.deepspeed_config
        if model is not None:
            if hasattr(model, "config"):
                hidden_size = (
                    max(model.config.hidden_sizes)
                    if getattr(model.config, "hidden_sizes", None)
                    else getattr(model.config, "hidden_size", None)
                )
                if hidden_size is not None and config_kwargs["zero_optimization"]["stage"] == 3:
                    # Note that `stage3_prefetch_bucket_size` can produce DeepSpeed messages like: `Invalidate trace cache @ step 0: expected module 1, but got module 0`
                    # This is expected and is not an error, see: https://github.com/microsoft/DeepSpeed/discussions/4081
                    config_kwargs.update(
                        {
                            "zero_optimization.reduce_bucket_size": hidden_size * hidden_size,
                            "zero_optimization.stage3_param_persistence_threshold": 10 * hidden_size,
                            "zero_optimization.stage3_prefetch_bucket_size": 0.9 * hidden_size * hidden_size,
                        }
                    )

        # If ZeRO-3 is used, we shard both the active and reference model.
        # Otherwise, we assume the reference model fits in memory and is initialized on each device with ZeRO disabled (stage 0)
        if config_kwargs["zero_optimization"]["stage"] != 3:
            config_kwargs["zero_optimization"]["stage"] = 0
        model, *_ = deepspeed.initialize(model=model, config=config_kwargs)
        model.eval()
        return model

File Path: trl/trainer/ddpo_config.py
Content:
import warnings
from dataclasses import dataclass, field
from typing import Optional

from ..core import flatten_dict
from ..import_utils import is_bitsandbytes_available, is_torchvision_available


@dataclass
class DDPOConfig(object):
    """
    Configuration class for DDPOTrainer
    """

    run_name: Optional[str] = field(
        default="",
        metadata={"help": "Run name for wandb logging and checkpoint saving."},
    )
    seed: Optional[int] = field(default=42, metadata={"help": "Random seed for reproducibility."})
    logdir: Optional[str] = field(
        default="logs",
        metadata={"help": "Top-level logging directory for checkpoint saving."},
    )
    log_with: Optional[str] = field(
        default=None,
        metadata={
            "help": "Log with either 'wandb' or 'tensorboard', check  https://huggingface.co/docs/accelerate/usage_guides/tracking for more details"
        },
    )
    tracker_kwargs: Optional[dict] = field(
        default_factory=dict,
        metadata={"help": "Keyword arguments for the tracker (e.g. wandb_project)"},
    )
    accelerator_kwargs: Optional[dict] = field(
        default_factory=dict,
        metadata={"help": "Keyword arguments for the accelerator"},
    )
    project_kwargs: Optional[dict] = field(
        default_factory=dict,
        metadata={"help": "Keyword arguments for the accelerator project config (e.g. `logging_dir`)"},
    )
    tracker_project_name: Optional[str] = field(
        default="trl", metadata={"help": "Name of project to use for tracking"}
    )
    num_epochs: Optional[int] = field(default=100, metadata={"help": "Number of epochs to train."})
    save_freq: Optional[int] = field(
        default=1,
        metadata={"help": "Number of epochs between saving model checkpoints."},
    )
    num_checkpoint_limit: Optional[int] = field(
        default=5,
        metadata={"help": "Number of checkpoints to keep before overwriting old ones."},
    )
    mixed_precision: Optional[str] = field(default="fp16", metadata={"help": "Mixed precision training."})
    allow_tf32: Optional[bool] = field(default=True, metadata={"help": "Allow tf32 on Ampere GPUs."})
    resume_from: Optional[str] = field(default="", metadata={"help": "Resume training from a checkpoint."})
    sample_num_steps: Optional[int] = field(default=50, metadata={"help": "Number of sampler inference steps."})
    sample_eta: Optional[float] = field(default=1.0, metadata={"help": "Eta parameter for the DDIM sampler."})
    sample_guidance_scale: Optional[float] = field(default=5.0, metadata={"help": "Classifier-free guidance weight."})
    sample_batch_size: Optional[int] = field(
        default=1, metadata={"help": "Batch size (per GPU!) to use for sampling."}
    )
    sample_num_batches_per_epoch: Optional[int] = field(
        default=2, metadata={"help": "Number of batches to sample per epoch."}
    )
    train_batch_size: Optional[int] = field(default=1, metadata={"help": "Batch size (per GPU!) to use for training."})
    train_use_8bit_adam: Optional[bool] = field(
        default=False,
        metadata={"help": "Whether to use the 8bit Adam optimizer from bitsandbytes."},
    )
    train_learning_rate: Optional[float] = field(default=3e-4, metadata={"help": "Learning rate."})
    train_adam_beta1: Optional[float] = field(default=0.9, metadata={"help": "Adam beta1."})
    train_adam_beta2: Optional[float] = field(default=0.999, metadata={"help": "Adam beta2."})
    train_adam_weight_decay: Optional[float] = field(default=1e-4, metadata={"help": "Adam weight decay."})
    train_adam_epsilon: Optional[float] = field(default=1e-8, metadata={"help": "Adam epsilon."})
    train_gradient_accumulation_steps: Optional[int] = field(
        default=1, metadata={"help": "Number of gradient accumulation steps."}
    )
    train_max_grad_norm: Optional[float] = field(
        default=1.0, metadata={"help": "Maximum gradient norm for gradient clipping."}
    )
    train_num_inner_epochs: Optional[int] = field(
        default=1, metadata={"help": "Number of inner epochs per outer epoch."}
    )
    train_cfg: Optional[bool] = field(
        default=True,
        metadata={"help": "Whether or not to use classifier-free guidance during training."},
    )
    train_adv_clip_max: Optional[float] = field(default=5, metadata={"help": "Clip advantages to the range."})
    train_clip_range: Optional[float] = field(default=1e-4, metadata={"help": "The PPO clip range."})
    train_timestep_fraction: Optional[float] = field(
        default=1.0, metadata={"help": "The fraction of timesteps to train on."}
    )

    per_prompt_stat_tracking: Optional[bool] = field(
        default=False,
        metadata={"help": "Whether to track statistics for each prompt separately."},
    )

    per_prompt_stat_tracking_buffer_size: Optional[int] = field(
        default=16,
        metadata={"help": "Number of reward values to store in the buffer for each prompt."},
    )
    per_prompt_stat_tracking_min_count: Optional[int] = field(
        default=16,
        metadata={"help": "The minimum number of reward values to store in the buffer."},
    )
    async_reward_computation: Optional[bool] = field(
        default=False,
        metadata={"help": "Whether to compute rewards asynchronously."},
    )
    max_workers: Optional[int] = field(
        default=2,
        metadata={"help": "The maximum number of workers to use for async reward computation."},
    )
    negative_prompts: Optional[str] = field(
        default="",
        metadata={"help": "Comma-separated list of prompts to use as negative examples."},
    )

    def to_dict(self):
        output_dict = {}
        for key, value in self.__dict__.items():
            output_dict[key] = value
        return flatten_dict(output_dict)

    def __post_init__(self):
        if self.log_with not in ["wandb", "tensorboard"]:
            warnings.warn(
                ("Accelerator tracking only supports image logging if `log_with` is set to 'wandb' or 'tensorboard'.")
            )

        if self.log_with == "wandb" and not is_torchvision_available():
            warnings.warn("Wandb image logging requires torchvision to be installed")

        if self.train_use_8bit_adam and not is_bitsandbytes_available():
            raise ImportError(
                "You need to install bitsandbytes to use 8bit Adam. "
                "You can install it with `pip install bitsandbytes`."
            )

File Path: trl/trainer/ddpo_trainer.py
Content:
# Copyright 2023 DDPO-pytorch authors (Kevin Black), metric-space, The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
from collections import defaultdict
from concurrent import futures
from typing import Any, Callable, Optional, Tuple
from warnings import warn

import torch
from accelerate import Accelerator
from accelerate.logging import get_logger
from accelerate.utils import ProjectConfiguration, set_seed

from ..models import DDPOStableDiffusionPipeline
from . import BaseTrainer, DDPOConfig
from .utils import PerPromptStatTracker


logger = get_logger(__name__)


class DDPOTrainer(BaseTrainer):
    """
    The DDPOTrainer uses Deep Diffusion Policy Optimization to optimise diffusion models.
    Note, this trainer is heavily inspired by the work here: https://github.com/kvablack/ddpo-pytorch
    As of now only Stable Diffusion based pipelines are supported

    Attributes:
        **config** (`DDPOConfig`) -- Configuration object for DDPOTrainer. Check the documentation of `PPOConfig` for more
         details.
        **reward_function** (Callable[[torch.Tensor, Tuple[str], Tuple[Any]], torch.Tensor]) -- Reward function to be used
        **prompt_function** (Callable[[], Tuple[str, Any]]) -- Function to generate prompts to guide model
        **sd_pipeline** (`DDPOStableDiffusionPipeline`) -- Stable Diffusion pipeline to be used for training.
        **image_samples_hook** (Optional[Callable[[Any, Any, Any], Any]]) -- Hook to be called to log images
    """

    def __init__(
        self,
        config: DDPOConfig,
        reward_function: Callable[[torch.Tensor, Tuple[str], Tuple[Any]], torch.Tensor],
        prompt_function: Callable[[], Tuple[str, Any]],
        sd_pipeline: DDPOStableDiffusionPipeline,
        image_samples_hook: Optional[Callable[[Any, Any, Any], Any]] = None,
    ):
        if image_samples_hook is None:
            warn("No image_samples_hook provided; no images will be logged")

        self.prompt_fn = prompt_function
        self.reward_fn = reward_function
        self.config = config
        self.image_samples_callback = image_samples_hook

        accelerator_project_config = ProjectConfiguration(**self.config.project_kwargs)

        if self.config.resume_from:
            self.config.resume_from = os.path.normpath(os.path.expanduser(self.config.resume_from))
            if "checkpoint_" not in os.path.basename(self.config.resume_from):
                # get the most recent checkpoint in this directory
                checkpoints = list(
                    filter(
                        lambda x: "checkpoint_" in x,
                        os.listdir(self.config.resume_from),
                    )
                )
                if len(checkpoints) == 0:
                    raise ValueError(f"No checkpoints found in {self.config.resume_from}")
                checkpoint_numbers = sorted([int(x.split("_")[-1]) for x in checkpoints])
                self.config.resume_from = os.path.join(
                    self.config.resume_from,
                    f"checkpoint_{checkpoint_numbers[-1]}",
                )

                accelerator_project_config.iteration = checkpoint_numbers[-1] + 1

        # number of timesteps within each trajectory to train on
        self.num_train_timesteps = int(self.config.sample_num_steps * self.config.train_timestep_fraction)

        self.accelerator = Accelerator(
            log_with=self.config.log_with,
            mixed_precision=self.config.mixed_precision,
            project_config=accelerator_project_config,
            # we always accumulate gradients across timesteps; we want config.train.gradient_accumulation_steps to be the
            # number of *samples* we accumulate across, so we need to multiply by the number of training timesteps to get
            # the total number of optimizer steps to accumulate across.
            gradient_accumulation_steps=self.config.train_gradient_accumulation_steps * self.num_train_timesteps,
            **self.config.accelerator_kwargs,
        )

        is_okay, message = self._config_check()
        if not is_okay:
            raise ValueError(message)

        is_using_tensorboard = config.log_with is not None and config.log_with == "tensorboard"

        if self.accelerator.is_main_process:
            self.accelerator.init_trackers(
                self.config.tracker_project_name,
                config=dict(ddpo_trainer_config=config.to_dict()) if not is_using_tensorboard else config.to_dict(),
                init_kwargs=self.config.tracker_kwargs,
            )

        logger.info(f"\n{config}")

        set_seed(self.config.seed, device_specific=True)

        self.sd_pipeline = sd_pipeline

        self.sd_pipeline.set_progress_bar_config(
            position=1,
            disable=not self.accelerator.is_local_main_process,
            leave=False,
            desc="Timestep",
            dynamic_ncols=True,
        )

        # For mixed precision training we cast all non-trainable weights (vae, non-lora text_encoder and non-lora unet) to half-precision
        # as these weights are only used for inference, keeping weights in full precision is not required.
        if self.accelerator.mixed_precision == "fp16":
            inference_dtype = torch.float16
        elif self.accelerator.mixed_precision == "bf16":
            inference_dtype = torch.bfloat16
        else:
            inference_dtype = torch.float32

        self.sd_pipeline.vae.to(self.accelerator.device, dtype=inference_dtype)
        self.sd_pipeline.text_encoder.to(self.accelerator.device, dtype=inference_dtype)
        self.sd_pipeline.unet.to(self.accelerator.device, dtype=inference_dtype)

        trainable_layers = self.sd_pipeline.get_trainable_layers()

        self.accelerator.register_save_state_pre_hook(self._save_model_hook)
        self.accelerator.register_load_state_pre_hook(self._load_model_hook)

        # Enable TF32 for faster training on Ampere GPUs,
        # cf https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices
        if self.config.allow_tf32:
            torch.backends.cuda.matmul.allow_tf32 = True

        self.optimizer = self._setup_optimizer(trainable_layers.parameters())

        self.neg_prompt_embed = self.sd_pipeline.text_encoder(
            self.sd_pipeline.tokenizer(
                [""] if self.config.negative_prompts is None else self.config.negative_prompts,
                return_tensors="pt",
                padding="max_length",
                truncation=True,
                max_length=self.sd_pipeline.tokenizer.model_max_length,
            ).input_ids.to(self.accelerator.device)
        )[0]

        if config.per_prompt_stat_tracking:
            self.stat_tracker = PerPromptStatTracker(
                config.per_prompt_stat_tracking_buffer_size,
                config.per_prompt_stat_tracking_min_count,
            )

        # NOTE: for some reason, autocast is necessary for non-lora training but for lora training it isn't necessary and it uses
        # more memory
        self.autocast = self.sd_pipeline.autocast or self.accelerator.autocast

        self.trainable_layers, self.optimizer = self.accelerator.prepare(trainable_layers, self.optimizer)

        if self.config.async_reward_computation:
            self.executor = futures.ThreadPoolExecutor(max_workers=config.max_workers)

        if config.resume_from:
            logger.info(f"Resuming from {config.resume_from}")
            self.accelerator.load_state(config.resume_from)
            self.first_epoch = int(config.resume_from.split("_")[-1]) + 1
        else:
            self.first_epoch = 0

    def compute_rewards(self, prompt_image_pairs, is_async=False):
        if not is_async:
            rewards = []
            for images, prompts, prompt_metadata in prompt_image_pairs:
                reward, reward_metadata = self.reward_fn(images, prompts, prompt_metadata)
                rewards.append(
                    (
                        torch.as_tensor(reward, device=self.accelerator.device),
                        reward_metadata,
                    )
                )
        else:
            rewards = self.executor.map(lambda x: self.reward_fn(*x), prompt_image_pairs)
            rewards = [
                (torch.as_tensor(reward.result(), device=self.accelerator.device), reward_metadata.result())
                for reward, reward_metadata in rewards
            ]

        return zip(*rewards)

    def step(self, epoch: int, global_step: int):
        """
        Perform a single step of training.

        Args:
            epoch (int): The current epoch.
            global_step (int): The current global step.

        Side Effects:
            - Model weights are updated
            - Logs the statistics to the accelerator trackers.
            - If `self.image_samples_callback` is not None, it will be called with the prompt_image_pairs, global_step, and the accelerator tracker.

        Returns:
            global_step (int): The updated global step.

        """
        samples, prompt_image_data = self._generate_samples(
            iterations=self.config.sample_num_batches_per_epoch,
            batch_size=self.config.sample_batch_size,
        )

        # collate samples into dict where each entry has shape (num_batches_per_epoch * sample.batch_size, ...)
        samples = {k: torch.cat([s[k] for s in samples]) for k in samples[0].keys()}
        rewards, rewards_metadata = self.compute_rewards(
            prompt_image_data, is_async=self.config.async_reward_computation
        )

        for i, image_data in enumerate(prompt_image_data):
            image_data.extend([rewards[i], rewards_metadata[i]])

        if self.image_samples_callback is not None:
            self.image_samples_callback(prompt_image_data, global_step, self.accelerator.trackers[0])

        rewards = torch.cat(rewards)
        rewards = self.accelerator.gather(rewards).cpu().numpy()

        self.accelerator.log(
            {
                "reward": rewards,
                "epoch": epoch,
                "reward_mean": rewards.mean(),
                "reward_std": rewards.std(),
            },
            step=global_step,
        )

        if self.config.per_prompt_stat_tracking:
            # gather the prompts across processes
            prompt_ids = self.accelerator.gather(samples["prompt_ids"]).cpu().numpy()
            prompts = self.sd_pipeline.tokenizer.batch_decode(prompt_ids, skip_special_tokens=True)
            advantages = self.stat_tracker.update(prompts, rewards)
        else:
            advantages = (rewards - rewards.mean()) / (rewards.std() + 1e-8)

        # ungather advantages;  keep the entries corresponding to the samples on this process
        samples["advantages"] = (
            torch.as_tensor(advantages)
            .reshape(self.accelerator.num_processes, -1)[self.accelerator.process_index]
            .to(self.accelerator.device)
        )

        del samples["prompt_ids"]

        total_batch_size, num_timesteps = samples["timesteps"].shape

        for inner_epoch in range(self.config.train_num_inner_epochs):
            # shuffle samples along batch dimension
            perm = torch.randperm(total_batch_size, device=self.accelerator.device)
            samples = {k: v[perm] for k, v in samples.items()}

            # shuffle along time dimension independently for each sample
            # still trying to understand the code below
            perms = torch.stack(
                [torch.randperm(num_timesteps, device=self.accelerator.device) for _ in range(total_batch_size)]
            )

            for key in ["timesteps", "latents", "next_latents", "log_probs"]:
                samples[key] = samples[key][
                    torch.arange(total_batch_size, device=self.accelerator.device)[:, None],
                    perms,
                ]

            original_keys = samples.keys()
            original_values = samples.values()
            # rebatch them as user defined train_batch_size is different from sample_batch_size
            reshaped_values = [v.reshape(-1, self.config.train_batch_size, *v.shape[1:]) for v in original_values]

            # Transpose the list of original values
            transposed_values = zip(*reshaped_values)
            # Create new dictionaries for each row of transposed values
            samples_batched = [dict(zip(original_keys, row_values)) for row_values in transposed_values]

            self.sd_pipeline.unet.train()
            global_step = self._train_batched_samples(inner_epoch, epoch, global_step, samples_batched)
            # ensure optimization step at the end of the inner epoch
            if not self.accelerator.sync_gradients:
                raise ValueError(
                    "Optimization step should have been performed by this point. Please check calculated gradient accumulation settings."
                )

        if epoch != 0 and epoch % self.config.save_freq == 0 and self.accelerator.is_main_process:
            self.accelerator.save_state()

        return global_step

    def calculate_loss(self, latents, timesteps, next_latents, log_probs, advantages, embeds):
        """
        Calculate the loss for a batch of an unpacked sample

        Args:
            latents (torch.Tensor):
                The latents sampled from the diffusion model, shape: [batch_size, num_steps, ...]
            timesteps (torch.Tensor):
                The timesteps sampled from the diffusion model, shape: [batch_size]
            next_latents (torch.Tensor):
                The next latents sampled from the diffusion model, shape: [batch_size, num_steps, ...]
            log_probs (torch.Tensor):
                The log probabilities of the latents, shape: [batch_size]
            advantages (torch.Tensor):
                The advantages of the latents, shape: [batch_size]
            embeds (torch.Tensor):
                The embeddings of the prompts, shape: [2*batch_size or batch_size, ...]
                Note: the "or" is because if train_cfg is True, the expectation is that negative prompts are concatenated to the embeds

        Returns:
            loss (torch.Tensor), approx_kl (torch.Tensor), clipfrac (torch.Tensor)
            (all of these are of shape (1,))
        """
        with self.autocast():
            if self.config.train_cfg:
                noise_pred = self.sd_pipeline.unet(
                    torch.cat([latents] * 2),
                    torch.cat([timesteps] * 2),
                    embeds,
                ).sample
                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
                noise_pred = noise_pred_uncond + self.config.sample_guidance_scale * (
                    noise_pred_text - noise_pred_uncond
                )
            else:
                noise_pred = self.sd_pipeline.unet(
                    latents,
                    timesteps,
                    embeds,
                ).sample
            # compute the log prob of next_latents given latents under the current model

            scheduler_step_output = self.sd_pipeline.scheduler_step(
                noise_pred,
                timesteps,
                latents,
                eta=self.config.sample_eta,
                prev_sample=next_latents,
            )

            log_prob = scheduler_step_output.log_probs

        advantages = torch.clamp(
            advantages,
            -self.config.train_adv_clip_max,
            self.config.train_adv_clip_max,
        )

        ratio = torch.exp(log_prob - log_probs)

        loss = self.loss(advantages, self.config.train_clip_range, ratio)

        approx_kl = 0.5 * torch.mean((log_prob - log_probs) ** 2)

        clipfrac = torch.mean((torch.abs(ratio - 1.0) > self.config.train_clip_range).float())

        return loss, approx_kl, clipfrac

    def loss(
        self,
        advantages: torch.Tensor,
        clip_range: float,
        ratio: torch.Tensor,
    ):
        unclipped_loss = -advantages * ratio
        clipped_loss = -advantages * torch.clamp(
            ratio,
            1.0 - clip_range,
            1.0 + clip_range,
        )
        return torch.mean(torch.maximum(unclipped_loss, clipped_loss))

    def _setup_optimizer(self, trainable_layers_parameters):
        if self.config.train_use_8bit_adam:
            import bitsandbytes

            optimizer_cls = bitsandbytes.optim.AdamW8bit
        else:
            optimizer_cls = torch.optim.AdamW

        return optimizer_cls(
            trainable_layers_parameters,
            lr=self.config.train_learning_rate,
            betas=(self.config.train_adam_beta1, self.config.train_adam_beta2),
            weight_decay=self.config.train_adam_weight_decay,
            eps=self.config.train_adam_epsilon,
        )

    def _save_model_hook(self, models, weights, output_dir):
        self.sd_pipeline.save_checkpoint(models, weights, output_dir)
        weights.pop()  # ensures that accelerate doesn't try to handle saving of the model

    def _load_model_hook(self, models, input_dir):
        self.sd_pipeline.load_checkpoint(models, input_dir)
        models.pop()  # ensures that accelerate doesn't try to handle loading of the model

    def _generate_samples(self, iterations, batch_size):
        """
        Generate samples from the model

        Args:
            iterations (int): Number of iterations to generate samples for
            batch_size (int): Batch size to use for sampling

        Returns:
            samples (List[Dict[str, torch.Tensor]]), prompt_image_pairs (List[List[Any]])
        """
        samples = []
        prompt_image_pairs = []
        self.sd_pipeline.unet.eval()

        sample_neg_prompt_embeds = self.neg_prompt_embed.repeat(batch_size, 1, 1)

        for _ in range(iterations):
            prompts, prompt_metadata = zip(*[self.prompt_fn() for _ in range(batch_size)])

            prompt_ids = self.sd_pipeline.tokenizer(
                prompts,
                return_tensors="pt",
                padding="max_length",
                truncation=True,
                max_length=self.sd_pipeline.tokenizer.model_max_length,
            ).input_ids.to(self.accelerator.device)
            prompt_embeds = self.sd_pipeline.text_encoder(prompt_ids)[0]

            with self.autocast():
                sd_output = self.sd_pipeline(
                    prompt_embeds=prompt_embeds,
                    negative_prompt_embeds=sample_neg_prompt_embeds,
                    num_inference_steps=self.config.sample_num_steps,
                    guidance_scale=self.config.sample_guidance_scale,
                    eta=self.config.sample_eta,
                    output_type="pt",
                )

                images = sd_output.images
                latents = sd_output.latents
                log_probs = sd_output.log_probs

            latents = torch.stack(latents, dim=1)  # (batch_size, num_steps + 1, ...)
            log_probs = torch.stack(log_probs, dim=1)  # (batch_size, num_steps, 1)
            timesteps = self.sd_pipeline.scheduler.timesteps.repeat(batch_size, 1)  # (batch_size, num_steps)

            samples.append(
                {
                    "prompt_ids": prompt_ids,
                    "prompt_embeds": prompt_embeds,
                    "timesteps": timesteps,
                    "latents": latents[:, :-1],  # each entry is the latent before timestep t
                    "next_latents": latents[:, 1:],  # each entry is the latent after timestep t
                    "log_probs": log_probs,
                    "negative_prompt_embeds": sample_neg_prompt_embeds,
                }
            )
            prompt_image_pairs.append([images, prompts, prompt_metadata])

        return samples, prompt_image_pairs

    def _train_batched_samples(self, inner_epoch, epoch, global_step, batched_samples):
        """
        Train on a batch of samples. Main training segment

        Args:
            inner_epoch (int): The current inner epoch
            epoch (int): The current epoch
            global_step (int): The current global step
            batched_samples (List[Dict[str, torch.Tensor]]): The batched samples to train on

        Side Effects:
            - Model weights are updated
            - Logs the statistics to the accelerator trackers.

        Returns:
            global_step (int): The updated global step
        """
        info = defaultdict(list)
        for i, sample in enumerate(batched_samples):
            if self.config.train_cfg:
                # concat negative prompts to sample prompts to avoid two forward passes
                embeds = torch.cat([sample["negative_prompt_embeds"], sample["prompt_embeds"]])
            else:
                embeds = sample["prompt_embeds"]

            for j in range(self.num_train_timesteps):
                with self.accelerator.accumulate(self.sd_pipeline.unet):
                    loss, approx_kl, clipfrac = self.calculate_loss(
                        sample["latents"][:, j],
                        sample["timesteps"][:, j],
                        sample["next_latents"][:, j],
                        sample["log_probs"][:, j],
                        sample["advantages"],
                        embeds,
                    )
                    info["approx_kl"].append(approx_kl)
                    info["clipfrac"].append(clipfrac)
                    info["loss"].append(loss)

                    self.accelerator.backward(loss)
                    if self.accelerator.sync_gradients:
                        self.accelerator.clip_grad_norm_(
                            self.trainable_layers.parameters(),
                            self.config.train_max_grad_norm,
                        )
                    self.optimizer.step()
                    self.optimizer.zero_grad()

                # Checks if the accelerator has performed an optimization step behind the scenes
                if self.accelerator.sync_gradients:
                    # log training-related stuff
                    info = {k: torch.mean(torch.stack(v)) for k, v in info.items()}
                    info = self.accelerator.reduce(info, reduction="mean")
                    info.update({"epoch": epoch, "inner_epoch": inner_epoch})
                    self.accelerator.log(info, step=global_step)
                    global_step += 1
                    info = defaultdict(list)
        return global_step

    def _config_check(self) -> Tuple[bool, str]:
        samples_per_epoch = (
            self.config.sample_batch_size * self.accelerator.num_processes * self.config.sample_num_batches_per_epoch
        )
        total_train_batch_size = (
            self.config.train_batch_size
            * self.accelerator.num_processes
            * self.config.train_gradient_accumulation_steps
        )

        if not self.config.sample_batch_size >= self.config.train_batch_size:
            return (
                False,
                f"Sample batch size ({self.config.sample_batch_size}) must be greater than or equal to the train batch size ({self.config.train_batch_size})",
            )
        if not self.config.sample_batch_size % self.config.train_batch_size == 0:
            return (
                False,
                f"Sample batch size ({self.config.sample_batch_size}) must be divisible by the train batch size ({self.config.train_batch_size})",
            )
        if not samples_per_epoch % total_train_batch_size == 0:
            return (
                False,
                f"Number of samples per epoch ({samples_per_epoch}) must be divisible by the total train batch size ({total_train_batch_size})",
            )
        return True, ""

    def train(self, epochs: Optional[int] = None):
        """
        Train the model for a given number of epochs
        """
        global_step = 0
        if epochs is None:
            epochs = self.config.num_epochs
        for epoch in range(self.first_epoch, epochs):
            global_step = self.step(epoch, global_step)

    def _save_pretrained(self, save_directory):
        self.sd_pipeline.save_pretrained(save_directory)

File Path: trl/trainer/dpo_trainer.py
Content:
# DPO Authors: Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn 2023
# Copyright 2023 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import warnings
from collections import defaultdict
from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Union

import torch
import torch.nn as nn
import torch.nn.functional as F
from datasets import Dataset
from transformers import DataCollator, PreTrainedModel, PreTrainedTokenizerBase, Trainer, TrainingArguments
from transformers.trainer_callback import TrainerCallback

from ..import_utils import is_peft_available
from ..models import create_reference_model
from .utils import DPODataCollatorWithPadding, disable_dropout_in_model, pad_to_length


if is_peft_available():
    from peft import PeftModel, get_peft_model, prepare_model_for_kbit_training


class DPOTrainer(Trainer):
    r"""
    Initialize DPOTrainer.

    Args:
        model (`transformers.PreTrainedModel`):
            The model to train, preferably an `AutoModelForSequenceClassification`.
        ref_model (`PreTrainedModelWrapper`):
            Hugging Face transformer model with a casual language modelling head. Used for implicit reward computation and loss. If no
            reference model is provided, the trainer will create a reference model with the same architecture as the model to be optimized.
        beta (`float`, defaults to 0.1):
            The beta factor in DPO loss. Higher beta means less divergence from the initial policy.
        args (`transformers.TrainingArguments`):
            The arguments to use for training.
        data_collator (`transformers.DataCollator`):
            The data collator to use for training. If None is specified, the default data collator (`DPODataCollatorWithPadding`) will be used
            which will pad the sequences to the maximum length of the sequences in the batch, given a dataset of paired sequences.
        label_pad_token_id (`int`, defaults to `-100`):
            The label pad token id. This argument is required if you want to use the default data collator.
        padding_value (`int`, defaults to `0`):
            The padding value. This argument is required if you want to use the default data collator.
        truncation_mode (`str`, defaults to `keep_end`):
            The truncation mode to use, either `keep_end` or `keep_start`. This argument is required if you want to use the default data collator.
        train_dataset (`datasets.Dataset`):
            The dataset to use for training.
        eval_dataset (`datasets.Dataset`):
            The dataset to use for evaluation.
        tokenizer (`transformers.PreTrainedTokenizerBase`):
            The tokenizer to use for training. This argument is required if you want to use the default data collator.
        model_init (`Callable[[], transformers.PreTrainedModel]`):
            The model initializer to use for training. If None is specified, the default model initializer will be used.
        callbacks (`List[transformers.TrainerCallback]`):
            The callbacks to use for training.
        optimizers (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`):
            The optimizer and scheduler to use for training.
        preprocess_logits_for_metrics (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`):
            The function to use to preprocess the logits before computing the metrics.
        max_length (`int`, defaults to `None`):
            The maximum length of the sequences in the batch. This argument is required if you want to use the default data collator.
        max_prompt_length (`int`, defaults to `None`):
            The maximum length of the prompt. This argument is required if you want to use the default data collator.
        max_target_length (`int`, defaults to `None`):
            The maximum length of the target. This argument is required if you want to use the default data collator and your model is an encoder-decoder.
        peft_config (`Dict`, defaults to `None`):
            The PEFT configuration to use for training. If you pass a PEFT configuration, the model will be wrapped in a PEFT model.
        is_encoder_decoder (`Optional[bool]`, `optional`, defaults to `None`):
            If no model is provided, we need to know if the model_init returns an encoder-decoder.
        disable_dropout (`bool`, defaults to `True`):
            Whether or not to disable dropouts in `model` and `ref_model`.
    """

    def __init__(
        self,
        model: Union[PreTrainedModel, nn.Module] = None,
        ref_model: Optional[Union[PreTrainedModel, nn.Module]] = None,
        beta: float = 0.1,
        args: TrainingArguments = None,
        data_collator: Optional[DataCollator] = None,
        label_pad_token_id: int = -100,
        padding_value: int = 0,
        truncation_mode: str = "keep_end",
        train_dataset: Optional[Dataset] = None,
        eval_dataset: Optional[Union[Dataset, Dict[str, Dataset]]] = None,
        tokenizer: Optional[PreTrainedTokenizerBase] = None,
        model_init: Optional[Callable[[], PreTrainedModel]] = None,
        callbacks: Optional[List[TrainerCallback]] = None,
        optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (
            None,
            None,
        ),
        preprocess_logits_for_metrics: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None,
        max_length: Optional[int] = None,
        max_prompt_length: Optional[int] = None,
        max_target_length: Optional[int] = None,
        peft_config: Optional[Dict] = None,
        is_encoder_decoder: Optional[bool] = None,
        disable_dropout: bool = True,
    ):
        if not is_peft_available() and peft_config is not None:
            raise ValueError(
                "PEFT is not installed and you passed a `peft_config` in the trainer's kwargs, please install it to use the PEFT models"
            )
        elif is_peft_available() and peft_config is not None:
            if getattr(model, "is_loaded_in_8bit", False) or getattr(model, "is_loaded_in_4bit", False):
                model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=args.gradient_checkpointing)
            model = get_peft_model(model, peft_config)

        if model is not None:
            self.is_encoder_decoder = model.config.is_encoder_decoder
        elif is_encoder_decoder is None:
            raise ValueError("When no model is provided, you need to pass the parameter is_encoder_decoder.")
        else:
            self.is_encoder_decoder = is_encoder_decoder

        self.is_peft_model = is_peft_available() and isinstance(model, PeftModel)

        if ref_model:
            self.ref_model = ref_model
        elif self.is_peft_model:
            # The `model` with adapters turned off will be used as the reference model
            self.ref_model = None
        else:
            self.ref_model = create_reference_model(model)

        if data_collator is None:
            if tokenizer is None:
                raise ValueError(
                    "max_length or a tokenizer must be specified when using the default DPODataCollatorWithPadding"
                )
            if max_length is None:
                warnings.warn(
                    "When using DPODataCollatorWithPadding, you should set `max_length` in the DPOTrainer's init"
                    " it will be set to `512` by default, but you should do it yourself in the future.",
                    UserWarning,
                )
                max_length = 512
            if max_prompt_length is None:
                warnings.warn(
                    "When using DPODataCollatorWithPadding, you should set `max_prompt_length` in the DPOTrainer's init"
                    " it will be set to `128` by default, but you should do it yourself in the future.",
                    UserWarning,
                )
                max_prompt_length = 128

            if max_target_length is None and self.is_encoder_decoder:
                warnings.warn(
                    "When using DPODataCollatorWithPadding with an encoder decoder architecture, you should set `max_target_length` in the DPOTrainer's init"
                    " it will be set to `128` by default, but you should do it yourself in the future.",
                    UserWarning,
                )
                max_target_length = 128

            data_collator = DPODataCollatorWithPadding(
                tokenizer,
                max_length=max_length,
                max_prompt_length=max_prompt_length,
                label_pad_token_id=label_pad_token_id,
                padding_value=padding_value,
                truncation_mode=truncation_mode,
                is_encoder_decoder=self.is_encoder_decoder,
                max_target_length=max_target_length,
            )

            if args.remove_unused_columns:
                args.remove_unused_columns = False
                # warn users
                warnings.warn(
                    "When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments"
                    " we have set it for you, but you should do it yourself in the future.",
                    UserWarning,
                )

            self.use_dpo_data_collator = True
        else:
            self.use_dpo_data_collator = False

        if disable_dropout:
            disable_dropout_in_model(model)
            if self.ref_model is not None:
                disable_dropout_in_model(self.ref_model)

        self.label_pad_token_id = label_pad_token_id
        self.padding_value = padding_value

        self.beta = beta

        self._stored_metrics = defaultdict(lambda: defaultdict(list))

        super().__init__(
            model,
            args,
            data_collator,
            train_dataset,
            eval_dataset,
            tokenizer,
            model_init,
            None,
            callbacks,
            optimizers,
            preprocess_logits_for_metrics,
        )

        if not hasattr(self, "accelerator"):
            raise AttributeError(
                "Your `Trainer` does not have an `accelerator` object. Consider upgrading `transformers`."
            )

        if self.ref_model is None:
            if not hasattr(self.accelerator.unwrap_model(self.model), "disable_adapter"):
                raise ValueError(
                    "You are using a `peft` version that does not support `disable_adapter`. Please update your `peft` version to the latest version."
                )
        else:
            if self.is_deepspeed_enabled:
                # Read more about the issue in https://github.com/huggingface/trl/pull/687
                self.ref_model = self.accelerator._prepare_deepspeed(self.ref_model)[0]
                self.ref_model.eval()
            else:
                self.ref_model = self.accelerator.prepare_model(self.ref_model, evaluation_mode=True)

    def concatenated_inputs(self, batch: Dict[str, Union[List, torch.LongTensor]]) -> Dict[str, torch.LongTensor]:
        """Concatenate the chosen and rejected inputs into a single tensor.

        Args:
            batch: A batch of data. Must contain the keys 'chosen_input_ids' and 'rejected_input_ids', which are tensors of shape (batch_size, sequence_length).

        Returns:
            A dictionary containing the concatenated inputs under the key 'concatenated_input_ids'.
        """
        concatenated_batch = {}

        if self.is_encoder_decoder:
            max_length = max(batch["chosen_labels"].shape[1], batch["rejected_labels"].shape[1])
        else:
            max_length = max(batch["chosen_input_ids"].shape[1], batch["rejected_input_ids"].shape[1])

        for k in batch:
            if k.startswith("chosen") and isinstance(batch[k], torch.Tensor):
                pad_value = self.label_pad_token_id if "labels" in k or self.is_encoder_decoder else self.padding_value
                concatenated_key = k.replace("chosen", "concatenated")
                concatenated_batch[concatenated_key] = pad_to_length(batch[k], max_length, pad_value=pad_value)
        for k in batch:
            if k.startswith("rejected") and isinstance(batch[k], torch.Tensor):
                pad_value = self.label_pad_token_id if "labels" in k or self.is_encoder_decoder else self.padding_value
                concatenated_key = k.replace("rejected", "concatenated")
                concatenated_batch[concatenated_key] = torch.cat(
                    (
                        concatenated_batch[concatenated_key],
                        pad_to_length(batch[k], max_length, pad_value=pad_value),
                    ),
                    dim=0,
                ).to(self.accelerator.device)

        if self.is_encoder_decoder:
            concatenated_batch["concatenated_input_ids"] = batch["prompt_input_ids"].repeat(2, 1)
            concatenated_batch["concatenated_attention_mask"] = batch["prompt_attention_mask"].repeat(2, 1)

        return concatenated_batch

    def dpo_loss(
        self,
        policy_chosen_logps: torch.FloatTensor,
        policy_rejected_logps: torch.FloatTensor,
        reference_chosen_logps: torch.FloatTensor,
        reference_rejected_logps: torch.FloatTensor,
        reference_free: bool = False,
    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:
        """Compute the DPO loss for a batch of policy and reference model log probabilities.

        Args:
            policy_chosen_logps: Log probabilities of the policy model for the chosen responses. Shape: (batch_size,)
            policy_rejected_logps: Log probabilities of the policy model for the rejected responses. Shape: (batch_size,)
            reference_chosen_logps: Log probabilities of the reference model for the chosen responses. Shape: (batch_size,)
            reference_rejected_logps: Log probabilities of the reference model for the rejected responses. Shape: (batch_size,)
            beta: Temperature parameter for the DPO loss, typically something in the range of 0.1 to 0.5. We ignore the reference model as beta -> 0.
            reference_free: If True, we ignore the _provided_ reference model and implicitly use a reference model that assigns equal probability to all responses.

        Returns:
            A tuple of three tensors: (losses, chosen_rewards, rejected_rewards).
            The losses tensor contains the DPO loss for each example in the batch.
            The chosen_rewards and rejected_rewards tensors contain the rewards for the chosen and rejected responses, respectively.
        """
        pi_logratios = policy_chosen_logps - policy_rejected_logps
        ref_logratios = reference_chosen_logps - reference_rejected_logps

        if reference_free:
            ref_logratios = 0

        logits = pi_logratios - ref_logratios

        losses = -F.logsigmoid(self.beta * logits)
        chosen_rewards = self.beta * (policy_chosen_logps - reference_chosen_logps).detach()
        rejected_rewards = self.beta * (policy_rejected_logps - reference_rejected_logps).detach()

        return losses, chosen_rewards, rejected_rewards

    def _get_batch_logps(
        self,
        logits: torch.FloatTensor,
        labels: torch.LongTensor,
        average_log_prob: bool = False,
    ) -> torch.FloatTensor:
        """Compute the log probabilities of the given labels under the given logits.

        Args:
            logits: Logits of the model (unnormalized). Shape: (batch_size, sequence_length, vocab_size)
            labels: Labels for which to compute the log probabilities. Label tokens with a value of label_pad_token_id are ignored. Shape: (batch_size, sequence_length)
            average_log_prob: If True, return the average log probability per (non-masked) token. Otherwise, return the sum of the log probabilities of the (non-masked) tokens.

        Returns:
            A tensor of shape (batch_size,) containing the average/sum log probabilities of the given labels under the given logits.
        """
        if logits.shape[:-1] != labels.shape:
            raise ValueError("Logits (batch and sequence length dim) and labels must have the same shape.")

        if not self.is_encoder_decoder:
            labels = labels[:, 1:].clone()
            logits = logits[:, :-1, :]
        loss_mask = labels != self.label_pad_token_id

        # dummy token; we'll ignore the losses on these tokens later
        labels[labels == self.label_pad_token_id] = 0

        per_token_logps = torch.gather(logits.log_softmax(-1), dim=2, index=labels.unsqueeze(2)).squeeze(2)

        if average_log_prob:
            return (per_token_logps * loss_mask).sum(-1) / loss_mask.sum(-1)
        else:
            return (per_token_logps * loss_mask).sum(-1)

    def concatenated_forward(
        self, model: nn.Module, batch: Dict[str, Union[List, torch.LongTensor]]
    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:
        """Run the given model on the given batch of inputs, concatenating the chosen and rejected inputs together.

        We do this to avoid doing two forward passes, because it's faster for FSDP.
        """
        concatenated_batch = self.concatenated_inputs(batch)
        len_chosen = batch["chosen_labels"].shape[0]

        model_kwargs = (
            {
                "labels": concatenated_batch["concatenated_labels"],
                "decoder_input_ids": concatenated_batch.pop("concatenated_decoder_input_ids", None),
            }
            if self.is_encoder_decoder
            else {}
        )
        all_logits = model(
            concatenated_batch["concatenated_input_ids"],
            attention_mask=concatenated_batch["concatenated_attention_mask"],
            **model_kwargs,
        ).logits.to(torch.float32)

        all_logps = self._get_batch_logps(
            all_logits,
            concatenated_batch["concatenated_labels"],
            average_log_prob=False,
        )

        chosen_logps = all_logps[:len_chosen]
        rejected_logps = all_logps[len_chosen:]

        chosen_logits = all_logits[:len_chosen]
        rejected_logits = all_logits[len_chosen:]

        return (chosen_logps, rejected_logps, chosen_logits, rejected_logits)

    def get_batch_metrics(
        self,
        model,
        batch: Dict[str, Union[List, torch.LongTensor]],
        train_eval: Literal["train", "eval"] = "train",
    ):
        """Compute the DPO loss and other metrics for the given batch of inputs for train or test."""
        metrics = {}

        (
            policy_chosen_logps,
            policy_rejected_logps,
            policy_chosen_logits,
            policy_rejected_logits,
        ) = self.concatenated_forward(model, batch)
        with torch.no_grad():
            if self.ref_model is None:
                with self.accelerator.unwrap_model(self.model).disable_adapter():
                    (
                        reference_chosen_logps,
                        reference_rejected_logps,
                        _,
                        _,
                    ) = self.concatenated_forward(self.model, batch)
            else:
                (
                    reference_chosen_logps,
                    reference_rejected_logps,
                    _,
                    _,
                ) = self.concatenated_forward(self.ref_model, batch)

        losses, chosen_rewards, rejected_rewards = self.dpo_loss(
            policy_chosen_logps,
            policy_rejected_logps,
            reference_chosen_logps,
            reference_rejected_logps,
        )
        reward_accuracies = (chosen_rewards > rejected_rewards).float()

        prefix = "eval_" if train_eval == "eval" else ""
        metrics[f"{prefix}rewards/chosen"] = chosen_rewards.cpu().numpy().mean()
        metrics[f"{prefix}rewards/rejected"] = rejected_rewards.cpu().numpy().mean()
        metrics[f"{prefix}rewards/accuracies"] = reward_accuracies.cpu().numpy().mean()
        metrics[f"{prefix}rewards/margins"] = (chosen_rewards - rejected_rewards).cpu().numpy().mean()
        metrics[f"{prefix}logps/rejected"] = policy_rejected_logps.detach().cpu().numpy().mean()
        metrics[f"{prefix}logps/chosen"] = policy_chosen_logps.detach().cpu().numpy().mean()
        metrics[f"{prefix}logits/rejected"] = policy_rejected_logits.detach().cpu().numpy().mean()
        metrics[f"{prefix}logits/chosen"] = policy_chosen_logits.detach().cpu().numpy().mean()

        return losses.mean(), metrics

    def compute_loss(
        self,
        model: Union[PreTrainedModel, nn.Module],
        inputs: Dict[str, Union[torch.Tensor, Any]],
        return_outputs=False,
    ) -> Union[torch.Tensor, Tuple[torch.Tensor, Dict[str, torch.Tensor]]]:
        if not self.use_dpo_data_collator:
            warnings.warn(
                "compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than "
                "DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator"
            )
        loss, metrics = self.get_batch_metrics(model, inputs, train_eval="train")

        # force log the metrics
        if self.accelerator.is_main_process:
            self.store_metrics(metrics, train_eval="train")

        if return_outputs:
            return (loss, metrics)
        return loss

    def get_batch_samples(self, model, batch: Dict[str, torch.LongTensor]) -> Tuple[str, str]:
        """Generate samples from the model and reference model for the given batch of inputs."""

        policy_output = model.generate(
            batch["prompt_input_ids"],
            attention_mask=batch["prompt_attention_mask"],
            max_length=self.config.max_length,
            do_sample=True,
            pad_token_id=self.tokenizer.pad_token_id,
        )

        if self.ref_model is None:
            with self.accelerator.unwrap_model(self.model).disable_adapter():
                reference_output = self.model.generate(
                    batch["prompt_input_ids"],
                    attention_mask=batch["prompt_attention_mask"],
                    max_length=self.config.max_length,
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id,
                )
        else:
            reference_output = self.ref_model.generate(
                batch["prompt_input_ids"],
                attention_mask=batch["prompt_attention_mask"],
                max_length=self.config.max_length,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
            )

        policy_output = pad_to_length(policy_output, self.config.max_length, self.tokenizer.pad_token_id)
        policy_output_decoded = self.tokenizer.batch_decode(policy_output, skip_special_tokens=True)

        reference_output = pad_to_length(reference_output, self.config.max_length, self.tokenizer.pad_token_id)
        reference_output_decoded = self.tokenizer.batch_decode(reference_output, skip_special_tokens=True)

        return policy_output_decoded, reference_output_decoded

    def prediction_step(
        self,
        model: Union[PreTrainedModel, nn.Module],
        inputs: Dict[str, Union[torch.Tensor, Any]],
        prediction_loss_only: bool,
        ignore_keys: Optional[List[str]] = None,
    ):
        if not self.use_dpo_data_collator:
            warnings.warn(
                "prediction_step is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than "
                "DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator"
            )
        if ignore_keys is None:
            if hasattr(model, "config"):
                ignore_keys = getattr(model.config, "keys_to_ignore_at_inference", [])
            else:
                ignore_keys = []

        with torch.no_grad():
            loss, metrics = self.get_batch_metrics(model, inputs, train_eval="eval")

        # force log the metrics
        if self.accelerator.is_main_process:
            self.store_metrics(metrics, train_eval="eval")

        if prediction_loss_only:
            return (loss.detach(), None, None)

        # logits for the chosen and rejected samples from model
        logits_dict = {
            "eval_logits/chosen": metrics["eval_logits/chosen"],
            "eval_logits/rejected": metrics["eval_logits/rejected"],
        }
        logits = tuple(v for k, v in logits_dict.items() if k not in ignore_keys)
        logits = torch.stack(logits).mean(axis=1)
        labels = torch.zeros(logits.shape[0])

        return (loss.detach(), logits, labels)

    def store_metrics(self, metrics: Dict[str, float], train_eval: Literal["train", "eval"] = "train") -> None:
        for key, value in metrics.items():
            self._stored_metrics[train_eval][key].append(value)

    def log(self, logs: Dict[str, float]) -> None:
        """
        Log `logs` on the various objects watching training, including stored metrics.

        Args:
            logs (`Dict[str, float]`):
                The values to log.
        """
        # logs either has 'loss' or 'eval_loss'
        train_eval = "train" if "loss" in logs else "eval"
        # Add averaged stored metrics to logs
        for key, metrics in self._stored_metrics[train_eval].items():
            logs[key] = torch.tensor(metrics).mean().item()
        del self._stored_metrics[train_eval]
        return super().log(logs)

File Path: trl/trainer/ppo_config.py
Content:
# Copyright 2022 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import os
import sys
import warnings
from dataclasses import dataclass, field
from typing import Literal, Optional

import numpy as np
import tyro

from trl.trainer.utils import exact_div

from ..core import flatten_dict


@dataclass
class PPOConfig:
    """
    Configuration class for PPOTrainer
    """

    exp_name: str = os.path.basename(sys.argv[0])[: -len(".py")]
    """the name of this experiment (by default is the file name without the extension name)"""
    task_name: Optional[str] = None
    """Name of task to use - used only for tracking purposes"""
    model_name: Optional[str] = None
    """Name of model to use - used only for tracking purposes"""
    query_dataset: Optional[str] = None
    """Name of dataset to query - used only for tracking purposes"""
    reward_model: Optional[str] = None
    """The reward model to use - used only for tracking purposes"""
    steps: int = 20000
    """Number of training steps"""
    learning_rate: float = 1e-5
    """Adam learning rate"""
    adap_kl_ctrl: bool = True
    """Use adaptive KL control, otherwise linear"""
    init_kl_coef: Optional[float] = 0.2
    """Initial KL penalty coefficient (used for adaptive and linear control)"""
    kl_penalty: Literal["kl", "abs", "mse", "full"] = "kl"
    """kl penalty options: 'kl': model_logp - ref_logp,  'abs': abs(kl),  'mse': mean squared error mse(kl) and 'full': the actual kl for all tokens in the distribution"""
    target: Optional[float] = 6
    """Target KL value for adaptive KL control"""
    horizon: Optional[float] = 10000
    """Horizon for adaptive KL control"""
    gamma: float = 1
    """Gamma parameter for advantage calculation"""
    lam: float = 0.95
    """Lambda parameter for advantage calculation"""
    cliprange: float = 0.2
    """Range for clipping in PPO policy gradient loss"""
    cliprange_value: float = 0.2
    """Range for clipping values in loss calculation"""
    vf_coef: float = 0.1
    """Scaling factor for value loss"""
    batch_size: int = 256
    """Number of samples per optimisation step"""
    forward_batch_size: Optional[int] = None
    """DEPRECATED: use `mini_batch_size` instead, which does the same thing."""
    mini_batch_size: int = 1
    """Number of samples optimized in each mini batch"""
    gradient_accumulation_steps: int = 1
    """The number of gradient accumulation steps"""
    world_size: tyro.conf.Suppress[int] = None
    """The world size for distributed training"""
    backward_batch_size: tyro.conf.Suppress[int] = None
    """TO BE FILLED In RUNTIME: Number of samples optimized in an `optimizer.step()` call"""
    global_backward_batch_size: tyro.conf.Suppress[int] = None
    """TO BE FILLED In RUNTIME: the effective `backward_batch_size` across all processes"""
    global_batch_size: tyro.conf.Suppress[int] = None
    """TO BE FILLED In RUNTIME: the effective `batch_size` across all processes"""
    ppo_epochs: int = 4
    """Number of optimisation epochs per batch of samples"""
    remove_unused_columns: bool = True
    """Remove unused columns from the dataset if `datasets.Dataset` is used"""
    log_with: Optional[Literal["wandb", "tensorboard"]] = None
    """Log with either 'wandb' or 'tensorboard', check  https://huggingface.co/docs/accelerate/usage_guides/tracking for more details"""
    tracker_kwargs: dict = field(default_factory=dict)
    """Keyword arguments for the tracker (e.g. wandb_project)"""
    accelerator_kwargs: dict = field(default_factory=dict)
    """Keyword arguments for the accelerator"""
    project_kwargs: dict = field(default_factory=dict)
    """Keyword arguments for the accelerator project config (e.g. `logging_dir`)"""
    tracker_project_name: str = "trl"
    """Name of project to use for tracking"""
    max_grad_norm: Optional[float] = None
    """Maximum gradient norm for gradient clipping"""
    seed: int = 0
    """Seed value for random generations"""
    optimize_cuda_cache: bool = False
    """Optimize CUDA cache for slightly more memory-efficient training"""
    early_stopping: bool = False
    """Whether to stop the PPO optimization loop early is the KL too high"""
    target_kl: float = 1
    """Stop early if we exceed this value by over 50%"""
    push_to_hub_if_best_kwargs: dict = field(default_factory=dict)
    """Keyword arguments for pushing model to the hub during training (e.g. repo_id)"""
    compare_steps: int = 1
    """Number of steps between comparison of the current reward with the best seen so far"""
    ratio_threshold: float = 10.0
    """Skip mini-batches with high PPO ratios that can cause loss spikes"""
    use_score_scaling: bool = False
    """Use score scaling"""
    use_score_norm: bool = False
    """Use score normalization. Only applicable if use_score_scaling is True"""
    score_clip: Optional[float] = None
    """Score clipping"""
    soft_risk_alpha: float = 0.05
    """Risk level alpha"""
    risk_level_scheduler: float = 0.98 
    """Risk level scheduler: 0.8 means the soft risk level reaches α after ρ = 80% of the training """

    # Model detection
    is_encoder_decoder: Optional[tyro.conf.Suppress[bool]] = None
    """TO BE FILLED In RUNTIME: Whether the model is an encoder-decoder model"""
    is_peft_model: Optional[tyro.conf.Suppress[bool]] = None
    """TO BE FILLED In RUNTIME: Whether the model is a PEFT model"""

    def __post_init__(self):
        if self.forward_batch_size is not None:
            warnings.warn(
                "Note that using `forward_batch_size` is deprecated, use `mini_batch_size` instead. By setting it you overwrite `mini_batch_size` which affects both the batch size during forward passes and also the mini batch size for PPO optimization."
            )
            self.mini_batch_size = self.forward_batch_size

        self.backward_batch_size = self.mini_batch_size * self.gradient_accumulation_steps
        exact_div(
            self.batch_size,
            self.backward_batch_size,
            "`batch_size`",
            "`mini_batch_size * gradient_accumulation_steps`",
            "`batch_size` must be a multiple of `mini_batch_size * gradient_accumulation_steps`",
        )

        # check if wandb is installed
        if self.log_with == "wandb":
            # raise error if wandb is not installed
            try:
                import wandb  # noqa: F401

            except ImportError:
                raise ImportError(
                    "Please install wandb to use wandb logging. You can do this by running `pip install wandb`."
                )

        self.total_ppo_epochs = int(np.ceil(self.steps / self.batch_size))
        assert self.kl_penalty in ["kl", "abs", "mse", "full"]

    def to_dict(self):
        output_dict = {}
        for key, value in self.__dict__.items():
            output_dict[key] = value
        return flatten_dict(output_dict)

File Path: trl/trainer/ppo_trainer.py
Content:
# Copyright 2022 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import inspect
import math
import os
import time
import typing
import warnings
from typing import Callable, List, Optional, Union

import datasets
import numpy as np
import torch
import torch.nn.functional as F
from accelerate import Accelerator
from accelerate.utils import ProjectConfiguration, is_deepspeed_available
from datasets import Dataset
from huggingface_hub import whoami
from packaging import version
from torch.optim import Adam
from transformers import (
    DataCollatorForLanguageModeling,
    PreTrainedTokenizer,
    PreTrainedTokenizerBase,
    PreTrainedTokenizerFast,
)

from ..core import (
    WANDB_PADDING,
    PPODecorators,
    clip_by_value,
    convert_to_scalar,
    entropy_from_logits,
    flatten_dict,
    logprobs_from_logits,
    masked_mean,
    masked_var,
    masked_whiten,
    set_seed,
    stack_dicts,
    stats_to_np,
)
from ..import_utils import is_torch_greater_2_0
from ..models import SUPPORTED_ARCHITECTURES, PreTrainedModelWrapper, create_reference_model
from . import AdaptiveKLController, BaseTrainer, FixedKLController, PPOConfig, RunningMoments

import pdb

if is_deepspeed_available():
    import deepspeed

MODEL_CARD_TEMPLATE = """---
license: apache-2.0
tags:
- trl
- transformers
- reinforcement-learning
---

# {model_name}

This is a [TRL language model](https://github.com/huggingface/trl) that has been fine-tuned with reinforcement learning to
 guide the model outputs according to a value, function, or human feedback. The model can be used for text generation.

## Usage

To use this model for inference, first install the TRL library:

```bash
python -m pip install trl
```

You can then generate text as follows:

```python
from transformers import pipeline

generator = pipeline("text-generation", model="{model_id}")
outputs = generator("Hello, my llama is cute")
```

If you want to use the model for training or to obtain the outputs from the value head, load the model as follows:

```python
from transformers import AutoTokenizer
from trl import AutoModelForCausalLMWithValueHead

tokenizer = AutoTokenizer.from_pretrained("{model_id}")
model = AutoModelForCausalLMWithValueHead.from_pretrained("{model_id}")

inputs = tokenizer("Hello, my llama is cute", return_tensors="pt")
outputs = model(**inputs, labels=inputs["input_ids"])
```
"""


class PPOTrainer(BaseTrainer):
    """
    The PPOTrainer uses Proximal Policy Optimization to optimise language models.
    Note, this trainer is heavily inspired by the original OpenAI learning to summarize work here:
    https://github.com/openai/summarize-from-feedback

    Attributes:
        **config** (`PPOConfig`) -- Configuration object for PPOTrainer. Check the documentation of `PPOConfig` for more
            details.
        **model** (`PreTrainedModelWrapper`) -- Model to be optimized, Hugging Face transformer model with a value head.
            Check the documentation of `PreTrainedModelWrapper` for more details.
        **ref_model** (`PreTrainedModelWrapper`, *optional*) -- Reference model to be used for KL penalty, Hugging Face
            transformer model with a casual language modelling head. Check the documentation of `PreTrainedModelWrapper`
            for more details. If no reference model is provided, the trainer will create a reference model with the same
             architecture as the model to be optimized with shared layers.
        **tokenizer** (`PreTrainedTokenizerBase`) -- Tokenizer to be used for encoding the
            data. Check the documentation of `transformers.PreTrainedTokenizer` and
            `transformers.PreTrainedTokenizerFast` for more details.
        **dataset** (Union[`torch.utils.data.Dataset`, `datasets.Dataset`], *optional*) -- PyTorch dataset or Hugging
            Face dataset. This is used to create a PyTorch dataloader. If no dataset is provided, the dataloader must be
             created outside the trainer users needs to design their own dataloader and make sure the batch
            size that is used is the same as the one specified in the configuration object.
        **optimizer** (`torch.optim.Optimizer`, *optional*) -- Optimizer to be used for training. If no optimizer is
            provided, the trainer will create an Adam optimizer with the learning rate specified in the configuration
            object.
        **data_collator** (DataCollatorForLanguageModeling, *optional*) -- Data collator to be used for training and
            passed along the dataloader
        **num_shared_layers** (int, *optional*) -- Number of layers to be shared between the model and the reference
            model, if no reference model is passed. If no number is provided, all the layers will be shared.
        **lr_scheduler** (`torch.optim.lr_scheduler`, *optional*) -- Learning rate scheduler to be used for training.
    """

    def __init__(
        self,
        config: PPOConfig = None,
        model: PreTrainedModelWrapper = None,
        ref_model: Optional[PreTrainedModelWrapper] = None,
        tokenizer: PreTrainedTokenizerBase = None,
        dataset: Optional[Union[torch.utils.data.Dataset, Dataset]] = None,
        optimizer: Optional[torch.optim.Optimizer] = None,
        data_collator: Optional[typing.Callable] = None,
        num_shared_layers: Optional[int] = None,
        lr_scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,
    ):
        """
        Initialize PPOTrainer.

        Args:
            config (`PPOConfig`):
                Configuration object for PPOTrainer. Check the documentation of `PPOConfig` for more details.
            model (`PreTrainedModelWrapper`):
                Hugging Face transformer model with a value head.
            ref_model (`PreTrainedModelWrapper`):
                Hugging Face transformer model with a casual language modelling head. Used for KL penalty
            tokenizer (`transformers.PreTrainedTokenizerBase`):
                Hugging Face tokenizer
            dataset (Optional[Union[`torch.utils.data.Dataset`, `datasets.Dataset`]]):
                PyTorch dataset or Hugging Face dataset. If a Hugging Face dataset is passed, the dataset
                will be preprocessed by removing the columns that are not used by the model. If none is passed,
                a warning will be raised in a multi-GPU setting.
            optimizer (Optional[`torch.optim.Optimizer`]):
                Optimizer used for training. If `None`, the `Adam` is used as default.
            data_collator (Optional[function]):
                Data collator function.
            num_shared_layers (Optional[int]):
                Number of shared layers between the model and the reference model. If `None`, all layers are shared.
                used only if `ref_model` is `None`.
            lr_scheduler (Optional[`torch.optim.lr_scheduler`]):
                Learning rate scheduler used for training.
        """
        super().__init__(config)

        # initial seed for reproducible experiments
        set_seed(config.seed)

        # Step 0: check positional arguments validity
        if not isinstance(config, PPOConfig):
            raise ValueError(f"config must be a PPOConfig, got {type(config)}")
        if not isinstance(tokenizer, (PreTrainedTokenizerBase)):
            raise ValueError(
                f"tokenizer must be a PreTrainedTokenizerBase like a PreTrainedTokenizer or a PreTrainedTokenizerFast, got {type(tokenizer)}"
            )
        if not isinstance(model, (SUPPORTED_ARCHITECTURES)):
            raise ValueError(
                f"model must be a PreTrainedModelWrapper, got {type(model)} - supported architectures are: {SUPPORTED_ARCHITECTURES}"
            )
        # Step 1: Initialize Accelerator
        self.accelerator = Accelerator(
            log_with=config.log_with,
            gradient_accumulation_steps=config.gradient_accumulation_steps,
            project_config=ProjectConfiguration(**config.project_kwargs),
            **config.accelerator_kwargs,
        )

        # Step 1.1 Runtime variables filled by the accelerator
        config.world_size = self.accelerator.num_processes
        config.global_backward_batch_size = config.backward_batch_size * config.world_size
        config.global_batch_size = config.batch_size * config.world_size

        self.model = model
        self.model_params = filter(lambda p: p.requires_grad, self.model.parameters())
        self.is_encoder_decoder = hasattr(self.model, "is_encoder_decoder")
        self.is_peft_model = getattr(self.model, "is_peft_model", False)
        config.is_encoder_decoder = self.is_encoder_decoder
        config.is_peft_model = self.is_peft_model

        is_using_tensorboard = config.log_with is not None and config.log_with == "tensorboard"
        self.accelerator.init_trackers(
            config.tracker_project_name,
            config=dict(trl_ppo_trainer_config=config.to_dict()) if not is_using_tensorboard else config.to_dict(),
            init_kwargs=config.tracker_kwargs,
        )
        self.is_using_text_environment = getattr(config, "use_text_environment", False)

        if isinstance(ref_model, SUPPORTED_ARCHITECTURES):
            self.ref_model = ref_model
            if num_shared_layers is not None:
                warnings.warn(
                    "num_shared_layers is ignored when ref_model is provided. Two different models are used for the "
                    "model and the reference model and no layers are shared.",
                    UserWarning,
                )
        elif ref_model is None and not self.is_peft_model:
            self.ref_model = create_reference_model(self.model, num_shared_layers=num_shared_layers)
        elif self.is_peft_model:
            self.ref_model = None
        else:
            raise ValueError(
                f"ref_model must be a PreTrainedModelWrapper or `None`, got {type(ref_model)} - supported "
                f"architectures are: {SUPPORTED_ARCHITECTURES} "
            )

        if not (isinstance(tokenizer, PreTrainedTokenizer) or isinstance(tokenizer, PreTrainedTokenizerFast)):
            raise ValueError(
                "tokenizer must be a transformers.PreTrainedTokenizer or transformers.PreTrainedTokenizerFast"
            )
        self.tokenizer = tokenizer

        if dataset is not None and not (isinstance(dataset, torch.utils.data.Dataset) or isinstance(dataset, Dataset)):
            raise ValueError("dataset must be a torch.utils.data.Dataset or datasets.Dataset")
        elif dataset is None:
            warnings.warn(
                "No dataset is provided. Make sure to set config.batch_size to the correct value before training.",
                UserWarning,
            )
        self.dataset = dataset
        self._signature_columns = None
        if self.dataset is not None:
            self.dataloader = self.prepare_dataloader(self.dataset, data_collator)
        elif self.dataset is None and self.accelerator.num_processes > 1:
            warnings.warn(
                "No dataset is provided. In a multi-GPU setting, this will lead to an error. You should"
                " prepare your dataloader yourself with `dataloader = ppo_trainer.accelerator.prepare(dataloader)`"
                " and using `torch.utils.data.DataLoader`, or pass a dataset to the `PPOTrainer`. Please "
                " refer to the documentation for more details.",
                UserWarning,
            )
            self.dataloader = None
        else:
            self.dataloader = None

        # Step 3: Initialize optimizer and data collator
        self.data_collator = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)
        if optimizer is None:
            self.optimizer = Adam(
                filter(lambda p: p.requires_grad, self.model.parameters()),
                lr=self.config.learning_rate,
            )
        else:
            self.optimizer = optimizer

        self.lr_scheduler = lr_scheduler
        if self.lr_scheduler is not None:
            lr_scheduler_class = (
                torch.optim.lr_scheduler._LRScheduler
                if not is_torch_greater_2_0()
                else torch.optim.lr_scheduler.LRScheduler
            )

            if not isinstance(self.lr_scheduler, lr_scheduler_class):
                raise ValueError(
                    "lr_scheduler must be a torch.optim.lr_scheduler._LRScheduler or torch.optim.lr_scheduler.LRScheduler (for torch >= 2.0)"
                )

        if self.config.adap_kl_ctrl:
            self.kl_ctl = AdaptiveKLController(self.config.init_kl_coef, self.config.target, self.config.horizon)
        else:
            self.kl_ctl = FixedKLController(self.config.init_kl_coef)

        # Safety checkers for DS integration
        is_deepspeed_used = self.accelerator.distributed_type == "DEEPSPEED" and hasattr(
            self.accelerator.state, "deepspeed_plugin"
        )

        (
            self.model,
            self.optimizer,
            self.data_collator,
            self.dataloader,
            self.lr_scheduler,
        ) = self.accelerator.prepare(
            self.model,
            self.optimizer,
            self.data_collator,
            self.dataloader,
            self.lr_scheduler,
        )
        if is_deepspeed_used:
            # Quantized models are already set on the correct device
            if not self.is_peft_model and not (
                getattr(self.ref_model.pretrained_model, "is_loaded_in_8bit", False)
                or getattr(self.ref_model.pretrained_model, "is_loaded_in_4bit", False)
            ):
                self.ref_model = self._prepare_deepspeed(self.ref_model)
        else:
            self.ref_model = self.accelerator.prepare(self.ref_model)

        # In a distributed setup, only logging needs to be performed on the main process
        # check: https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html
        # or: https://discuss.pytorch.org/t/use-distributed-data-parallel-correctly/82500/11
        self.is_distributed = self.accelerator.distributed_type == "MULTI_GPU"

        # init the current step
        self.current_step = 0

        # init variables for pushing model to hub
        if config.push_to_hub_if_best_kwargs:
            if "repo_id" not in config.push_to_hub_if_best_kwargs:
                raise ValueError("You have to specify repo_id in order to push the model to the hub!")
            self.push_to_hub_kwargs = config.push_to_hub_if_best_kwargs
            self.compare_step = 0
            self.highest_reward = torch.tensor(-float("inf"))

        # post process for PP
        if not getattr(self.model, "is_sequential_parallel", False):
            self.current_device = self.accelerator.device
        else:
            self.current_device = torch.device("cuda:0")

        PPODecorators.optimize_cuda_cache = self.config.optimize_cuda_cache

        self.running = RunningMoments(self.accelerator)

    def _filter_kwargs(self, kwargs, target_func):
        """
        filter the keyword arguments that are supported by the target function.

        Args:
            kwargs (dict):
                Keyword arguments
            target_func (function):
                Target function
        """
        return {k: v for k, v in kwargs.items() if k in inspect.signature(target_func).parameters.keys()}

    def prepare_dataloader(self, dataset: Union[torch.utils.data.Dataset, Dataset], data_collator=None):
        """
        Prepare the dataloader for training.

        Args:
            dataset (Union[`torch.utils.data.Dataset`, `datasets.Dataset`]):
                PyTorch dataset or Hugging Face dataset. If a Hugging Face dataset is passed, the dataset
                will be preprocessed by removing the columns that are not used by the model.
            data_collator (Optional[function]):
                Data collator function.

        Returns:
            `torch.utils.data.DataLoader`: PyTorch dataloader
        """
        if isinstance(dataset, Dataset):
            dataset = self._remove_unused_columns(dataset)
        dataloader = torch.utils.data.DataLoader(
            dataset,
            batch_size=self.config.batch_size,
            collate_fn=data_collator,
            shuffle=True,
            drop_last=True,
        )
        return dataloader

    # Adapted from transformers.Trainer._set_signature_columns_if_needed
    def _set_signature_columns_if_needed(self):
        if self._signature_columns is None:
            # Inspect model forward signature to keep only the arguments it accepts.
            signature = inspect.signature(self.model.forward)
            self._signature_columns = list(signature.parameters.keys())
            # label => sentiment | we need query and response for logging purpose
            self._signature_columns += ["label", "query", "response"]

    # Adapted from transformers.Trainer._remove_unused_columns
    def _remove_unused_columns(self, dataset: "Dataset"):
        if not self.config.remove_unused_columns:
            return dataset
        self._set_signature_columns_if_needed()
        signature_columns = self._signature_columns

        ignored_columns = list(set(dataset.column_names) - set(signature_columns))

        columns = [k for k in signature_columns if k in dataset.column_names]

        if version.parse(datasets.__version__) < version.parse("1.4.0"):
            dataset.set_format(
                type=dataset.format["type"],
                columns=columns,
                format_kwargs=dataset.format["format_kwargs"],
            )
            return dataset
        else:
            return dataset.remove_columns(ignored_columns)

    def generate(
        self,
        query_tensor: Union[torch.Tensor, List[torch.Tensor]],
        length_sampler: Callable = None,
        batch_size: int = 4,
        return_prompt: bool = True,
        **generation_kwargs,
    ):
        """
        Generate response with the model given the query tensor.
        call the `generate` method of the model.

        Args:
            query_tensor (`torch.LongTensor`):
                A tensor of shape (`batch_size`, `seq_len`) containing query tokens.
            generation_kwargs (dict[str, Any]):
                Keyword arguments for generation.
            length_sampler (`Callable`, *optional*):
                Callable that returns the number of newly generated tokens.
            batch_size (`int`, *optional):
                Batch size used for generation, defaults to `4`.
            return_prompt (`bool`, *optional*):
                If set to `False` the prompt is not returned but only the newly generated tokens, defaults to `True`.

        Returns:
            `torch.LongTensor`: A tensor of shape (`batch_size`, `gen_len`) containing response tokens.
        """

        if isinstance(query_tensor, List):
            return self._generate_batched(
                query_tensor,
                length_sampler=length_sampler,
                batch_size=batch_size,
                return_prompt=return_prompt,
                **generation_kwargs,
            )

        else:
            if length_sampler is not None:
                generation_kwargs["max_new_tokens"] = length_sampler()
            response = self.accelerator.unwrap_model(self.model).generate(
                input_ids=query_tensor.unsqueeze(dim=0), **generation_kwargs
            )

            if not return_prompt and not self.is_encoder_decoder:
                return response[:, query_tensor.shape[0] :]
            return response

    def _generate_batched(
        self,
        query_tensors: List[torch.Tensor],
        length_sampler: Callable = None,
        batch_size: int = 4,
        return_prompt: bool = True,
        pad_to_multiple_of: int = None,
        remove_padding: bool = True,
        **generation_kwargs,
    ):
        outputs = []

        padding_side_default = self.tokenizer.padding_side
        if not self.is_encoder_decoder:
            self.tokenizer.padding_side = "left"

        # in case we have fewer examples than bs
        batch_size = min(len(query_tensors), batch_size)

        for i in range(0, len(query_tensors), batch_size):
            if length_sampler is not None:
                generation_kwargs["max_new_tokens"] = length_sampler()

            # prevent overflow if query tensors are not even multiple of bs
            end_index = min(len(query_tensors), i + batch_size)

            batch = query_tensors[i:end_index]
            batch_mask = [torch.ones_like(element) for element in batch]
            inputs = {"input_ids": batch, "attention_mask": batch_mask}

            padded_inputs = self.tokenizer.pad(
                inputs,
                padding=True,
                max_length=None,
                pad_to_multiple_of=pad_to_multiple_of,
                return_tensors="pt",
            ).to(self.current_device)

            generations = self.accelerator.unwrap_model(self.model).generate(**padded_inputs, **generation_kwargs)

            for generation, mask in zip(generations, padded_inputs["attention_mask"]):
                if not self.is_encoder_decoder:
                    output = generation[(1 - mask).sum() :]  # remove padding
                else:
                    output = generation

                if not return_prompt and not self.is_encoder_decoder:
                    output = output[(mask).sum() :]  # remove prompt

                if remove_padding and self.tokenizer.eos_token_id in output:
                    pad_mask = output == self.tokenizer.eos_token_id
                    pad_start = torch.nonzero(pad_mask, as_tuple=False)[0, 0].item()
                    output = output[: pad_start + 1]  # keep the eos token at the end

                outputs.append(output)

        self.tokenizer.padding_side = padding_side_default
        return outputs

    def _step_safety_checker(
        self,
        batch_size: int,
        queries: List[torch.LongTensor],
        responses: List[torch.LongTensor],
        scores: List[torch.FloatTensor],
        masks: Optional[List[torch.LongTensor]] = None,
    ):
        """
        Check if the input data is valid for training.

        Args:
            batch_size (int):
                Batch size from the config file.
            queries (List[`torch.LongTensor`]):
                List of tensors containing the encoded queries of shape (`query_length`)
            responses (List[`torch.LongTensor`]):
                List of tensors containing the encoded responses of shape (`response_length`)
            scores (List[`torch.FloatTensor`]):
                List of tensors containing the scores.
            masks (List[`torch.LongTensor`], *optional*):
                list of optional tensors containing the masks of shape (`query_length` + `response_length`)
        Returns:
            `tuple`: The input processed data.
        """
        for name, tensor_list in zip(["queries", "responses", "scores"], [queries, responses, scores]):
            if not isinstance(tensor_list, list):
                raise ValueError(f"{name} must be a list of tensors - got {type(tensor_list)}")
            if not isinstance(tensor_list[0], torch.Tensor):
                raise ValueError(f"Elements in {name} must be tensors - got {type(tensor_list[0])}")
            if batch_size is not None and len(tensor_list) != batch_size:
                raise ValueError(
                    f"Batch size ({batch_size}) does not match number of examples - but got {len(tensor_list)} for: {name}"
                )

        # add queries, scores and responses on the correct device
        queries = [tensor.to(self.current_device) for tensor in queries]
        responses = [tensor.to(self.current_device) for tensor in responses]
        scores = [tensor.to(self.current_device) for tensor in scores]
        masks = [tensor.to(self.current_device) for tensor in masks] if masks is not None else None

        # squeeze scores if needed
        for i, score in enumerate(scores):
            if score.dim() > 1:
                raise ValueError(f"Scores must be 1-dimensional - got {score.dim()} for {score}")
            elif score.dim() == 1:
                scores[i] = score.squeeze()

        return queries, responses, scores, masks

    @PPODecorators.empty_cuda_cache()
    def step(
        self,
        queries: List[torch.LongTensor],
        responses: List[torch.LongTensor],
        scores: List[torch.FloatTensor],
        risk_alpha: List[torch.LongTensor],
        response_masks: Optional[List[torch.LongTensor]] = None
    ):
        """
        Run a PPO optimisation step given a list of queries, model responses, and rewards.

        Args:
            queries (List[`torch.LongTensor`]):
                List of tensors containing the encoded queries of shape (`query_length`)
            responses (List[`torch.LongTensor`]):
                List of tensors containing the encoded responses of shape (`response_length`)
            scores (List[`torch.FloatTensor`]):
                List of tensors containing the scores.
            response_masks (List[`torch.FloatTensor`], *optional*)):
                List of tensors containing masks of the response tokens.

        Returns:
            `dict[str, Any]`: A summary of the training statistics
        """

        bs = self.config.batch_size

        queries, responses, scores, response_masks = self._step_safety_checker(
            bs, queries, responses, scores, response_masks
        )
        scores = torch.tensor(scores, device=self.current_device)
        if self.config.use_score_scaling:
            # Score scaling
            scores_mean, scores_std = self.running.update(scores)
            tensor_to_kwargs = dict(dtype=scores.dtype, device=scores.device)
            score_scaling_factor = self.running.std.to(**tensor_to_kwargs) + torch.finfo(scores.dtype).eps
            if self.config.use_score_norm:
                scores = (scores - self.running.mean.to(**tensor_to_kwargs)) / score_scaling_factor
            else:
                scores /= score_scaling_factor

        if self.config.score_clip is not None:
            # Score clipping
            scores_dtype = scores.dtype
            scores = torch.clip(scores.float(), -self.config.score_clip, self.config.score_clip).to(dtype=scores_dtype)

        # if we want to push best model to the hub
        if hasattr(self, "highest_reward"):
            if self.compare_step % self.config.compare_steps == 0:
                curr_mean_reward = scores.mean()
                # if the best reward ever seen
                if curr_mean_reward > self.highest_reward:
                    self.highest_reward = curr_mean_reward
                    # push model to hub
                    self.push_to_hub(**self.push_to_hub_kwargs)
            self.compare_step += 1

        timing = dict()
        t0 = time.time()

        t = time.time()

        model_inputs = self.prepare_model_inputs(queries, responses)

        # performs padding in distributed setting 
        # what about padding in non-distributed setting then? 
        if self.is_distributed:

            # if response length = 42, left padding adds 6 zeros to the left of attention mask to reach desired total length
            pad_first = self.tokenizer.padding_side == "left"

            model_inputs["input_ids"] = self.accelerator.pad_across_processes(
                model_inputs["input_ids"],
                dim=1,
                pad_index=self.tokenizer.pad_token_id,
                pad_first=pad_first,
            )
            model_inputs["attention_mask"] = self.accelerator.pad_across_processes(
                model_inputs["attention_mask"], dim=1, pad_index=0, pad_first=pad_first
            )
            if self.is_encoder_decoder:
                model_inputs["decoder_input_ids"] = self.accelerator.pad_across_processes(
                    model_inputs["decoder_input_ids"],
                    dim=1,
                    pad_index=self.tokenizer.pad_token_id,
                    pad_first=pad_first,
                )
                model_inputs["decoder_attention_mask"] = self.accelerator.pad_across_processes(
                    model_inputs["decoder_attention_mask"],
                    dim=1,
                    pad_index=0,
                    pad_first=pad_first,
                )

        model_inputs_names = list(model_inputs.keys())

        full_kl_penalty = self.config.kl_penalty == "full"

        with torch.no_grad():
            all_logprobs, logits_or_none, values, masks = self.batched_forward_pass(
                self.model,
                queries,
                responses,
                model_inputs,
                response_masks=response_masks,
                return_logits=full_kl_penalty,
            )

            # for when the model is a peft model
            if self.is_peft_model and hasattr(
                self.accelerator.unwrap_model(self.model).pretrained_model,
                "disable_adapter",
            ):
                with self.accelerator.unwrap_model(self.model).pretrained_model.disable_adapter():
                    ref_logprobs, ref_logits_or_none, _, _ = self.batched_forward_pass(
                        self.model, queries, responses, model_inputs, return_logits=full_kl_penalty
                    )
            elif self.is_peft_model and not hasattr(self.model.pretrained_model, "disable_adapter"):
                raise ValueError(
                    "You are using a `peft` version that does not support `disable_adapter`. Please update your `peft` version to the latest version."
                )

            else:
                ref_logprobs, ref_logits_or_none, _, _ = self.batched_forward_pass(
                    self.ref_model, queries, responses, model_inputs, return_logits=full_kl_penalty
                )

        timing["time/ppo/forward_pass"] = time.time() - t

        with torch.no_grad():
            t = time.time()
            if full_kl_penalty:
                active_full_logprobs = logprobs_from_logits(logits_or_none, None, gather=False)
                ref_full_logprobs = logprobs_from_logits(ref_logits_or_none, None, gather=False)

                rewards, non_score_reward = self.compute_rewards(
                    scores, active_full_logprobs, ref_full_logprobs, masks
                )
            else:
                rewards, non_score_reward = self.compute_rewards(scores, all_logprobs, ref_logprobs, masks)
            timing["time/ppo/compute_rewards"] = time.time() - t

            t = time.time()

            pdb.set_trace()
            values, advantages, returns, risk_masks = self.compute_advantages(values, rewards, masks, risk_alpha)
            timing["time/ppo/compute_advantages"] = time.time() - t

        # upcast to float32 to avoid dataset issues
        batch_dict = {
            "queries": queries,
            "responses": responses,
            "logprobs": all_logprobs.to(torch.float32),
            "values": values.to(torch.float32),
            "masks": risk_masks,
            "advantages": advantages,
            "returns": returns,
        }
        batch_dict.update(model_inputs)

        t = time.time()
        all_stats = []
        early_stop = False
        for _ in range(self.config.ppo_epochs):
            if early_stop:
                break
            b_inds = np.random.permutation(bs)
            for backward_batch_start in range(0, bs, self.config.backward_batch_size):
                backward_batch_end = backward_batch_start + self.config.backward_batch_size
                backward_batch_inds = b_inds[backward_batch_start:backward_batch_end]

                for mini_batch_start in range(0, self.config.backward_batch_size, self.config.mini_batch_size):
                    mini_batch_end = mini_batch_start + self.config.mini_batch_size
                    mini_batch_inds = backward_batch_inds[mini_batch_start:mini_batch_end]
                    mini_batch_dict = {
                        "logprobs": batch_dict["logprobs"][mini_batch_inds],
                        "values": batch_dict["values"][mini_batch_inds],
                        "masks": batch_dict["masks"][mini_batch_inds],
                        # hacks: the queries and responses are ragged.
                        "queries": [batch_dict["queries"][i] for i in mini_batch_inds],
                        "responses": [batch_dict["responses"][i] for i in mini_batch_inds],
                        "advantages": batch_dict["advantages"][mini_batch_inds],
                        "returns": batch_dict["returns"][mini_batch_inds],
                    }
                    for k in model_inputs_names:
                        mini_batch_dict[k] = batch_dict[k][mini_batch_inds]
                    with self.accelerator.accumulate(self.model):
                        model_inputs = {k: mini_batch_dict[k] for k in model_inputs_names}

                        logprobs, logits, vpreds, _ = self.batched_forward_pass(
                            self.model,
                            mini_batch_dict["queries"],
                            mini_batch_dict["responses"],
                            model_inputs,
                            return_logits=True,
                        )
                        train_stats = self.train_minibatch(
                            mini_batch_dict["logprobs"],
                            mini_batch_dict["values"],
                            logprobs,
                            logits,
                            vpreds,
                            mini_batch_dict["masks"],
                            mini_batch_dict["advantages"],
                            mini_batch_dict["returns"],
                        )
                        all_stats.append(train_stats)

            # typically, early stopping is done at the epoch level
            if self.config.early_stopping:
                policykl = train_stats["policy/policykl"]
                early_stop = self._early_stop(policykl)
                if early_stop:
                    break

        timing["time/ppo/optimize_step"] = time.time() - t

        t = time.time()
        train_stats = stack_dicts(all_stats)

        # reshape advantages/ratios such that they are not averaged.
        train_stats["policy/advantages"] = torch.flatten(train_stats["policy/advantages"]).unsqueeze(0)
        train_stats["policy/advantages"] = torch.nan_to_num(train_stats["policy/advantages"], WANDB_PADDING)
        train_stats["policy/ratio"] = torch.flatten(train_stats["policy/ratio"]).unsqueeze(0)

        stats = self.record_step_stats(
            scores=scores,
            logprobs=all_logprobs,
            ref_logprobs=ref_logprobs,
            non_score_reward=non_score_reward,
            train_stats=train_stats,
            kl_coef=self.kl_ctl.value,
            masks=masks,
            queries=queries,
            responses=responses,
        )
        # Gather/Reduce stats from all processes
        if self.is_distributed:
            stats = self.gather_stats(stats)
        stats = stats_to_np(stats)
        timing["time/ppo/calc_stats"] = time.time() - t
        stats["ppo/learning_rate"] = self.optimizer.param_groups[0]["lr"]

        # Update the KL control - multiply the batch_size by the number of processes
        self.kl_ctl.update(
            stats["objective/kl"],
            self.config.batch_size * self.accelerator.num_processes,
        )

        # Log the total ppo time
        timing["time/ppo/total"] = time.time() - t0
        stats.update(timing)

        # post-process stats for tensorboard and other loggers
        if self.config.log_with != "wandb":
            stats = convert_to_scalar(stats)

        if self.lr_scheduler is not None:
            self.lr_scheduler.step()

        return stats

    def _early_stop(self, policykl):
        r"""
        Handles the early stopping logic. If the policy KL is greater than the target KL, then the gradient is zeroed and
        the optimization step is skipped.
        This also handles the multi-gpu case where the policy KL is averaged across all processes.

        Args:
            policy_kl (torch.Tensor):
                the policy KL

        Returns:
            `bool`: whether to early stop or not
        """
        early_stop = False
        if not self.config.early_stopping:
            return early_stop

        if not self.is_distributed and policykl > 1.5 * self.config.target_kl:
            self.optimizer.zero_grad()
            early_stop = True
        elif self.is_distributed:
            import torch.distributed as dist

            # Wait for all processes to finish
            dist.barrier()

            # all gather the policykl
            dist.all_reduce(policykl, dist.ReduceOp.SUM)
            policykl /= self.accelerator.num_processes

            if policykl > 1.5 * self.config.target_kl:
                self.optimizer.zero_grad()
                early_stop = True
        return early_stop

    def gather_stats(self, stats):
        """
        Gather stats from all processes. Useful in the context of distributed training.

        Args:
            stats (dict[str, Any]):
            a dictionary of stats to be gathered. The stats should contain torch tensors.

        Returns:
            `dict[str, Any]`: A dictionary of stats with the tensors gathered.
        """
        import torch.distributed as dist

        # Wait for all processes to finish
        dist.barrier()

        for k, v in stats.items():
            if isinstance(v, torch.Tensor):
                dist.all_reduce(v.to(self.accelerator.device), dist.ReduceOp.SUM)
                v /= self.accelerator.num_processes
            stats[k] = v
        return stats

    def prepare_model_inputs(self, queries: torch.Tensor, responses: torch.Tensor):
        if self.is_encoder_decoder:
            input_data = self.data_collator(
                [{"input_ids": q, "attention_mask": torch.ones_like(q)} for q in queries]
            ).to(self.current_device)

            decoder_inputs = self.data_collator(
                [{"input_ids": r, "attention_mask": torch.ones_like(r)} for r in responses]
            ).to(self.current_device)

            input_data["decoder_input_ids"] = decoder_inputs["input_ids"]
            input_data["decoder_attention_mask"] = decoder_inputs["attention_mask"]
        else:
            input_ids = [torch.cat([q, r]) for q, r in zip(queries, responses)]
            # attention mask is 1 only for the query + actual response lengths and not (64+48=112)
            input_data = self.data_collator(
                [{"input_ids": ids, "attention_mask": torch.ones_like(ids)} for ids in input_ids]
            ).to(self.current_device)

        input_data.pop("labels", None)  # we don't want to compute LM losses
        return input_data

    @PPODecorators.empty_cuda_cache()
    def batched_forward_pass(
        self,
        model: PreTrainedModelWrapper,
        queries: torch.Tensor,
        responses: torch.Tensor,
        model_inputs: dict,
        return_logits: bool = False,
        response_masks: Optional[torch.Tensor] = None,
    ):
        """
        Calculate model outputs in multiple batches.

        Args:
            queries (`torch.LongTensor`):
                List of tensors containing the encoded queries, shape (`batch_size`, `query_length`)
            responses (`torch.LongTensor`):
                List of tensors containing the encoded responses, shape (`batch_size`, `response_length`)
            return_logits (`bool`, *optional*, defaults to `False`):
                Whether to return all_logits. Set to `False` if logits are not needed to reduce memory consumption.
        Returns:
            (tuple):
                - all_logprobs (`torch.FloatTensor`): Log probabilities of the responses,
                    shape (`batch_size`, `response_length`)
                - all_ref_logprobs (`torch.FloatTensor`): Log probabilities of the responses,
                    shape (`batch_size`, `response_length`)
                - all_values (`torch.FloatTensor`): Values of the responses, shape (`batch_size`, `response_length`)
        """
        bs = len(queries)
        fbs = self.config.mini_batch_size
        all_logprobs = []
        all_logits = []
        all_masks = []
        all_values = []

        model.eval()


        for i in range(math.ceil(bs / fbs)):
            input_kwargs = {key: value[i * fbs : (i + 1) * fbs] for key, value in model_inputs.items()}
            query_batch = queries[i * fbs : (i + 1) * fbs]
            response_batch = responses[i * fbs : (i + 1) * fbs]
            if response_masks is not None:
                response_masks_batch = response_masks[i * fbs : (i + 1) * fbs]
            logits, _, values = model(**input_kwargs)

            if self.is_encoder_decoder:
                input_ids = input_kwargs["decoder_input_ids"]
                attention_mask = input_kwargs["decoder_attention_mask"]
            else:
                input_ids = input_kwargs["input_ids"]
                attention_mask = input_kwargs["attention_mask"]

            # logits at step
            # why are we reducing size by 1
            # because at the current time step/token, logits are over the next token 
            # thats why logits[:, 0, :] is for input_ids[:, 1] 
            logprobs = logprobs_from_logits(logits[:, :-1, :], input_ids[:, 1:])
            masks = torch.zeros_like(attention_mask)
            # the following would mean that the last element of masks is always zero
            masks[:, :-1] = attention_mask[:, 1:]

            #For a single sequence model, start position is the end of the query. End is start + response length.
            #It handles optional left padding that may need to be offset if present.
            #The masks tensor is used to mask the loss - set to 0 before start and after end index, 1 inside.
            #If a separate response mask is passed in, that is combined with the masks tensor to further mask just the response portion.

            for j in range(len(query_batch)):
                if self.is_encoder_decoder:
                    # Decoder sentence starts always in the index 1 after padding in the Enc-Dec Models
                    start = 1
                    end = attention_mask[j, :].sum() - 1
                else:
                    start = len(query_batch[j]) - 1  # logprobs starts from the second query token
                    am_start = start
                    if attention_mask[j, 0] == 0:  # offset left padding
                        am_start += attention_mask[j, :].nonzero()[0]
                    end = start + len(response_batch[j])
                    am_end = am_start + len(response_batch[j])
                    if response_masks is not None:
                        response_masks_batch[j] = torch.cat(
                            (torch.zeros_like(query_batch[j]), response_masks_batch[j])
                        )[1:]

                masks[j, :start] = 0
                masks[j, end:] = 0
                if response_masks is not None:
                    # this was incorrect
                    masks[j, am_start:am_end] = masks[j, am_start:am_end] * response_masks_batch[j][start:end]

            if return_logits:
                all_logits.append(logits)
            else:
                del logits
            all_values.append(values)
            all_logprobs.append(logprobs)
            all_masks.append(masks)

        return (
            torch.cat(all_logprobs),
            torch.cat(all_logits)[:, :-1] if return_logits else None,
            torch.cat(all_values)[:, :-1],
            torch.cat(all_masks)[:, :-1],
        )

    @PPODecorators.empty_cuda_cache()
    def train_minibatch(
        self,
        old_logprobs: torch.FloatTensor,
        values: torch.FloatTensor,
        logprobs: torch.FloatTensor,
        logits: torch.FloatTensor,
        vpreds: torch.FloatTensor,
        mask: torch.LongTensor,
        advantages: torch.FloatTensor,
        returns: torch.FloatTensor,
    ):
        """
        Train one PPO minibatch

        Args:
            logprobs (`torch.FloatTensor`):
                Log probabilities of the model, shape [batch_size, response_length]
            values (`torch.FloatTensor`):
                Values of the value head, shape [batch_size, response_length]
            query (`torch.LongTensor`):
                Encoded queries, shape [batch_size, query_length]
            response (`torch.LongTensor`):
                Encoded responses, shape [batch_size, response_length]
            model_input (`torch.LongTensor`):
                Concatenated queries and responses, shape [batch_size, query_length+response_length]

        Returns:
            train_stats (dict[str, `torch.Tensor`]):
                Dictionary of training statistics
        """
        self.model.train()
        loss_p, loss_v, train_stats = self.loss(
            old_logprobs, values, logits, vpreds, logprobs, mask, advantages, returns
        )
        loss = loss_p + loss_v
        self.accelerator.backward(loss)
        if self.config.max_grad_norm is not None:
            if self.accelerator.sync_gradients:
                self.accelerator.clip_grad_norm_(self.model_params, self.config.max_grad_norm)
        self.optimizer.step()
        # we call optimizer.zero_grad() every time and let `accelerator` handle accumulation
        # see https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation#the-finished-code
        self.optimizer.zero_grad()
        return train_stats

    def compute_rewards(
        self,
        scores: torch.FloatTensor,
        logprobs: torch.FloatTensor,
        ref_logprobs: torch.FloatTensor,
        masks: torch.LongTensor,
    ):
        """
        Compute per token rewards from scores and KL-penalty.

        Args:
            scores (`torch.FloatTensor`):
                Scores from the reward model, shape (`batch_size`)
            logprobs (`torch.FloatTensor`):
                Log probabilities of the model, shape (`batch_size`, `response_length`)
            ref_logprobs (`torch.FloatTensor`):
                Log probabilities of the reference model, shape (`batch_size`, `response_length`)
        """
        rewards, non_score_rewards = [], []
        i = 0
        for score, logprob, ref_logprob, mask in zip(scores, logprobs, ref_logprobs, masks):
            #print(i, len(mask.nonzero()))
            # compute KL penalty (from difference in logprobs)
            kl = self._kl_penalty(logprob, ref_logprob)
            non_score_reward = -self.kl_ctl.value * kl
            non_score_rewards.append(non_score_reward)
            reward = non_score_reward.clone()
            if len(mask.nonzero()) == 0:
                last_non_masked_index = 0
            else: 
                last_non_masked_index = mask.nonzero()[-1]

            # reward is preference model score + KL penalty
            reward[last_non_masked_index] += score
            rewards.append(reward)
            i += 1
        return torch.stack(rewards), torch.stack(non_score_rewards)

    def _kl_penalty(self, logprob: torch.FloatTensor, ref_logprob: torch.FloatTensor) -> torch.FloatTensor:
        if self.config.kl_penalty == "kl":
            return logprob - ref_logprob

        if self.config.kl_penalty == "abs":
            return (logprob - ref_logprob).abs()

        if self.config.kl_penalty == "mse":
            return 0.5 * (logprob - ref_logprob).square()

        if self.config.kl_penalty == "full":
            # Flip is required due to this issue? :https://github.com/pytorch/pytorch/issues/57459
            return F.kl_div(ref_logprob, logprob, log_target=True, reduction="none").sum(-1)

        raise NotImplementedError

    def find_alpha_return(
        self: torch.FloatTensor,
        traj_returns: torch.FloatTensor,
        mask: torch.FloatTensor,
        risk_alpha: torch.LongTensor,
    ):
        sorted_returns, indices = torch.sort(traj_returns)
        q_risk_alpha = sorted_returns[risk_alpha-1]

        for ii in range(traj_returns.shape[0]):
            if traj_returns[ii] > q_risk_alpha:
                #print('here', risk_alpha, ii)
                mask[ii,:] = torch.zeros_like(mask[ii,:], dtype=int)

        return q_risk_alpha, mask


    def compute_advantages(
        self: torch.FloatTensor,
        values: torch.FloatTensor,
        rewards: torch.FloatTensor,
        mask: torch.FloatTensor,
        risk_alpha: torch.LongTensor,
    ):
        lastgaelam = 0
        advantages_reversed = []
        gen_len = rewards.shape[-1]

        values = values * mask
        rewards = rewards * mask

        for t in reversed(range(gen_len)):
            nextvalues = values[:, t + 1] if t < gen_len - 1 else 0.0
            delta = rewards[:, t] + self.config.gamma * nextvalues - values[:, t]
            lastgaelam = delta + self.config.gamma * self.config.lam * lastgaelam
            advantages_reversed.append(lastgaelam)
        advantages = torch.stack(advantages_reversed[::-1]).transpose(0, 1)

        returns = advantages + values
        # select 63rd index element of returns in the batch
        traj_returns = returns[:,63]
        # find alpha_return and create new mask based on alpha_return
        alpha_return, masks = self.find_alpha_return(traj_returns, mask, risk_alpha)
        # return this 'masks' so that step() can pass this mask to loss()

        # Whiten / normalize the unmasked elements
        advantages = masked_whiten(advantages, mask)
        advantages = advantages.detach()
        return values, advantages, returns, masks

    def loss(
        self,
        old_logprobs: torch.FloatTensor,
        values: torch.FloatTensor,
        logits: torch.FloatTensor,
        vpreds: torch.FloatTensor,
        logprobs: torch.FloatTensor,
        mask: torch.LongTensor,
        advantages: torch.FloatTensor,
        returns: torch.FloatTensor,
    ):
        """
        Calculate policy and value losses.

        Args:
            old_logprobs (`torch.FloatTensor`):
                Log probabilities of the model, shape (`batch_size`, `response_length`)
            values (`torch.FloatTensor`):
                Values of the value head, shape (`batch_size`, `response_length`)
            rewards (`torch.FloatTensor`):
                Rewards from the reward model, shape (`batch_size`, `response_length`)
            logits (`torch.FloatTensor`):
                Logits of the model, shape (`batch_size`, `response_length`, `vocab_size`)
            v_pred (`torch.FloatTensor`):
                Values of the value head, shape (`batch_size`, `response_length`)
            logprobs (`torch.FloatTensor`):
                Log probabilities of the model, shape (`batch_size`, `response_length`)
        """

        vpredclipped = clip_by_value(
            vpreds,
            values - self.config.cliprange_value,
            values + self.config.cliprange_value,
        )

        vf_losses1 = (vpreds - returns) ** 2
        vf_losses2 = (vpredclipped - returns) ** 2
        vf_loss = 0.5 * masked_mean(torch.max(vf_losses1, vf_losses2), mask)
        vf_clipfrac = masked_mean(torch.gt(vf_losses2, vf_losses1).float(), mask)

        ratio = torch.exp(logprobs - old_logprobs)

        pg_losses = -advantages * ratio
        pg_losses2 = -advantages * torch.clamp(ratio, 1.0 - self.config.cliprange, 1.0 + self.config.cliprange)

        pg_loss = masked_mean(torch.max(pg_losses, pg_losses2), mask)
        pg_clipfrac = masked_mean(torch.gt(pg_losses2, pg_losses).float(), mask)

        loss = pg_loss + self.config.vf_coef * vf_loss

        avg_ratio = masked_mean(ratio, mask).item()
        if avg_ratio > self.config.ratio_threshold:
            warnings.warn(
                f"The average ratio of batch ({avg_ratio:.2f}) exceeds threshold {self.config.ratio_threshold:.2f}. Skipping batch."
            )
            pg_loss = pg_loss * 0.0
            vf_loss = vf_loss * 0.0
            loss = loss * 0.0

        entropy = masked_mean(entropy_from_logits(logits), mask)

        approxkl = 0.5 * masked_mean((logprobs - old_logprobs) ** 2, mask)
        policykl = masked_mean(old_logprobs - logprobs, mask)

        return_mean, return_var = masked_mean(returns, mask), masked_var(returns, mask)
        value_mean, value_var = masked_mean(values, mask), masked_var(values, mask)

        stats = dict(
            loss=dict(policy=pg_loss.detach(), value=vf_loss.detach(), total=loss.detach()),
            policy=dict(
                entropy=entropy.detach(),
                approxkl=approxkl.detach(),
                policykl=policykl.detach(),
                clipfrac=pg_clipfrac.detach(),
                advantages=advantages.detach(),
                advantages_mean=masked_mean(advantages, mask).detach(),
                ratio=ratio.detach(),
            ),
            returns=dict(mean=return_mean.detach(), var=return_var.detach()),
            val=dict(
                vpred=masked_mean(vpreds, mask).detach(),
                error=masked_mean((vpreds - returns) ** 2, mask).detach(),
                clipfrac=vf_clipfrac.detach(),
                mean=value_mean.detach(),
                var=value_var.detach(),
            ),
        )
        return pg_loss, self.config.vf_coef * vf_loss, flatten_dict(stats)

    def record_step_stats(self, kl_coef: float, **data):
        """
        Record training step statistics.


        Args:
            kl_coef (`float`):
                KL coefficient
            data (`dict`):
                Dictionary of training step data

        Returns:
            stats (`dict`):
                Dictionary of training step statistics
        """
        mask = data.pop("masks")

        kl_list = ((data["logprobs"] - data["ref_logprobs"]) * mask).sum(axis=-1)
        mean_kl = kl_list.mean()
        mean_entropy = (-data["logprobs"] * mask).sum(axis=-1).mean()

        mean_non_score_reward = masked_mean(
            data["non_score_reward"], mask
        )  # non_score_reward is size `batch_size`, `response_length`
        mean_scores = data["scores"].mean()  # scores is size `batch_size`
        std_scores = data["scores"].std()

        if mean_kl.item() < -1.0:
            # warn users
            warnings.warn(
                f"KL divergence is starting to become negative: {mean_kl.item():.2f} - this might be a precursor for failed training."
                " sometimes this happens because the generation kwargs are not correctly set. Please make sure"
                " that the generation kwargs are set correctly, or review your training hyperparameters."
            )

        stats = {
            "objective/kl": mean_kl,
            "objective/kl_dist": kl_list,
            "objective/logprobs": data["logprobs"],
            "objective/ref_logprobs": data["ref_logprobs"],
            "objective/kl_coef": kl_coef,
            "objective/entropy": mean_entropy,
            "ppo/mean_non_score_reward": mean_non_score_reward,
            "ppo/mean_scores": mean_scores,
            "ppo/std_scores": std_scores,
        }

        # Log text properties
        query_lens = torch.tensor([len(query) for query in data["queries"]], dtype=torch.float)
        response_lens = torch.tensor([len(response) for response in data["responses"]], dtype=torch.float)

        stats["tokens/queries_len_mean"] = torch.mean(query_lens).cpu().numpy().item()
        stats["tokens/queries_len_std"] = torch.std(query_lens).cpu().numpy().item()
        stats["tokens/queries_dist"] = query_lens.cpu().numpy()
        stats["tokens/responses_len_mean"] = torch.mean(response_lens).cpu().numpy().item()
        stats["tokens/responses_len_std"] = torch.std(response_lens).cpu().numpy().item()
        stats["tokens/responses_dist"] = response_lens.cpu().numpy()

        for k, v in data["train_stats"].items():
            print(k)
            stats[f"ppo/{k}"] = torch.mean(v, axis=0)
        stats["ppo/val/var_explained"] = 1 - stats["ppo/val/error"] / stats["ppo/returns/var"]
        return stats

    def log_stats(
        self,
        stats: dict,
        batch: dict,
        rewards: List[torch.FloatTensor],
        columns_to_log: List[str] = ["query", "response"],
    ):
        """
        A function that logs all the training stats. Call it at the end of each epoch.

        Args:
            stats (dict[str, Any]):
                A dictionary of training stats.
            batch (dict[str, Any]):
                A dictionary of batch data, this contains the queries and responses.
            rewards (`List[torch.FloatTensor]`):
                A tensor of rewards.
        """
        # Log only if we are in the main process
        if self.accelerator.is_main_process:
            logs = {}

            # Log stats
            if not isinstance(rewards, torch.Tensor):
                rewards = torch.tensor(rewards).to(self.current_device)

            if "query" not in batch.keys() and "response" not in batch.keys():
                # warn the user that the game logs will not be logged
                warnings.warn(
                    "The game logs will not be logged because the batch does not contain the keys 'query' and "
                    "'response'. "
                )
            elif self.config.log_with == "wandb":
                import wandb

                if any([column_to_log not in batch.keys() for column_to_log in columns_to_log]):
                    raise ValueError(f"Columns to log {columns_to_log} are not present in the batch {batch.keys()}.")

                batch_list = [batch[column_to_log] for column_to_log in columns_to_log]

                table_rows = [list(r) for r in zip(*batch_list, rewards.cpu().tolist())]
                logs.update({"game_log": wandb.Table(columns=[*columns_to_log, "reward"], rows=table_rows)})
            # All reduce rewards if distributed
            if self.is_distributed:
                import torch.distributed as dist

                dist.barrier()

                dist.all_reduce(rewards, op=torch.distributed.ReduceOp.SUM)
                rewards /= self.accelerator.num_processes

            logs.update(stats)

            # manually cast in fp32 for bf16 torch tensors
            for k, v in logs.items():
                if isinstance(v, torch.Tensor) and v.dtype == torch.bfloat16:
                    logs[k] = v.float()

            logs["env/reward_mean"] = torch.mean(rewards).cpu().numpy().item()
            logs["env/reward_std"] = torch.std(rewards).cpu().numpy().item()
            logs["env/reward_dist"] = rewards.cpu().numpy()

            if self.config.log_with == "tensorboard":
                # update the current step
                self.current_step += 1

            self.accelerator.log(
                logs,
                step=self.current_step if self.config.log_with == "tensorboard" else None,
            )

        else:
            if self.is_distributed:
                import torch.distributed as dist

                if not isinstance(rewards, torch.Tensor):
                    rewards = torch.tensor(rewards).to(self.current_device)

                dist.barrier()
                dist.all_reduce(rewards, op=torch.distributed.ReduceOp.SUM)

    def create_model_card(self, path: str, model_name: Optional[str] = "TRL Model") -> None:
        """Creates and saves a model card for a TRL model.

        Args:
            path (`str`): The path to save the model card to.
            model_name (`str`, *optional*): The name of the model, defaults to `TRL Model`.
        """
        try:
            user = whoami()["name"]
        # handle the offline case
        except:  # noqa
            warnings.warn("Cannot retrieve user information assuming you are running in offline mode.")
            return

        if not os.path.exists(path):
            os.makedirs(path)

        model_card_content = MODEL_CARD_TEMPLATE.format(model_name=model_name, model_id=f"{user}/{path}")
        with open(os.path.join(path, "README.md"), "w", encoding="utf-8") as f:
            f.write(model_card_content)

    def _save_pretrained(self, save_directory: str) -> None:
        self.accelerator.unwrap_model(self.model).save_pretrained(save_directory)
        self.tokenizer.save_pretrained(save_directory)
        self.create_model_card(save_directory)

    def _show_tokens(self, tokens, masks):
        from rich import print
        from rich.text import Text

        text = Text()

        for i, (token, mask) in enumerate(zip(tokens, masks)):
            if mask == 1:
                text.append(self.tokenizer.decode(token.item()), style="black on deep_sky_blue1")
                text.append(" ")
            else:
                text.append(self.tokenizer.decode(token.item()), style="black on cyan3")
                text.append(" ")
        print(text)

    def _prepare_deepspeed(self, model: PreTrainedModelWrapper):
        # Adapted from accelerate: https://github.com/huggingface/accelerate/blob/739b135f8367becb67ffaada12fe76e3aa60fefd/src/accelerate/accelerator.py#L1473
        deepspeed_plugin = self.accelerator.state.deepspeed_plugin
        config_kwargs = deepspeed_plugin.deepspeed_config
        if model is not None:
            if hasattr(model, "config"):
                hidden_size = (
                    max(model.config.hidden_sizes)
                    if getattr(model.config, "hidden_sizes", None)
                    else getattr(model.config, "hidden_size", None)
                )
                if hidden_size is not None and config_kwargs["zero_optimization"]["stage"] == 3:
                    # Note that `stage3_prefetch_bucket_size` can produce DeepSpeed messages like: `Invalidate trace cache @ step 0: expected module 1, but got module 0`
                    # This is expected and is not an error, see: https://github.com/microsoft/DeepSpeed/discussions/4081
                    config_kwargs.update(
                        {
                            "zero_optimization.reduce_bucket_size": hidden_size * hidden_size,
                            "zero_optimization.stage3_param_persistence_threshold": 10 * hidden_size,
                            "zero_optimization.stage3_prefetch_bucket_size": 0.9 * hidden_size * hidden_size,
                        }
                    )

        # If ZeRO-3 is used, we shard both the active and reference model.
        # Otherwise, we assume the reference model fits in memory and is initialized on each device with ZeRO disabled (stage 0)
        if config_kwargs["zero_optimization"]["stage"] != 3:
            config_kwargs["zero_optimization"]["stage"] = 0
        model, *_ = deepspeed.initialize(model=model, config=config_kwargs)
        model.eval()
        return model

File Path: trl/trainer/ppo_trainer_2.py
Content:
# Copyright 2022 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import inspect
import math
import os
import time
import typing
import warnings
from typing import Callable, List, Optional, Union

import datasets
import numpy as np
import torch
import torch.nn.functional as F
from accelerate import Accelerator
from accelerate.utils import ProjectConfiguration, is_deepspeed_available
from datasets import Dataset
from huggingface_hub import whoami
from packaging import version
from torch.optim import Adam
from transformers import (
    DataCollatorForLanguageModeling,
    PreTrainedTokenizer,
    PreTrainedTokenizerBase,
    PreTrainedTokenizerFast,
)

from ..core import (
    WANDB_PADDING,
    PPODecorators,
    clip_by_value,
    convert_to_scalar,
    entropy_from_logits,
    flatten_dict,
    logprobs_from_logits,
    masked_mean,
    masked_var,
    masked_whiten,
    set_seed,
    stack_dicts,
    stats_to_np,
)
from ..import_utils import is_torch_greater_2_0
from ..models import SUPPORTED_ARCHITECTURES, PreTrainedModelWrapper, create_reference_model
from . import AdaptiveKLController, BaseTrainer, FixedKLController, PPOConfig, RunningMoments

import pdb

if is_deepspeed_available():
    import deepspeed

MODEL_CARD_TEMPLATE = """---
license: apache-2.0
tags:
- trl
- transformers
- reinforcement-learning
---

# {model_name}

This is a [TRL language model](https://github.com/huggingface/trl) that has been fine-tuned with reinforcement learning to
 guide the model outputs according to a value, function, or human feedback. The model can be used for text generation.

## Usage

To use this model for inference, first install the TRL library:

```bash
python -m pip install trl
```

You can then generate text as follows:

```python
from transformers import pipeline

generator = pipeline("text-generation", model="{model_id}")
outputs = generator("Hello, my llama is cute")
```

If you want to use the model for training or to obtain the outputs from the value head, load the model as follows:

```python
from transformers import AutoTokenizer
from trl import AutoModelForCausalLMWithValueHead

tokenizer = AutoTokenizer.from_pretrained("{model_id}")
model = AutoModelForCausalLMWithValueHead.from_pretrained("{model_id}")

inputs = tokenizer("Hello, my llama is cute", return_tensors="pt")
outputs = model(**inputs, labels=inputs["input_ids"])
```
"""


class PPOTrainer(BaseTrainer):
    """
    The PPOTrainer uses Proximal Policy Optimization to optimise language models.
    Note, this trainer is heavily inspired by the original OpenAI learning to summarize work here:
    https://github.com/openai/summarize-from-feedback

    Attributes:
        **config** (`PPOConfig`) -- Configuration object for PPOTrainer. Check the documentation of `PPOConfig` for more
            details.
        **model** (`PreTrainedModelWrapper`) -- Model to be optimized, Hugging Face transformer model with a value head.
            Check the documentation of `PreTrainedModelWrapper` for more details.
        **ref_model** (`PreTrainedModelWrapper`, *optional*) -- Reference model to be used for KL penalty, Hugging Face
            transformer model with a casual language modelling head. Check the documentation of `PreTrainedModelWrapper`
            for more details. If no reference model is provided, the trainer will create a reference model with the same
             architecture as the model to be optimized with shared layers.
        **tokenizer** (`PreTrainedTokenizerBase`) -- Tokenizer to be used for encoding the
            data. Check the documentation of `transformers.PreTrainedTokenizer` and
            `transformers.PreTrainedTokenizerFast` for more details.
        **dataset** (Union[`torch.utils.data.Dataset`, `datasets.Dataset`], *optional*) -- PyTorch dataset or Hugging
            Face dataset. This is used to create a PyTorch dataloader. If no dataset is provided, the dataloader must be
             created outside the trainer users needs to design their own dataloader and make sure the batch
            size that is used is the same as the one specified in the configuration object.
        **optimizer** (`torch.optim.Optimizer`, *optional*) -- Optimizer to be used for training. If no optimizer is
            provided, the trainer will create an Adam optimizer with the learning rate specified in the configuration
            object.
        **data_collator** (DataCollatorForLanguageModeling, *optional*) -- Data collator to be used for training and
            passed along the dataloader
        **num_shared_layers** (int, *optional*) -- Number of layers to be shared between the model and the reference
            model, if no reference model is passed. If no number is provided, all the layers will be shared.
        **lr_scheduler** (`torch.optim.lr_scheduler`, *optional*) -- Learning rate scheduler to be used for training.
    """

    def __init__(
        self,
        config: PPOConfig = None,
        model: PreTrainedModelWrapper = None,
        ref_model: Optional[PreTrainedModelWrapper] = None,
        tokenizer: PreTrainedTokenizerBase = None,
        dataset: Optional[Union[torch.utils.data.Dataset, Dataset]] = None,
        optimizer: Optional[torch.optim.Optimizer] = None,
        data_collator: Optional[typing.Callable] = None,
        num_shared_layers: Optional[int] = None,
        lr_scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,
    ):
        """
        Initialize PPOTrainer.

        Args:
            config (`PPOConfig`):
                Configuration object for PPOTrainer. Check the documentation of `PPOConfig` for more details.
            model (`PreTrainedModelWrapper`):
                Hugging Face transformer model with a value head.
            ref_model (`PreTrainedModelWrapper`):
                Hugging Face transformer model with a casual language modelling head. Used for KL penalty
            tokenizer (`transformers.PreTrainedTokenizerBase`):
                Hugging Face tokenizer
            dataset (Optional[Union[`torch.utils.data.Dataset`, `datasets.Dataset`]]):
                PyTorch dataset or Hugging Face dataset. If a Hugging Face dataset is passed, the dataset
                will be preprocessed by removing the columns that are not used by the model. If none is passed,
                a warning will be raised in a multi-GPU setting.
            optimizer (Optional[`torch.optim.Optimizer`]):
                Optimizer used for training. If `None`, the `Adam` is used as default.
            data_collator (Optional[function]):
                Data collator function.
            num_shared_layers (Optional[int]):
                Number of shared layers between the model and the reference model. If `None`, all layers are shared.
                used only if `ref_model` is `None`.
            lr_scheduler (Optional[`torch.optim.lr_scheduler`]):
                Learning rate scheduler used for training.
        """
        super().__init__(config)

        # initial seed for reproducible experiments
        set_seed(config.seed)

        # Step 0: check positional arguments validity
        if not isinstance(config, PPOConfig):
            raise ValueError(f"config must be a PPOConfig, got {type(config)}")
        if not isinstance(tokenizer, (PreTrainedTokenizerBase)):
            raise ValueError(
                f"tokenizer must be a PreTrainedTokenizerBase like a PreTrainedTokenizer or a PreTrainedTokenizerFast, got {type(tokenizer)}"
            )
        if not isinstance(model, (SUPPORTED_ARCHITECTURES)):
            raise ValueError(
                f"model must be a PreTrainedModelWrapper, got {type(model)} - supported architectures are: {SUPPORTED_ARCHITECTURES}"
            )
        # Step 1: Initialize Accelerator
        self.accelerator = Accelerator(
            log_with=config.log_with,
            gradient_accumulation_steps=config.gradient_accumulation_steps,
            project_config=ProjectConfiguration(**config.project_kwargs),
            **config.accelerator_kwargs,
        )

        # Step 1.1 Runtime variables filled by the accelerator
        config.world_size = self.accelerator.num_processes
        config.global_backward_batch_size = config.backward_batch_size * config.world_size
        config.global_batch_size = config.batch_size * config.world_size

        self.model = model
        self.model_params = filter(lambda p: p.requires_grad, self.model.parameters())
        self.is_encoder_decoder = hasattr(self.model, "is_encoder_decoder")
        self.is_peft_model = getattr(self.model, "is_peft_model", False)
        config.is_encoder_decoder = self.is_encoder_decoder
        config.is_peft_model = self.is_peft_model

        is_using_tensorboard = config.log_with is not None and config.log_with == "tensorboard"
        self.accelerator.init_trackers(
            config.tracker_project_name,
            config=dict(trl_ppo_trainer_config=config.to_dict()) if not is_using_tensorboard else config.to_dict(),
            init_kwargs=config.tracker_kwargs,
        )
        self.is_using_text_environment = getattr(config, "use_text_environment", False)

        if isinstance(ref_model, SUPPORTED_ARCHITECTURES):
            self.ref_model = ref_model
            if num_shared_layers is not None:
                warnings.warn(
                    "num_shared_layers is ignored when ref_model is provided. Two different models are used for the "
                    "model and the reference model and no layers are shared.",
                    UserWarning,
                )
        elif ref_model is None and not self.is_peft_model:
            self.ref_model = create_reference_model(self.model, num_shared_layers=num_shared_layers)
        elif self.is_peft_model:
            self.ref_model = None
        else:
            raise ValueError(
                f"ref_model must be a PreTrainedModelWrapper or `None`, got {type(ref_model)} - supported "
                f"architectures are: {SUPPORTED_ARCHITECTURES} "
            )

        if not (isinstance(tokenizer, PreTrainedTokenizer) or isinstance(tokenizer, PreTrainedTokenizerFast)):
            raise ValueError(
                "tokenizer must be a transformers.PreTrainedTokenizer or transformers.PreTrainedTokenizerFast"
            )
        self.tokenizer = tokenizer

        if dataset is not None and not (isinstance(dataset, torch.utils.data.Dataset) or isinstance(dataset, Dataset)):
            raise ValueError("dataset must be a torch.utils.data.Dataset or datasets.Dataset")
        elif dataset is None:
            warnings.warn(
                "No dataset is provided. Make sure to set config.batch_size to the correct value before training.",
                UserWarning,
            )
        self.dataset = dataset
        self._signature_columns = None
        if self.dataset is not None:
            self.dataloader = self.prepare_dataloader(self.dataset, data_collator)
        elif self.dataset is None and self.accelerator.num_processes > 1:
            warnings.warn(
                "No dataset is provided. In a multi-GPU setting, this will lead to an error. You should"
                " prepare your dataloader yourself with `dataloader = ppo_trainer.accelerator.prepare(dataloader)`"
                " and using `torch.utils.data.DataLoader`, or pass a dataset to the `PPOTrainer`. Please "
                " refer to the documentation for more details.",
                UserWarning,
            )
            self.dataloader = None
        else:
            self.dataloader = None

        # Step 3: Initialize optimizer and data collator
        self.data_collator = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)
        if optimizer is None:
            self.optimizer = Adam(
                filter(lambda p: p.requires_grad, self.model.parameters()),
                lr=self.config.learning_rate,
            )
        else:
            self.optimizer = optimizer

        self.lr_scheduler = lr_scheduler
        if self.lr_scheduler is not None:
            lr_scheduler_class = (
                torch.optim.lr_scheduler._LRScheduler
                if not is_torch_greater_2_0()
                else torch.optim.lr_scheduler.LRScheduler
            )

            if not isinstance(self.lr_scheduler, lr_scheduler_class):
                raise ValueError(
                    "lr_scheduler must be a torch.optim.lr_scheduler._LRScheduler or torch.optim.lr_scheduler.LRScheduler (for torch >= 2.0)"
                )

        if self.config.adap_kl_ctrl:
            self.kl_ctl = AdaptiveKLController(self.config.init_kl_coef, self.config.target, self.config.horizon)
        else:
            self.kl_ctl = FixedKLController(self.config.init_kl_coef)

        # Safety checkers for DS integration
        is_deepspeed_used = self.accelerator.distributed_type == "DEEPSPEED" and hasattr(
            self.accelerator.state, "deepspeed_plugin"
        )

        (
            self.model,
            self.optimizer,
            self.data_collator,
            self.dataloader,
            self.lr_scheduler,
        ) = self.accelerator.prepare(
            self.model,
            self.optimizer,
            self.data_collator,
            self.dataloader,
            self.lr_scheduler,
        )
        if is_deepspeed_used:
            # Quantized models are already set on the correct device
            if not self.is_peft_model and not (
                getattr(self.ref_model.pretrained_model, "is_loaded_in_8bit", False)
                or getattr(self.ref_model.pretrained_model, "is_loaded_in_4bit", False)
            ):
                self.ref_model = self._prepare_deepspeed(self.ref_model)
        else:
            self.ref_model = self.accelerator.prepare(self.ref_model)

        # In a distributed setup, only logging needs to be performed on the main process
        # check: https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html
        # or: https://discuss.pytorch.org/t/use-distributed-data-parallel-correctly/82500/11
        self.is_distributed = self.accelerator.distributed_type == "MULTI_GPU"

        # init the current step
        self.current_step = 0

        # init variables for pushing model to hub
        if config.push_to_hub_if_best_kwargs:
            if "repo_id" not in config.push_to_hub_if_best_kwargs:
                raise ValueError("You have to specify repo_id in order to push the model to the hub!")
            self.push_to_hub_kwargs = config.push_to_hub_if_best_kwargs
            self.compare_step = 0
            self.highest_reward = torch.tensor(-float("inf"))

        # post process for PP
        if not getattr(self.model, "is_sequential_parallel", False):
            self.current_device = self.accelerator.device
        else:
            self.current_device = torch.device("cuda:0")

        PPODecorators.optimize_cuda_cache = self.config.optimize_cuda_cache

        self.running = RunningMoments(self.accelerator)

    def _filter_kwargs(self, kwargs, target_func):
        """
        filter the keyword arguments that are supported by the target function.

        Args:
            kwargs (dict):
                Keyword arguments
            target_func (function):
                Target function
        """
        return {k: v for k, v in kwargs.items() if k in inspect.signature(target_func).parameters.keys()}

    def prepare_dataloader(self, dataset: Union[torch.utils.data.Dataset, Dataset], data_collator=None):
        """
        Prepare the dataloader for training.

        Args:
            dataset (Union[`torch.utils.data.Dataset`, `datasets.Dataset`]):
                PyTorch dataset or Hugging Face dataset. If a Hugging Face dataset is passed, the dataset
                will be preprocessed by removing the columns that are not used by the model.
            data_collator (Optional[function]):
                Data collator function.

        Returns:
            `torch.utils.data.DataLoader`: PyTorch dataloader
        """
        if isinstance(dataset, Dataset):
            dataset = self._remove_unused_columns(dataset)
        dataloader = torch.utils.data.DataLoader(
            dataset,
            batch_size=self.config.batch_size,
            collate_fn=data_collator,
            shuffle=True,
            drop_last=True,
        )
        return dataloader

    # Adapted from transformers.Trainer._set_signature_columns_if_needed
    def _set_signature_columns_if_needed(self):
        if self._signature_columns is None:
            # Inspect model forward signature to keep only the arguments it accepts.
            signature = inspect.signature(self.model.forward)
            self._signature_columns = list(signature.parameters.keys())
            # label => sentiment | we need query and response for logging purpose
            self._signature_columns += ["label", "query", "response"]

    # Adapted from transformers.Trainer._remove_unused_columns
    def _remove_unused_columns(self, dataset: "Dataset"):
        if not self.config.remove_unused_columns:
            return dataset
        self._set_signature_columns_if_needed()
        signature_columns = self._signature_columns

        ignored_columns = list(set(dataset.column_names) - set(signature_columns))

        columns = [k for k in signature_columns if k in dataset.column_names]

        if version.parse(datasets.__version__) < version.parse("1.4.0"):
            dataset.set_format(
                type=dataset.format["type"],
                columns=columns,
                format_kwargs=dataset.format["format_kwargs"],
            )
            return dataset
        else:
            return dataset.remove_columns(ignored_columns)

    def generate(
        self,
        query_tensor: Union[torch.Tensor, List[torch.Tensor]],
        length_sampler: Callable = None,
        batch_size: int = 4,
        return_prompt: bool = True,
        **generation_kwargs,
    ):
        """
        Generate response with the model given the query tensor.
        call the `generate` method of the model.

        Args:
            query_tensor (`torch.LongTensor`):
                A tensor of shape (`batch_size`, `seq_len`) containing query tokens.
            generation_kwargs (dict[str, Any]):
                Keyword arguments for generation.
            length_sampler (`Callable`, *optional*):
                Callable that returns the number of newly generated tokens.
            batch_size (`int`, *optional):
                Batch size used for generation, defaults to `4`.
            return_prompt (`bool`, *optional*):
                If set to `False` the prompt is not returned but only the newly generated tokens, defaults to `True`.

        Returns:
            `torch.LongTensor`: A tensor of shape (`batch_size`, `gen_len`) containing response tokens.
        """

        if isinstance(query_tensor, List):
            return self._generate_batched(
                query_tensor,
                length_sampler=length_sampler,
                batch_size=batch_size,
                return_prompt=return_prompt,
                **generation_kwargs,
            )

        else:
            if length_sampler is not None:
                generation_kwargs["max_new_tokens"] = length_sampler()
            response = self.accelerator.unwrap_model(self.model).generate(
                input_ids=query_tensor.unsqueeze(dim=0), **generation_kwargs
            )

            if not return_prompt and not self.is_encoder_decoder:
                return response[:, query_tensor.shape[0] :]
            return response

    def _generate_batched(
        self,
        query_tensors: List[torch.Tensor],
        length_sampler: Callable = None,
        batch_size: int = 4,
        return_prompt: bool = True,
        pad_to_multiple_of: int = None,
        remove_padding: bool = True,
        **generation_kwargs,
    ):
        outputs = []

        padding_side_default = self.tokenizer.padding_side
        if not self.is_encoder_decoder:
            self.tokenizer.padding_side = "left"

        # in case we have fewer examples than bs
        batch_size = min(len(query_tensors), batch_size)

        for i in range(0, len(query_tensors), batch_size):
            if length_sampler is not None:
                generation_kwargs["max_new_tokens"] = length_sampler()

            # prevent overflow if query tensors are not even multiple of bs
            end_index = min(len(query_tensors), i + batch_size)

            batch = query_tensors[i:end_index]
            batch_mask = [torch.ones_like(element) for element in batch]
            inputs = {"input_ids": batch, "attention_mask": batch_mask}

            padded_inputs = self.tokenizer.pad(
                inputs,
                padding=True,
                max_length=None,
                pad_to_multiple_of=pad_to_multiple_of,
                return_tensors="pt",
            ).to(self.current_device)

            generations = self.accelerator.unwrap_model(self.model).generate(**padded_inputs, **generation_kwargs)

            for generation, mask in zip(generations, padded_inputs["attention_mask"]):
                if not self.is_encoder_decoder:
                    output = generation[(1 - mask).sum() :]  # remove padding
                else:
                    output = generation

                if not return_prompt and not self.is_encoder_decoder:
                    output = output[(mask).sum() :]  # remove prompt

                if remove_padding and self.tokenizer.eos_token_id in output:
                    pad_mask = output == self.tokenizer.eos_token_id
                    pad_start = torch.nonzero(pad_mask, as_tuple=False)[0, 0].item()
                    output = output[: pad_start + 1]  # keep the eos token at the end

                outputs.append(output)

        self.tokenizer.padding_side = padding_side_default
        return outputs

    def _step_safety_checker(
        self,
        batch_size: int,
        queries: List[torch.LongTensor],
        responses: List[torch.LongTensor],
        scores: List[torch.FloatTensor],
        masks: Optional[List[torch.LongTensor]] = None,
    ):
        """
        Check if the input data is valid for training.

        Args:
            batch_size (int):
                Batch size from the config file.
            queries (List[`torch.LongTensor`]):
                List of tensors containing the encoded queries of shape (`query_length`)
            responses (List[`torch.LongTensor`]):
                List of tensors containing the encoded responses of shape (`response_length`)
            scores (List[`torch.FloatTensor`]):
                List of tensors containing the scores.
            masks (List[`torch.LongTensor`], *optional*):
                list of optional tensors containing the masks of shape (`query_length` + `response_length`)
        Returns:
            `tuple`: The input processed data.
        """
        for name, tensor_list in zip(["queries", "responses", "scores"], [queries, responses, scores]):
            if not isinstance(tensor_list, list):
                raise ValueError(f"{name} must be a list of tensors - got {type(tensor_list)}")
            if not isinstance(tensor_list[0], torch.Tensor):
                raise ValueError(f"Elements in {name} must be tensors - got {type(tensor_list[0])}")
            if batch_size is not None and len(tensor_list) != batch_size:
                raise ValueError(
                    f"Batch size ({batch_size}) does not match number of examples - but got {len(tensor_list)} for: {name}"
                )

        # add queries, scores and responses on the correct device
        queries = [tensor.to(self.current_device) for tensor in queries]
        responses = [tensor.to(self.current_device) for tensor in responses]
        scores = [tensor.to(self.current_device) for tensor in scores]
        masks = [tensor.to(self.current_device) for tensor in masks] if masks is not None else None

        # squeeze scores if needed
        for i, score in enumerate(scores):
            if score.dim() > 1:
                raise ValueError(f"Scores must be 1-dimensional - got {score.dim()} for {score}")
            elif score.dim() == 1:
                scores[i] = score.squeeze()

        return queries, responses, scores, masks

    @PPODecorators.empty_cuda_cache()
    def step(
        self,
        queries: List[torch.LongTensor],
        responses: List[torch.LongTensor],
        scores: List[torch.FloatTensor],
        risk_alpha: List[torch.LongTensor],
        response_masks: Optional[List[torch.LongTensor]] = None
    ):
        """
        Run a PPO optimisation step given a list of queries, model responses, and rewards.

        Args:
            queries (List[`torch.LongTensor`]):
                List of tensors containing the encoded queries of shape (`query_length`)
            responses (List[`torch.LongTensor`]):
                List of tensors containing the encoded responses of shape (`response_length`)
            scores (List[`torch.FloatTensor`]):
                List of tensors containing the scores.
            response_masks (List[`torch.FloatTensor`], *optional*)):
                List of tensors containing masks of the response tokens.

        Returns:
            `dict[str, Any]`: A summary of the training statistics
        """

        bs = self.config.batch_size

        queries, responses, scores, response_masks = self._step_safety_checker(
            bs, queries, responses, scores, response_masks
        )
        scores = torch.tensor(scores, device=self.current_device)
        if self.config.use_score_scaling:
            # Score scaling
            scores_mean, scores_std = self.running.update(scores)
            tensor_to_kwargs = dict(dtype=scores.dtype, device=scores.device)
            score_scaling_factor = self.running.std.to(**tensor_to_kwargs) + torch.finfo(scores.dtype).eps
            if self.config.use_score_norm:
                scores = (scores - self.running.mean.to(**tensor_to_kwargs)) / score_scaling_factor
            else:
                scores /= score_scaling_factor

        if self.config.score_clip is not None:
            # Score clipping
            scores_dtype = scores.dtype
            scores = torch.clip(scores.float(), -self.config.score_clip, self.config.score_clip).to(dtype=scores_dtype)

        # if we want to push best model to the hub
        if hasattr(self, "highest_reward"):
            if self.compare_step % self.config.compare_steps == 0:
                curr_mean_reward = scores.mean()
                # if the best reward ever seen
                if curr_mean_reward > self.highest_reward:
                    self.highest_reward = curr_mean_reward
                    # push model to hub
                    self.push_to_hub(**self.push_to_hub_kwargs)
            self.compare_step += 1

        timing = dict()
        t0 = time.time()

        t = time.time()

        model_inputs = self.prepare_model_inputs(queries, responses)

        # performs padding in distributed setting 
        # what about padding in non-distributed setting then? 
        if self.is_distributed:

            # if response length = 42, left padding adds 6 zeros to the left of attention mask to reach desired total length
            pad_first = self.tokenizer.padding_side == "left"

            model_inputs["input_ids"] = self.accelerator.pad_across_processes(
                model_inputs["input_ids"],
                dim=1,
                pad_index=self.tokenizer.pad_token_id,
                pad_first=pad_first,
            )
            model_inputs["attention_mask"] = self.accelerator.pad_across_processes(
                model_inputs["attention_mask"], dim=1, pad_index=0, pad_first=pad_first
            )
            if self.is_encoder_decoder:
                model_inputs["decoder_input_ids"] = self.accelerator.pad_across_processes(
                    model_inputs["decoder_input_ids"],
                    dim=1,
                    pad_index=self.tokenizer.pad_token_id,
                    pad_first=pad_first,
                )
                model_inputs["decoder_attention_mask"] = self.accelerator.pad_across_processes(
                    model_inputs["decoder_attention_mask"],
                    dim=1,
                    pad_index=0,
                    pad_first=pad_first,
                )

        model_inputs_names = list(model_inputs.keys())

        full_kl_penalty = self.config.kl_penalty == "full"

        with torch.no_grad():
            all_logprobs, logits_or_none, values, masks = self.batched_forward_pass(
                self.model,
                queries,
                responses,
                model_inputs,
                response_masks=response_masks,
                return_logits=full_kl_penalty,
            )

            # for when the model is a peft model
            if self.is_peft_model and hasattr(
                self.accelerator.unwrap_model(self.model).pretrained_model,
                "disable_adapter",
            ):
                with self.accelerator.unwrap_model(self.model).pretrained_model.disable_adapter():
                    ref_logprobs, ref_logits_or_none, _, _ = self.batched_forward_pass(
                        self.model, queries, responses, model_inputs, return_logits=full_kl_penalty
                    )
            elif self.is_peft_model and not hasattr(self.model.pretrained_model, "disable_adapter"):
                raise ValueError(
                    "You are using a `peft` version that does not support `disable_adapter`. Please update your `peft` version to the latest version."
                )

            else:
                ref_logprobs, ref_logits_or_none, _, _ = self.batched_forward_pass(
                    self.ref_model, queries, responses, model_inputs, return_logits=full_kl_penalty
                )

        timing["time/ppo/forward_pass"] = time.time() - t

        with torch.no_grad():
            t = time.time()
            if full_kl_penalty:
                active_full_logprobs = logprobs_from_logits(logits_or_none, None, gather=False)
                ref_full_logprobs = logprobs_from_logits(ref_logits_or_none, None, gather=False)

                rewards, non_score_reward = self.compute_rewards(
                    scores, active_full_logprobs, ref_full_logprobs, masks
                )
            else:
                rewards, non_score_reward = self.compute_rewards(scores, all_logprobs, ref_logprobs, masks)
            timing["time/ppo/compute_rewards"] = time.time() - t

            t = time.time()

            values, advantages, returns, masks = self.compute_advantages(values, rewards, masks, risk_alpha)
            timing["time/ppo/compute_advantages"] = time.time() - t

        # upcast to float32 to avoid dataset issues
        batch_dict = {
            "queries": queries,
            "responses": responses,
            "logprobs": all_logprobs.to(torch.float32),
            "values": values.to(torch.float32),
            "masks": masks,
            "advantages": advantages,
            "returns": returns,
        }
        batch_dict.update(model_inputs)

        t = time.time()
        all_stats = []
        early_stop = False
        for _ in range(self.config.ppo_epochs):
            if early_stop:
                break
            b_inds = np.random.permutation(bs)
            for backward_batch_start in range(0, bs, self.config.backward_batch_size):
                backward_batch_end = backward_batch_start + self.config.backward_batch_size
                backward_batch_inds = b_inds[backward_batch_start:backward_batch_end]

                for mini_batch_start in range(0, self.config.backward_batch_size, self.config.mini_batch_size):
                    mini_batch_end = mini_batch_start + self.config.mini_batch_size
                    mini_batch_inds = backward_batch_inds[mini_batch_start:mini_batch_end]
                    mini_batch_dict = {
                        "logprobs": batch_dict["logprobs"][mini_batch_inds],
                        "values": batch_dict["values"][mini_batch_inds],
                        "masks": batch_dict["masks"][mini_batch_inds],
                        # hacks: the queries and responses are ragged.
                        "queries": [batch_dict["queries"][i] for i in mini_batch_inds],
                        "responses": [batch_dict["responses"][i] for i in mini_batch_inds],
                        "advantages": batch_dict["advantages"][mini_batch_inds],
                        "returns": batch_dict["returns"][mini_batch_inds],
                    }
                    for k in model_inputs_names:
                        mini_batch_dict[k] = batch_dict[k][mini_batch_inds]
                    with self.accelerator.accumulate(self.model):
                        model_inputs = {k: mini_batch_dict[k] for k in model_inputs_names}

                        logprobs, logits, vpreds, _ = self.batched_forward_pass(
                            self.model,
                            mini_batch_dict["queries"],
                            mini_batch_dict["responses"],
                            model_inputs,
                            return_logits=True,
                        )
                        train_stats = self.train_minibatch(
                            mini_batch_dict["logprobs"],
                            mini_batch_dict["values"],
                            logprobs,
                            logits,
                            vpreds,
                            mini_batch_dict["masks"],
                            mini_batch_dict["advantages"],
                            mini_batch_dict["returns"],
                        )
                        all_stats.append(train_stats)

            # typically, early stopping is done at the epoch level
            if self.config.early_stopping:
                policykl = train_stats["policy/policykl"]
                early_stop = self._early_stop(policykl)
                if early_stop:
                    break

        timing["time/ppo/optimize_step"] = time.time() - t

        t = time.time()
        train_stats = stack_dicts(all_stats)

        # reshape advantages/ratios such that they are not averaged.
        train_stats["policy/advantages"] = torch.flatten(train_stats["policy/advantages"]).unsqueeze(0)
        train_stats["policy/advantages"] = torch.nan_to_num(train_stats["policy/advantages"], WANDB_PADDING)
        train_stats["policy/ratio"] = torch.flatten(train_stats["policy/ratio"]).unsqueeze(0)

        stats = self.record_step_stats(
            scores=scores,
            logprobs=all_logprobs,
            ref_logprobs=ref_logprobs,
            non_score_reward=non_score_reward,
            train_stats=train_stats,
            kl_coef=self.kl_ctl.value,
            masks=masks,
            queries=queries,
            responses=responses,
        )
        # Gather/Reduce stats from all processes
        if self.is_distributed:
            stats = self.gather_stats(stats)
        stats = stats_to_np(stats)
        timing["time/ppo/calc_stats"] = time.time() - t
        stats["ppo/learning_rate"] = self.optimizer.param_groups[0]["lr"]

        # Update the KL control - multiply the batch_size by the number of processes
        self.kl_ctl.update(
            stats["objective/kl"],
            self.config.batch_size * self.accelerator.num_processes,
        )

        # Log the total ppo time
        timing["time/ppo/total"] = time.time() - t0
        stats.update(timing)

        # post-process stats for tensorboard and other loggers
        if self.config.log_with != "wandb":
            stats = convert_to_scalar(stats)

        if self.lr_scheduler is not None:
            self.lr_scheduler.step()

        return stats

    def _early_stop(self, policykl):
        r"""
        Handles the early stopping logic. If the policy KL is greater than the target KL, then the gradient is zeroed and
        the optimization step is skipped.
        This also handles the multi-gpu case where the policy KL is averaged across all processes.

        Args:
            policy_kl (torch.Tensor):
                the policy KL

        Returns:
            `bool`: whether to early stop or not
        """
        early_stop = False
        if not self.config.early_stopping:
            return early_stop

        if not self.is_distributed and policykl > 1.5 * self.config.target_kl:
            self.optimizer.zero_grad()
            early_stop = True
        elif self.is_distributed:
            import torch.distributed as dist

            # Wait for all processes to finish
            dist.barrier()

            # all gather the policykl
            dist.all_reduce(policykl, dist.ReduceOp.SUM)
            policykl /= self.accelerator.num_processes

            if policykl > 1.5 * self.config.target_kl:
                self.optimizer.zero_grad()
                early_stop = True
        return early_stop

    def gather_stats(self, stats):
        """
        Gather stats from all processes. Useful in the context of distributed training.

        Args:
            stats (dict[str, Any]):
            a dictionary of stats to be gathered. The stats should contain torch tensors.

        Returns:
            `dict[str, Any]`: A dictionary of stats with the tensors gathered.
        """
        import torch.distributed as dist

        # Wait for all processes to finish
        dist.barrier()

        for k, v in stats.items():
            if isinstance(v, torch.Tensor):
                dist.all_reduce(v.to(self.accelerator.device), dist.ReduceOp.SUM)
                v /= self.accelerator.num_processes
            stats[k] = v
        return stats

    def prepare_model_inputs(self, queries: torch.Tensor, responses: torch.Tensor):
        if self.is_encoder_decoder:
            input_data = self.data_collator(
                [{"input_ids": q, "attention_mask": torch.ones_like(q)} for q in queries]
            ).to(self.current_device)

            decoder_inputs = self.data_collator(
                [{"input_ids": r, "attention_mask": torch.ones_like(r)} for r in responses]
            ).to(self.current_device)

            input_data["decoder_input_ids"] = decoder_inputs["input_ids"]
            input_data["decoder_attention_mask"] = decoder_inputs["attention_mask"]
        else:
            input_ids = [torch.cat([q, r]) for q, r in zip(queries, responses)]
            # attention mask is 1 only for the query + actual response lengths and not (64+48=112)
            input_data = self.data_collator(
                [{"input_ids": ids, "attention_mask": torch.ones_like(ids)} for ids in input_ids]
            ).to(self.current_device)

        input_data.pop("labels", None)  # we don't want to compute LM losses
        return input_data

    @PPODecorators.empty_cuda_cache()
    def batched_forward_pass(
        self,
        model: PreTrainedModelWrapper,
        queries: torch.Tensor,
        responses: torch.Tensor,
        model_inputs: dict,
        return_logits: bool = False,
        response_masks: Optional[torch.Tensor] = None,
    ):
        """
        Calculate model outputs in multiple batches.

        Args:
            queries (`torch.LongTensor`):
                List of tensors containing the encoded queries, shape (`batch_size`, `query_length`)
            responses (`torch.LongTensor`):
                List of tensors containing the encoded responses, shape (`batch_size`, `response_length`)
            return_logits (`bool`, *optional*, defaults to `False`):
                Whether to return all_logits. Set to `False` if logits are not needed to reduce memory consumption.
        Returns:
            (tuple):
                - all_logprobs (`torch.FloatTensor`): Log probabilities of the responses,
                    shape (`batch_size`, `response_length`)
                - all_ref_logprobs (`torch.FloatTensor`): Log probabilities of the responses,
                    shape (`batch_size`, `response_length`)
                - all_values (`torch.FloatTensor`): Values of the responses, shape (`batch_size`, `response_length`)
        """
        bs = len(queries)
        fbs = self.config.mini_batch_size
        all_logprobs = []
        all_logits = []
        all_masks = []
        all_values = []

        model.eval()


        for i in range(math.ceil(bs / fbs)):
            input_kwargs = {key: value[i * fbs : (i + 1) * fbs] for key, value in model_inputs.items()}
            query_batch = queries[i * fbs : (i + 1) * fbs]
            response_batch = responses[i * fbs : (i + 1) * fbs]
            if response_masks is not None:
                response_masks_batch = response_masks[i * fbs : (i + 1) * fbs]
            logits, _, values = model(**input_kwargs)

            if self.is_encoder_decoder:
                input_ids = input_kwargs["decoder_input_ids"]
                attention_mask = input_kwargs["decoder_attention_mask"]
            else:
                input_ids = input_kwargs["input_ids"]
                attention_mask = input_kwargs["attention_mask"]

            # logits at step
            # why are we reducing size by 1
            # because at the current time step/token, logits are over the next token 
            # thats why logits[:, 0, :] is for input_ids[:, 1] 
            logprobs = logprobs_from_logits(logits[:, :-1, :], input_ids[:, 1:])
            masks = torch.zeros_like(attention_mask)
            # the following would mean that the last element of masks is always zero
            masks[:, :-1] = attention_mask[:, 1:]

            #For a single sequence model, start position is the end of the query. End is start + response length.
            #It handles optional left padding that may need to be offset if present.
            #The masks tensor is used to mask the loss - set to 0 before start and after end index, 1 inside.
            #If a separate response mask is passed in, that is combined with the masks tensor to further mask just the response portion.

            for j in range(len(query_batch)):
                if self.is_encoder_decoder:
                    # Decoder sentence starts always in the index 1 after padding in the Enc-Dec Models
                    start = 1
                    end = attention_mask[j, :].sum() - 1
                else:
                    start = len(query_batch[j]) - 1  # logprobs starts from the second query token
                    am_start = start
                    if attention_mask[j, 0] == 0:  # offset left padding
                        am_start += attention_mask[j, :].nonzero()[0]
                    end = start + len(response_batch[j])
                    am_end = am_start + len(response_batch[j])
                    if response_masks is not None:
                        response_masks_batch[j] = torch.cat(
                            (torch.zeros_like(query_batch[j]), response_masks_batch[j])
                        )[1:]

                masks[j, :start] = 0
                masks[j, end:] = 0
                if response_masks is not None:
                    # this was incorrect
                    masks[j, am_start:am_end] = masks[j, am_start:am_end] * response_masks_batch[j][start:end]

            if return_logits:
                all_logits.append(logits)
            else:
                del logits
            all_values.append(values)
            all_logprobs.append(logprobs)
            all_masks.append(masks)

        return (
            torch.cat(all_logprobs),
            torch.cat(all_logits)[:, :-1] if return_logits else None,
            torch.cat(all_values)[:, :-1],
            torch.cat(all_masks)[:, :-1],
        )

    @PPODecorators.empty_cuda_cache()
    def train_minibatch(
        self,
        old_logprobs: torch.FloatTensor,
        values: torch.FloatTensor,
        logprobs: torch.FloatTensor,
        logits: torch.FloatTensor,
        vpreds: torch.FloatTensor,
        mask: torch.LongTensor,
        advantages: torch.FloatTensor,
        returns: torch.FloatTensor,
    ):
        """
        Train one PPO minibatch

        Args:
            logprobs (`torch.FloatTensor`):
                Log probabilities of the model, shape [batch_size, response_length]
            values (`torch.FloatTensor`):
                Values of the value head, shape [batch_size, response_length]
            query (`torch.LongTensor`):
                Encoded queries, shape [batch_size, query_length]
            response (`torch.LongTensor`):
                Encoded responses, shape [batch_size, response_length]
            model_input (`torch.LongTensor`):
                Concatenated queries and responses, shape [batch_size, query_length+response_length]

        Returns:
            train_stats (dict[str, `torch.Tensor`]):
                Dictionary of training statistics
        """
        self.model.train()
        loss_p, loss_v, train_stats = self.loss(
            old_logprobs, values, logits, vpreds, logprobs, mask, advantages, returns
        )
        loss = loss_p + loss_v
        self.accelerator.backward(loss)
        if self.config.max_grad_norm is not None:
            if self.accelerator.sync_gradients:
                self.accelerator.clip_grad_norm_(self.model_params, self.config.max_grad_norm)
        self.optimizer.step()
        # we call optimizer.zero_grad() every time and let `accelerator` handle accumulation
        # see https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation#the-finished-code
        self.optimizer.zero_grad()
        return train_stats

    def compute_rewards(
        self,
        scores: torch.FloatTensor,
        logprobs: torch.FloatTensor,
        ref_logprobs: torch.FloatTensor,
        masks: torch.LongTensor,
    ):
        """
        Compute per token rewards from scores and KL-penalty.

        Args:
            scores (`torch.FloatTensor`):
                Scores from the reward model, shape (`batch_size`)
            logprobs (`torch.FloatTensor`):
                Log probabilities of the model, shape (`batch_size`, `response_length`)
            ref_logprobs (`torch.FloatTensor`):
                Log probabilities of the reference model, shape (`batch_size`, `response_length`)
        """
        rewards, non_score_rewards = [], []
        i = 0
        for score, logprob, ref_logprob, mask in zip(scores, logprobs, ref_logprobs, masks):
            #print(i, len(mask.nonzero()))
            # compute KL penalty (from difference in logprobs)
            kl = self._kl_penalty(logprob, ref_logprob)
            non_score_reward = -self.kl_ctl.value * kl
            non_score_rewards.append(non_score_reward)
            reward = non_score_reward.clone()
            if len(mask.nonzero()) == 0:
                last_non_masked_index = 0
            else: 
                last_non_masked_index = mask.nonzero()[-1]

            # reward is preference model score + KL penalty
            reward[last_non_masked_index] += score
            rewards.append(reward)
            i += 1
        return torch.stack(rewards), torch.stack(non_score_rewards)

    def _kl_penalty(self, logprob: torch.FloatTensor, ref_logprob: torch.FloatTensor) -> torch.FloatTensor:
        if self.config.kl_penalty == "kl":
            return logprob - ref_logprob

        if self.config.kl_penalty == "abs":
            return (logprob - ref_logprob).abs()

        if self.config.kl_penalty == "mse":
            return 0.5 * (logprob - ref_logprob).square()

        if self.config.kl_penalty == "full":
            # Flip is required due to this issue? :https://github.com/pytorch/pytorch/issues/57459
            return F.kl_div(ref_logprob, logprob, log_target=True, reduction="none").sum(-1)

        raise NotImplementedError

    def find_alpha_return(
        self: torch.FloatTensor,
        traj_returns: torch.FloatTensor,
        mask: torch.FloatTensor,
        risk_alpha: torch.LongTensor,
    ):
        sorted_returns, indices = torch.sort(traj_returns)
        q_risk_alpha = sorted_returns[risk_alpha-1]

        for ii in range(traj_returns.shape[0]):
            if traj_returns[ii] > q_risk_alpha:
                #print('here', risk_alpha, ii)
                mask[ii,:] = torch.zeros_like(mask[ii,:], dtype=int)

        return q_risk_alpha, mask


    def compute_advantages(
        self: torch.FloatTensor,
        values: torch.FloatTensor,
        rewards: torch.FloatTensor,
        mask: torch.FloatTensor,
        risk_alpha: torch.LongTensor,
    ):
        lastgaelam = 0
        advantages_reversed = []
        gen_len = rewards.shape[-1]

        values = values * mask
        rewards = rewards * mask

        for t in reversed(range(gen_len)):
            nextvalues = values[:, t + 1] if t < gen_len - 1 else 0.0
            delta = rewards[:, t] + self.config.gamma * nextvalues - values[:, t]
            lastgaelam = delta + self.config.gamma * self.config.lam * lastgaelam
            advantages_reversed.append(lastgaelam)
        advantages = torch.stack(advantages_reversed[::-1]).transpose(0, 1)

        returns = advantages + values
        # select 63rd index element of returns in the batch
        traj_returns = returns[:,63]
        # find alpha_return and create new mask based on alpha_return
        alpha_return, masks = self.find_alpha_return(traj_returns, mask, risk_alpha)
        # return this 'masks' so that step() can pass this mask to loss()

        # Whiten / normalize the unmasked elements
        advantages = masked_whiten(advantages, mask)
        advantages = advantages.detach()
        return values, advantages, returns, masks

    def loss(
        self,
        old_logprobs: torch.FloatTensor,
        values: torch.FloatTensor,
        logits: torch.FloatTensor,
        vpreds: torch.FloatTensor,
        logprobs: torch.FloatTensor,
        mask: torch.LongTensor,
        advantages: torch.FloatTensor,
        returns: torch.FloatTensor,
    ):
        """
        Calculate policy and value losses.

        Args:
            old_logprobs (`torch.FloatTensor`):
                Log probabilities of the model, shape (`batch_size`, `response_length`)
            values (`torch.FloatTensor`):
                Values of the value head, shape (`batch_size`, `response_length`)
            rewards (`torch.FloatTensor`):
                Rewards from the reward model, shape (`batch_size`, `response_length`)
            logits (`torch.FloatTensor`):
                Logits of the model, shape (`batch_size`, `response_length`, `vocab_size`)
            v_pred (`torch.FloatTensor`):
                Values of the value head, shape (`batch_size`, `response_length`)
            logprobs (`torch.FloatTensor`):
                Log probabilities of the model, shape (`batch_size`, `response_length`)
        """

        vpredclipped = clip_by_value(
            vpreds,
            values - self.config.cliprange_value,
            values + self.config.cliprange_value,
        )

        vf_losses1 = (vpreds - returns) ** 2
        vf_losses2 = (vpredclipped - returns) ** 2
        vf_loss = 0.5 * masked_mean(torch.max(vf_losses1, vf_losses2), mask)
        vf_clipfrac = masked_mean(torch.gt(vf_losses2, vf_losses1).float(), mask)

        ratio = torch.exp(logprobs - old_logprobs)

        pg_losses = -advantages * ratio
        pg_losses2 = -advantages * torch.clamp(ratio, 1.0 - self.config.cliprange, 1.0 + self.config.cliprange)

        pg_loss = masked_mean(torch.max(pg_losses, pg_losses2), mask)
        pg_clipfrac = masked_mean(torch.gt(pg_losses2, pg_losses).float(), mask)

        loss = pg_loss + self.config.vf_coef * vf_loss

        avg_ratio = masked_mean(ratio, mask).item()
        if avg_ratio > self.config.ratio_threshold:
            warnings.warn(
                f"The average ratio of batch ({avg_ratio:.2f}) exceeds threshold {self.config.ratio_threshold:.2f}. Skipping batch."
            )
            pg_loss = pg_loss * 0.0
            vf_loss = vf_loss * 0.0
            loss = loss * 0.0

        entropy = masked_mean(entropy_from_logits(logits), mask)

        approxkl = 0.5 * masked_mean((logprobs - old_logprobs) ** 2, mask)
        policykl = masked_mean(old_logprobs - logprobs, mask)

        return_mean, return_var = masked_mean(returns, mask), masked_var(returns, mask)
        value_mean, value_var = masked_mean(values, mask), masked_var(values, mask)

        stats = dict(
            loss=dict(policy=pg_loss.detach(), value=vf_loss.detach(), total=loss.detach()),
            policy=dict(
                entropy=entropy.detach(),
                approxkl=approxkl.detach(),
                policykl=policykl.detach(),
                clipfrac=pg_clipfrac.detach(),
                advantages=advantages.detach(),
                advantages_mean=masked_mean(advantages, mask).detach(),
                ratio=ratio.detach(),
            ),
            returns=dict(mean=return_mean.detach(), var=return_var.detach()),
            val=dict(
                vpred=masked_mean(vpreds, mask).detach(),
                error=masked_mean((vpreds - returns) ** 2, mask).detach(),
                clipfrac=vf_clipfrac.detach(),
                mean=value_mean.detach(),
                var=value_var.detach(),
            ),
        )
        return pg_loss, self.config.vf_coef * vf_loss, flatten_dict(stats)

    def record_step_stats(self, kl_coef: float, **data):
        """
        Record training step statistics.


        Args:
            kl_coef (`float`):
                KL coefficient
            data (`dict`):
                Dictionary of training step data

        Returns:
            stats (`dict`):
                Dictionary of training step statistics
        """
        mask = data.pop("masks")

        kl_list = ((data["logprobs"] - data["ref_logprobs"]) * mask).sum(axis=-1)
        mean_kl = kl_list.mean()
        mean_entropy = (-data["logprobs"] * mask).sum(axis=-1).mean()

        mean_non_score_reward = masked_mean(
            data["non_score_reward"], mask
        )  # non_score_reward is size `batch_size`, `response_length`
        mean_scores = data["scores"].mean()  # scores is size `batch_size`
        std_scores = data["scores"].std()

        if mean_kl.item() < -1.0:
            # warn users
            warnings.warn(
                f"KL divergence is starting to become negative: {mean_kl.item():.2f} - this might be a precursor for failed training."
                " sometimes this happens because the generation kwargs are not correctly set. Please make sure"
                " that the generation kwargs are set correctly, or review your training hyperparameters."
            )

        stats = {
            "objective/kl": mean_kl,
            "objective/kl_dist": kl_list,
            "objective/logprobs": data["logprobs"],
            "objective/ref_logprobs": data["ref_logprobs"],
            "objective/kl_coef": kl_coef,
            "objective/entropy": mean_entropy,
            "ppo/mean_non_score_reward": mean_non_score_reward,
            "ppo/mean_scores": mean_scores,
            "ppo/std_scores": std_scores,
        }

        # Log text properties
        query_lens = torch.tensor([len(query) for query in data["queries"]], dtype=torch.float)
        response_lens = torch.tensor([len(response) for response in data["responses"]], dtype=torch.float)

        stats["tokens/queries_len_mean"] = torch.mean(query_lens).cpu().numpy().item()
        stats["tokens/queries_len_std"] = torch.std(query_lens).cpu().numpy().item()
        stats["tokens/queries_dist"] = query_lens.cpu().numpy()
        stats["tokens/responses_len_mean"] = torch.mean(response_lens).cpu().numpy().item()
        stats["tokens/responses_len_std"] = torch.std(response_lens).cpu().numpy().item()
        stats["tokens/responses_dist"] = response_lens.cpu().numpy()

        for k, v in data["train_stats"].items():
            stats[f"ppo/{k}"] = torch.mean(v, axis=0)
        stats["ppo/val/var_explained"] = 1 - stats["ppo/val/error"] / stats["ppo/returns/var"]
        return stats

    def log_stats(
        self,
        stats: dict,
        batch: dict,
        rewards: List[torch.FloatTensor],
        columns_to_log: List[str] = ["query", "response"],
    ):
        """
        A function that logs all the training stats. Call it at the end of each epoch.

        Args:
            stats (dict[str, Any]):
                A dictionary of training stats.
            batch (dict[str, Any]):
                A dictionary of batch data, this contains the queries and responses.
            rewards (`List[torch.FloatTensor]`):
                A tensor of rewards.
        """
        # Log only if we are in the main process
        if self.accelerator.is_main_process:
            logs = {}

            # Log stats
            if not isinstance(rewards, torch.Tensor):
                rewards = torch.tensor(rewards).to(self.current_device)

            if "query" not in batch.keys() and "response" not in batch.keys():
                # warn the user that the game logs will not be logged
                warnings.warn(
                    "The game logs will not be logged because the batch does not contain the keys 'query' and "
                    "'response'. "
                )
            elif self.config.log_with == "wandb":
                import wandb

                if any([column_to_log not in batch.keys() for column_to_log in columns_to_log]):
                    raise ValueError(f"Columns to log {columns_to_log} are not present in the batch {batch.keys()}.")

                batch_list = [batch[column_to_log] for column_to_log in columns_to_log]

                table_rows = [list(r) for r in zip(*batch_list, rewards.cpu().tolist())]
                logs.update({"game_log": wandb.Table(columns=[*columns_to_log, "reward"], rows=table_rows)})
            # All reduce rewards if distributed
            if self.is_distributed:
                import torch.distributed as dist

                dist.barrier()

                dist.all_reduce(rewards, op=torch.distributed.ReduceOp.SUM)
                rewards /= self.accelerator.num_processes

            logs.update(stats)

            # manually cast in fp32 for bf16 torch tensors
            for k, v in logs.items():
                if isinstance(v, torch.Tensor) and v.dtype == torch.bfloat16:
                    logs[k] = v.float()

            logs["env/reward_mean"] = torch.mean(rewards).cpu().numpy().item()
            logs["env/reward_std"] = torch.std(rewards).cpu().numpy().item()
            logs["env/reward_dist"] = rewards.cpu().numpy()

            if self.config.log_with == "tensorboard":
                # update the current step
                self.current_step += 1

            self.accelerator.log(
                logs,
                step=self.current_step if self.config.log_with == "tensorboard" else None,
            )

        else:
            if self.is_distributed:
                import torch.distributed as dist

                if not isinstance(rewards, torch.Tensor):
                    rewards = torch.tensor(rewards).to(self.current_device)

                dist.barrier()
                dist.all_reduce(rewards, op=torch.distributed.ReduceOp.SUM)

    def create_model_card(self, path: str, model_name: Optional[str] = "TRL Model") -> None:
        """Creates and saves a model card for a TRL model.

        Args:
            path (`str`): The path to save the model card to.
            model_name (`str`, *optional*): The name of the model, defaults to `TRL Model`.
        """
        try:
            user = whoami()["name"]
        # handle the offline case
        except:  # noqa
            warnings.warn("Cannot retrieve user information assuming you are running in offline mode.")
            return

        if not os.path.exists(path):
            os.makedirs(path)

        model_card_content = MODEL_CARD_TEMPLATE.format(model_name=model_name, model_id=f"{user}/{path}")
        with open(os.path.join(path, "README.md"), "w", encoding="utf-8") as f:
            f.write(model_card_content)

    def _save_pretrained(self, save_directory: str) -> None:
        self.accelerator.unwrap_model(self.model).save_pretrained(save_directory)
        self.tokenizer.save_pretrained(save_directory)
        self.create_model_card(save_directory)

    def _show_tokens(self, tokens, masks):
        from rich import print
        from rich.text import Text

        text = Text()

        for i, (token, mask) in enumerate(zip(tokens, masks)):
            if mask == 1:
                text.append(self.tokenizer.decode(token.item()), style="black on deep_sky_blue1")
                text.append(" ")
            else:
                text.append(self.tokenizer.decode(token.item()), style="black on cyan3")
                text.append(" ")
        print(text)

    def _prepare_deepspeed(self, model: PreTrainedModelWrapper):
        # Adapted from accelerate: https://github.com/huggingface/accelerate/blob/739b135f8367becb67ffaada12fe76e3aa60fefd/src/accelerate/accelerator.py#L1473
        deepspeed_plugin = self.accelerator.state.deepspeed_plugin
        config_kwargs = deepspeed_plugin.deepspeed_config
        if model is not None:
            if hasattr(model, "config"):
                hidden_size = (
                    max(model.config.hidden_sizes)
                    if getattr(model.config, "hidden_sizes", None)
                    else getattr(model.config, "hidden_size", None)
                )
                if hidden_size is not None and config_kwargs["zero_optimization"]["stage"] == 3:
                    # Note that `stage3_prefetch_bucket_size` can produce DeepSpeed messages like: `Invalidate trace cache @ step 0: expected module 1, but got module 0`
                    # This is expected and is not an error, see: https://github.com/microsoft/DeepSpeed/discussions/4081
                    config_kwargs.update(
                        {
                            "zero_optimization.reduce_bucket_size": hidden_size * hidden_size,
                            "zero_optimization.stage3_param_persistence_threshold": 10 * hidden_size,
                            "zero_optimization.stage3_prefetch_bucket_size": 0.9 * hidden_size * hidden_size,
                        }
                    )

        # If ZeRO-3 is used, we shard both the active and reference model.
        # Otherwise, we assume the reference model fits in memory and is initialized on each device with ZeRO disabled (stage 0)
        if config_kwargs["zero_optimization"]["stage"] != 3:
            config_kwargs["zero_optimization"]["stage"] = 0
        model, *_ = deepspeed.initialize(model=model, config=config_kwargs)
        model.eval()
        return model

File Path: trl/trainer/ppo_trainer_original.py
Content:
# Copyright 2022 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import inspect
import math
import os
import time
import typing
import warnings
from typing import Callable, List, Optional, Union

import datasets
import numpy as np
import torch
import torch.nn.functional as F
from accelerate import Accelerator
from accelerate.utils import ProjectConfiguration, is_deepspeed_available
from datasets import Dataset
from huggingface_hub import whoami
from packaging import version
from torch.optim import Adam
from transformers import (
    DataCollatorForLanguageModeling,
    PreTrainedTokenizer,
    PreTrainedTokenizerBase,
    PreTrainedTokenizerFast,
)

from ..core import (
    WANDB_PADDING,
    PPODecorators,
    clip_by_value,
    convert_to_scalar,
    entropy_from_logits,
    flatten_dict,
    logprobs_from_logits,
    masked_mean,
    masked_var,
    masked_whiten,
    set_seed,
    stack_dicts,
    stats_to_np,
)
from ..import_utils import is_torch_greater_2_0
from ..models import SUPPORTED_ARCHITECTURES, PreTrainedModelWrapper, create_reference_model
from . import AdaptiveKLController, BaseTrainer, FixedKLController, PPOConfig, RunningMoments

import pdb

if is_deepspeed_available():
    import deepspeed

MODEL_CARD_TEMPLATE = """---
license: apache-2.0
tags:
- trl
- transformers
- reinforcement-learning
---

# {model_name}

This is a [TRL language model](https://github.com/huggingface/trl) that has been fine-tuned with reinforcement learning to
 guide the model outputs according to a value, function, or human feedback. The model can be used for text generation.

## Usage

To use this model for inference, first install the TRL library:

```bash
python -m pip install trl
```

You can then generate text as follows:

```python
from transformers import pipeline

generator = pipeline("text-generation", model="{model_id}")
outputs = generator("Hello, my llama is cute")
```

If you want to use the model for training or to obtain the outputs from the value head, load the model as follows:

```python
from transformers import AutoTokenizer
from trl import AutoModelForCausalLMWithValueHead

tokenizer = AutoTokenizer.from_pretrained("{model_id}")
model = AutoModelForCausalLMWithValueHead.from_pretrained("{model_id}")

inputs = tokenizer("Hello, my llama is cute", return_tensors="pt")
outputs = model(**inputs, labels=inputs["input_ids"])
```
"""


class PPOTrainer(BaseTrainer):
    """
    The PPOTrainer uses Proximal Policy Optimization to optimise language models.
    Note, this trainer is heavily inspired by the original OpenAI learning to summarize work here:
    https://github.com/openai/summarize-from-feedback

    Attributes:
        **config** (`PPOConfig`) -- Configuration object for PPOTrainer. Check the documentation of `PPOConfig` for more
            details.
        **model** (`PreTrainedModelWrapper`) -- Model to be optimized, Hugging Face transformer model with a value head.
            Check the documentation of `PreTrainedModelWrapper` for more details.
        **ref_model** (`PreTrainedModelWrapper`, *optional*) -- Reference model to be used for KL penalty, Hugging Face
            transformer model with a casual language modelling head. Check the documentation of `PreTrainedModelWrapper`
            for more details. If no reference model is provided, the trainer will create a reference model with the same
             architecture as the model to be optimized with shared layers.
        **tokenizer** (`PreTrainedTokenizerBase`) -- Tokenizer to be used for encoding the
            data. Check the documentation of `transformers.PreTrainedTokenizer` and
            `transformers.PreTrainedTokenizerFast` for more details.
        **dataset** (Union[`torch.utils.data.Dataset`, `datasets.Dataset`], *optional*) -- PyTorch dataset or Hugging
            Face dataset. This is used to create a PyTorch dataloader. If no dataset is provided, the dataloader must be
             created outside the trainer users needs to design their own dataloader and make sure the batch
            size that is used is the same as the one specified in the configuration object.
        **optimizer** (`torch.optim.Optimizer`, *optional*) -- Optimizer to be used for training. If no optimizer is
            provided, the trainer will create an Adam optimizer with the learning rate specified in the configuration
            object.
        **data_collator** (DataCollatorForLanguageModeling, *optional*) -- Data collator to be used for training and
            passed along the dataloader
        **num_shared_layers** (int, *optional*) -- Number of layers to be shared between the model and the reference
            model, if no reference model is passed. If no number is provided, all the layers will be shared.
        **lr_scheduler** (`torch.optim.lr_scheduler`, *optional*) -- Learning rate scheduler to be used for training.
    """

    def __init__(
        self,
        config: PPOConfig = None,
        model: PreTrainedModelWrapper = None,
        ref_model: Optional[PreTrainedModelWrapper] = None,
        tokenizer: PreTrainedTokenizerBase = None,
        dataset: Optional[Union[torch.utils.data.Dataset, Dataset]] = None,
        optimizer: Optional[torch.optim.Optimizer] = None,
        data_collator: Optional[typing.Callable] = None,
        num_shared_layers: Optional[int] = None,
        lr_scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,
    ):
        """
        Initialize PPOTrainer.

        Args:
            config (`PPOConfig`):
                Configuration object for PPOTrainer. Check the documentation of `PPOConfig` for more details.
            model (`PreTrainedModelWrapper`):
                Hugging Face transformer model with a value head.
            ref_model (`PreTrainedModelWrapper`):
                Hugging Face transformer model with a casual language modelling head. Used for KL penalty
            tokenizer (`transformers.PreTrainedTokenizerBase`):
                Hugging Face tokenizer
            dataset (Optional[Union[`torch.utils.data.Dataset`, `datasets.Dataset`]]):
                PyTorch dataset or Hugging Face dataset. If a Hugging Face dataset is passed, the dataset
                will be preprocessed by removing the columns that are not used by the model. If none is passed,
                a warning will be raised in a multi-GPU setting.
            optimizer (Optional[`torch.optim.Optimizer`]):
                Optimizer used for training. If `None`, the `Adam` is used as default.
            data_collator (Optional[function]):
                Data collator function.
            num_shared_layers (Optional[int]):
                Number of shared layers between the model and the reference model. If `None`, all layers are shared.
                used only if `ref_model` is `None`.
            lr_scheduler (Optional[`torch.optim.lr_scheduler`]):
                Learning rate scheduler used for training.
        """
        super().__init__(config)

        # initial seed for reproducible experiments
        set_seed(config.seed)

        # Step 0: check positional arguments validity
        if not isinstance(config, PPOConfig):
            raise ValueError(f"config must be a PPOConfig, got {type(config)}")
        if not isinstance(tokenizer, (PreTrainedTokenizerBase)):
            raise ValueError(
                f"tokenizer must be a PreTrainedTokenizerBase like a PreTrainedTokenizer or a PreTrainedTokenizerFast, got {type(tokenizer)}"
            )
        if not isinstance(model, (SUPPORTED_ARCHITECTURES)):
            raise ValueError(
                f"model must be a PreTrainedModelWrapper, got {type(model)} - supported architectures are: {SUPPORTED_ARCHITECTURES}"
            )
        # Step 1: Initialize Accelerator
        self.accelerator = Accelerator(
            log_with=config.log_with,
            gradient_accumulation_steps=config.gradient_accumulation_steps,
            project_config=ProjectConfiguration(**config.project_kwargs),
            **config.accelerator_kwargs,
        )

        # Step 1.1 Runtime variables filled by the accelerator
        config.world_size = self.accelerator.num_processes
        config.global_backward_batch_size = config.backward_batch_size * config.world_size
        config.global_batch_size = config.batch_size * config.world_size

        self.model = model
        self.model_params = filter(lambda p: p.requires_grad, self.model.parameters())
        self.is_encoder_decoder = hasattr(self.model, "is_encoder_decoder")
        self.is_peft_model = getattr(self.model, "is_peft_model", False)
        config.is_encoder_decoder = self.is_encoder_decoder
        config.is_peft_model = self.is_peft_model

        is_using_tensorboard = config.log_with is not None and config.log_with == "tensorboard"
        self.accelerator.init_trackers(
            config.tracker_project_name,
            config=dict(trl_ppo_trainer_config=config.to_dict()) if not is_using_tensorboard else config.to_dict(),
            init_kwargs=config.tracker_kwargs,
        )
        self.is_using_text_environment = getattr(config, "use_text_environment", False)

        if isinstance(ref_model, SUPPORTED_ARCHITECTURES):
            self.ref_model = ref_model
            if num_shared_layers is not None:
                warnings.warn(
                    "num_shared_layers is ignored when ref_model is provided. Two different models are used for the "
                    "model and the reference model and no layers are shared.",
                    UserWarning,
                )
        elif ref_model is None and not self.is_peft_model:
            self.ref_model = create_reference_model(self.model, num_shared_layers=num_shared_layers)
        elif self.is_peft_model:
            self.ref_model = None
        else:
            raise ValueError(
                f"ref_model must be a PreTrainedModelWrapper or `None`, got {type(ref_model)} - supported "
                f"architectures are: {SUPPORTED_ARCHITECTURES} "
            )

        if not (isinstance(tokenizer, PreTrainedTokenizer) or isinstance(tokenizer, PreTrainedTokenizerFast)):
            raise ValueError(
                "tokenizer must be a transformers.PreTrainedTokenizer or transformers.PreTrainedTokenizerFast"
            )
        self.tokenizer = tokenizer

        if dataset is not None and not (isinstance(dataset, torch.utils.data.Dataset) or isinstance(dataset, Dataset)):
            raise ValueError("dataset must be a torch.utils.data.Dataset or datasets.Dataset")
        elif dataset is None:
            warnings.warn(
                "No dataset is provided. Make sure to set config.batch_size to the correct value before training.",
                UserWarning,
            )
        self.dataset = dataset
        self._signature_columns = None
        if self.dataset is not None:
            self.dataloader = self.prepare_dataloader(self.dataset, data_collator)
        elif self.dataset is None and self.accelerator.num_processes > 1:
            warnings.warn(
                "No dataset is provided. In a multi-GPU setting, this will lead to an error. You should"
                " prepare your dataloader yourself with `dataloader = ppo_trainer.accelerator.prepare(dataloader)`"
                " and using `torch.utils.data.DataLoader`, or pass a dataset to the `PPOTrainer`. Please "
                " refer to the documentation for more details.",
                UserWarning,
            )
            self.dataloader = None
        else:
            self.dataloader = None

        # Step 3: Initialize optimizer and data collator
        self.data_collator = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)
        if optimizer is None:
            self.optimizer = Adam(
                filter(lambda p: p.requires_grad, self.model.parameters()),
                lr=self.config.learning_rate,
            )
        else:
            self.optimizer = optimizer

        self.lr_scheduler = lr_scheduler
        if self.lr_scheduler is not None:
            lr_scheduler_class = (
                torch.optim.lr_scheduler._LRScheduler
                if not is_torch_greater_2_0()
                else torch.optim.lr_scheduler.LRScheduler
            )

            if not isinstance(self.lr_scheduler, lr_scheduler_class):
                raise ValueError(
                    "lr_scheduler must be a torch.optim.lr_scheduler._LRScheduler or torch.optim.lr_scheduler.LRScheduler (for torch >= 2.0)"
                )

        if self.config.adap_kl_ctrl:
            self.kl_ctl = AdaptiveKLController(self.config.init_kl_coef, self.config.target, self.config.horizon)
        else:
            self.kl_ctl = FixedKLController(self.config.init_kl_coef)

        # Safety checkers for DS integration
        is_deepspeed_used = self.accelerator.distributed_type == "DEEPSPEED" and hasattr(
            self.accelerator.state, "deepspeed_plugin"
        )

        (
            self.model,
            self.optimizer,
            self.data_collator,
            self.dataloader,
            self.lr_scheduler,
        ) = self.accelerator.prepare(
            self.model,
            self.optimizer,
            self.data_collator,
            self.dataloader,
            self.lr_scheduler,
        )
        if is_deepspeed_used:
            # Quantized models are already set on the correct device
            if not self.is_peft_model and not (
                getattr(self.ref_model.pretrained_model, "is_loaded_in_8bit", False)
                or getattr(self.ref_model.pretrained_model, "is_loaded_in_4bit", False)
            ):
                self.ref_model = self._prepare_deepspeed(self.ref_model)
        else:
            self.ref_model = self.accelerator.prepare(self.ref_model)

        # In a distributed setup, only logging needs to be performed on the main process
        # check: https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html
        # or: https://discuss.pytorch.org/t/use-distributed-data-parallel-correctly/82500/11
        self.is_distributed = self.accelerator.distributed_type == "MULTI_GPU"

        # init the current step
        self.current_step = 0

        # init variables for pushing model to hub
        if config.push_to_hub_if_best_kwargs:
            if "repo_id" not in config.push_to_hub_if_best_kwargs:
                raise ValueError("You have to specify repo_id in order to push the model to the hub!")
            self.push_to_hub_kwargs = config.push_to_hub_if_best_kwargs
            self.compare_step = 0
            self.highest_reward = torch.tensor(-float("inf"))

        # post process for PP
        if not getattr(self.model, "is_sequential_parallel", False):
            self.current_device = self.accelerator.device
        else:
            self.current_device = torch.device("cuda:0")

        PPODecorators.optimize_cuda_cache = self.config.optimize_cuda_cache

        self.running = RunningMoments(self.accelerator)

    def _filter_kwargs(self, kwargs, target_func):
        """
        filter the keyword arguments that are supported by the target function.

        Args:
            kwargs (dict):
                Keyword arguments
            target_func (function):
                Target function
        """
        return {k: v for k, v in kwargs.items() if k in inspect.signature(target_func).parameters.keys()}

    def prepare_dataloader(self, dataset: Union[torch.utils.data.Dataset, Dataset], data_collator=None):
        """
        Prepare the dataloader for training.

        Args:
            dataset (Union[`torch.utils.data.Dataset`, `datasets.Dataset`]):
                PyTorch dataset or Hugging Face dataset. If a Hugging Face dataset is passed, the dataset
                will be preprocessed by removing the columns that are not used by the model.
            data_collator (Optional[function]):
                Data collator function.

        Returns:
            `torch.utils.data.DataLoader`: PyTorch dataloader
        """
        if isinstance(dataset, Dataset):
            dataset = self._remove_unused_columns(dataset)
        dataloader = torch.utils.data.DataLoader(
            dataset,
            batch_size=self.config.batch_size,
            collate_fn=data_collator,
            shuffle=True,
            drop_last=True,
        )
        return dataloader

    # Adapted from transformers.Trainer._set_signature_columns_if_needed
    def _set_signature_columns_if_needed(self):
        if self._signature_columns is None:
            # Inspect model forward signature to keep only the arguments it accepts.
            signature = inspect.signature(self.model.forward)
            self._signature_columns = list(signature.parameters.keys())
            # label => sentiment | we need query and response for logging purpose
            self._signature_columns += ["label", "query", "response"]

    # Adapted from transformers.Trainer._remove_unused_columns
    def _remove_unused_columns(self, dataset: "Dataset"):
        if not self.config.remove_unused_columns:
            return dataset
        self._set_signature_columns_if_needed()
        signature_columns = self._signature_columns

        ignored_columns = list(set(dataset.column_names) - set(signature_columns))

        columns = [k for k in signature_columns if k in dataset.column_names]

        if version.parse(datasets.__version__) < version.parse("1.4.0"):
            dataset.set_format(
                type=dataset.format["type"],
                columns=columns,
                format_kwargs=dataset.format["format_kwargs"],
            )
            return dataset
        else:
            return dataset.remove_columns(ignored_columns)

    def generate(
        self,
        query_tensor: Union[torch.Tensor, List[torch.Tensor]],
        length_sampler: Callable = None,
        batch_size: int = 4,
        return_prompt: bool = True,
        **generation_kwargs,
    ):
        """
        Generate response with the model given the query tensor.
        call the `generate` method of the model.

        Args:
            query_tensor (`torch.LongTensor`):
                A tensor of shape (`batch_size`, `seq_len`) containing query tokens.
            generation_kwargs (dict[str, Any]):
                Keyword arguments for generation.
            length_sampler (`Callable`, *optional*):
                Callable that returns the number of newly generated tokens.
            batch_size (`int`, *optional):
                Batch size used for generation, defaults to `4`.
            return_prompt (`bool`, *optional*):
                If set to `False` the prompt is not returned but only the newly generated tokens, defaults to `True`.

        Returns:
            `torch.LongTensor`: A tensor of shape (`batch_size`, `gen_len`) containing response tokens.
        """

        if isinstance(query_tensor, List):
            return self._generate_batched(
                query_tensor,
                length_sampler=length_sampler,
                batch_size=batch_size,
                return_prompt=return_prompt,
                **generation_kwargs,
            )

        else:
            if length_sampler is not None:
                generation_kwargs["max_new_tokens"] = length_sampler()
            response = self.accelerator.unwrap_model(self.model).generate(
                input_ids=query_tensor.unsqueeze(dim=0), **generation_kwargs
            )

            if not return_prompt and not self.is_encoder_decoder:
                return response[:, query_tensor.shape[0] :]
            return response

    def _generate_batched(
        self,
        query_tensors: List[torch.Tensor],
        length_sampler: Callable = None,
        batch_size: int = 4,
        return_prompt: bool = True,
        pad_to_multiple_of: int = None,
        remove_padding: bool = True,
        **generation_kwargs,
    ):
        outputs = []

        padding_side_default = self.tokenizer.padding_side
        if not self.is_encoder_decoder:
            self.tokenizer.padding_side = "left"

        # in case we have fewer examples than bs
        batch_size = min(len(query_tensors), batch_size)

        for i in range(0, len(query_tensors), batch_size):
            if length_sampler is not None:
                generation_kwargs["max_new_tokens"] = length_sampler()

            # prevent overflow if query tensors are not even multiple of bs
            end_index = min(len(query_tensors), i + batch_size)

            batch = query_tensors[i:end_index]
            batch_mask = [torch.ones_like(element) for element in batch]
            inputs = {"input_ids": batch, "attention_mask": batch_mask}

            padded_inputs = self.tokenizer.pad(
                inputs,
                padding=True,
                max_length=None,
                pad_to_multiple_of=pad_to_multiple_of,
                return_tensors="pt",
            ).to(self.current_device)

            generations = self.accelerator.unwrap_model(self.model).generate(**padded_inputs, **generation_kwargs)

            for generation, mask in zip(generations, padded_inputs["attention_mask"]):
                if not self.is_encoder_decoder:
                    output = generation[(1 - mask).sum() :]  # remove padding
                else:
                    output = generation

                if not return_prompt and not self.is_encoder_decoder:
                    output = output[(mask).sum() :]  # remove prompt

                if remove_padding and self.tokenizer.eos_token_id in output:
                    pad_mask = output == self.tokenizer.eos_token_id
                    pad_start = torch.nonzero(pad_mask, as_tuple=False)[0, 0].item()
                    output = output[: pad_start + 1]  # keep the eos token at the end

                outputs.append(output)

        self.tokenizer.padding_side = padding_side_default
        return outputs

    def _step_safety_checker(
        self,
        batch_size: int,
        queries: List[torch.LongTensor],
        responses: List[torch.LongTensor],
        scores: List[torch.FloatTensor],
        masks: Optional[List[torch.LongTensor]] = None,
    ):
        """
        Check if the input data is valid for training.

        Args:
            batch_size (int):
                Batch size from the config file.
            queries (List[`torch.LongTensor`]):
                List of tensors containing the encoded queries of shape (`query_length`)
            responses (List[`torch.LongTensor`]):
                List of tensors containing the encoded responses of shape (`response_length`)
            scores (List[`torch.FloatTensor`]):
                List of tensors containing the scores.
            masks (List[`torch.LongTensor`], *optional*):
                list of optional tensors containing the masks of shape (`query_length` + `response_length`)
        Returns:
            `tuple`: The input processed data.
        """
        for name, tensor_list in zip(["queries", "responses", "scores"], [queries, responses, scores]):
            if not isinstance(tensor_list, list):
                raise ValueError(f"{name} must be a list of tensors - got {type(tensor_list)}")
            if not isinstance(tensor_list[0], torch.Tensor):
                raise ValueError(f"Elements in {name} must be tensors - got {type(tensor_list[0])}")
            if batch_size is not None and len(tensor_list) != batch_size:
                raise ValueError(
                    f"Batch size ({batch_size}) does not match number of examples - but got {len(tensor_list)} for: {name}"
                )

        # add queries, scores and responses on the correct device
        queries = [tensor.to(self.current_device) for tensor in queries]
        responses = [tensor.to(self.current_device) for tensor in responses]
        scores = [tensor.to(self.current_device) for tensor in scores]
        masks = [tensor.to(self.current_device) for tensor in masks] if masks is not None else None

        # squeeze scores if needed
        for i, score in enumerate(scores):
            if score.dim() > 1:
                raise ValueError(f"Scores must be 1-dimensional - got {score.dim()} for {score}")
            elif score.dim() == 1:
                scores[i] = score.squeeze()

        return queries, responses, scores, masks

    @PPODecorators.empty_cuda_cache()
    def step(
        self,
        queries: List[torch.LongTensor],
        responses: List[torch.LongTensor],
        scores: List[torch.FloatTensor],
        response_masks: Optional[List[torch.LongTensor]] = None,
    ):
        """
        Run a PPO optimisation step given a list of queries, model responses, and rewards.

        Args:
            queries (List[`torch.LongTensor`]):
                List of tensors containing the encoded queries of shape (`query_length`)
            responses (List[`torch.LongTensor`]):
                List of tensors containing the encoded responses of shape (`response_length`)
            scores (List[`torch.FloatTensor`]):
                List of tensors containing the scores.
            response_masks (List[`torch.FloatTensor`], *optional*)):
                List of tensors containing masks of the response tokens.

        Returns:
            `dict[str, Any]`: A summary of the training statistics
        """

        bs = self.config.batch_size

        queries, responses, scores, response_masks = self._step_safety_checker(
            bs, queries, responses, scores, response_masks
        )
        scores = torch.tensor(scores, device=self.current_device)
        if self.config.use_score_scaling:
            # Score scaling
            scores_mean, scores_std = self.running.update(scores)
            tensor_to_kwargs = dict(dtype=scores.dtype, device=scores.device)
            score_scaling_factor = self.running.std.to(**tensor_to_kwargs) + torch.finfo(scores.dtype).eps
            if self.config.use_score_norm:
                scores = (scores - self.running.mean.to(**tensor_to_kwargs)) / score_scaling_factor
            else:
                scores /= score_scaling_factor

        if self.config.score_clip is not None:
            # Score clipping
            scores_dtype = scores.dtype
            scores = torch.clip(scores.float(), -self.config.score_clip, self.config.score_clip).to(dtype=scores_dtype)

        # if we want to push best model to the hub
        if hasattr(self, "highest_reward"):
            if self.compare_step % self.config.compare_steps == 0:
                curr_mean_reward = scores.mean()
                # if the best reward ever seen
                if curr_mean_reward > self.highest_reward:
                    self.highest_reward = curr_mean_reward
                    # push model to hub
                    self.push_to_hub(**self.push_to_hub_kwargs)
            self.compare_step += 1

        timing = dict()
        t0 = time.time()

        t = time.time()

        model_inputs = self.prepare_model_inputs(queries, responses)

        # performs padding in distributed setting 
        # what about padding in non-distributed setting then? 
        if self.is_distributed:

            # if response length = 42, left padding adds 6 zeros to the left of attention mask to reach desired total length
            pad_first = self.tokenizer.padding_side == "left"

            model_inputs["input_ids"] = self.accelerator.pad_across_processes(
                model_inputs["input_ids"],
                dim=1,
                pad_index=self.tokenizer.pad_token_id,
                pad_first=pad_first,
            )
            model_inputs["attention_mask"] = self.accelerator.pad_across_processes(
                model_inputs["attention_mask"], dim=1, pad_index=0, pad_first=pad_first
            )
            if self.is_encoder_decoder:
                model_inputs["decoder_input_ids"] = self.accelerator.pad_across_processes(
                    model_inputs["decoder_input_ids"],
                    dim=1,
                    pad_index=self.tokenizer.pad_token_id,
                    pad_first=pad_first,
                )
                model_inputs["decoder_attention_mask"] = self.accelerator.pad_across_processes(
                    model_inputs["decoder_attention_mask"],
                    dim=1,
                    pad_index=0,
                    pad_first=pad_first,
                )

        model_inputs_names = list(model_inputs.keys())

        full_kl_penalty = self.config.kl_penalty == "full"

        with torch.no_grad():
            all_logprobs, logits_or_none, values, masks = self.batched_forward_pass(
                self.model,
                queries,
                responses,
                model_inputs,
                response_masks=response_masks,
                return_logits=full_kl_penalty,
            )

            # for when the model is a peft model
            if self.is_peft_model and hasattr(
                self.accelerator.unwrap_model(self.model).pretrained_model,
                "disable_adapter",
            ):
                with self.accelerator.unwrap_model(self.model).pretrained_model.disable_adapter():
                    ref_logprobs, ref_logits_or_none, _, _ = self.batched_forward_pass(
                        self.model, queries, responses, model_inputs, return_logits=full_kl_penalty
                    )
            elif self.is_peft_model and not hasattr(self.model.pretrained_model, "disable_adapter"):
                raise ValueError(
                    "You are using a `peft` version that does not support `disable_adapter`. Please update your `peft` version to the latest version."
                )

            else:
                ref_logprobs, ref_logits_or_none, _, _ = self.batched_forward_pass(
                    self.ref_model, queries, responses, model_inputs, return_logits=full_kl_penalty
                )

        timing["time/ppo/forward_pass"] = time.time() - t

        with torch.no_grad():
            t = time.time()
            if full_kl_penalty:
                active_full_logprobs = logprobs_from_logits(logits_or_none, None, gather=False)
                ref_full_logprobs = logprobs_from_logits(ref_logits_or_none, None, gather=False)

                rewards, non_score_reward = self.compute_rewards(
                    scores, active_full_logprobs, ref_full_logprobs, masks
                )
            else:
                rewards, non_score_reward = self.compute_rewards(scores, all_logprobs, ref_logprobs, masks)
            timing["time/ppo/compute_rewards"] = time.time() - t

            t = time.time()
            values, advantages, returns = self.compute_advantages(values, rewards, masks)
            timing["time/ppo/compute_advantages"] = time.time() - t

        # upcast to float32 to avoid dataset issues
        batch_dict = {
            "queries": queries,
            "responses": responses,
            "logprobs": all_logprobs.to(torch.float32),
            "values": values.to(torch.float32),
            "masks": masks,
            "advantages": advantages,
            "returns": returns,
        }
        batch_dict.update(model_inputs)

        t = time.time()
        all_stats = []
        early_stop = False
        for _ in range(self.config.ppo_epochs):
            if early_stop:
                break
            b_inds = np.random.permutation(bs)
            for backward_batch_start in range(0, bs, self.config.backward_batch_size):
                backward_batch_end = backward_batch_start + self.config.backward_batch_size
                backward_batch_inds = b_inds[backward_batch_start:backward_batch_end]

                for mini_batch_start in range(0, self.config.backward_batch_size, self.config.mini_batch_size):
                    mini_batch_end = mini_batch_start + self.config.mini_batch_size
                    mini_batch_inds = backward_batch_inds[mini_batch_start:mini_batch_end]
                    mini_batch_dict = {
                        "logprobs": batch_dict["logprobs"][mini_batch_inds],
                        "values": batch_dict["values"][mini_batch_inds],
                        "masks": batch_dict["masks"][mini_batch_inds],
                        # hacks: the queries and responses are ragged.
                        "queries": [batch_dict["queries"][i] for i in mini_batch_inds],
                        "responses": [batch_dict["responses"][i] for i in mini_batch_inds],
                        "advantages": batch_dict["advantages"][mini_batch_inds],
                        "returns": batch_dict["returns"][mini_batch_inds],
                    }
                    for k in model_inputs_names:
                        mini_batch_dict[k] = batch_dict[k][mini_batch_inds]
                    with self.accelerator.accumulate(self.model):
                        model_inputs = {k: mini_batch_dict[k] for k in model_inputs_names}

                        logprobs, logits, vpreds, _ = self.batched_forward_pass(
                            self.model,
                            mini_batch_dict["queries"],
                            mini_batch_dict["responses"],
                            model_inputs,
                            return_logits=True,
                        )
                        train_stats = self.train_minibatch(
                            mini_batch_dict["logprobs"],
                            mini_batch_dict["values"],
                            logprobs,
                            logits,
                            vpreds,
                            mini_batch_dict["masks"],
                            mini_batch_dict["advantages"],
                            mini_batch_dict["returns"],
                        )
                        all_stats.append(train_stats)

            # typically, early stopping is done at the epoch level
            if self.config.early_stopping:
                policykl = train_stats["policy/policykl"]
                early_stop = self._early_stop(policykl)
                if early_stop:
                    break

        timing["time/ppo/optimize_step"] = time.time() - t

        t = time.time()
        train_stats = stack_dicts(all_stats)

        # reshape advantages/ratios such that they are not averaged.
        train_stats["policy/advantages"] = torch.flatten(train_stats["policy/advantages"]).unsqueeze(0)
        train_stats["policy/advantages"] = torch.nan_to_num(train_stats["policy/advantages"], WANDB_PADDING)
        train_stats["policy/ratio"] = torch.flatten(train_stats["policy/ratio"]).unsqueeze(0)

        stats = self.record_step_stats(
            scores=scores,
            logprobs=all_logprobs,
            ref_logprobs=ref_logprobs,
            non_score_reward=non_score_reward,
            train_stats=train_stats,
            kl_coef=self.kl_ctl.value,
            masks=masks,
            queries=queries,
            responses=responses,
        )
        # Gather/Reduce stats from all processes
        if self.is_distributed:
            stats = self.gather_stats(stats)
        stats = stats_to_np(stats)
        timing["time/ppo/calc_stats"] = time.time() - t
        stats["ppo/learning_rate"] = self.optimizer.param_groups[0]["lr"]

        # Update the KL control - multiply the batch_size by the number of processes
        self.kl_ctl.update(
            stats["objective/kl"],
            self.config.batch_size * self.accelerator.num_processes,
        )

        # Log the total ppo time
        timing["time/ppo/total"] = time.time() - t0
        stats.update(timing)

        # post-process stats for tensorboard and other loggers
        if self.config.log_with != "wandb":
            stats = convert_to_scalar(stats)

        if self.lr_scheduler is not None:
            self.lr_scheduler.step()

        return stats

    def _early_stop(self, policykl):
        r"""
        Handles the early stopping logic. If the policy KL is greater than the target KL, then the gradient is zeroed and
        the optimization step is skipped.
        This also handles the multi-gpu case where the policy KL is averaged across all processes.

        Args:
            policy_kl (torch.Tensor):
                the policy KL

        Returns:
            `bool`: whether to early stop or not
        """
        early_stop = False
        if not self.config.early_stopping:
            return early_stop

        if not self.is_distributed and policykl > 1.5 * self.config.target_kl:
            self.optimizer.zero_grad()
            early_stop = True
        elif self.is_distributed:
            import torch.distributed as dist

            # Wait for all processes to finish
            dist.barrier()

            # all gather the policykl
            dist.all_reduce(policykl, dist.ReduceOp.SUM)
            policykl /= self.accelerator.num_processes

            if policykl > 1.5 * self.config.target_kl:
                self.optimizer.zero_grad()
                early_stop = True
        return early_stop

    def gather_stats(self, stats):
        """
        Gather stats from all processes. Useful in the context of distributed training.

        Args:
            stats (dict[str, Any]):
            a dictionary of stats to be gathered. The stats should contain torch tensors.

        Returns:
            `dict[str, Any]`: A dictionary of stats with the tensors gathered.
        """
        import torch.distributed as dist

        # Wait for all processes to finish
        dist.barrier()

        for k, v in stats.items():
            if isinstance(v, torch.Tensor):
                dist.all_reduce(v.to(self.accelerator.device), dist.ReduceOp.SUM)
                v /= self.accelerator.num_processes
            stats[k] = v
        return stats

    def prepare_model_inputs(self, queries: torch.Tensor, responses: torch.Tensor):
        if self.is_encoder_decoder:
            input_data = self.data_collator(
                [{"input_ids": q, "attention_mask": torch.ones_like(q)} for q in queries]
            ).to(self.current_device)

            decoder_inputs = self.data_collator(
                [{"input_ids": r, "attention_mask": torch.ones_like(r)} for r in responses]
            ).to(self.current_device)

            input_data["decoder_input_ids"] = decoder_inputs["input_ids"]
            input_data["decoder_attention_mask"] = decoder_inputs["attention_mask"]
        else:
            input_ids = [torch.cat([q, r]) for q, r in zip(queries, responses)]
            # attention mask is 1 only for the query + actual response lengths and not (64+48=112)
            input_data = self.data_collator(
                [{"input_ids": ids, "attention_mask": torch.ones_like(ids)} for ids in input_ids]
            ).to(self.current_device)

        input_data.pop("labels", None)  # we don't want to compute LM losses
        return input_data

    @PPODecorators.empty_cuda_cache()
    def batched_forward_pass(
        self,
        model: PreTrainedModelWrapper,
        queries: torch.Tensor,
        responses: torch.Tensor,
        model_inputs: dict,
        return_logits: bool = False,
        response_masks: Optional[torch.Tensor] = None,
    ):
        """
        Calculate model outputs in multiple batches.

        Args:
            queries (`torch.LongTensor`):
                List of tensors containing the encoded queries, shape (`batch_size`, `query_length`)
            responses (`torch.LongTensor`):
                List of tensors containing the encoded responses, shape (`batch_size`, `response_length`)
            return_logits (`bool`, *optional*, defaults to `False`):
                Whether to return all_logits. Set to `False` if logits are not needed to reduce memory consumption.
        Returns:
            (tuple):
                - all_logprobs (`torch.FloatTensor`): Log probabilities of the responses,
                    shape (`batch_size`, `response_length`)
                - all_ref_logprobs (`torch.FloatTensor`): Log probabilities of the responses,
                    shape (`batch_size`, `response_length`)
                - all_values (`torch.FloatTensor`): Values of the responses, shape (`batch_size`, `response_length`)
        """
        bs = len(queries)
        fbs = self.config.mini_batch_size
        all_logprobs = []
        all_logits = []
        all_masks = []
        all_values = []

        model.eval()


        for i in range(math.ceil(bs / fbs)):
            input_kwargs = {key: value[i * fbs : (i + 1) * fbs] for key, value in model_inputs.items()}
            query_batch = queries[i * fbs : (i + 1) * fbs]
            response_batch = responses[i * fbs : (i + 1) * fbs]
            if response_masks is not None:
                response_masks_batch = response_masks[i * fbs : (i + 1) * fbs]
            logits, _, values = model(**input_kwargs)

            if self.is_encoder_decoder:
                input_ids = input_kwargs["decoder_input_ids"]
                attention_mask = input_kwargs["decoder_attention_mask"]
            else:
                input_ids = input_kwargs["input_ids"]
                attention_mask = input_kwargs["attention_mask"]

            # logits at step
            # why are we reducing size by 1
            # because at the current time step/token, logits are over the next token 
            # thats why logits[:, 0, :] is for input_ids[:, 1] 
            logprobs = logprobs_from_logits(logits[:, :-1, :], input_ids[:, 1:])
            masks = torch.zeros_like(attention_mask)
            # the following would mean that the last element of masks is always zero
            masks[:, :-1] = attention_mask[:, 1:]

            #For a single sequence model, start position is the end of the query. End is start + response length.
            #It handles optional left padding that may need to be offset if present.
            #The masks tensor is used to mask the loss - set to 0 before start and after end index, 1 inside.
            #If a separate response mask is passed in, that is combined with the masks tensor to further mask just the response portion.

            for j in range(len(query_batch)):
                if self.is_encoder_decoder:
                    # Decoder sentence starts always in the index 1 after padding in the Enc-Dec Models
                    start = 1
                    end = attention_mask[j, :].sum() - 1
                else:
                    start = len(query_batch[j]) - 1  # logprobs starts from the second query token
                    am_start = start
                    if attention_mask[j, 0] == 0:  # offset left padding
                        am_start += attention_mask[j, :].nonzero()[0]
                    end = start + len(response_batch[j])
                    am_end = am_start + len(response_batch[j])
                    if response_masks is not None:
                        response_masks_batch[j] = torch.cat(
                            (torch.zeros_like(query_batch[j]), response_masks_batch[j])
                        )[1:]

                masks[j, :start] = 0
                masks[j, end:] = 0
                if response_masks is not None:
                    # this was incorrect
                    masks[j, am_start:am_end] = masks[j, am_start:am_end] * response_masks_batch[j][start:end]

            if return_logits:
                all_logits.append(logits)
            else:
                del logits
            all_values.append(values)
            all_logprobs.append(logprobs)
            all_masks.append(masks)

        return (
            torch.cat(all_logprobs),
            torch.cat(all_logits)[:, :-1] if return_logits else None,
            torch.cat(all_values)[:, :-1],
            torch.cat(all_masks)[:, :-1],
        )

    @PPODecorators.empty_cuda_cache()
    def train_minibatch(
        self,
        old_logprobs: torch.FloatTensor,
        values: torch.FloatTensor,
        logprobs: torch.FloatTensor,
        logits: torch.FloatTensor,
        vpreds: torch.FloatTensor,
        mask: torch.LongTensor,
        advantages: torch.FloatTensor,
        returns: torch.FloatTensor,
    ):
        """
        Train one PPO minibatch

        Args:
            logprobs (`torch.FloatTensor`):
                Log probabilities of the model, shape [batch_size, response_length]
            values (`torch.FloatTensor`):
                Values of the value head, shape [batch_size, response_length]
            query (`torch.LongTensor`):
                Encoded queries, shape [batch_size, query_length]
            response (`torch.LongTensor`):
                Encoded responses, shape [batch_size, response_length]
            model_input (`torch.LongTensor`):
                Concatenated queries and responses, shape [batch_size, query_length+response_length]

        Returns:
            train_stats (dict[str, `torch.Tensor`]):
                Dictionary of training statistics
        """
        self.model.train()
        loss_p, loss_v, train_stats = self.loss(
            old_logprobs, values, logits, vpreds, logprobs, mask, advantages, returns
        )
        loss = loss_p + loss_v
        self.accelerator.backward(loss)
        if self.config.max_grad_norm is not None:
            if self.accelerator.sync_gradients:
                self.accelerator.clip_grad_norm_(self.model_params, self.config.max_grad_norm)
        self.optimizer.step()
        # we call optimizer.zero_grad() every time and let `accelerator` handle accumulation
        # see https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation#the-finished-code
        self.optimizer.zero_grad()
        return train_stats

    def compute_rewards(
        self,
        scores: torch.FloatTensor,
        logprobs: torch.FloatTensor,
        ref_logprobs: torch.FloatTensor,
        masks: torch.LongTensor,
    ):
        """
        Compute per token rewards from scores and KL-penalty.

        Args:
            scores (`torch.FloatTensor`):
                Scores from the reward model, shape (`batch_size`)
            logprobs (`torch.FloatTensor`):
                Log probabilities of the model, shape (`batch_size`, `response_length`)
            ref_logprobs (`torch.FloatTensor`):
                Log probabilities of the reference model, shape (`batch_size`, `response_length`)
        """
        rewards, non_score_rewards = [], []
        i = 0
        for score, logprob, ref_logprob, mask in zip(scores, logprobs, ref_logprobs, masks):
            #print(i, len(mask.nonzero()))
            # compute KL penalty (from difference in logprobs)
            kl = self._kl_penalty(logprob, ref_logprob)
            non_score_reward = -self.kl_ctl.value * kl
            non_score_rewards.append(non_score_reward)
            reward = non_score_reward.clone()
            if len(mask.nonzero()) == 0:
                last_non_masked_index = 0
            else: 
                last_non_masked_index = mask.nonzero()[-1]

            # reward is preference model score + KL penalty
            reward[last_non_masked_index] += score
            rewards.append(reward)
            i += 1
        return torch.stack(rewards), torch.stack(non_score_rewards)

    def _kl_penalty(self, logprob: torch.FloatTensor, ref_logprob: torch.FloatTensor) -> torch.FloatTensor:
        if self.config.kl_penalty == "kl":
            return logprob - ref_logprob

        if self.config.kl_penalty == "abs":
            return (logprob - ref_logprob).abs()

        if self.config.kl_penalty == "mse":
            return 0.5 * (logprob - ref_logprob).square()

        if self.config.kl_penalty == "full":
            # Flip is required due to this issue? :https://github.com/pytorch/pytorch/issues/57459
            return F.kl_div(ref_logprob, logprob, log_target=True, reduction="none").sum(-1)

        raise NotImplementedError

    def compute_advantages(
        self: torch.FloatTensor,
        values: torch.FloatTensor,
        rewards: torch.FloatTensor,
        mask: torch.FloatTensor,
    ):
        lastgaelam = 0
        advantages_reversed = []
        gen_len = rewards.shape[-1]

        values = values * mask
        rewards = rewards * mask

        for t in reversed(range(gen_len)):
            nextvalues = values[:, t + 1] if t < gen_len - 1 else 0.0
            delta = rewards[:, t] + self.config.gamma * nextvalues - values[:, t]
            lastgaelam = delta + self.config.gamma * self.config.lam * lastgaelam
            advantages_reversed.append(lastgaelam)
        advantages = torch.stack(advantages_reversed[::-1]).transpose(0, 1)

        returns = advantages + values
        advantages = masked_whiten(advantages, mask)
        advantages = advantages.detach()
        return values, advantages, returns

    def loss(
        self,
        old_logprobs: torch.FloatTensor,
        values: torch.FloatTensor,
        logits: torch.FloatTensor,
        vpreds: torch.FloatTensor,
        logprobs: torch.FloatTensor,
        mask: torch.LongTensor,
        advantages: torch.FloatTensor,
        returns: torch.FloatTensor,
    ):
        """
        Calculate policy and value losses.

        Args:
            old_logprobs (`torch.FloatTensor`):
                Log probabilities of the model, shape (`batch_size`, `response_length`)
            values (`torch.FloatTensor`):
                Values of the value head, shape (`batch_size`, `response_length`)
            rewards (`torch.FloatTensor`):
                Rewards from the reward model, shape (`batch_size`, `response_length`)
            logits (`torch.FloatTensor`):
                Logits of the model, shape (`batch_size`, `response_length`, `vocab_size`)
            v_pred (`torch.FloatTensor`):
                Values of the value head, shape (`batch_size`, `response_length`)
            logprobs (`torch.FloatTensor`):
                Log probabilities of the model, shape (`batch_size`, `response_length`)
        """

        vpredclipped = clip_by_value(
            vpreds,
            values - self.config.cliprange_value,
            values + self.config.cliprange_value,
        )

        vf_losses1 = (vpreds - returns) ** 2
        vf_losses2 = (vpredclipped - returns) ** 2
        vf_loss = 0.5 * masked_mean(torch.max(vf_losses1, vf_losses2), mask)
        vf_clipfrac = masked_mean(torch.gt(vf_losses2, vf_losses1).float(), mask)

        ratio = torch.exp(logprobs - old_logprobs)

        pg_losses = -advantages * ratio
        pg_losses2 = -advantages * torch.clamp(ratio, 1.0 - self.config.cliprange, 1.0 + self.config.cliprange)

        pg_loss = masked_mean(torch.max(pg_losses, pg_losses2), mask)
        pg_clipfrac = masked_mean(torch.gt(pg_losses2, pg_losses).float(), mask)

        loss = pg_loss + self.config.vf_coef * vf_loss

        avg_ratio = masked_mean(ratio, mask).item()
        if avg_ratio > self.config.ratio_threshold:
            warnings.warn(
                f"The average ratio of batch ({avg_ratio:.2f}) exceeds threshold {self.config.ratio_threshold:.2f}. Skipping batch."
            )
            pg_loss = pg_loss * 0.0
            vf_loss = vf_loss * 0.0
            loss = loss * 0.0

        entropy = masked_mean(entropy_from_logits(logits), mask)

        approxkl = 0.5 * masked_mean((logprobs - old_logprobs) ** 2, mask)
        policykl = masked_mean(old_logprobs - logprobs, mask)

        return_mean, return_var = masked_mean(returns, mask), masked_var(returns, mask)
        value_mean, value_var = masked_mean(values, mask), masked_var(values, mask)

        stats = dict(
            loss=dict(policy=pg_loss.detach(), value=vf_loss.detach(), total=loss.detach()),
            policy=dict(
                entropy=entropy.detach(),
                approxkl=approxkl.detach(),
                policykl=policykl.detach(),
                clipfrac=pg_clipfrac.detach(),
                advantages=advantages.detach(),
                advantages_mean=masked_mean(advantages, mask).detach(),
                ratio=ratio.detach(),
            ),
            returns=dict(mean=return_mean.detach(), var=return_var.detach()),
            val=dict(
                vpred=masked_mean(vpreds, mask).detach(),
                error=masked_mean((vpreds - returns) ** 2, mask).detach(),
                clipfrac=vf_clipfrac.detach(),
                mean=value_mean.detach(),
                var=value_var.detach(),
            ),
        )
        return pg_loss, self.config.vf_coef * vf_loss, flatten_dict(stats)

    def record_step_stats(self, kl_coef: float, **data):
        """
        Record training step statistics.


        Args:
            kl_coef (`float`):
                KL coefficient
            data (`dict`):
                Dictionary of training step data

        Returns:
            stats (`dict`):
                Dictionary of training step statistics
        """
        mask = data.pop("masks")

        kl_list = ((data["logprobs"] - data["ref_logprobs"]) * mask).sum(axis=-1)
        mean_kl = kl_list.mean()
        mean_entropy = (-data["logprobs"] * mask).sum(axis=-1).mean()

        mean_non_score_reward = masked_mean(
            data["non_score_reward"], mask
        )  # non_score_reward is size `batch_size`, `response_length`
        mean_scores = data["scores"].mean()  # scores is size `batch_size`
        std_scores = data["scores"].std()

        if mean_kl.item() < -1.0:
            # warn users
            warnings.warn(
                f"KL divergence is starting to become negative: {mean_kl.item():.2f} - this might be a precursor for failed training."
                " sometimes this happens because the generation kwargs are not correctly set. Please make sure"
                " that the generation kwargs are set correctly, or review your training hyperparameters."
            )

        stats = {
            "objective/kl": mean_kl,
            "objective/kl_dist": kl_list,
            "objective/logprobs": data["logprobs"],
            "objective/ref_logprobs": data["ref_logprobs"],
            "objective/kl_coef": kl_coef,
            "objective/entropy": mean_entropy,
            "ppo/mean_non_score_reward": mean_non_score_reward,
            "ppo/mean_scores": mean_scores,
            "ppo/std_scores": std_scores,
        }

        # Log text properties
        query_lens = torch.tensor([len(query) for query in data["queries"]], dtype=torch.float)
        response_lens = torch.tensor([len(response) for response in data["responses"]], dtype=torch.float)

        stats["tokens/queries_len_mean"] = torch.mean(query_lens).cpu().numpy().item()
        stats["tokens/queries_len_std"] = torch.std(query_lens).cpu().numpy().item()
        stats["tokens/queries_dist"] = query_lens.cpu().numpy()
        stats["tokens/responses_len_mean"] = torch.mean(response_lens).cpu().numpy().item()
        stats["tokens/responses_len_std"] = torch.std(response_lens).cpu().numpy().item()
        stats["tokens/responses_dist"] = response_lens.cpu().numpy()

        for k, v in data["train_stats"].items():
            stats[f"ppo/{k}"] = torch.mean(v, axis=0)
        stats["ppo/val/var_explained"] = 1 - stats["ppo/val/error"] / stats["ppo/returns/var"]
        return stats

    def log_stats(
        self,
        stats: dict,
        batch: dict,
        rewards: List[torch.FloatTensor],
        columns_to_log: List[str] = ["query", "response"],
    ):
        """
        A function that logs all the training stats. Call it at the end of each epoch.

        Args:
            stats (dict[str, Any]):
                A dictionary of training stats.
            batch (dict[str, Any]):
                A dictionary of batch data, this contains the queries and responses.
            rewards (`List[torch.FloatTensor]`):
                A tensor of rewards.
        """
        # Log only if we are in the main process
        if self.accelerator.is_main_process:
            logs = {}

            # Log stats
            if not isinstance(rewards, torch.Tensor):
                rewards = torch.tensor(rewards).to(self.current_device)

            if "query" not in batch.keys() and "response" not in batch.keys():
                # warn the user that the game logs will not be logged
                warnings.warn(
                    "The game logs will not be logged because the batch does not contain the keys 'query' and "
                    "'response'. "
                )
            elif self.config.log_with == "wandb":
                import wandb

                if any([column_to_log not in batch.keys() for column_to_log in columns_to_log]):
                    raise ValueError(f"Columns to log {columns_to_log} are not present in the batch {batch.keys()}.")

                batch_list = [batch[column_to_log] for column_to_log in columns_to_log]

                table_rows = [list(r) for r in zip(*batch_list, rewards.cpu().tolist())]
                logs.update({"game_log": wandb.Table(columns=[*columns_to_log, "reward"], rows=table_rows)})
            # All reduce rewards if distributed
            if self.is_distributed:
                import torch.distributed as dist

                dist.barrier()

                dist.all_reduce(rewards, op=torch.distributed.ReduceOp.SUM)
                rewards /= self.accelerator.num_processes

            logs.update(stats)

            # manually cast in fp32 for bf16 torch tensors
            for k, v in logs.items():
                if isinstance(v, torch.Tensor) and v.dtype == torch.bfloat16:
                    logs[k] = v.float()

            logs["env/reward_mean"] = torch.mean(rewards).cpu().numpy().item()
            logs["env/reward_std"] = torch.std(rewards).cpu().numpy().item()
            logs["env/reward_dist"] = rewards.cpu().numpy()

            if self.config.log_with == "tensorboard":
                # update the current step
                self.current_step += 1

            self.accelerator.log(
                logs,
                step=self.current_step if self.config.log_with == "tensorboard" else None,
            )

        else:
            if self.is_distributed:
                import torch.distributed as dist

                if not isinstance(rewards, torch.Tensor):
                    rewards = torch.tensor(rewards).to(self.current_device)

                dist.barrier()
                dist.all_reduce(rewards, op=torch.distributed.ReduceOp.SUM)

    def create_model_card(self, path: str, model_name: Optional[str] = "TRL Model") -> None:
        """Creates and saves a model card for a TRL model.

        Args:
            path (`str`): The path to save the model card to.
            model_name (`str`, *optional*): The name of the model, defaults to `TRL Model`.
        """
        try:
            user = whoami()["name"]
        # handle the offline case
        except:  # noqa
            warnings.warn("Cannot retrieve user information assuming you are running in offline mode.")
            return

        if not os.path.exists(path):
            os.makedirs(path)

        model_card_content = MODEL_CARD_TEMPLATE.format(model_name=model_name, model_id=f"{user}/{path}")
        with open(os.path.join(path, "README.md"), "w", encoding="utf-8") as f:
            f.write(model_card_content)

    def _save_pretrained(self, save_directory: str) -> None:
        self.accelerator.unwrap_model(self.model).save_pretrained(save_directory)
        self.tokenizer.save_pretrained(save_directory)
        self.create_model_card(save_directory)

    def _show_tokens(self, tokens, masks):
        from rich import print
        from rich.text import Text

        text = Text()

        for i, (token, mask) in enumerate(zip(tokens, masks)):
            if mask == 1:
                text.append(self.tokenizer.decode(token.item()), style="black on deep_sky_blue1")
                text.append(" ")
            else:
                text.append(self.tokenizer.decode(token.item()), style="black on cyan3")
                text.append(" ")
        print(text)

    def _prepare_deepspeed(self, model: PreTrainedModelWrapper):
        # Adapted from accelerate: https://github.com/huggingface/accelerate/blob/739b135f8367becb67ffaada12fe76e3aa60fefd/src/accelerate/accelerator.py#L1473
        deepspeed_plugin = self.accelerator.state.deepspeed_plugin
        config_kwargs = deepspeed_plugin.deepspeed_config
        if model is not None:
            if hasattr(model, "config"):
                hidden_size = (
                    max(model.config.hidden_sizes)
                    if getattr(model.config, "hidden_sizes", None)
                    else getattr(model.config, "hidden_size", None)
                )
                if hidden_size is not None and config_kwargs["zero_optimization"]["stage"] == 3:
                    # Note that `stage3_prefetch_bucket_size` can produce DeepSpeed messages like: `Invalidate trace cache @ step 0: expected module 1, but got module 0`
                    # This is expected and is not an error, see: https://github.com/microsoft/DeepSpeed/discussions/4081
                    config_kwargs.update(
                        {
                            "zero_optimization.reduce_bucket_size": hidden_size * hidden_size,
                            "zero_optimization.stage3_param_persistence_threshold": 10 * hidden_size,
                            "zero_optimization.stage3_prefetch_bucket_size": 0.9 * hidden_size * hidden_size,
                        }
                    )

        # If ZeRO-3 is used, we shard both the active and reference model.
        # Otherwise, we assume the reference model fits in memory and is initialized on each device with ZeRO disabled (stage 0)
        if config_kwargs["zero_optimization"]["stage"] != 3:
            config_kwargs["zero_optimization"]["stage"] = 0
        model, *_ = deepspeed.initialize(model=model, config=config_kwargs)
        model.eval()
        return model

File Path: trl/trainer/reward_trainer.py
Content:
# Copyright 2023 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import warnings
from dataclasses import FrozenInstanceError, replace
from typing import Any, Callable, Dict, List, Optional, Tuple, Union

import torch
import torch.nn as nn
from datasets import Dataset
from transformers import DataCollator, PreTrainedModel, PreTrainedTokenizerBase, Trainer, TrainingArguments
from transformers.trainer_callback import TrainerCallback
from transformers.trainer_pt_utils import nested_detach
from transformers.trainer_utils import EvalPrediction

from ..import_utils import is_peft_available
from .training_configs import RewardConfig
from .utils import PeftSavingCallback, RewardDataCollatorWithPadding, compute_accuracy


if is_peft_available():
    from peft import PeftModel, get_peft_model, prepare_model_for_kbit_training


class RewardTrainer(Trainer):
    r"""
    The RewardTrainer can be used to train your custom Reward Model. It is a subclass of the
    `transformers.Trainer` class and inherits all of its attributes and methods. It is recommended to use
    an `AutoModelForSequenceClassification` as the reward model. The reward model should be trained on a dataset
    of paired examples, where each example is a tuple of two sequences. The reward model should be trained to
    predict which example in the pair is more relevant to the task at hand.

    The reward trainer expects a very specific format for the dataset. The dataset should contain two 4 entries at least
    if you don't use the default `RewardDataCollatorWithPadding` data collator. The entries should be named
    - `input_ids_chosen`
    - `attention_mask_chosen`
    - `input_ids_rejected`
    - `attention_mask_rejected`

    Optionally, you can also pass a `margin` entry to the dataset. This entry should contain the margin used to modulate the
    loss of the reward model as outlined in https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/.
    If you don't pass a margin, no margin will be used.
    """

    def __init__(
        self,
        model: Union[PreTrainedModel, nn.Module] = None,
        args: Optional[RewardConfig] = None,
        data_collator: Optional[DataCollator] = None,
        train_dataset: Optional[Dataset] = None,
        eval_dataset: Optional[Union[Dataset, Dict[str, Dataset]]] = None,
        tokenizer: Optional[PreTrainedTokenizerBase] = None,
        model_init: Optional[Callable[[], PreTrainedModel]] = None,
        compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None,
        callbacks: Optional[List[TrainerCallback]] = None,
        optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (
            None,
            None,
        ),
        preprocess_logits_for_metrics: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None,
        max_length: Optional[int] = None,
        peft_config: Optional[Dict] = None,
    ):
        """
        Initialize RewardTrainer.

        Args:
            model (`transformers.PreTrainedModel`):
                The model to train, preferably an `AutoModelForSequenceClassification`.
            args (`RewardConfig`):
                The arguments to use for training.
            data_collator (`transformers.DataCollator`):
                The data collator to use for training. If None is specified, the default data collator (`RewardDataCollatorWithPadding`) will be used
                which will pad the sequences to the maximum length of the sequences in the batch, given a dataset of paired sequences.
            train_dataset (`datasets.Dataset`):
                The dataset to use for training.
            eval_dataset (`datasets.Dataset`):
                The dataset to use for evaluation.
            tokenizer (`transformers.PreTrainedTokenizerBase`):
                The tokenizer to use for training. This argument is required if you want to use the default data collator.
            model_init (`Callable[[], transformers.PreTrainedModel]`):
                The model initializer to use for training. If None is specified, the default model initializer will be used.
            compute_metrics (`Callable[[transformers.EvalPrediction], Dict]`, *optional* defaults to `compute_accuracy`):
                The metrics to use for evaluation. If no metrics are specified, the default metric (`compute_accuracy`) will be used.
            callbacks (`List[transformers.TrainerCallback]`):
                The callbacks to use for training.
            optimizers (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`):
                The optimizer and scheduler to use for training.
            preprocess_logits_for_metrics (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`):
                The function to use to preprocess the logits before computing the metrics.
            peft_config (`Dict`, defaults to `None`):
                The PEFT configuration to use for training. If you pass a PEFT configuration, the model will be wrapped in a PEFT model.
        """
        if type(args) == TrainingArguments:
            warnings.warn(
                "Using `transformers.TrainingArguments` for `args` is deprecated and will be removed in a future version. Please use `RewardConfig` instead.",
                FutureWarning,
            )
            if max_length is not None:
                warnings.warn(
                    "The `max_length` argument is deprecated and will be removed in a future version. Please use the `RewardConfig` to set `max_length` instead.",
                    FutureWarning,
                )
        else:
            if max_length is not None and args.max_length is not None:
                raise ValueError(
                    "You cannot specify both `max_length` and `args.max_length`. Please use the `RewardConfig` to set `max_length` once."
                )
            if max_length is not None and args.max_length is None:
                warnings.warn(
                    "The `max_length` argument is deprecated and will be removed in a future version. Please use the `RewardConfig` to set `max_length` instead.",
                    FutureWarning,
                )
        if not is_peft_available() and peft_config is not None:
            raise ValueError(
                "PEFT is not installed and you passed a `peft_config` in the trainer's kwargs, please install it to use the PEFT models"
            )
        elif is_peft_available() and peft_config is not None:
            if getattr(model, "is_loaded_in_8bit", False) or getattr(model, "is_quantized", False):
                model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=args.gradient_checkpointing)

            model = get_peft_model(model, peft_config)

        if is_peft_available() and callbacks is None and isinstance(model, PeftModel):
            callbacks = [PeftSavingCallback()]

        if compute_metrics is None:
            compute_metrics = compute_accuracy

        if data_collator is None:
            if tokenizer is None:
                raise ValueError(
                    "max_length or a tokenizer must be specified when using the default RewardDataCollatorWithPadding"
                )
            if type(args) == TrainingArguments:
                if max_length is None:
                    warnings.warn(
                        "When using RewardDataCollatorWithPadding, you should set `max_length` in RewardConfig."
                        " It will be set to `512` by default, but you should do it yourself in the future.",
                        UserWarning,
                    )
                    max_length = 512
            else:
                if max_length is None and args.max_length is None:
                    warnings.warn(
                        "When using RewardDataCollatorWithPadding, you should set `max_length` in RewardConfig."
                        " It will be set to `512` by default, but you should do it yourself in the future.",
                        UserWarning,
                    )
                    max_length = 512
                if max_length is None and args.max_length is not None:
                    max_length = args.max_length

            data_collator = RewardDataCollatorWithPadding(tokenizer, max_length=max_length)

            if args.remove_unused_columns:
                try:  # for bc before https://github.com/huggingface/transformers/pull/25435
                    args.remove_unused_columns = False
                except FrozenInstanceError:
                    args = replace(args, remove_unused_columns=False)
                # warn users
                warnings.warn(
                    "When using RewardDataCollatorWithPadding, you should set `remove_unused_columns=False` in your RewardConfig"
                    " we have set it for you, but you should do it yourself in the future.",
                    UserWarning,
                )

            self.use_reward_data_collator = True
        else:
            self.use_reward_data_collator = False
        super().__init__(
            model,
            args,
            data_collator,
            train_dataset,
            eval_dataset,
            tokenizer,
            model_init,
            compute_metrics,
            callbacks,
            optimizers,
            preprocess_logits_for_metrics,
        )

    def compute_loss(
        self,
        model: Union[PreTrainedModel, nn.Module],
        inputs: Dict[str, Union[torch.Tensor, Any]],
        return_outputs=False,
    ) -> Union[torch.Tensor, Tuple[torch.Tensor, Dict[str, torch.Tensor]]]:
        if not self.use_reward_data_collator:
            warnings.warn(
                "The current compute_loss is implemented for RewardDataCollatorWithPadding,"
                " if you are using a custom data collator make sure you know what you are doing or"
                " implement your own compute_loss method."
            )
        rewards_chosen = model(
            input_ids=inputs["input_ids_chosen"],
            attention_mask=inputs["attention_mask_chosen"],
        )[0]
        rewards_rejected = model(
            input_ids=inputs["input_ids_rejected"],
            attention_mask=inputs["attention_mask_rejected"],
        )[0]
        # calculate loss, optionally modulate with margin
        if "margin" in inputs:
            loss = -nn.functional.logsigmoid(rewards_chosen - rewards_rejected - inputs["margin"]).mean()
        else:
            loss = -nn.functional.logsigmoid(rewards_chosen - rewards_rejected).mean()

        if return_outputs:
            return loss, {
                "rewards_chosen": rewards_chosen,
                "rewards_rejected": rewards_rejected,
            }
        return loss

    def prediction_step(
        self,
        model: Union[PreTrainedModel, nn.Module],
        inputs: Dict[str, Union[torch.Tensor, Any]],
        prediction_loss_only: bool,
        ignore_keys: Optional[List[str]] = None,
    ) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:
        inputs = self._prepare_inputs(inputs)
        if ignore_keys is None:
            if hasattr(self.model, "config"):
                ignore_keys = getattr(self.model.config, "keys_to_ignore_at_inference", [])
            else:
                ignore_keys = []

        with torch.no_grad():
            loss, logits_dict = self.compute_loss(model, inputs, return_outputs=True)

        if prediction_loss_only:
            return (loss, None, None)

        loss = loss.detach()
        logits = tuple(v for k, v in logits_dict.items() if k not in ignore_keys)
        logits = nested_detach(logits)
        # Stack accepted against rejected, mean over logits
        # and softmax to get preferences between accepted and rejected to sum to 1
        logits = torch.stack(logits).mean(dim=2).softmax(dim=0).T

        labels = torch.zeros(logits.shape[0])
        labels = self._prepare_inputs(labels)

        return loss, logits, labels

File Path: trl/trainer/sft_trainer.py
Content:
# Copyright 2023 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import dataclasses
import warnings
from typing import Callable, Dict, List, Optional, Tuple, Union

import torch
import torch.nn as nn
from datasets import Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    DataCollator,
    DataCollatorForLanguageModeling,
    PreTrainedModel,
    PreTrainedTokenizerBase,
    Trainer,
    TrainingArguments,
)
from transformers.trainer_callback import TrainerCallback
from transformers.trainer_utils import EvalPrediction

from ..import_utils import is_peft_available
from .utils import ConstantLengthDataset, DataCollatorForCompletionOnlyLM, PeftSavingCallback


if is_peft_available():
    from peft import PeftConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training


class SFTTrainer(Trainer):
    r"""
    Class definition of the Supervised Finetuning Trainer (SFT Trainer).
    This class is a wrapper around the `transformers.Trainer` class and inherits all of its attributes and methods.
    The trainer takes care of properly initializing the PeftModel in case a user passes a `PeftConfig` object.

    Args:
        model (Union[`transformers.PreTrainedModel`, `nn.Module`, `str`]):
            The model to train, can be a `PreTrainedModel`, a `torch.nn.Module` or a string with the model name to
            load from cache or download. The model can be also converted to a `PeftModel` if a `PeftConfig` object is
            passed to the `peft_config` argument.
        args (Optional[`transformers.TrainingArguments`]):
            The arguments to tweak for training. Please refer to the official documentation of `transformers.TrainingArguments`
            for more information.
        data_collator (Optional[`transformers.DataCollator`]):
            The data collator to use for training.
        train_dataset (Optional[`datasets.Dataset`]):
            The dataset to use for training. We recommend users to use `trl.trainer.ConstantLengthDataset` to create their dataset.
        eval_dataset (Optional[Union[`datasets.Dataset`, Dict[`str`, `datasets.Dataset`]]]):
            The dataset to use for evaluation. We recommend users to use `trl.trainer.ConstantLengthDataset` to create their dataset.
        tokenizer (Optional[`transformers.PreTrainedTokenizer`]):
            The tokenizer to use for training. If not specified, the tokenizer associated to the model will be used.
        model_init (`Callable[[], transformers.PreTrainedModel]`):
                The model initializer to use for training. If None is specified, the default model initializer will be used.
        compute_metrics (`Callable[[transformers.EvalPrediction], Dict]`, *optional* defaults to `compute_accuracy`):
            The metrics to use for evaluation. If no metrics are specified, the default metric (`compute_accuracy`) will be used.
        callbacks (`List[transformers.TrainerCallback]`):
            The callbacks to use for training.
        optimizers (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`):
            The optimizer and scheduler to use for training.
        preprocess_logits_for_metrics (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`):
            The function to use to preprocess the logits before computing the metrics.
        peft_config (`Optional[PeftConfig]`):
            The PeftConfig object to use to initialize the PeftModel.
        dataset_text_field (`Optional[str]`):
            The name of the text field of the dataset, in case this is passed by a user, the trainer will automatically create a
            `ConstantLengthDataset` based on the `dataset_text_field` argument.
        formatting_func (`Optional[Callable]`):
            The formatting function to be used for creating the `ConstantLengthDataset`.
        max_seq_length (`Optional[int]`):
            The maximum sequence length to use for the `ConstantLengthDataset` and for automaticallty creating the Dataset. Defaults to `512`.
        infinite (`Optional[bool]`):
            Whether to use an infinite dataset or not. Defaults to `False`.
        num_of_sequences (`Optional[int]`):
            The number of sequences to use for the `ConstantLengthDataset`. Defaults to `1024`.
        chars_per_token (`Optional[float]`):
            The number of characters per token to use for the `ConstantLengthDataset`. Defaults to `3.6`. You can check how this is computed in the
            stack-llama example: https://github.com/huggingface/trl/blob/08f550674c553c36c51d1027613c29f14f3676a5/examples/stack_llama/scripts/supervised_finetuning.py#L53.
        packing (`Optional[bool]`):
            Used only in case `dataset_text_field` is passed. This argument is used by the `ConstantLengthDataset` to pack the sequences
            of the dataset.
        dataset_num_proc (`Optional[int]`):
            The number of workers to use to tokenize the data. Only used when `packing=False`. Defaults to None.
        dataset_batch_size (`int`):
            The number of examples to tokenize per batch. If batch_size <= 0 or batch_size == None,
            tokenize the full dataset as a single batch. Defaults to 1000.
    """

    def __init__(
        self,
        model: Union[PreTrainedModel, nn.Module, str] = None,
        args: TrainingArguments = None,
        data_collator: Optional[DataCollator] = None,
        train_dataset: Optional[Dataset] = None,
        eval_dataset: Optional[Union[Dataset, Dict[str, Dataset]]] = None,
        tokenizer: Optional[PreTrainedTokenizerBase] = None,
        model_init: Optional[Callable[[], PreTrainedModel]] = None,
        compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None,
        callbacks: Optional[List[TrainerCallback]] = None,
        optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None),
        preprocess_logits_for_metrics: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None,
        peft_config: Optional[Dict] = None,
        dataset_text_field: Optional[str] = None,
        packing: Optional[bool] = False,
        formatting_func: Optional[Callable] = None,
        max_seq_length: Optional[int] = None,
        infinite: Optional[bool] = False,
        num_of_sequences: Optional[int] = 1024,
        chars_per_token: Optional[float] = 3.6,
        dataset_num_proc: Optional[int] = None,
        dataset_batch_size: int = 1000,
    ):
        if isinstance(model, str):
            warnings.warn(
                "You passed a model_id to the SFTTrainer. This will automatically create an "
                "`AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you."
            )

        if packing and data_collator is not None and isinstance(data_collator, DataCollatorForCompletionOnlyLM):
            raise ValueError(
                "You passed a `DataCollatorForCompletionOnlyLM` to the SFTTrainer. This is not compatible with the `packing` argument."
            )

        supported_classes = (PreTrainedModel,) if not is_peft_available() else (PreTrainedModel, PeftModel)

        if is_peft_available() and peft_config is not None:
            if not isinstance(peft_config, PeftConfig):
                raise ValueError(
                    "If you want to use the PeftModel, you need to pass a PeftConfig object to the SFTTrainer."
                    f" and you passed a {type(peft_config)}."
                )

            if not isinstance(model, PeftModel):
                if not isinstance(model, PreTrainedModel):
                    model = AutoModelForCausalLM.from_pretrained(
                        model,
                    )

                if getattr(model, "is_loaded_in_8bit", False) or getattr(model, "is_loaded_in_4bit", False):
                    model = prepare_model_for_kbit_training(
                        model, use_gradient_checkpointing=args.gradient_checkpointing
                    )

                    args = dataclasses.replace(args, gradient_checkpointing=False)

                model = get_peft_model(model, peft_config)

            if callbacks is None:
                callbacks = [PeftSavingCallback]
        elif not isinstance(model, supported_classes):
            model = AutoModelForCausalLM.from_pretrained(model)

        if tokenizer is None:
            tokenizer = AutoTokenizer.from_pretrained(model.config._name_or_path)
            if getattr(tokenizer, "pad_token", None) is None:
                tokenizer.pad_token = tokenizer.eos_token

        if max_seq_length is None:
            # to overcome some issues with broken tokenizers
            max_seq_length = min(tokenizer.model_max_length, 1024)

            warnings.warn(
                f"You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to {max_seq_length}"
            )

        self.dataset_num_proc = dataset_num_proc
        self.dataset_batch_size = dataset_batch_size
        if not packing:
            if dataset_text_field is None and formatting_func is None:
                raise ValueError(
                    "You passed `packing=False` to the SFTTrainer, but you didn't pass a `dataset_text_field` or `formatting_func` argument."
                )

            if data_collator is None:
                data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

        if train_dataset is not None:
            train_dataset = self._prepare_dataset(
                train_dataset,
                tokenizer,
                packing,
                dataset_text_field,
                max_seq_length,
                formatting_func,
                infinite,
                num_of_sequences,
                chars_per_token,
            )
        if eval_dataset is not None:
            eval_dataset = self._prepare_dataset(
                eval_dataset,
                tokenizer,
                packing,
                dataset_text_field,
                max_seq_length,
                formatting_func,
                infinite,
                num_of_sequences,
                chars_per_token,
            )

        if tokenizer.padding_side is not None and tokenizer.padding_side != "right":
            warnings.warn(
                "You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to "
                "overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code."
            )

        super().__init__(
            model=model,
            args=args,
            data_collator=data_collator,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizer=tokenizer,
            model_init=model_init,
            compute_metrics=compute_metrics,
            callbacks=callbacks,
            optimizers=optimizers,
            preprocess_logits_for_metrics=preprocess_logits_for_metrics,
        )

        if self.args.max_steps > 0 and packing:
            warnings.warn(
                "You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached."
            )
            self.train_dataset.infinite = True
        elif self.args.max_steps == -1 and packing:
            self.train_dataset.infinite = False

    def _prepare_dataset(
        self,
        dataset,
        tokenizer,
        packing,
        dataset_text_field,
        max_seq_length,
        formatting_func,
        infinite,
        num_of_sequences,
        chars_per_token,
    ):
        if dataset is None:
            raise ValueError("The dataset should not be None")

        # check if torch dataset / dataloader and do nothing
        if isinstance(dataset, (torch.utils.data.IterableDataset, torch.utils.data.Dataset, ConstantLengthDataset)):
            return dataset

        if not packing:
            return self._prepare_non_packed_dataloader(
                tokenizer, dataset, dataset_text_field, max_seq_length, formatting_func
            )

        if dataset_text_field is not None or formatting_func is not None:
            if tokenizer is None:
                raise ValueError(
                    "You need to pass a tokenizer when using the SFT Trainer when passing a `dataset_text_field`."
                )

            return ConstantLengthDataset(
                tokenizer,
                dataset,
                dataset_text_field=dataset_text_field,
                formatting_func=formatting_func,
                seq_length=max_seq_length,
                infinite=infinite,
                num_of_sequences=num_of_sequences,
                chars_per_token=chars_per_token,
                eos_token_id=tokenizer.eos_token_id,
            )

        raise ValueError(
            "You need to pass a `dataset_text_field` or `formatting_func` argument to the SFTTrainer if you want to use the `ConstantLengthDataset`."
        )

    def _prepare_non_packed_dataloader(
        self, tokenizer, dataset, dataset_text_field, max_seq_len, formatting_func=None
    ):
        use_formatting_func = formatting_func is not None and dataset_text_field is None
        self._dataset_sanity_checked = False

        # Inspired from: https://huggingface.co/learn/nlp-course/chapter7/6?fw=pt
        def tokenize(element):
            outputs = tokenizer(
                element[dataset_text_field] if not use_formatting_func else formatting_func(element),
                truncation=True,
                padding=False,
                max_length=max_seq_len,
                return_overflowing_tokens=False,
                return_length=False,
            )

            if use_formatting_func and not self._dataset_sanity_checked:
                if not isinstance(formatting_func(element), list):
                    raise ValueError(
                        "The `formatting_func` should return a list of processed strings since it can lead to silent bugs."
                    )
                else:
                    self._dataset_sanity_checked = True

            return {"input_ids": outputs["input_ids"], "attention_mask": outputs["attention_mask"]}

        tokenized_dataset = dataset.map(
            tokenize,
            batched=True,
            remove_columns=dataset.column_names,
            num_proc=self.dataset_num_proc,
            batch_size=self.dataset_batch_size,
        )

        return tokenized_dataset

File Path: trl/trainer/sr_ppo_config.py
Content:
# Copyright 2022 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import os
import sys
import warnings
from dataclasses import dataclass, field
from typing import Literal, Optional

import numpy as np
import tyro
from trl.trainer.utils import exact_div

from ..core import flatten_dict


@dataclass
class SR_PPOConfig:
    """
    Configuration class for PPOTrainer
    """

    exp_name: str = os.path.basename(sys.argv[0])[: -len(".py")]
    """the name of this experiment (by default is the file name without the extension name)"""
    task_name: Optional[str] = None
    """Name of task to use - used only for tracking purposes"""
    model_name: Optional[str] = None
    """Name of model to use - used only for tracking purposes"""
    query_dataset: Optional[str] = None
    """Name of dataset to query - used only for tracking purposes"""
    reward_model: Optional[str] = None
    """The reward model to use - used only for tracking purposes"""
    steps: int = 20000
    """Number of training steps"""
    learning_rate: float = 1e-5
    """Adam learning rate"""
    adap_kl_ctrl: bool = True
    """Use adaptive KL control, otherwise linear"""
    init_kl_coef: Optional[float] = 0.2
    """Initial KL penalty coefficient (used for adaptive and linear control)"""
    kl_penalty: Literal["kl", "abs", "mse", "full"] = "kl"
    """kl penalty options: 'kl': model_logp - ref_logp,  'abs': abs(kl),  'mse': mean squared error mse(kl) and 'full': the actual kl for all tokens in the distribution"""
    target: Optional[float] = 6
    """Target KL value for adaptive KL control"""
    horizon: Optional[float] = 10000
    """Horizon for adaptive KL control"""
    gamma: float = 1
    """Gamma parameter for advantage calculation"""
    lam: float = 0.95
    """Lambda parameter for advantage calculation"""
    cliprange: float = 0.2
    """Range for clipping in PPO policy gradient loss"""
    cliprange_value: float = 0.2
    """Range for clipping values in loss calculation"""
    vf_coef: float = 0.1
    """Scaling factor for value loss"""
    batch_size: int = 256
    """Number of samples per optimisation step"""
    forward_batch_size: Optional[int] = None
    """DEPRECATED: use `mini_batch_size` instead, which does the same thing."""
    mini_batch_size: int = 1
    """Number of samples optimized in each mini batch"""
    gradient_accumulation_steps: int = 1
    """The number of gradient accumulation steps"""
    world_size: tyro.conf.Suppress[int] = None
    """The world size for distributed training"""
    backward_batch_size: tyro.conf.Suppress[int] = None
    """TO BE FILLED In RUNTIME: Number of samples optimized in an `optimizer.step()` call"""
    global_backward_batch_size: tyro.conf.Suppress[int] = None
    """TO BE FILLED In RUNTIME: the effective `backward_batch_size` across all processes"""
    global_batch_size: tyro.conf.Suppress[int] = None
    """TO BE FILLED In RUNTIME: the effective `batch_size` across all processes"""
    ppo_epochs: int = 4
    """Number of optimisation epochs per batch of samples"""
    remove_unused_columns: bool = True
    """Remove unused columns from the dataset if `datasets.Dataset` is used"""
    log_with: Optional[Literal["wandb", "tensorboard"]] = None
    """Log with either 'wandb' or 'tensorboard', check  https://huggingface.co/docs/accelerate/usage_guides/tracking for more details"""
    tracker_kwargs: dict = field(default_factory=dict)
    """Keyword arguments for the tracker (e.g. wandb_project)"""
    accelerator_kwargs: dict = field(default_factory=dict)
    """Keyword arguments for the accelerator"""
    project_kwargs: dict = field(default_factory=dict)
    """Keyword arguments for the accelerator project config (e.g. `logging_dir`)"""
    tracker_project_name: str = "trl"
    """Name of project to use for tracking"""
    max_grad_norm: Optional[float] = None
    """Maximum gradient norm for gradient clipping"""
    seed: int = 0
    """Seed value for random generations"""
    optimize_cuda_cache: bool = False
    """Optimize CUDA cache for slightly more memory-efficient training"""
    early_stopping: bool = False
    """Whether to stop the PPO optimization loop early is the KL too high"""
    target_kl: float = 1
    """Stop early if we exceed this value by over 50%"""
    push_to_hub_if_best_kwargs: dict = field(default_factory=dict)
    """Keyword arguments for pushing model to the hub during training (e.g. repo_id)"""
    compare_steps: int = 1
    """Number of steps between comparison of the current reward with the best seen so far"""
    ratio_threshold: float = 10.0
    """Skip mini-batches with high PPO ratios that can cause loss spikes"""
    use_score_scaling: bool = False
    """Use score scaling"""
    use_score_norm: bool = False
    """Use score normalization. Only applicable if use_score_scaling is True"""
    score_clip: Optional[float] = None
    """Score clipping"""
    soft_risk_alpha: float = 0.05
    """Risk level alpha"""
    risk_level_scheduler: float = 0.8 
    """Risk level scheduler: 0.8 means the soft risk level reaches α after ρ = 80% of the training """

    # Model detection
    is_encoder_decoder: Optional[tyro.conf.Suppress[bool]] = None
    """TO BE FILLED In RUNTIME: Whether the model is an encoder-decoder model"""
    is_peft_model: Optional[tyro.conf.Suppress[bool]] = None
    """TO BE FILLED In RUNTIME: Whether the model is a PEFT model"""

    def __post_init__(self):
        if self.forward_batch_size is not None:
            warnings.warn(
                "Note that using `forward_batch_size` is deprecated, use `mini_batch_size` instead. By setting it you overwrite `mini_batch_size` which affects both the batch size during forward passes and also the mini batch size for PPO optimization."
            )
            self.mini_batch_size = self.forward_batch_size

        self.backward_batch_size = self.mini_batch_size * self.gradient_accumulation_steps
        exact_div(
            self.batch_size,
            self.backward_batch_size,
            "`batch_size`",
            "`mini_batch_size * gradient_accumulation_steps`",
            "`batch_size` must be a multiple of `mini_batch_size * gradient_accumulation_steps`",
        )

        # check if wandb is installed
        if self.log_with == "wandb":
            # raise error if wandb is not installed
            try:
                import wandb  # noqa: F401

            except ImportError:
                raise ImportError(
                    "Please install wandb to use wandb logging. You can do this by running `pip install wandb`."
                )

        self.total_ppo_epochs = int(np.ceil(self.steps / self.batch_size))
        assert self.kl_penalty in ["kl", "abs", "mse", "full"]

    def to_dict(self):
        output_dict = {}
        for key, value in self.__dict__.items():
            output_dict[key] = value
        return flatten_dict(output_dict)

File Path: trl/trainer/sr_ppo_trainer.py
Content:
# Copyright 2022 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import inspect
import math
import os
import time
import typing
import warnings
from typing import Callable, List, Optional, Union

import datasets
import numpy as np
import torch
import torch.nn.functional as F
from accelerate import Accelerator
from accelerate.utils import ProjectConfiguration, is_deepspeed_available
from datasets import Dataset
from huggingface_hub import whoami
from packaging import version
from torch.optim import Adam
from transformers import (
    DataCollatorForLanguageModeling,
    PreTrainedTokenizer,
    PreTrainedTokenizerBase,
    PreTrainedTokenizerFast,
)

from ..core import (
    WANDB_PADDING,
    PPODecorators,
    clip_by_value,
    convert_to_scalar,
    entropy_from_logits,
    flatten_dict,
    logprobs_from_logits,
    masked_mean,
    masked_var,
    masked_whiten,
    set_seed,
    stack_dicts,
    stats_to_np,
)
from ..import_utils import is_torch_greater_2_0
from ..models import SUPPORTED_ARCHITECTURES, PreTrainedModelWrapper, create_reference_model
from . import AdaptiveKLController, BaseTrainer, FixedKLController, PPOConfig, RunningMoments

import pdb

if is_deepspeed_available():
    import deepspeed

MODEL_CARD_TEMPLATE = """---
license: apache-2.0
tags:
- trl
- transformers
- reinforcement-learning
---

# {model_name}

This is a [TRL language model](https://github.com/huggingface/trl) that has been fine-tuned with reinforcement learning to
 guide the model outputs according to a value, function, or human feedback. The model can be used for text generation.

## Usage

To use this model for inference, first install the TRL library:

```bash
python -m pip install trl
```

You can then generate text as follows:

```python
from transformers import pipeline

generator = pipeline("text-generation", model="{model_id}")
outputs = generator("Hello, my llama is cute")
```

If you want to use the model for training or to obtain the outputs from the value head, load the model as follows:

```python
from transformers import AutoTokenizer
from trl import AutoModelForCausalLMWithValueHead

tokenizer = AutoTokenizer.from_pretrained("{model_id}")
model = AutoModelForCausalLMWithValueHead.from_pretrained("{model_id}")

inputs = tokenizer("Hello, my llama is cute", return_tensors="pt")
outputs = model(**inputs, labels=inputs["input_ids"])
```
"""


class PPOTrainer(BaseTrainer):
    """
    The PPOTrainer uses Proximal Policy Optimization to optimise language models.
    Note, this trainer is heavily inspired by the original OpenAI learning to summarize work here:
    https://github.com/openai/summarize-from-feedback

    Attributes:
        **config** (`PPOConfig`) -- Configuration object for PPOTrainer. Check the documentation of `PPOConfig` for more
            details.
        **model** (`PreTrainedModelWrapper`) -- Model to be optimized, Hugging Face transformer model with a value head.
            Check the documentation of `PreTrainedModelWrapper` for more details.
        **ref_model** (`PreTrainedModelWrapper`, *optional*) -- Reference model to be used for KL penalty, Hugging Face
            transformer model with a casual language modelling head. Check the documentation of `PreTrainedModelWrapper`
            for more details. If no reference model is provided, the trainer will create a reference model with the same
             architecture as the model to be optimized with shared layers.
        **tokenizer** (`PreTrainedTokenizerBase`) -- Tokenizer to be used for encoding the
            data. Check the documentation of `transformers.PreTrainedTokenizer` and
            `transformers.PreTrainedTokenizerFast` for more details.
        **dataset** (Union[`torch.utils.data.Dataset`, `datasets.Dataset`], *optional*) -- PyTorch dataset or Hugging
            Face dataset. This is used to create a PyTorch dataloader. If no dataset is provided, the dataloader must be
             created outside the trainer users needs to design their own dataloader and make sure the batch
            size that is used is the same as the one specified in the configuration object.
        **optimizer** (`torch.optim.Optimizer`, *optional*) -- Optimizer to be used for training. If no optimizer is
            provided, the trainer will create an Adam optimizer with the learning rate specified in the configuration
            object.
        **data_collator** (DataCollatorForLanguageModeling, *optional*) -- Data collator to be used for training and
            passed along the dataloader
        **num_shared_layers** (int, *optional*) -- Number of layers to be shared between the model and the reference
            model, if no reference model is passed. If no number is provided, all the layers will be shared.
        **lr_scheduler** (`torch.optim.lr_scheduler`, *optional*) -- Learning rate scheduler to be used for training.
    """

    def __init__(
        self,
        config: PPOConfig = None,
        model: PreTrainedModelWrapper = None,
        ref_model: Optional[PreTrainedModelWrapper] = None,
        tokenizer: PreTrainedTokenizerBase = None,
        dataset: Optional[Union[torch.utils.data.Dataset, Dataset]] = None,
        optimizer: Optional[torch.optim.Optimizer] = None,
        data_collator: Optional[typing.Callable] = None,
        num_shared_layers: Optional[int] = None,
        lr_scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,
    ):
        """
        Initialize PPOTrainer.

        Args:
            config (`PPOConfig`):
                Configuration object for PPOTrainer. Check the documentation of `PPOConfig` for more details.
            model (`PreTrainedModelWrapper`):
                Hugging Face transformer model with a value head.
            ref_model (`PreTrainedModelWrapper`):
                Hugging Face transformer model with a casual language modelling head. Used for KL penalty
            tokenizer (`transformers.PreTrainedTokenizerBase`):
                Hugging Face tokenizer
            dataset (Optional[Union[`torch.utils.data.Dataset`, `datasets.Dataset`]]):
                PyTorch dataset or Hugging Face dataset. If a Hugging Face dataset is passed, the dataset
                will be preprocessed by removing the columns that are not used by the model. If none is passed,
                a warning will be raised in a multi-GPU setting.
            optimizer (Optional[`torch.optim.Optimizer`]):
                Optimizer used for training. If `None`, the `Adam` is used as default.
            data_collator (Optional[function]):
                Data collator function.
            num_shared_layers (Optional[int]):
                Number of shared layers between the model and the reference model. If `None`, all layers are shared.
                used only if `ref_model` is `None`.
            lr_scheduler (Optional[`torch.optim.lr_scheduler`]):
                Learning rate scheduler used for training.
        """
        super().__init__(config)

        # initial seed for reproducible experiments
        set_seed(config.seed)

        # Step 0: check positional arguments validity
        if not isinstance(config, PPOConfig):
            raise ValueError(f"config must be a PPOConfig, got {type(config)}")
        if not isinstance(tokenizer, (PreTrainedTokenizerBase)):
            raise ValueError(
                f"tokenizer must be a PreTrainedTokenizerBase like a PreTrainedTokenizer or a PreTrainedTokenizerFast, got {type(tokenizer)}"
            )
        if not isinstance(model, (SUPPORTED_ARCHITECTURES)):
            raise ValueError(
                f"model must be a PreTrainedModelWrapper, got {type(model)} - supported architectures are: {SUPPORTED_ARCHITECTURES}"
            )
        # Step 1: Initialize Accelerator
        self.accelerator = Accelerator(
            log_with=config.log_with,
            gradient_accumulation_steps=config.gradient_accumulation_steps,
            project_config=ProjectConfiguration(**config.project_kwargs),
            **config.accelerator_kwargs,
        )

        # Step 1.1 Runtime variables filled by the accelerator
        config.world_size = self.accelerator.num_processes
        config.global_backward_batch_size = config.backward_batch_size * config.world_size
        config.global_batch_size = config.batch_size * config.world_size

        self.model = model
        self.model_params = filter(lambda p: p.requires_grad, self.model.parameters())
        self.is_encoder_decoder = hasattr(self.model, "is_encoder_decoder")
        self.is_peft_model = getattr(self.model, "is_peft_model", False)
        config.is_encoder_decoder = self.is_encoder_decoder
        config.is_peft_model = self.is_peft_model

        is_using_tensorboard = config.log_with is not None and config.log_with == "tensorboard"
        self.accelerator.init_trackers(
            config.tracker_project_name,
            config=dict(trl_ppo_trainer_config=config.to_dict()) if not is_using_tensorboard else config.to_dict(),
            init_kwargs=config.tracker_kwargs,
        )
        self.is_using_text_environment = getattr(config, "use_text_environment", False)

        if isinstance(ref_model, SUPPORTED_ARCHITECTURES):
            self.ref_model = ref_model
            if num_shared_layers is not None:
                warnings.warn(
                    "num_shared_layers is ignored when ref_model is provided. Two different models are used for the "
                    "model and the reference model and no layers are shared.",
                    UserWarning,
                )
        elif ref_model is None and not self.is_peft_model:
            self.ref_model = create_reference_model(self.model, num_shared_layers=num_shared_layers)
        elif self.is_peft_model:
            self.ref_model = None
        else:
            raise ValueError(
                f"ref_model must be a PreTrainedModelWrapper or `None`, got {type(ref_model)} - supported "
                f"architectures are: {SUPPORTED_ARCHITECTURES} "
            )

        if not (isinstance(tokenizer, PreTrainedTokenizer) or isinstance(tokenizer, PreTrainedTokenizerFast)):
            raise ValueError(
                "tokenizer must be a transformers.PreTrainedTokenizer or transformers.PreTrainedTokenizerFast"
            )
        self.tokenizer = tokenizer

        if dataset is not None and not (isinstance(dataset, torch.utils.data.Dataset) or isinstance(dataset, Dataset)):
            raise ValueError("dataset must be a torch.utils.data.Dataset or datasets.Dataset")
        elif dataset is None:
            warnings.warn(
                "No dataset is provided. Make sure to set config.batch_size to the correct value before training.",
                UserWarning,
            )
        self.dataset = dataset
        self._signature_columns = None
        if self.dataset is not None:
            self.dataloader = self.prepare_dataloader(self.dataset, data_collator)
        elif self.dataset is None and self.accelerator.num_processes > 1:
            warnings.warn(
                "No dataset is provided. In a multi-GPU setting, this will lead to an error. You should"
                " prepare your dataloader yourself with `dataloader = ppo_trainer.accelerator.prepare(dataloader)`"
                " and using `torch.utils.data.DataLoader`, or pass a dataset to the `PPOTrainer`. Please "
                " refer to the documentation for more details.",
                UserWarning,
            )
            self.dataloader = None
        else:
            self.dataloader = None

        # Step 3: Initialize optimizer and data collator
        self.data_collator = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)
        if optimizer is None:
            self.optimizer = Adam(
                filter(lambda p: p.requires_grad, self.model.parameters()),
                lr=self.config.learning_rate,
            )
        else:
            self.optimizer = optimizer

        self.lr_scheduler = lr_scheduler
        if self.lr_scheduler is not None:
            lr_scheduler_class = (
                torch.optim.lr_scheduler._LRScheduler
                if not is_torch_greater_2_0()
                else torch.optim.lr_scheduler.LRScheduler
            )

            if not isinstance(self.lr_scheduler, lr_scheduler_class):
                raise ValueError(
                    "lr_scheduler must be a torch.optim.lr_scheduler._LRScheduler or torch.optim.lr_scheduler.LRScheduler (for torch >= 2.0)"
                )

        if self.config.adap_kl_ctrl:
            self.kl_ctl = AdaptiveKLController(self.config.init_kl_coef, self.config.target, self.config.horizon)
        else:
            self.kl_ctl = FixedKLController(self.config.init_kl_coef)

        # Safety checkers for DS integration
        is_deepspeed_used = self.accelerator.distributed_type == "DEEPSPEED" and hasattr(
            self.accelerator.state, "deepspeed_plugin"
        )

        (
            self.model,
            self.optimizer,
            self.data_collator,
            self.dataloader,
            self.lr_scheduler,
        ) = self.accelerator.prepare(
            self.model,
            self.optimizer,
            self.data_collator,
            self.dataloader,
            self.lr_scheduler,
        )
        if is_deepspeed_used:
            # Quantized models are already set on the correct device
            if not self.is_peft_model and not (
                getattr(self.ref_model.pretrained_model, "is_loaded_in_8bit", False)
                or getattr(self.ref_model.pretrained_model, "is_loaded_in_4bit", False)
            ):
                self.ref_model = self._prepare_deepspeed(self.ref_model)
        else:
            self.ref_model = self.accelerator.prepare(self.ref_model)

        # In a distributed setup, only logging needs to be performed on the main process
        # check: https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html
        # or: https://discuss.pytorch.org/t/use-distributed-data-parallel-correctly/82500/11
        self.is_distributed = self.accelerator.distributed_type == "MULTI_GPU"

        # init the current step
        self.current_step = 0

        # init variables for pushing model to hub
        if config.push_to_hub_if_best_kwargs:
            if "repo_id" not in config.push_to_hub_if_best_kwargs:
                raise ValueError("You have to specify repo_id in order to push the model to the hub!")
            self.push_to_hub_kwargs = config.push_to_hub_if_best_kwargs
            self.compare_step = 0
            self.highest_reward = torch.tensor(-float("inf"))

        # post process for PP
        if not getattr(self.model, "is_sequential_parallel", False):
            self.current_device = self.accelerator.device
        else:
            self.current_device = torch.device("cuda:0")

        PPODecorators.optimize_cuda_cache = self.config.optimize_cuda_cache

        self.running = RunningMoments(self.accelerator)

    def _filter_kwargs(self, kwargs, target_func):
        """
        filter the keyword arguments that are supported by the target function.

        Args:
            kwargs (dict):
                Keyword arguments
            target_func (function):
                Target function
        """
        return {k: v for k, v in kwargs.items() if k in inspect.signature(target_func).parameters.keys()}

    def prepare_dataloader(self, dataset: Union[torch.utils.data.Dataset, Dataset], data_collator=None):
        """
        Prepare the dataloader for training.

        Args:
            dataset (Union[`torch.utils.data.Dataset`, `datasets.Dataset`]):
                PyTorch dataset or Hugging Face dataset. If a Hugging Face dataset is passed, the dataset
                will be preprocessed by removing the columns that are not used by the model.
            data_collator (Optional[function]):
                Data collator function.

        Returns:
            `torch.utils.data.DataLoader`: PyTorch dataloader
        """
        if isinstance(dataset, Dataset):
            dataset = self._remove_unused_columns(dataset)
        dataloader = torch.utils.data.DataLoader(
            dataset,
            batch_size=self.config.batch_size,
            collate_fn=data_collator,
            shuffle=True,
            drop_last=True,
        )
        return dataloader

    # Adapted from transformers.Trainer._set_signature_columns_if_needed
    def _set_signature_columns_if_needed(self):
        if self._signature_columns is None:
            # Inspect model forward signature to keep only the arguments it accepts.
            signature = inspect.signature(self.model.forward)
            self._signature_columns = list(signature.parameters.keys())
            # label => sentiment | we need query and response for logging purpose
            self._signature_columns += ["label", "query", "response"]

    # Adapted from transformers.Trainer._remove_unused_columns
    def _remove_unused_columns(self, dataset: "Dataset"):
        if not self.config.remove_unused_columns:
            return dataset
        self._set_signature_columns_if_needed()
        signature_columns = self._signature_columns

        ignored_columns = list(set(dataset.column_names) - set(signature_columns))

        columns = [k for k in signature_columns if k in dataset.column_names]

        if version.parse(datasets.__version__) < version.parse("1.4.0"):
            dataset.set_format(
                type=dataset.format["type"],
                columns=columns,
                format_kwargs=dataset.format["format_kwargs"],
            )
            return dataset
        else:
            return dataset.remove_columns(ignored_columns)

    def generate(
        self,
        query_tensor: Union[torch.Tensor, List[torch.Tensor]],
        length_sampler: Callable = None,
        batch_size: int = 4,
        return_prompt: bool = True,
        **generation_kwargs,
    ):
        """
        Generate response with the model given the query tensor.
        call the `generate` method of the model.

        Args:
            query_tensor (`torch.LongTensor`):
                A tensor of shape (`batch_size`, `seq_len`) containing query tokens.
            generation_kwargs (dict[str, Any]):
                Keyword arguments for generation.
            length_sampler (`Callable`, *optional*):
                Callable that returns the number of newly generated tokens.
            batch_size (`int`, *optional):
                Batch size used for generation, defaults to `4`.
            return_prompt (`bool`, *optional*):
                If set to `False` the prompt is not returned but only the newly generated tokens, defaults to `True`.

        Returns:
            `torch.LongTensor`: A tensor of shape (`batch_size`, `gen_len`) containing response tokens.
        """

        if isinstance(query_tensor, List):
            return self._generate_batched(
                query_tensor,
                length_sampler=length_sampler,
                batch_size=batch_size,
                return_prompt=return_prompt,
                **generation_kwargs,
            )

        else:
            if length_sampler is not None:
                generation_kwargs["max_new_tokens"] = length_sampler()
            response = self.accelerator.unwrap_model(self.model).generate(
                input_ids=query_tensor.unsqueeze(dim=0), **generation_kwargs
            )

            if not return_prompt and not self.is_encoder_decoder:
                return response[:, query_tensor.shape[0] :]
            return response

    def _generate_batched(
        self,
        query_tensors: List[torch.Tensor],
        length_sampler: Callable = None,
        batch_size: int = 4,
        return_prompt: bool = True,
        pad_to_multiple_of: int = None,
        remove_padding: bool = True,
        **generation_kwargs,
    ):
        outputs = []

        padding_side_default = self.tokenizer.padding_side
        if not self.is_encoder_decoder:
            self.tokenizer.padding_side = "left"

        # in case we have fewer examples than bs
        batch_size = min(len(query_tensors), batch_size)

        for i in range(0, len(query_tensors), batch_size):
            if length_sampler is not None:
                generation_kwargs["max_new_tokens"] = length_sampler()

            # prevent overflow if query tensors are not even multiple of bs
            end_index = min(len(query_tensors), i + batch_size)

            batch = query_tensors[i:end_index]
            batch_mask = [torch.ones_like(element) for element in batch]
            inputs = {"input_ids": batch, "attention_mask": batch_mask}

            padded_inputs = self.tokenizer.pad(
                inputs,
                padding=True,
                max_length=None,
                pad_to_multiple_of=pad_to_multiple_of,
                return_tensors="pt",
            ).to(self.current_device)

            generations = self.accelerator.unwrap_model(self.model).generate(**padded_inputs, **generation_kwargs)

            for generation, mask in zip(generations, padded_inputs["attention_mask"]):
                if not self.is_encoder_decoder:
                    output = generation[(1 - mask).sum() :]  # remove padding
                else:
                    output = generation

                if not return_prompt and not self.is_encoder_decoder:
                    output = output[(mask).sum() :]  # remove prompt

                if remove_padding and self.tokenizer.eos_token_id in output:
                    pad_mask = output == self.tokenizer.eos_token_id
                    pad_start = torch.nonzero(pad_mask, as_tuple=False)[0, 0].item()
                    output = output[: pad_start + 1]  # keep the eos token at the end

                outputs.append(output)

        self.tokenizer.padding_side = padding_side_default
        return outputs

    def _step_safety_checker(
        self,
        batch_size: int,
        queries: List[torch.LongTensor],
        responses: List[torch.LongTensor],
        scores: List[torch.FloatTensor],
        masks: Optional[List[torch.LongTensor]] = None,
    ):
        """
        Check if the input data is valid for training.

        Args:
            batch_size (int):
                Batch size from the config file.
            queries (List[`torch.LongTensor`]):
                List of tensors containing the encoded queries of shape (`query_length`)
            responses (List[`torch.LongTensor`]):
                List of tensors containing the encoded responses of shape (`response_length`)
            scores (List[`torch.FloatTensor`]):
                List of tensors containing the scores.
            masks (List[`torch.LongTensor`], *optional*):
                list of optional tensors containing the masks of shape (`query_length` + `response_length`)
        Returns:
            `tuple`: The input processed data.
        """
        for name, tensor_list in zip(["queries", "responses", "scores"], [queries, responses, scores]):
            if not isinstance(tensor_list, list):
                raise ValueError(f"{name} must be a list of tensors - got {type(tensor_list)}")
            if not isinstance(tensor_list[0], torch.Tensor):
                raise ValueError(f"Elements in {name} must be tensors - got {type(tensor_list[0])}")
            if batch_size is not None and len(tensor_list) != batch_size:
                raise ValueError(
                    f"Batch size ({batch_size}) does not match number of examples - but got {len(tensor_list)} for: {name}"
                )

        # add queries, scores and responses on the correct device
        queries = [tensor.to(self.current_device) for tensor in queries]
        responses = [tensor.to(self.current_device) for tensor in responses]
        scores = [tensor.to(self.current_device) for tensor in scores]
        masks = [tensor.to(self.current_device) for tensor in masks] if masks is not None else None

        # squeeze scores if needed
        for i, score in enumerate(scores):
            if score.dim() > 1:
                raise ValueError(f"Scores must be 1-dimensional - got {score.dim()} for {score}")
            elif score.dim() == 1:
                scores[i] = score.squeeze()

        return queries, responses, scores, masks

    @PPODecorators.empty_cuda_cache()
    def step(
        self,
        queries: List[torch.LongTensor],
        responses: List[torch.LongTensor],
        scores: List[torch.FloatTensor],
        response_masks: Optional[List[torch.LongTensor]] = None,
    ):
        """
        Run a PPO optimisation step given a list of queries, model responses, and rewards.

        Args:
            queries (List[`torch.LongTensor`]):
                List of tensors containing the encoded queries of shape (`query_length`)
            responses (List[`torch.LongTensor`]):
                List of tensors containing the encoded responses of shape (`response_length`)
            scores (List[`torch.FloatTensor`]):
                List of tensors containing the scores.
            response_masks (List[`torch.FloatTensor`], *optional*)):
                List of tensors containing masks of the response tokens.

        Returns:
            `dict[str, Any]`: A summary of the training statistics
        """

        #pdb.set_trace()
        bs = self.config.batch_size

        queries, responses, scores, response_masks = self._step_safety_checker(
            bs, queries, responses, scores, response_masks
        )
        scores = torch.tensor(scores, device=self.current_device)
        if self.config.use_score_scaling:
            # Score scaling
            scores_mean, scores_std = self.running.update(scores)
            tensor_to_kwargs = dict(dtype=scores.dtype, device=scores.device)
            score_scaling_factor = self.running.std.to(**tensor_to_kwargs) + torch.finfo(scores.dtype).eps
            if self.config.use_score_norm:
                scores = (scores - self.running.mean.to(**tensor_to_kwargs)) / score_scaling_factor
            else:
                scores /= score_scaling_factor

        if self.config.score_clip is not None:
            # Score clipping
            scores_dtype = scores.dtype
            scores = torch.clip(scores.float(), -self.config.score_clip, self.config.score_clip).to(dtype=scores_dtype)

        # if we want to push best model to the hub
        if hasattr(self, "highest_reward"):
            if self.compare_step % self.config.compare_steps == 0:
                curr_mean_reward = scores.mean()
                # if the best reward ever seen
                if curr_mean_reward > self.highest_reward:
                    self.highest_reward = curr_mean_reward
                    # push model to hub
                    self.push_to_hub(**self.push_to_hub_kwargs)
            self.compare_step += 1

        timing = dict()
        t0 = time.time()

        t = time.time()

        #pdb.set_trace()

        model_inputs = self.prepare_model_inputs(queries, responses)

        #pdb.set_trace()

        if self.is_distributed:
            pad_first = self.tokenizer.padding_side == "left"

            model_inputs["input_ids"] = self.accelerator.pad_across_processes(
                model_inputs["input_ids"],
                dim=1,
                pad_index=self.tokenizer.pad_token_id,
                pad_first=pad_first,
            )
            model_inputs["attention_mask"] = self.accelerator.pad_across_processes(
                model_inputs["attention_mask"], dim=1, pad_index=0, pad_first=pad_first
            )
            if self.is_encoder_decoder:
                model_inputs["decoder_input_ids"] = self.accelerator.pad_across_processes(
                    model_inputs["decoder_input_ids"],
                    dim=1,
                    pad_index=self.tokenizer.pad_token_id,
                    pad_first=pad_first,
                )
                model_inputs["decoder_attention_mask"] = self.accelerator.pad_across_processes(
                    model_inputs["decoder_attention_mask"],
                    dim=1,
                    pad_index=0,
                    pad_first=pad_first,
                )

        model_inputs_names = list(model_inputs.keys())

        full_kl_penalty = self.config.kl_penalty == "full"

        with torch.no_grad():
            all_logprobs, logits_or_none, values, masks = self.batched_forward_pass(
                self.model,
                queries,
                responses,
                model_inputs,
                response_masks=response_masks,
                return_logits=full_kl_penalty,
            )
            # for when the model is a peft model
            if self.is_peft_model and hasattr(
                self.accelerator.unwrap_model(self.model).pretrained_model,
                "disable_adapter",
            ):
                with self.accelerator.unwrap_model(self.model).pretrained_model.disable_adapter():
                    ref_logprobs, ref_logits_or_none, _, _ = self.batched_forward_pass(
                        self.model, queries, responses, model_inputs, return_logits=full_kl_penalty
                    )
            elif self.is_peft_model and not hasattr(self.model.pretrained_model, "disable_adapter"):
                raise ValueError(
                    "You are using a `peft` version that does not support `disable_adapter`. Please update your `peft` version to the latest version."
                )

            else:
                ref_logprobs, ref_logits_or_none, _, _ = self.batched_forward_pass(
                    self.ref_model, queries, responses, model_inputs, return_logits=full_kl_penalty
                )

        timing["time/ppo/forward_pass"] = time.time() - t

        with torch.no_grad():
            t = time.time()
            if full_kl_penalty:
                active_full_logprobs = logprobs_from_logits(logits_or_none, None, gather=False)
                ref_full_logprobs = logprobs_from_logits(ref_logits_or_none, None, gather=False)

                rewards, non_score_reward = self.compute_rewards(
                    scores, active_full_logprobs, ref_full_logprobs, masks
                )
            else:
                rewards, non_score_reward = self.compute_rewards(scores, all_logprobs, ref_logprobs, masks)
            timing["time/ppo/compute_rewards"] = time.time() - t

            t = time.time()
            values, advantages, returns = self.compute_advantages(values, rewards, masks)
            timing["time/ppo/compute_advantages"] = time.time() - t

        # upcast to float32 to avoid dataset issues
        batch_dict = {
            "queries": queries,
            "responses": responses,
            "logprobs": all_logprobs.to(torch.float32),
            "values": values.to(torch.float32),
            "masks": masks,
            "advantages": advantages,
            "returns": returns,
        }
        batch_dict.update(model_inputs)

        t = time.time()
        all_stats = []
        early_stop = False
        for _ in range(self.config.ppo_epochs):
            if early_stop:
                break
            b_inds = np.random.permutation(bs)
            for backward_batch_start in range(0, bs, self.config.backward_batch_size):
                backward_batch_end = backward_batch_start + self.config.backward_batch_size
                backward_batch_inds = b_inds[backward_batch_start:backward_batch_end]

                for mini_batch_start in range(0, self.config.backward_batch_size, self.config.mini_batch_size):
                    mini_batch_end = mini_batch_start + self.config.mini_batch_size
                    mini_batch_inds = backward_batch_inds[mini_batch_start:mini_batch_end]
                    mini_batch_dict = {
                        "logprobs": batch_dict["logprobs"][mini_batch_inds],
                        "values": batch_dict["values"][mini_batch_inds],
                        "masks": batch_dict["masks"][mini_batch_inds],
                        # hacks: the queries and responses are ragged.
                        "queries": [batch_dict["queries"][i] for i in mini_batch_inds],
                        "responses": [batch_dict["responses"][i] for i in mini_batch_inds],
                        "advantages": batch_dict["advantages"][mini_batch_inds],
                        "returns": batch_dict["returns"][mini_batch_inds],
                    }
                    for k in model_inputs_names:
                        mini_batch_dict[k] = batch_dict[k][mini_batch_inds]
                    with self.accelerator.accumulate(self.model):
                        model_inputs = {k: mini_batch_dict[k] for k in model_inputs_names}

                        logprobs, logits, vpreds, _ = self.batched_forward_pass(
                            self.model,
                            mini_batch_dict["queries"],
                            mini_batch_dict["responses"],
                            model_inputs,
                            return_logits=True,
                        )
                        train_stats = self.train_minibatch(
                            mini_batch_dict["logprobs"],
                            mini_batch_dict["values"],
                            logprobs,
                            logits,
                            vpreds,
                            mini_batch_dict["masks"],
                            mini_batch_dict["advantages"],
                            mini_batch_dict["returns"],
                        )
                        all_stats.append(train_stats)

            # typically, early stopping is done at the epoch level
            if self.config.early_stopping:
                policykl = train_stats["policy/policykl"]
                early_stop = self._early_stop(policykl)
                if early_stop:
                    break

        timing["time/ppo/optimize_step"] = time.time() - t

        t = time.time()
        train_stats = stack_dicts(all_stats)

        # reshape advantages/ratios such that they are not averaged.
        train_stats["policy/advantages"] = torch.flatten(train_stats["policy/advantages"]).unsqueeze(0)
        train_stats["policy/advantages"] = torch.nan_to_num(train_stats["policy/advantages"], WANDB_PADDING)
        train_stats["policy/ratio"] = torch.flatten(train_stats["policy/ratio"]).unsqueeze(0)

        stats = self.record_step_stats(
            scores=scores,
            logprobs=all_logprobs,
            ref_logprobs=ref_logprobs,
            non_score_reward=non_score_reward,
            train_stats=train_stats,
            kl_coef=self.kl_ctl.value,
            masks=masks,
            queries=queries,
            responses=responses,
        )
        # Gather/Reduce stats from all processes
        if self.is_distributed:
            stats = self.gather_stats(stats)
        stats = stats_to_np(stats)
        timing["time/ppo/calc_stats"] = time.time() - t
        stats["ppo/learning_rate"] = self.optimizer.param_groups[0]["lr"]

        # Update the KL control - multiply the batch_size by the number of processes
        self.kl_ctl.update(
            stats["objective/kl"],
            self.config.batch_size * self.accelerator.num_processes,
        )

        # Log the total ppo time
        timing["time/ppo/total"] = time.time() - t0
        stats.update(timing)

        # post-process stats for tensorboard and other loggers
        if self.config.log_with != "wandb":
            stats = convert_to_scalar(stats)

        if self.lr_scheduler is not None:
            self.lr_scheduler.step()

        return stats

    def _early_stop(self, policykl):
        r"""
        Handles the early stopping logic. If the policy KL is greater than the target KL, then the gradient is zeroed and
        the optimization step is skipped.
        This also handles the multi-gpu case where the policy KL is averaged across all processes.

        Args:
            policy_kl (torch.Tensor):
                the policy KL

        Returns:
            `bool`: whether to early stop or not
        """
        early_stop = False
        if not self.config.early_stopping:
            return early_stop

        if not self.is_distributed and policykl > 1.5 * self.config.target_kl:
            self.optimizer.zero_grad()
            early_stop = True
        elif self.is_distributed:
            import torch.distributed as dist

            # Wait for all processes to finish
            dist.barrier()

            # all gather the policykl
            dist.all_reduce(policykl, dist.ReduceOp.SUM)
            policykl /= self.accelerator.num_processes

            if policykl > 1.5 * self.config.target_kl:
                self.optimizer.zero_grad()
                early_stop = True
        return early_stop

    def gather_stats(self, stats):
        """
        Gather stats from all processes. Useful in the context of distributed training.

        Args:
            stats (dict[str, Any]):
            a dictionary of stats to be gathered. The stats should contain torch tensors.

        Returns:
            `dict[str, Any]`: A dictionary of stats with the tensors gathered.
        """
        import torch.distributed as dist

        # Wait for all processes to finish
        dist.barrier()

        for k, v in stats.items():
            if isinstance(v, torch.Tensor):
                dist.all_reduce(v.to(self.accelerator.device), dist.ReduceOp.SUM)
                v /= self.accelerator.num_processes
            stats[k] = v
        return stats

    def prepare_model_inputs(self, queries: torch.Tensor, responses: torch.Tensor):
        if self.is_encoder_decoder:
            input_data = self.data_collator(
                [{"input_ids": q, "attention_mask": torch.ones_like(q)} for q in queries]
            ).to(self.current_device)

            decoder_inputs = self.data_collator(
                [{"input_ids": r, "attention_mask": torch.ones_like(r)} for r in responses]
            ).to(self.current_device)

            input_data["decoder_input_ids"] = decoder_inputs["input_ids"]
            input_data["decoder_attention_mask"] = decoder_inputs["attention_mask"]
        else:
            input_ids = [torch.cat([q, r]) for q, r in zip(queries, responses)]
            input_data = self.data_collator(
                [{"input_ids": ids, "attention_mask": torch.ones_like(ids)} for ids in input_ids]
            ).to(self.current_device)

        input_data.pop("labels", None)  # we don't want to compute LM losses
        return input_data

    @PPODecorators.empty_cuda_cache()
    def batched_forward_pass(
        self,
        model: PreTrainedModelWrapper,
        queries: torch.Tensor,
        responses: torch.Tensor,
        model_inputs: dict,
        return_logits: bool = False,
        response_masks: Optional[torch.Tensor] = None,
    ):
        """
        Calculate model outputs in multiple batches.

        Args:
            queries (`torch.LongTensor`):
                List of tensors containing the encoded queries, shape (`batch_size`, `query_length`)
            responses (`torch.LongTensor`):
                List of tensors containing the encoded responses, shape (`batch_size`, `response_length`)
            return_logits (`bool`, *optional*, defaults to `False`):
                Whether to return all_logits. Set to `False` if logits are not needed to reduce memory consumption.
        Returns:
            (tuple):
                - all_logprobs (`torch.FloatTensor`): Log probabilities of the responses,
                    shape (`batch_size`, `response_length`)
                - all_ref_logprobs (`torch.FloatTensor`): Log probabilities of the responses,
                    shape (`batch_size`, `response_length`)
                - all_values (`torch.FloatTensor`): Values of the responses, shape (`batch_size`, `response_length`)
        """
        bs = len(queries)
        fbs = self.config.mini_batch_size
        all_logprobs = []
        all_logits = []
        all_masks = []
        all_values = []

        model.eval()

        #pdb.set_trace()
        for i in range(math.ceil(bs / fbs)):
            input_kwargs = {key: value[i * fbs : (i + 1) * fbs] for key, value in model_inputs.items()}
            query_batch = queries[i * fbs : (i + 1) * fbs]
            response_batch = responses[i * fbs : (i + 1) * fbs]
            if response_masks is not None:
                response_masks_batch = response_masks[i * fbs : (i + 1) * fbs]
            logits, _, values = model(**input_kwargs)

            if self.is_encoder_decoder:
                input_ids = input_kwargs["decoder_input_ids"]
                attention_mask = input_kwargs["decoder_attention_mask"]
            else:
                input_ids = input_kwargs["input_ids"]
                attention_mask = input_kwargs["attention_mask"]


            # logits at step
            logprobs = logprobs_from_logits(logits[:, :-1, :], input_ids[:, 1:])
            masks = torch.zeros_like(attention_mask)
            masks[:, :-1] = attention_mask[:, 1:]

            for j in range(len(query_batch)):
                if self.is_encoder_decoder:
                    # Decoder sentence starts always in the index 1 after padding in the Enc-Dec Models
                    start = 1
                    end = attention_mask[j, :].sum() - 1
                else:
                    start = len(query_batch[j]) - 1  # logprobs starts from the second query token
                    if attention_mask[j, 0] == 0:  # offset left padding
                        start += attention_mask[j, :].nonzero()[0]
                    end = start + len(response_batch[j])
                    if response_masks is not None:
                        response_masks_batch[j] = torch.cat(
                            (torch.zeros_like(query_batch[j]), response_masks_batch[j])
                        )[1:]

                masks[j, :start] = 0
                masks[j, end:] = 0
                if response_masks is not None:
                    masks[j, start:end] = masks[j, start:end] * response_masks_batch[j][start:end]

            if return_logits:
                all_logits.append(logits)
            else:
                del logits
            all_values.append(values)
            all_logprobs.append(logprobs)
            all_masks.append(masks)

        return (
            torch.cat(all_logprobs),
            torch.cat(all_logits)[:, :-1] if return_logits else None,
            torch.cat(all_values)[:, :-1],
            torch.cat(all_masks)[:, :-1],
        )

    @PPODecorators.empty_cuda_cache()
    def train_minibatch(
        self,
        old_logprobs: torch.FloatTensor,
        values: torch.FloatTensor,
        logprobs: torch.FloatTensor,
        logits: torch.FloatTensor,
        vpreds: torch.FloatTensor,
        mask: torch.LongTensor,
        advantages: torch.FloatTensor,
        returns: torch.FloatTensor,
    ):
        """
        Train one PPO minibatch

        Args:
            logprobs (`torch.FloatTensor`):
                Log probabilities of the model, shape [batch_size, response_length]
            values (`torch.FloatTensor`):
                Values of the value head, shape [batch_size, response_length]
            query (`torch.LongTensor`):
                Encoded queries, shape [batch_size, query_length]
            response (`torch.LongTensor`):
                Encoded responses, shape [batch_size, response_length]
            model_input (`torch.LongTensor`):
                Concatenated queries and responses, shape [batch_size, query_length+response_length]

        Returns:
            train_stats (dict[str, `torch.Tensor`]):
                Dictionary of training statistics
        """
        self.model.train()
        loss_p, loss_v, train_stats = self.loss(
            old_logprobs, values, logits, vpreds, logprobs, mask, advantages, returns
        )
        loss = loss_p + loss_v
        self.accelerator.backward(loss)
        if self.config.max_grad_norm is not None:
            if self.accelerator.sync_gradients:
                self.accelerator.clip_grad_norm_(self.model_params, self.config.max_grad_norm)
        self.optimizer.step()
        # we call optimizer.zero_grad() every time and let `accelerator` handle accumulation
        # see https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation#the-finished-code
        self.optimizer.zero_grad()
        return train_stats

    def compute_rewards(
        self,
        scores: torch.FloatTensor,
        logprobs: torch.FloatTensor,
        ref_logprobs: torch.FloatTensor,
        masks: torch.LongTensor,
    ):
        """
        Compute per token rewards from scores and KL-penalty.

        Args:
            scores (`torch.FloatTensor`):
                Scores from the reward model, shape (`batch_size`)
            logprobs (`torch.FloatTensor`):
                Log probabilities of the model, shape (`batch_size`, `response_length`)
            ref_logprobs (`torch.FloatTensor`):
                Log probabilities of the reference model, shape (`batch_size`, `response_length`)
        """
        rewards, non_score_rewards = [], []
        for score, logprob, ref_logprob, mask in zip(scores, logprobs, ref_logprobs, masks):
            # compute KL penalty (from difference in logprobs)
            kl = self._kl_penalty(logprob, ref_logprob)
            non_score_reward = -self.kl_ctl.value * kl
            non_score_rewards.append(non_score_reward)
            reward = non_score_reward.clone()
            last_non_masked_index = mask.nonzero()[-1]

            # reward is preference model score + KL penalty
            reward[last_non_masked_index] += score
            rewards.append(reward)
        return torch.stack(rewards), torch.stack(non_score_rewards)

    def _kl_penalty(self, logprob: torch.FloatTensor, ref_logprob: torch.FloatTensor) -> torch.FloatTensor:
        if self.config.kl_penalty == "kl":
            return logprob - ref_logprob

        if self.config.kl_penalty == "abs":
            return (logprob - ref_logprob).abs()

        if self.config.kl_penalty == "mse":
            return 0.5 * (logprob - ref_logprob).square()

        if self.config.kl_penalty == "full":
            # Flip is required due to this issue? :https://github.com/pytorch/pytorch/issues/57459
            return F.kl_div(ref_logprob, logprob, log_target=True, reduction="none").sum(-1)

        raise NotImplementedError

    def compute_advantages(
        self: torch.FloatTensor,
        values: torch.FloatTensor,
        rewards: torch.FloatTensor,
        mask: torch.FloatTensor,
    ):
        lastgaelam = 0
        advantages_reversed = []
        gen_len = rewards.shape[-1]

        pdb.set_trace()
        values = values * mask
        rewards = rewards * mask

        for t in reversed(range(gen_len)):
            nextvalues = values[:, t + 1] if t < gen_len - 1 else 0.0
            delta = rewards[:, t] + self.config.gamma * nextvalues - values[:, t]
            lastgaelam = delta + self.config.gamma * self.config.lam * lastgaelam
            advantages_reversed.append(lastgaelam)
        advantages = torch.stack(advantages_reversed[::-1]).transpose(0, 1)

        returns = advantages + values
        advantages = masked_whiten(advantages, mask)
        advantages = advantages.detach()
        return values, advantages, returns

    def loss(
        self,
        old_logprobs: torch.FloatTensor,
        values: torch.FloatTensor,
        logits: torch.FloatTensor,
        vpreds: torch.FloatTensor,
        logprobs: torch.FloatTensor,
        mask: torch.LongTensor,
        advantages: torch.FloatTensor,
        returns: torch.FloatTensor,
        **kwargs
    ):
        """
        Calculate policy and value losses.

        Args:
            old_logprobs (`torch.FloatTensor`):
                Log probabilities of the model, shape (`batch_size`, `response_length`)
            values (`torch.FloatTensor`):
                Values of the value head, shape (`batch_size`, `response_length`)
            rewards (`torch.FloatTensor`):
                Rewards from the reward model, shape (`batch_size`, `response_length`)
            logits (`torch.FloatTensor`):
                Logits of the model, shape (`batch_size`, `response_length`, `vocab_size`)
            v_pred (`torch.FloatTensor`):
                Values of the value head, shape (`batch_size`, `response_length`)
            logprobs (`torch.FloatTensor`):
                Log probabilities of the model, shape (`batch_size`, `response_length`)
        """

        vpredclipped = clip_by_value(
            vpreds,
            values - self.config.cliprange_value,
            values + self.config.cliprange_value,
        )

        #pdb.set_trace()

        vf_losses1 = (vpreds - returns) ** 2
        vf_losses2 = (vpredclipped - returns) ** 2
        vf_loss = 0.5 * masked_mean(torch.max(vf_losses1, vf_losses2), mask)
        vf_clipfrac = masked_mean(torch.gt(vf_losses2, vf_losses1).float(), mask)

        ratio = torch.exp(logprobs - old_logprobs)

        pg_losses = -advantages * ratio
        pg_losses2 = -advantages * torch.clamp(ratio, 1.0 - self.config.cliprange, 1.0 + self.config.cliprange)

        pg_loss = masked_mean(torch.max(pg_losses, pg_losses2), mask)
        pg_clipfrac = masked_mean(torch.gt(pg_losses2, pg_losses).float(), mask)

        loss = pg_loss + self.config.vf_coef * vf_loss

        avg_ratio = masked_mean(ratio, mask).item()
        if avg_ratio > self.config.ratio_threshold:
            warnings.warn(
                f"The average ratio of batch ({avg_ratio:.2f}) exceeds threshold {self.config.ratio_threshold:.2f}. Skipping batch."
            )
            pg_loss = pg_loss * 0.0
            vf_loss = vf_loss * 0.0
            loss = loss * 0.0

        entropy = masked_mean(entropy_from_logits(logits), mask)

        approxkl = 0.5 * masked_mean((logprobs - old_logprobs) ** 2, mask)
        policykl = masked_mean(old_logprobs - logprobs, mask)

        return_mean, return_var = masked_mean(returns, mask), masked_var(returns, mask)
        value_mean, value_var = masked_mean(values, mask), masked_var(values, mask)

        stats = dict(
            loss=dict(policy=pg_loss.detach(), value=vf_loss.detach(), total=loss.detach()),
            policy=dict(
                entropy=entropy.detach(),
                approxkl=approxkl.detach(),
                policykl=policykl.detach(),
                clipfrac=pg_clipfrac.detach(),
                advantages=advantages.detach(),
                advantages_mean=masked_mean(advantages, mask).detach(),
                ratio=ratio.detach(),
            ),
            returns=dict(mean=return_mean.detach(), var=return_var.detach()),
            val=dict(
                vpred=masked_mean(vpreds, mask).detach(),
                error=masked_mean((vpreds - returns) ** 2, mask).detach(),
                clipfrac=vf_clipfrac.detach(),
                mean=value_mean.detach(),
                var=value_var.detach(),
            ),
        )
        return pg_loss, self.config.vf_coef * vf_loss, flatten_dict(stats)

    def record_step_stats(self, kl_coef: float, **data):
        """
        Record training step statistics.


        Args:
            kl_coef (`float`):
                KL coefficient
            data (`dict`):
                Dictionary of training step data

        Returns:
            stats (`dict`):
                Dictionary of training step statistics
        """
        mask = data.pop("masks")

        kl_list = ((data["logprobs"] - data["ref_logprobs"]) * mask).sum(axis=-1)
        mean_kl = kl_list.mean()
        mean_entropy = (-data["logprobs"] * mask).sum(axis=-1).mean()

        mean_non_score_reward = masked_mean(
            data["non_score_reward"], mask
        )  # non_score_reward is size `batch_size`, `response_length`
        mean_scores = data["scores"].mean()  # scores is size `batch_size`
        std_scores = data["scores"].std()

        if mean_kl.item() < -1.0:
            # warn users
            warnings.warn(
                f"KL divergence is starting to become negative: {mean_kl.item():.2f} - this might be a precursor for failed training."
                " sometimes this happens because the generation kwargs are not correctly set. Please make sure"
                " that the generation kwargs are set correctly, or review your training hyperparameters."
            )

        stats = {
            "objective/kl": mean_kl,
            "objective/kl_dist": kl_list,
            "objective/logprobs": data["logprobs"],
            "objective/ref_logprobs": data["ref_logprobs"],
            "objective/kl_coef": kl_coef,
            "objective/entropy": mean_entropy,
            "ppo/mean_non_score_reward": mean_non_score_reward,
            "ppo/mean_scores": mean_scores,
            "ppo/std_scores": std_scores,
        }

        # Log text properties
        query_lens = torch.tensor([len(query) for query in data["queries"]], dtype=torch.float)
        response_lens = torch.tensor([len(response) for response in data["responses"]], dtype=torch.float)

        stats["tokens/queries_len_mean"] = torch.mean(query_lens).cpu().numpy().item()
        stats["tokens/queries_len_std"] = torch.std(query_lens).cpu().numpy().item()
        stats["tokens/queries_dist"] = query_lens.cpu().numpy()
        stats["tokens/responses_len_mean"] = torch.mean(response_lens).cpu().numpy().item()
        stats["tokens/responses_len_std"] = torch.std(response_lens).cpu().numpy().item()
        stats["tokens/responses_dist"] = response_lens.cpu().numpy()

        for k, v in data["train_stats"].items():
            stats[f"ppo/{k}"] = torch.mean(v, axis=0)
        stats["ppo/val/var_explained"] = 1 - stats["ppo/val/error"] / stats["ppo/returns/var"]
        return stats

    def log_stats(
        self,
        stats: dict,
        batch: dict,
        rewards: List[torch.FloatTensor],
        columns_to_log: List[str] = ["query", "response"],
    ):
        """
        A function that logs all the training stats. Call it at the end of each epoch.

        Args:
            stats (dict[str, Any]):
                A dictionary of training stats.
            batch (dict[str, Any]):
                A dictionary of batch data, this contains the queries and responses.
            rewards (`List[torch.FloatTensor]`):
                A tensor of rewards.
        """
        # Log only if we are in the main process
        if self.accelerator.is_main_process:
            logs = {}

            # Log stats
            if not isinstance(rewards, torch.Tensor):
                rewards = torch.tensor(rewards).to(self.current_device)

            if "query" not in batch.keys() and "response" not in batch.keys():
                # warn the user that the game logs will not be logged
                warnings.warn(
                    "The game logs will not be logged because the batch does not contain the keys 'query' and "
                    "'response'. "
                )
            elif self.config.log_with == "wandb":
                import wandb

                if any([column_to_log not in batch.keys() for column_to_log in columns_to_log]):
                    raise ValueError(f"Columns to log {columns_to_log} are not present in the batch {batch.keys()}.")

                batch_list = [batch[column_to_log] for column_to_log in columns_to_log]

                table_rows = [list(r) for r in zip(*batch_list, rewards.cpu().tolist())]
                logs.update({"game_log": wandb.Table(columns=[*columns_to_log, "reward"], rows=table_rows)})
            # All reduce rewards if distributed
            if self.is_distributed:
                import torch.distributed as dist

                dist.barrier()

                dist.all_reduce(rewards, op=torch.distributed.ReduceOp.SUM)
                rewards /= self.accelerator.num_processes

            logs.update(stats)

            # manually cast in fp32 for bf16 torch tensors
            for k, v in logs.items():
                if isinstance(v, torch.Tensor) and v.dtype == torch.bfloat16:
                    logs[k] = v.float()

            logs["env/reward_mean"] = torch.mean(rewards).cpu().numpy().item()
            logs["env/reward_std"] = torch.std(rewards).cpu().numpy().item()
            logs["env/reward_dist"] = rewards.cpu().numpy()

            if self.config.log_with == "tensorboard":
                # update the current step
                self.current_step += 1

            self.accelerator.log(
                logs,
                step=self.current_step if self.config.log_with == "tensorboard" else None,
            )

        else:
            if self.is_distributed:
                import torch.distributed as dist

                if not isinstance(rewards, torch.Tensor):
                    rewards = torch.tensor(rewards).to(self.current_device)

                dist.barrier()
                dist.all_reduce(rewards, op=torch.distributed.ReduceOp.SUM)

    def create_model_card(self, path: str, model_name: Optional[str] = "TRL Model") -> None:
        """Creates and saves a model card for a TRL model.

        Args:
            path (`str`): The path to save the model card to.
            model_name (`str`, *optional*): The name of the model, defaults to `TRL Model`.
        """
        try:
            user = whoami()["name"]
        # handle the offline case
        except:  # noqa
            warnings.warn("Cannot retrieve user information assuming you are running in offline mode.")
            return

        if not os.path.exists(path):
            os.makedirs(path)

        model_card_content = MODEL_CARD_TEMPLATE.format(model_name=model_name, model_id=f"{user}/{path}")
        with open(os.path.join(path, "README.md"), "w", encoding="utf-8") as f:
            f.write(model_card_content)

    def _save_pretrained(self, save_directory: str) -> None:
        self.accelerator.unwrap_model(self.model).save_pretrained(save_directory)
        self.tokenizer.save_pretrained(save_directory)
        self.create_model_card(save_directory)

    def _show_tokens(self, tokens, masks):
        from rich import print
        from rich.text import Text

        text = Text()

        for i, (token, mask) in enumerate(zip(tokens, masks)):
            if mask == 1:
                text.append(self.tokenizer.decode(token.item()), style="black on deep_sky_blue1")
                text.append(" ")
            else:
                text.append(self.tokenizer.decode(token.item()), style="black on cyan3")
                text.append(" ")
        print(text)

    def _prepare_deepspeed(self, model: PreTrainedModelWrapper):
        # Adapted from accelerate: https://github.com/huggingface/accelerate/blob/739b135f8367becb67ffaada12fe76e3aa60fefd/src/accelerate/accelerator.py#L1473
        deepspeed_plugin = self.accelerator.state.deepspeed_plugin
        config_kwargs = deepspeed_plugin.deepspeed_config
        if model is not None:
            if hasattr(model, "config"):
                hidden_size = (
                    max(model.config.hidden_sizes)
                    if getattr(model.config, "hidden_sizes", None)
                    else getattr(model.config, "hidden_size", None)
                )
                if hidden_size is not None and config_kwargs["zero_optimization"]["stage"] == 3:
                    # Note that `stage3_prefetch_bucket_size` can produce DeepSpeed messages like: `Invalidate trace cache @ step 0: expected module 1, but got module 0`
                    # This is expected and is not an error, see: https://github.com/microsoft/DeepSpeed/discussions/4081
                    config_kwargs.update(
                        {
                            "zero_optimization.reduce_bucket_size": hidden_size * hidden_size,
                            "zero_optimization.stage3_param_persistence_threshold": 10 * hidden_size,
                            "zero_optimization.stage3_prefetch_bucket_size": 0.9 * hidden_size * hidden_size,
                        }
                    )

        # If ZeRO-3 is used, we shard both the active and reference model.
        # Otherwise, we assume the reference model fits in memory and is initialized on each device with ZeRO disabled (stage 0)
        if config_kwargs["zero_optimization"]["stage"] != 3:
            config_kwargs["zero_optimization"]["stage"] = 0
        model, *_ = deepspeed.initialize(model=model, config=config_kwargs)
        model.eval()
        return model

File Path: trl/trainer/training_configs.py
Content:
# coding=utf-8
# coding=utf-8
# Copyright 2023 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass, field
from typing import Optional

from transformers import TrainingArguments


@dataclass
class RewardConfig(TrainingArguments):
    """
    RewardConfig collects all training arguments related to the [`RewardTrainer`] class.

    Using [`HfArgumentParser`] we can turn this class into
    [argparse](https://docs.python.org/3/library/argparse#module-argparse) arguments that can be specified on the
    command line.

    Parameters:
        max_length (`int`, *optional*, defaults to `None`):
            The maximum length of the sequences in the batch. This argument is required if you want to use the default data collator.
        gradient_checkpointing (`bool`, *optional*, defaults to `True`):
                If True, use gradient checkpointing to save memory at the expense of slower backward pass.
    """

    max_length: Optional[int] = field(
        default=None,
        metadata={
            "help": "The maximum length of the sequences in the batch. This argument is required if you want to use the default data collator."
        },
    )
    gradient_checkpointing: Optional[bool] = field(
        default=True,
        metadata={
            "help": "If True, use gradient checkpointing to save memory at the expense of slower backward pass."
        },
    )

File Path: trl/trainer/utils.py
Content:
# Copyright 2022 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import os
import random
import warnings
from collections import deque
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple, Union

import numpy as np
import torch
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import IterableDataset
from transformers import DataCollatorForLanguageModeling, PreTrainedModel, PreTrainedTokenizerBase, TrainerCallback


class AdaptiveKLController:
    """
    Adaptive KL controller described in the paper:
    https://arxiv.org/pdf/1909.08593.pdf
    """

    def __init__(self, init_kl_coef, target, horizon):
        self.value = init_kl_coef
        self.target = target
        self.horizon = horizon

    def update(self, current, n_steps):
        target = self.target
        proportional_error = np.clip(current / target - 1, -0.2, 0.2)
        mult = 1 + proportional_error * n_steps / self.horizon
        self.value *= mult


class FixedKLController:
    """Fixed KL controller."""

    def __init__(self, kl_coef):
        self.value = kl_coef

    def update(self, current, n_steps):
        pass

class BeaverLambdaController:
    """
    Lambda controller described in Eqn 31 of the paper:
    https://arxiv.org/pdf/2310.12773.pdf
    """

    def __init__(self, init_kl_coef, target, horizon):
        self.value = init_kl_coef
        self.target = target
        self.horizon = horizon

    def update(self, current, n_steps):
        target = self.target
        proportional_error = np.clip(current / target - 1, -0.2, 0.2)
        mult = 1 + proportional_error * n_steps / self.horizon
        self.value *= mult


class DataCollatorForCompletionOnlyLM(DataCollatorForLanguageModeling):
    """
    Data collator used for completion tasks. It ensures that all the tokens of the labels are set to an 'ignore_index'
    when they do not come from the assistant. This ensure that the loss is only
    calculated on the completion made by the assistant.

    Args:
        instruction_template (`Optional[str]`): the template form that indicates the start of the human instruction, typically something like
            '### Human:\n'. Useful for assistant-style conversation datasets
        response_template (`Union[str, List[int]]`): the template form that indicates the start of the response, typically something like
            '### Response:\n'. It can also be passed as tokenized ids, which can be useful when using a tokenizer that encodes the response
            differently if it does not have proper context.
        mlm (`bool`, *optional*, defaults to `False`): Whether or not to use masked language modeling in the underlying
            `DataCollatorForLanguageModeling` class. Note that this option currently has no effect but is present
             for flexibility and backwards-compatibility.
        ignore_index (`int`, *optional*, defaults to `-100`):
            The index to use to ignore the initial tokens with
    """

    def __init__(
        self,
        response_template: Union[str, List[int]],
        instruction_template: Optional[str] = None,
        *args,
        mlm: bool = False,
        ignore_index: int = -100,
        **kwargs,
    ):
        super().__init__(*args, mlm=mlm, **kwargs)
        self.instruction_template = instruction_template
        self.response_template = response_template
        self.ignore_index = ignore_index
        if type(response_template) == list:
            # The user already provides the token ids
            self.response_token_ids = response_template
        else:
            self.response_token_ids = self.tokenizer.encode(self.response_template, add_special_tokens=False)

    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:
        batch = super().torch_call(examples)

        if self.instruction_template is None:
            for i in range(len(examples)):
                response_token_ids_start_idx = None

                for idx in np.where(batch["labels"][i] == self.response_token_ids[0])[0]:
                    # `response_token_ids` is `'### Response:\n'`, here we are just making sure that the token IDs match
                    if (
                        self.response_token_ids
                        == batch["labels"][i][idx : idx + len(self.response_token_ids)].tolist()
                    ):
                        response_token_ids_start_idx = idx

                if response_token_ids_start_idx is None:
                    warnings.warn(
                        f"Could not find response key `{self.response_template}` in the "
                        f'following instance: {self.tokenizer.decode(batch["input_ids"][i])} '
                        f"This instance will be ignored in loss calculation. "
                        f"Note, if this happens often, consider increasing the `max_seq_length`."
                    )
                    batch["labels"][i, :] = self.ignore_index
                else:
                    response_token_ids_end_idx = response_token_ids_start_idx + len(self.response_token_ids)

                    # Make pytorch loss function ignore all tokens up through the end of the response key
                    batch["labels"][i, :response_token_ids_end_idx] = self.ignore_index

        else:
            for i in range(len(examples)):
                response_token_ids_idxs = []
                human_token_ids_idxs = []

                for assistant_idx in np.where(batch["labels"][i] == self.response_token_ids[0])[0]:
                    # find the indexes of the start of a response.
                    if (
                        self.response_token_ids
                        == batch["labels"][i][assistant_idx : assistant_idx + len(self.response_token_ids)].tolist()
                    ):
                        response_token_ids_idxs.append(assistant_idx + len(self.response_token_ids))

                if len(response_token_ids_idxs) == 0:
                    warnings.warn(
                        f"Could not find response key `{self.response_template}` in the "
                        f'following instance: {self.tokenizer.decode(batch["input_ids"][i])} '
                        f"This instance will be ignored in loss calculation. "
                        f"Note, if this happens often, consider increasing the `max_seq_length`."
                    )
                    batch["labels"][i, :] = self.ignore_index

                human_token_ids = self.tokenizer.encode(self.instruction_template, add_special_tokens=False)
                for human_idx in np.where(batch["labels"][i] == human_token_ids[0])[0]:
                    # find the indexes of the start of a human answer.
                    if human_token_ids == batch["labels"][i][human_idx : human_idx + len(human_token_ids)].tolist():
                        human_token_ids_idxs.append(human_idx)

                if len(human_token_ids_idxs) == 0:
                    warnings.warn(
                        f"Could not find instruction key `{self.instruction_template}` in the "
                        f'following instance: {self.tokenizer.decode(batch["input_ids"][i])} '
                        f"This instance will be ignored in loss calculation. "
                        f"Note, if this happens often, consider increasing the `max_seq_length`."
                    )
                    batch["labels"][i, :] = self.ignore_index

                for idx, (start, end) in enumerate(zip(human_token_ids_idxs, response_token_ids_idxs)):
                    # Make pytorch loss function ignore all non response tokens
                    if idx != 0:
                        batch["labels"][i, start:end] = self.ignore_index
                    else:
                        batch["labels"][i, :end] = self.ignore_index

                if len(response_token_ids_idxs) < len(human_token_ids_idxs):
                    batch["labels"][i, human_token_ids_idxs[-1] :] = self.ignore_index

        return batch


@dataclass
class RewardDataCollatorWithPadding:
    r"""
    Reward DataCollator class that pads the inputs to the maximum length of the batch.
    Args:
        tokenizer (`PreTrainedTokenizerBase`):
            The tokenizer used for encoding the data.
        padding (`Union[bool, str, `PaddingStrategy`]`, `optional`, defaults to `True`):
            padding_strategy to pass to the tokenizer.
        max_length (`Optional[int]`, `optional`, defaults to `None`):
            The maximum length of the sequence to be processed.
        pad_to_multiple_of (`Optional[int]`, `optional`, defaults to `None`):
            If set will pad the sequence to a multiple of the provided value.
        return_tensors (`str`, `optional`, defaults to `"pt"`):
            The tensor type to use.
    """
    tokenizer: PreTrainedTokenizerBase
    padding: Union[bool, str] = True
    max_length: Optional[int] = None
    pad_to_multiple_of: Optional[int] = None
    return_tensors: str = "pt"

    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:
        features_chosen = []
        features_rejected = []
        margin = []
        # check if we have a margin. If we do, we need to batch it as well
        has_margin = "margin" in features[0]
        for feature in features:
            # check if the keys are named as expected
            if (
                "input_ids_chosen" not in feature
                or "input_ids_rejected" not in feature
                or "attention_mask_chosen" not in feature
                or "attention_mask_rejected" not in feature
            ):
                raise ValueError(
                    "The features should include `input_ids_chosen`, `attention_mask_chosen`, `input_ids_rejected` and `attention_mask_rejected`"
                )

            features_chosen.append(
                {
                    "input_ids": feature["input_ids_chosen"],
                    "attention_mask": feature["attention_mask_chosen"],
                }
            )
            features_rejected.append(
                {
                    "input_ids": feature["input_ids_rejected"],
                    "attention_mask": feature["attention_mask_rejected"],
                }
            )
            if has_margin:
                margin.append(feature["margin"])
        batch_chosen = self.tokenizer.pad(
            features_chosen,
            padding=self.padding,
            max_length=self.max_length,
            pad_to_multiple_of=self.pad_to_multiple_of,
            return_tensors=self.return_tensors,
        )
        batch_rejected = self.tokenizer.pad(
            features_rejected,
            padding=self.padding,
            max_length=self.max_length,
            pad_to_multiple_of=self.pad_to_multiple_of,
            return_tensors=self.return_tensors,
        )
        batch = {
            "input_ids_chosen": batch_chosen["input_ids"],
            "attention_mask_chosen": batch_chosen["attention_mask"],
            "input_ids_rejected": batch_rejected["input_ids"],
            "attention_mask_rejected": batch_rejected["attention_mask"],
            "return_loss": True,
        }
        if has_margin:
            margin = torch.tensor(margin, dtype=torch.float)
            batch["margin"] = margin
        return batch


@dataclass
class DPODataCollatorWithPadding:
    r"""
    DPO DataCollator class that pads the inputs to the maximum length of the batch.
    Args:
        tokenizer (`PreTrainedTokenizerBase`):
            The tokenizer used for encoding the data.
        model (Optional[`PreTrainedModel`]):
            The model that is being trained. If set and has the *prepare_decoder_input_ids_from_labels*, use it to
            prepare the *decoder_input_ids*.
        padding (`Union[bool, str, `PaddingStrategy`]`, `optional`, defaults to `True`):
            padding_strategy to pass to the tokenizer.
        max_length (`Optional[int]`, `optional`, defaults to `None`):
            The maximum length of the sequence to be processed.
        max_prompt_length (`Optional[int]`, `optional`, defaults to `None`):
            The maximum length of the prompt to be processed.
        label_pad_token_id (`int`, defaults to -100):
            The label used for masking.
        padding_value (`int`, defaults to 0):
            The value used for padding.
        is_encoder_decoder (`Optional[bool]`, `optional`, defaults to `None`):
            Whether or not you model has an encoder_decoder architecture.
        max_target_length (`Optional[int]`, `optional`, defaults to `None`):
            The maximum length of the target to be processed. Only useful for encoder-decoder architectures.
        truncation_mode: (`str`, defaults to "keep_end"):
            The truncation mode to use when truncating the prompt.
    """
    tokenizer: PreTrainedTokenizerBase
    model: Optional[PreTrainedModel] = None
    padding: Union[bool, str] = True
    max_length: Optional[int] = None
    max_prompt_length: Optional[int] = None
    label_pad_token_id: int = -100
    padding_value: int = 0
    truncation_mode: str = "keep_end"
    is_encoder_decoder: Optional[bool] = False
    max_target_length: Optional[int] = None

    def tokenize_batch_element(
        self,
        prompt: str,
        chosen: str,
        rejected: str,
    ) -> Dict:
        """Tokenize a single batch element.

        At this stage, we don't convert to PyTorch tensors yet; we just handle the truncation
            in case the prompt + chosen or prompt + rejected responses is/are too long. First
            we truncate the prompt; if we're still too long, we truncate the chosen/rejected.

        We also create the labels for the chosen/rejected responses, which are of length equal to
            the sum of the length of the prompt and the chosen/rejected response, with
            label_pad_token_id  for the prompt tokens.
        """
        batch = {}

        if not self.is_encoder_decoder:
            chosen_tokens = self.tokenizer(chosen, add_special_tokens=False)
            rejected_tokens = self.tokenizer(rejected, add_special_tokens=False)
            prompt_tokens = self.tokenizer(prompt, add_special_tokens=False)

            eos_token_id = self.tokenizer.eos_token_id
            # Get indices in list prompt_tokens["input_ids"] that equals the EOS token (often 0)
            eos_indices_prompt = [i for i, x in enumerate(prompt_tokens["input_ids"]) if x == eos_token_id]
            # attention mask these indices to eos_token_id
            new_attention_mask = [
                0 if i in eos_indices_prompt else p for i, p in enumerate(prompt_tokens["attention_mask"])
            ]
            prompt_tokens["attention_mask"] = new_attention_mask

            # do the same for chosen and rejected
            eos_indices_chosen = [i for i, x in enumerate(chosen_tokens["input_ids"]) if x == eos_token_id]
            new_attention_mask_c = [
                0 if i in eos_indices_chosen else p for i, p in enumerate(chosen_tokens["attention_mask"])
            ]
            chosen_tokens["attention_mask"] = new_attention_mask_c

            eos_indices_rejected = [i for i, x in enumerate(rejected_tokens["input_ids"]) if x == eos_token_id]
            new_attention_mask_r = [
                0 if i in eos_indices_rejected else p for i, p in enumerate(rejected_tokens["attention_mask"])
            ]
            rejected_tokens["attention_mask"] = new_attention_mask_r

            # add EOS token to end of prompt
            chosen_tokens["input_ids"].append(self.tokenizer.eos_token_id)
            chosen_tokens["attention_mask"].append(1)

            rejected_tokens["input_ids"].append(self.tokenizer.eos_token_id)
            rejected_tokens["attention_mask"].append(1)

            longer_response_length = max(len(chosen_tokens["input_ids"]), len(rejected_tokens["input_ids"]))

            # if combined sequence is too long, truncate the prompt
            if len(prompt_tokens["input_ids"]) + longer_response_length > self.max_length:
                if self.truncation_mode == "keep_start":
                    prompt_tokens = {k: v[: self.max_prompt_length] for k, v in prompt_tokens.items()}
                elif self.truncation_mode == "keep_end":
                    prompt_tokens = {k: v[-self.max_prompt_length :] for k, v in prompt_tokens.items()}
                else:
                    raise ValueError(f"Unknown truncation mode: {self.truncation_mode}")

            # if that's still too long, truncate the response
            if len(prompt_tokens["input_ids"]) + longer_response_length > self.max_length:
                chosen_tokens = {k: v[: self.max_length - self.max_prompt_length] for k, v in chosen_tokens.items()}
                rejected_tokens = {
                    k: v[: self.max_length - self.max_prompt_length] for k, v in rejected_tokens.items()
                }

            # Create labels
            chosen_sequence_tokens = {k: prompt_tokens[k] + chosen_tokens[k] for k in chosen_tokens}
            rejected_sequence_tokens = {k: prompt_tokens[k] + rejected_tokens[k] for k in rejected_tokens}
            chosen_sequence_tokens["labels"] = chosen_sequence_tokens["input_ids"][:]
            chosen_sequence_tokens["labels"][: len(prompt_tokens["input_ids"])] = [self.label_pad_token_id] * len(
                prompt_tokens["input_ids"]
            )
            rejected_sequence_tokens["labels"] = rejected_sequence_tokens["input_ids"][:]
            rejected_sequence_tokens["labels"][: len(prompt_tokens["input_ids"])] = [self.label_pad_token_id] * len(
                prompt_tokens["input_ids"]
            )

            for k, toks in {
                "chosen": chosen_sequence_tokens,
                "rejected": rejected_sequence_tokens,
                "prompt": prompt_tokens,
            }.items():
                for type_key, tokens in toks.items():
                    if type_key == "token_type_ids":
                        continue
                    batch[f"{k}_{type_key}"] = tokens

        else:
            chosen_tokens = self.tokenizer(
                chosen, truncation=True, max_length=self.max_target_length, add_special_tokens=True
            )
            rejected_tokens = self.tokenizer(
                rejected, truncation=True, max_length=self.max_target_length, add_special_tokens=True
            )
            prompt_tokens = self.tokenizer(
                prompt, truncation=True, max_length=self.max_prompt_length, add_special_tokens=True
            )

            batch["chosen_labels"] = chosen_tokens["input_ids"]
            batch["rejected_labels"] = rejected_tokens["input_ids"]
            batch["prompt_input_ids"] = prompt_tokens["input_ids"]
            batch["prompt_attention_mask"] = prompt_tokens["attention_mask"]

            if self.model is not None and hasattr(self.model, "prepare_decoder_input_ids_from_labels"):
                batch["rejected_decoder_input_ids"] = self.model.prepare_decoder_input_ids_from_labels(
                    labels=batch["rejected_labels"]
                )
                batch["chosen_decoder_input_ids"] = self.model.prepare_decoder_input_ids_from_labels(
                    labels=batch["chosen_labels"]
                )

        batch["prompt"] = prompt
        batch["chosen"] = prompt + chosen
        batch["rejected"] = prompt + rejected
        batch["chosen_response_only"] = chosen
        batch["rejected_response_only"] = rejected

        return batch

    def collate(self, batch):
        # first, pad everything to the same length
        padded_batch = {}
        for k in batch[0].keys():
            if k.endswith("_input_ids") or k.endswith("_attention_mask") or k.endswith("_labels"):
                if self.is_encoder_decoder:
                    to_pad = [torch.LongTensor(ex[k]) for ex in batch]

                    if (k.startswith("prompt")) and (k.endswith("input_ids")):
                        padding_value = self.tokenizer.pad_token_id
                    elif k.endswith("_attention_mask"):
                        padding_value = 0
                    elif (k.startswith("chosen")) or (k.startswith("rejected")) or ("decoder" in k):
                        padding_value = self.label_pad_token_id
                    else:
                        raise ValueError(f"Unexpected key in batch '{k}'")
                    padded_batch[k] = pad_sequence(to_pad, batch_first=True, padding_value=padding_value)
                else:
                    # adapted from https://stackoverflow.com/questions/73256206
                    if "prompt" in k:
                        to_pad = [torch.LongTensor(ex[k][::-1]) for ex in batch]
                    else:
                        to_pad = [torch.LongTensor(ex[k]) for ex in batch]
                    if k.endswith("_input_ids"):
                        padding_value = self.tokenizer.pad_token_id
                    elif k.endswith("_labels"):
                        padding_value = self.label_pad_token_id
                    elif k.endswith("_attention_mask"):
                        padding_value = self.padding_value
                    else:
                        raise ValueError(f"Unexpected key in batch '{k}'")

                    padded_batch[k] = pad_sequence(to_pad, batch_first=True, padding_value=padding_value)
                    # for the prompt, flip back so padding is on left side
                    if "prompt" in k:
                        padded_batch[k] = padded_batch[k].flip(dims=[1])
            else:
                padded_batch[k] = [ex[k] for ex in batch]

        return padded_batch

    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:
        tokenized_batch = []

        for feature in features:
            prompt = feature["prompt"]
            chosen = feature["chosen"]
            rejected = feature["rejected"]

            batch_element = self.tokenize_batch_element(prompt, chosen, rejected)
            tokenized_batch.append(batch_element)

        # return collated batch
        return self.collate(tokenized_batch)


class ConstantLengthDataset(IterableDataset):
    """
    Iterable dataset that returns constant length chunks of tokens from stream of text files.
    The dataset also formats the text before tokenization with a specific format that is provided
    by the user.

        Args:
            tokenizer (`transformers.PreTrainedTokenizer`):
                The processor used for processing the data.
            dataset (`dataset.Dataset`):
                Dataset with text files.
            dataset_text_field (`str`, **optional**):
                Name of the field in the dataset that contains the text. Used only if `formatting_func` is `None`.
            formatting_func (`Callable`, **optional**):
                Function that formats the text before tokenization. Usually it is recommended to have follows a certain
                pattern such as `"### Question: {question}\n ### Answer: {answer}\n"`
            infinite (`bool`, *optional*, defaults to `False`):
                If True the iterator is reset after dataset reaches end else stops.
            seq_length (`int`, *optional*, defaults to `1024`):
                Length of token sequences to return.
            num_of_sequences (`int`, *optional*, defaults to `1024`):
                Number of token sequences to keep in buffer.
            chars_per_token (`int`, *optional*, defaults to `3.6`):
                Number of characters per token used to estimate number of tokens in text buffer.
            eos_token_id (`int`, *optional*, defaults to `0`):
                Id of the end of sequence token if the passed tokenizer does not have an EOS token.
            shuffle ('bool', *optional*, defaults to True)
                Shuffle the examples before they are returned
    """

    def __init__(
        self,
        tokenizer,
        dataset,
        dataset_text_field=None,
        formatting_func=None,
        infinite=False,
        seq_length=1024,
        num_of_sequences=1024,
        chars_per_token=3.6,
        eos_token_id=0,
        shuffle=True,
    ):
        self.tokenizer = tokenizer

        if tokenizer.eos_token_id is None:
            warnings.warn(
                "The passed tokenizer does not have an EOS token. We will use the passed eos_token_id instead which corresponds"
                f" to {eos_token_id}. If this is not the correct EOS token, make sure to pass the correct eos_token_id."
            )

        self.concat_token_id = tokenizer.eos_token_id if tokenizer.eos_token_id else eos_token_id
        self.dataset = dataset
        self.seq_length = seq_length
        self.infinite = infinite
        self.current_size = 0
        self.max_buffer_size = seq_length * chars_per_token * num_of_sequences
        self.shuffle = shuffle
        if formatting_func is None:
            self.formatting_func = lambda x: x[dataset_text_field]
        else:
            self.formatting_func = formatting_func

        if formatting_func is not None:
            formatting_func_signature = formatting_func.__code__.co_varnames
            if len(formatting_func_signature) > 1:
                warnings.warn(
                    "The passed formatting_func has more than one argument. Usually that function should have a single argument `example`"
                    " which corresponds to the dictionary returned by each element of the dataset. Make sure you know what you are doing."
                )

    def __len__(self):
        return len(self.dataset)

    def __iter__(self):
        iterator = iter(self.dataset)
        more_examples = True
        while more_examples:
            buffer, buffer_len = [], 0
            while True:
                if buffer_len >= self.max_buffer_size:
                    break
                try:
                    buffer.append(self.formatting_func(next(iterator)))
                    buffer_len += len(buffer[-1])
                except StopIteration:
                    if self.infinite:
                        iterator = iter(self.dataset)
                        warnings.warn("The dataset reached end and the iterator is reset to the start.")
                    else:
                        more_examples = False
                        break
            tokenized_inputs = self.tokenizer(buffer, truncation=False)["input_ids"]
            all_token_ids = []
            for tokenized_input in tokenized_inputs:
                all_token_ids.extend(tokenized_input + [self.concat_token_id])
            examples = []
            for i in range(0, len(all_token_ids), self.seq_length):
                input_ids = all_token_ids[i : i + self.seq_length]
                if len(input_ids) == self.seq_length:
                    examples.append(input_ids)
            if self.shuffle:
                random.shuffle(examples)
            for example in examples:
                self.current_size += 1
                yield {
                    "input_ids": torch.LongTensor(example),
                    "labels": torch.LongTensor(example),
                }


class PeftSavingCallback(TrainerCallback):
    def on_save(self, args, state, control, **kwargs):
        if args.should_save:
            checkpoint_path = os.path.join(args.output_dir, f"checkpoint-{state.global_step}")
            kwargs["model"].save_pretrained(checkpoint_path)

            if "pytorch_model.bin" in os.listdir(checkpoint_path):
                os.remove(os.path.join(checkpoint_path, "pytorch_model.bin"))


class RunningMoments:
    def __init__(self, accelerator):
        """
        Calculates the running mean and standard deviation of a data stream. Reference:
        https://github.com/OpenLMLab/MOSS-RLHF/blob/40b91eb2f2b71b16919addede0341d2bef70825d/utils.py#L75
        """
        self.mean = 0
        self.std = 1
        self.var = 1
        self.count = 1e-24
        self.accelerator = accelerator

    @torch.no_grad()
    def update(self, xs: torch.Tensor) -> Tuple[float, float]:
        """
        Updates running moments from batch's moments computed across ranks
        """
        if self.accelerator.use_distributed:
            xs_mean, xs_var, xs_count = get_global_statistics(self.accelerator, xs)
        else:
            xs_count = xs.numel()
            xs_var, xs_mean = torch.var_mean(xs, unbiased=False)
        xs_mean, xs_var = xs_mean.float(), xs_var.float()

        delta = xs_mean - self.mean
        tot_count = self.count + xs_count

        new_sum = xs_var * xs_count
        # correct old_sum deviation accounting for the new mean
        old_sum = self.var * self.count + delta**2 * self.count * xs_count / tot_count
        tot_sum = old_sum + new_sum

        self.mean += delta * xs_count / tot_count
        self.var = tot_sum / tot_count
        self.std = (self.var * tot_count / (tot_count - 1)).float().sqrt()
        self.count = tot_count

        return xs_mean.item(), (xs_var * xs_count / (xs_count - 1)).float().sqrt().item()


@torch.no_grad()
def get_global_statistics(accelerator, xs: torch.Tensor, mask=None, device="cpu") -> Tuple[float, float, int]:
    """
    Computes element-wise mean and variance of the tensor across processes. Reference:
    https://github.com/OpenLMLab/MOSS-RLHF/blob/40b91eb2f2b71b16919addede0341d2bef70825d/utils.py#L57C1-L73C75
    """
    xs = xs.to(accelerator.device)
    sum_and_count = torch.tensor([xs.sum(), (xs.numel() if mask is None else mask.sum())], device=xs.device)
    sum_and_count = accelerator.reduce(sum_and_count)
    global_sum, count = sum_and_count
    global_mean = global_sum / count

    sum_var = torch.sum(((xs - global_mean) ** 2).mul(1 if mask is None else mask))
    sum_var = accelerator.reduce(sum_var)
    global_var = sum_var / count

    return global_mean.to(device), global_var.to(device), count.to(device)


def compute_accuracy(eval_pred) -> Dict[str, float]:
    predictions, labels = eval_pred
    # Here, predictions is rewards_chosen and rewards_rejected.
    # We want to see how much of the time rewards_chosen > rewards_rejected.
    predictions = np.argmax(predictions, axis=1)

    accuracy = np.array(predictions == labels, dtype=float).mean().item()
    return {"accuracy": accuracy}


def pad_to_length(tensor: torch.Tensor, length: int, pad_value: Union[int, float], dim: int = -1) -> torch.Tensor:
    if tensor.size(dim) >= length:
        return tensor
    else:
        pad_size = list(tensor.shape)
        pad_size[dim] = length - tensor.size(dim)
        return torch.cat(
            [
                tensor,
                pad_value * torch.ones(*pad_size, dtype=tensor.dtype, device=tensor.device),
            ],
            dim=dim,
        )


def disable_dropout_in_model(model: torch.nn.Module) -> None:
    for module in model.modules():
        if isinstance(module, torch.nn.Dropout):
            module.p = 0


def exact_div(a, b, a_str, b_str, custom_error_message=""):
    q = a // b
    if a != q * b:
        raise ValueError(f"{custom_error_message}, {a_str}={a}, {b_str}={b}, inexact division: {a} / {b} = {a / b}")
    return q


# copied from https://github.com/kvablack/ddpo-pytorch/blob/main/ddpo_pytorch/stat_tracking.py#L5
class PerPromptStatTracker:
    r"""
    Class for tracking statistics per prompt. Mainly used to calculate advantage for the DPPO algorithm

    Args:
        buffer_size (`int`):
            Size of the buffer to keep for each prompt.
        min_count (`int`):
            Minimum number of samples to keep in the buffer before calculating the mean and std.
    """

    def __init__(self, buffer_size, min_count):
        self.buffer_size = buffer_size
        self.min_count = min_count
        self.stats = {}

    def update(self, prompts, rewards):
        prompts = np.array(prompts)
        rewards = np.array(rewards)
        unique = np.unique(prompts)
        advantages = np.empty_like(rewards)
        for prompt in unique:
            prompt_rewards = rewards[prompts == prompt]
            if prompt not in self.stats:
                self.stats[prompt] = deque(maxlen=self.buffer_size)
            self.stats[prompt].extend(prompt_rewards)

            if len(self.stats[prompt]) < self.min_count:
                mean = np.mean(rewards)
                std = np.std(rewards) + 1e-6
            else:
                mean = np.mean(self.stats[prompt])
                std = np.std(self.stats[prompt]) + 1e-6
            advantages[prompts == prompt] = (prompt_rewards - mean) / std

        return advantages

    def get_stats(self):
        return {k: {"mean": np.mean(v), "std": np.std(v), "count": len(v)} for k, v in self.stats.items()}

Output:
{
    "experimental_code": "import torch\nimport math\nimport numpy as np\n\n# Assuming trl.core.masked_whiten is available, or implemented as follows:\ndef masked_whiten(values, mask, shift_mean=True):\n    # Simplified for extraction, actual implementation may vary slightly.\n    # Computes mean and variance only over masked elements before whitening.\n    mean = (values * mask).sum() / mask.sum()\n    centered_values = values - mean\n    variance = (centered_values**2 * mask).sum() / mask.sum()\n    if mask.sum() > 1:\n        variance = variance * (mask.sum() / (mask.sum() - 1)) # Bessel's correction\n    else:\n        variance = torch.ones_like(variance) # Avoid division by zero if only one element\n    whitened = (values - mean) * torch.rsqrt(variance + 1e-8)\n    if not shift_mean:\n        whitened += mean\n    return whitened\n\n# --- From examples/IMDB/training/soft_risk_ppo_auth1_2.py ---\n# This function dynamically calculates the number of worst-case trajectories (alpha_N_ceil) to consider in a batch\n# based on the current training epoch, total epochs, and specified risk parameters.\n# It implements a soft-risk scheduling mechanism, gradually adjusting the effective batch size for policy updates\n# to focus on lower-return trajectories as training progresses.\n# Parameters like args_risk_alpha, args_risk_rho, etc. simulate access to a ScriptArguments object.\ndef get_current_risk_level(\n    alpha_cfg: float,\n    rho_cfg: float,\n    m: int,\n    M: int,\n    args_risk_alpha: float,\n    args_risk_rho: float,\n    args_risk_n: int,\n    args_ppo_config_batch_size: int,\n    args_risk_scheduler: str\n) -> int:\n    alpha = args_risk_alpha  # User-defined risk alpha from ScriptArguments (e.2, 0.2)\n    rho = args_risk_rho      # User-defined risk rho from ScriptArguments (e.g., 0.8)\n\n    if args_risk_scheduler == \"old\": \n        if m <= args_risk_n:\n            return args_ppo_config_batch_size # Use full batch size initially\n        else:\n            # Gradual reduction based on the 'old' schedule\n            return math.ceil(args_ppo_config_batch_size * max(alpha, 1 - (1 - alpha) * (m - args_risk_n) / (rho * M)))   \n    else: # This branch is specifically targeted by the 'new_schedule' aspect of the original file path\n        if m <= args_risk_n: # Initial phase (e.g., if current_epoch <= risk_n)\n            val = args_ppo_config_batch_size # Use full batch size\n        elif m >= math.ceil(rho * M): # Final phase (e.g., if current_epoch >= rho * total_epochs)\n            val = math.ceil(alpha * args_ppo_config_batch_size) # Fixed minimum batch size\n        else: # Intermediate gradual reduction phase\n            K = (1 - alpha) / (math.ceil(rho * M) - args_risk_n) # Slope for linear decrease\n            val = math.ceil(args_ppo_config_batch_size * max(alpha, 1 - K * (m - args_risk_n)))\n        return val\n\n# --- From trl/trainer/ppo_trainer.py (modified for soft-risk PPO) ---\n# The following functions implement the Conditional Value at Risk (CVaR) mechanism\n# by selectively masking out higher-return trajectories during policy optimization.\n\n# This method identifies a CVaR threshold and updates the batch mask.\n# 'risk_alpha' (an integer, here 'alpha_N_ceil') specifies the number of lowest-return\n# trajectories to keep. The mask is modified to zero out elements of trajectories\n# whose total return is above the calculated threshold.\ndef find_alpha_return(\n    traj_returns: torch.FloatTensor,\n    mask: torch.FloatTensor,\n    risk_alpha: int,\n) -> tuple[torch.FloatTensor, torch.FloatTensor]:\n    # Sort returns to identify the threshold for worst-case scenarios\n    sorted_returns, _ = torch.sort(traj_returns)\n    # The threshold is the (risk_alpha-1)-th smallest return in the batch.\n    # All trajectories with returns less than or equal to this are considered \"worst-case\".\n    q_risk_alpha = sorted_returns[risk_alpha-1]\n\n    # Apply the mask: trajectories with returns greater than the threshold are excluded\n    for ii in range(traj_returns.shape[0]):\n        if traj_returns[ii] > q_risk_alpha:\n            mask[ii,:] = torch.zeros_like(mask[ii,:], dtype=int)\n\n    return q_risk_alpha, mask\n\n# This method calculates generalized advantage estimation (GAE) but integrates\n# the risk-averse selection logic. After computing initial returns, it calls\n# `find_alpha_return` to modify the mask, ensuring only specified worst-case\n# trajectories contribute to the final advantage calculation.\ndef compute_advantages(\n    values: torch.FloatTensor,\n    rewards: torch.FloatTensor,\n    mask: torch.FloatTensor,\n    risk_alpha: int, # This is 'alpha_N_ceil' from get_current_risk_level\n    config_gamma: float, \n    config_lam: float,\n) -> tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:\n    lastgaelam = 0\n    advantages_reversed = []\n    gen_len = rewards.shape[-1]\n\n    values = values * mask\n    rewards = rewards * mask\n\n    for t in reversed(range(gen_len)):\n        nextvalues = values[:, t + 1] if t < gen_len - 1 else 0.0\n        delta = rewards[:, t] + config_gamma * nextvalues - values[:, t]\n        lastgaelam = delta + config_gamma * config_lam * lastgaelam\n        advantages_reversed.append(lastgaelam)\n    advantages = torch.stack(advantages_reversed[::-1]).transpose(0, 1)\n\n    returns = advantages + values\n    # Assuming the last token index (63) represents the end of a response in a 64-token sequence for total return.\n    traj_returns = returns[:,63] \n\n    # Modify the mask based on the calculated CVaR threshold, filtering out non-worst-case trajectories.\n    _, masks = find_alpha_return(traj_returns, mask, risk_alpha)\n\n    # Whiten advantages using the *newly modified* mask to focus on selected trajectories.\n    advantages = masked_whiten(advantages, masks)\n    advantages = advantages.detach()\n    return values, advantages, returns, masks\n\n# Simplified representation of how these functions are integrated into the training loop:\n# In the main training loop (e.g., 'for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):')\n#   current_epoch = epoch\n#   total_training_epochs = 194 # This value is specifically used in the example script.\n#   # ScriptArguments values would be passed for args_risk_alpha, etc.\n#   # For this specific example, let's assume:\n#   # args_risk_alpha = 0.2\n#   # args_risk_rho = 0.8\n#   # args_risk_n = 240\n#   # args_ppo_config_batch_size = 128\n#   # args_risk_scheduler = 'new_schedule'\n#   # alpha_N_ceil is calculated here, which represents the effective number of samples to consider.\n#   alpha_N_ceil_computed = get_current_risk_level(\n#       alpha_cfg=0.05, rho_cfg=0.98, m=current_epoch, M=total_training_epochs,\n#       args_risk_alpha=0.2, args_risk_rho=0.8, args_risk_n=240, args_ppo_config_batch_size=128,\n#       args_risk_scheduler='new_schedule'\n#   )\n#\n#   # Then, this alpha_N_ceil_computed value is passed as 'risk_alpha' to the ppo_trainer's step method.\n#   # stats = ppo_trainer.step(query_tensors, response_tensors, rewards, alpha_N_ceil_computed)\n",
    "experimental_info": "The methodology adapts the Risk-Averse Reinforcement Learning (RARL) approach to the standard Reinforcement Learning with Human Feedback (RLHF) pipeline, optimizing the Conditional Value at Risk (CVaR) of the return instead of the expected value. It employs a soft-risk scheduling mechanism, inspired by the CeSoR algorithm, to gradually introduce risk-aversion. \n\n**Key Experimental Settings:**\n- **Base Policy Initialization**: The approach starts with a supervised fine-tuned (SFT) base policy. The specific model checkpoint used for this base is `/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_final/2024-01-24_21-17-41/checkpoint-100`.\n- **Reward Model**: A sentiment analysis pipeline `sentiment-analysis:lvwerra/distilbert-imdb` serves as the reward model.\n- **Optimization Algorithm**: Proximal Policy Optimization (PPO) is used for policy updates. The policy operates in an Actor-Critic setting with a base transformer extended by a language modeling head (actor) and a value function head (critic).\n- **KL-Divergence Regularization**: The `kl_penalty` is set to `\"kl\"` in the `PPOConfig`, which adds a KL-Divergence term to the reward function to regularize against the base policy.\n- **Risk-Aversion Mechanism (CVaR)**:\n    - **Soft-Risk Scheduling**: The system uses a `\"new_schedule\"` for introducing risk-aversion, dynamically adjusting the effective batch size used for policy updates based on training progress. This schedule is controlled by the following parameters:\n        - `risk_n`: `240`. This is the number of initial training steps/epochs where the full batch size is utilized, delaying the introduction of explicit risk-aversion.\n        - `risk_alpha`: `0.2`. This target risk level determines the percentage of worst-performing trajectories (e.g., 20% of lowest returns) that will be selected for policy updates during the fully risk-averse phase.\n        - `risk_rho`: `0.8`. This parameter dictates that the soft risk level will reach its target `risk_alpha` after 80% of the total policy training iterations.\n        - The total policy training steps (`M`) for the scheduling function is `194` (as observed in the example script `soft_risk_ppo_auth1_2.py`).\n    - **Dynamic Batch Size**: The `get_current_risk_level` function calculates `alpha_N_ceil` for each training step. This value starts at the full `batch_size` (`128`) for the first `240` epochs (`risk_n`). It then gradually decreases, reaching `alpha * batch_size` (approximately `0.2 * 128 = 26` trajectories) around `0.8 * 194 = 155` epochs.\n    - **Trajectory Selection (CVaR Optimization)**: The `find_alpha_return` method sorts trajectories by their total returns (specifically, the return at index 63, implying a fixed response length of 64 tokens) and identifies a threshold `q_risk_alpha`. Subsequently, the training `mask` is modified to exclude (set to zero) all trajectories whose returns are strictly greater than this `q_risk_alpha`. This procedure effectively prunes the training batch to focus policy updates exclusively on the `alpha_N_ceil` worst-performing trajectories (lowest returns), directly optimizing for CVaR.\n- **General Training Parameters**:\n    - `learning_rate`: `1.41e-5`\n    - `mini_batch_size`: `128`\n    - `batch_size`: `128` (This is the logical batch size before dynamic adjustment)\n    - `gradient_accumulation_steps`: `1`\n    - `early_stopping`: `False`\n    - `target_kl`: `6.0`\n    - `seed`: `0`\n    - `steps`: `128000` (Total PPO training steps configured, although the `M` parameter for risk scheduling is set to 194 in the example script, suggesting a shorter effective scheduling horizon or an inconsistency.)\n    - `logging_dir`: Typically `/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/imdb_final_1/sft_p_new_schedule/{date_n_time}`."
}
