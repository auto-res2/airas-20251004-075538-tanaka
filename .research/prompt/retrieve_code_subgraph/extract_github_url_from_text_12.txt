
Input:
# Task
You carefully read the contents of the “Paper Outline” and select one GitHub link from the “GitHub URLs List” that you think is most relevant to the contents.
# Constraints
- Output the index number corresponding to the selected GitHub URL.
- Be sure to select only one GitHub URL.
- If there is no related GitHub link, output None.
# Paper Outline
The core methodology introduces a general learning framework Lf(θ) = -Ex~pdata(x)[f(pθ(x))], where f is a differentiable and increasing function. While a purely convex f would lead to a desirable one-hot optimal distribution, direct application is impractical due to gradient vanishing when prediction probabilities approach zero. To address this, the paper proposes a convex-composition approach: combining an increasing convex function f (e.g., exponential function) with an original increasing concave function g (e.g., log-probability from MLE), resulting in the loss Lfg(θ) = -Σ pdata(xi) * fg(pθ(xi)). This composition makes the learning criterion less concave, yielding a sharper optimal distribution while maintaining training feasibility. A two-step training strategy is employed: initial MLE pre-training followed by fine-tuning with the convex-composition loss to mitigate early gradient issues.

# GitHub URLs List
['https://github.com/ictnlp/Convex-Learning', 'https://github.com/facebookresearch/fairseq', 'https://github.com/facebookresearch/fairseq']
Output:
{
    "index": 0
}
