
Input:
You are a researcher with expertise in engineering in the field of machine learning.

# Instructions
- The content described in “Repository Content” corresponds to the GitHub repository of the method described in “Method.”
- Please extract the following two pieces of information from “Repository Content”:
    - experimental_code：Extract the implementation sections that are directly related to the method described in “Method.”
    - experimental_info：Extract and output the experimental settings related to the method described in “Method.”

# Method
ScaleGrad modifies the standard MLE objective's gradient to encourage the use of novel tokens. The core idea is to maintain a dynamic set of 'novel tokens' at each decoding step during training, defined as tokens not yet observed in the current ground-truth sequence. The method then re-normalizes the softmax output probabilities: probabilities of novel tokens are scaled down by a hyper-parameter γ (where γ ∈ (0,1)), while probabilities of non-novel tokens are effectively scaled up. This re-scaling dynamically alters the gradients such that for a ground-truth token that is also novel, its gradient norm is increased, compelling the model to assign even higher probability to it. Conversely, for a non-ground-truth token that is non-novel, its gradient norm is increased, pushing the model to assign much lower probability. This mechanism encourages novelty without compromising the learning of target tokens. The paper also provides a gradient-level comparison with Unlikelihood (UL) training, highlighting ScaleGrad's robustness against potential issues where UL might decrease ground-truth token probabilities in certain conditions.

# Repository Content
File Path: custom/__init__.py
Content:
# Copyright (c) Facebook, Inc. and its affiliates.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
#
import importlib
import os

for file in os.listdir(os.path.dirname(__file__)):
    if file.endswith('.py') and not file.startswith('_'):
        task_name = file[:file.find('.py')]
        importlib.import_module('fairseq.custom.' + task_name)

File Path: custom/_evaluation.py
Content:
# Copyright (c) Facebook, Inc. and its affiliates.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
#

from fairseq import options, sequence_generator
from fairseq.custom import evaluate_utils
import argparse
from glob import glob
import os.path
import getpass
import sys
import shlex
import pickle
import tensorflow as tf
tf.logging.set_verbosity(tf.logging.ERROR)


def main():
    script_parser = argparse.ArgumentParser(description='Computes greedy completion, single-token prediction, and corresponding targets.')
    script_parser.add_argument('--data-dir', type=str, required=True)
    script_parser.add_argument('--base-dir', type=str, required=True)
    script_parser.add_argument('--eval-mode', choices=['all', 'completion', 'singletoken'], default='all')
    script_parser.add_argument('--data-prefix-length', type=int, default=50, help='Length of prefix')
    script_parser.add_argument('--batch-size-completions', type=int, default=128)
    script_parser.add_argument('--batch-size-single-prediction', type=int, default=1024)

    script_parser.add_argument('--completion-length', type=int, default=500,
                               help='The length of each generated sequence, not counting the prefix length')
    script_parser.add_argument('--model-path', type=str, required=True, help='The path to the folder with checkpoints')
    script_parser.add_argument('--save-path', type=str, required=True)
    script_parser.add_argument('--ckpt', choices=['best', 'last', 'all', 'step', 'epoch'], default='best')
    script_parser.add_argument('--ckpt-step', type=str, default=None)
    script_parser.add_argument('--ckpt-epoch', type=str, default=None)
    script_parser.add_argument('--data-split', choices=['train', 'valid', 'test'], default='valid')
    script_parser.add_argument('--num-samples', type=int, default=-1)
    script_parser.add_argument('--beam-size', type=int, default=1)
    script_parser.add_argument('--beam-ngram-block', type=int, default=0)
    script_parser.add_argument('--topp', type=float, default=0.0)
    script_parser.add_argument('--topk', type=int, default=1)
    script_parser.add_argument('--singletoken-topk', type=int, default=1)
    script_parser.add_argument('--singletoken-topp', type=float, default=0.0)

    high_level_args = script_parser.parse_args()

    if high_level_args.ckpt == 'last':
        checkpoints = glob(os.path.join(high_level_args.model_path, 'checkpoint_last.pt'))
    elif high_level_args.ckpt == 'best':
        checkpoints = glob(os.path.join(high_level_args.model_path, 'checkpoint_best.pt'))
    elif high_level_args.ckpt == 'step':
        checkpoints = glob(os.path.join(high_level_args.model_path, 'checkpoint_*_{}.pt'.format(high_level_args.ckpt_step)))
    elif high_level_args.ckpt == 'epoch':
        checkpoints = glob(
            os.path.join(high_level_args.model_path, 'checkpoint{}.pt'.format(high_level_args.ckpt_epoch)))
    elif high_level_args.ckpt == 'all':
        checkpoints = glob(os.path.join(high_level_args.model_path, 'checkpoint*'))

    print("Evaluating {} checkpoints.".format(len(checkpoints)))
    for i, checkpoint in enumerate(checkpoints):

        if high_level_args.eval_mode in ['all', 'completion']:
            num_tokens = high_level_args.data_prefix_length*high_level_args.batch_size_completions
            FAIRSEQ_OPTS = "--data {} \
                            --task language_modeling_with_generation \
                            --path {} \
                            --tokens-per-sample {} \
                            --max-tokens {} \
                            --sample-break-mode none \
                            --gen-subset {} \
                            --user-dir {}".format(high_level_args.data_dir, checkpoint,
                                                  num_tokens, num_tokens, high_level_args.data_split,
                                                  os.path.join(high_level_args.base_dir, 'fairseq/custom'))
            sys.argv = shlex.split(FAIRSEQ_OPTS)
            parser = options.get_generation_parser()
            args = options.parse_args_and_arch(parser)
            args.add_bos_token = False
            args.skip_invalid_size_inputs_valid_test = False

            task, model, generator, itr, step = evaluate_utils.load(args)

            task.dictionary.eos_index = len(task.dictionary) - 1
            task.dictionary.eos_word = task.dictionary.symbols[-1]

            fairseq_generator = sequence_generator.SequenceGenerator(tgt_dict=task.dictionary,
                                                                     beam_size=high_level_args.beam_size,
                                                                     no_repeat_ngram_size=high_level_args.beam_ngram_block,
                                                                     max_len_b=high_level_args.completion_length+high_level_args.data_prefix_length,
                                                                     )

            filename_suffix = '_{}__st_{}__spl_{}__pfx_{}__cmpl_{}__bs_cmpl_{}__bs_sprd_{}__bms_{}__bnb_{}__tpk_{}__tpp_{}__sttpk_{}__sttpp_{}__ckst_{}__ckep_{}__ckpt_{}'.format(
                os.path.basename(os.path.normpath(high_level_args.model_path)),
                step, high_level_args.data_split, high_level_args.data_prefix_length, high_level_args.completion_length,
                high_level_args.batch_size_completions, high_level_args.batch_size_single_prediction,
                high_level_args.beam_size, high_level_args.beam_ngram_block, high_level_args.topk, high_level_args.topp, high_level_args.singletoken_topk,
                high_level_args.singletoken_topp, high_level_args.ckpt_step, high_level_args.ckpt_epoch,
                high_level_args.ckpt)

            completions, gen_metrics, actual_metrics = evaluate_utils.generate_completions(
                model, generator, fairseq_generator, itr,
                high_level_args.data_prefix_length,
                high_level_args.completion_length,
                topk=high_level_args.topk,
                beam_size=high_level_args.beam_size,
                num_samples=high_level_args.num_samples,
                topp=high_level_args.topp)

            completion_tokens = [[task.dictionary[i] for i in sample] for sample in completions]
            completion_text = [' '.join(ts) for ts in completion_tokens]

            # dump generation to text file
            completion_output_filename = os.path.join(high_level_args.save_path,
                                                      'completions_{}.txt'.format(filename_suffix))
            with open(completion_output_filename, 'w') as f:
                for line in completion_text:
                    f.write(line)
                    f.write('\n')
                print("\tcompletions output file: %s" % completion_output_filename)


        if high_level_args.eval_mode in ['all', 'singletoken']:
            num_tokens = high_level_args.batch_size_single_prediction
            FAIRSEQ_OPTS = "--data {} \
                                        --task language_modeling_with_generation \
                                        --path {} \
                                        --tokens-per-sample {} \
                                        --max-tokens {} \
                                        --sample-break-mode none \
                                        --gen-subset {} \
                                        --user-dir {}".format(high_level_args.data_dir, checkpoint,
                                                              num_tokens, num_tokens, high_level_args.data_split,
                                                              os.path.join(high_level_args.base_dir,
                                                                           'fairseq/custom'))
            sys.argv = shlex.split(FAIRSEQ_OPTS)
            parser = options.get_generation_parser()
            args = options.parse_args_and_arch(parser)
            args.add_bos_token = False
            args.skip_invalid_size_inputs_valid_test = False

            task, model, generator, itr, step = evaluate_utils.load(args)

            single_predicted_tokens, target_tokens, metrics = evaluate_utils.eval_single_token_prediction(
                model, itr, task.target_dictionary, singletoken_topk=high_level_args.singletoken_topk,
                singletoken_topp=high_level_args.singletoken_topp)

            subset_metrics = {}
            subset_data = high_level_args.data_split

            for metric_name, value in metrics.items():
                subset_metrics[f'{subset_data}/{metric_name}'] = value
            subset_metrics['checkpoint_step'] = step

            filename_suffix = '_{}__st_{}__spl_{}__pfx_{}__cmpl_{}__bs_cmpl_{}__bs_sprd_{}__bms_{}__bnb_{}__tpk_{}__tpp_{}__sttpk_{}__sttpp_{}__ckst_{}__ckep_{}__ckpt_{}'.format(
                os.path.basename(os.path.normpath(high_level_args.model_path)),
                step, high_level_args.data_split, high_level_args.data_prefix_length, high_level_args.completion_length,
                high_level_args.batch_size_completions, high_level_args.batch_size_single_prediction,
                high_level_args.beam_size, high_level_args.beam_ngram_block, high_level_args.topk, high_level_args.topp,
                high_level_args.singletoken_topk,
                high_level_args.singletoken_topp, high_level_args.ckpt_step, high_level_args.ckpt_epoch,
                high_level_args.ckpt)

            single_token_predictions_filename = os.path.join(high_level_args.save_path, "single_token_predictions_{}.txt".format(filename_suffix))

            pkl_filename = os.path.join(high_level_args.save_path, "metrics_{}.pkl".format(filename_suffix))
            pickle.dump(subset_metrics, open(pkl_filename, 'wb'))

            with open(single_token_predictions_filename, 'w') as f:
                for single_predicted_tokens_sublist in single_predicted_tokens:
                    _single_token_text = [task.dictionary[i] for i in single_predicted_tokens_sublist]
                    f.write(' '.join(_single_token_text))
                    f.write('\n')

            target_filename = os.path.join(high_level_args.save_path, "targets_{}.txt".format(filename_suffix))

            with open(target_filename, 'w') as f:
                for target_tokens_sublist in target_tokens:
                    _target_text = [task.dictionary[i] for i in target_tokens_sublist]
                    f.write(' '.join(_target_text))
                    f.write('\n')


if __name__ == '__main__':
    main()

File Path: custom/baseline_cross_entropy.py
Content:
# Copyright (c) Facebook, Inc. and its affiliates.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
#

import math
import torch.nn.functional as F

from fairseq import utils

from fairseq.criterions import FairseqCriterion, register_criterion
from fairseq.custom.metrics import TrainingMetrics


@register_criterion('cross_entropy_wcustom_metrics')
class CrossEntropyCriterionWCustomMetrics(FairseqCriterion):

    def __init__(self, args, task):
        super().__init__(args, task)

    def forward(self, model, sample, reduce=True, compute_custom_metrics=True):
        """Compute the loss for the given sample.

        Returns a tuple with three elements:
        1) the loss
        2) the sample size, which is used as the denominator for the gradient
        3) logging outputs to display while training
        """
        net_output = model(**sample['net_input'])
        logits = net_output[0].view(-1, net_output[0].size(-1))
        target = model.get_targets(sample, net_output)
        target = target.view(-1)
        loss, _ = self.compute_loss(model, net_output, sample, reduce=reduce)
        sample_size = sample['target'].size(0) if self.args.sentence_avg else sample['ntokens']

        true_token_logits = -F.nll_loss(
            logits,
            target,
            ignore_index=self.padding_idx,
            reduction='none',
        )
        orig = utils.strip_pad(target, self.padding_idx)
        ntokens = orig.numel()

        logging_output = {
            'loss': utils.item(loss.data) if reduce else loss.data,
            'ntokens': sample['ntokens'],
            'nsentences': sample['target'].size(0),
            'sample_size': sample_size,
        }
        if compute_custom_metrics:
            custom_output = TrainingMetrics.ranking_metrics(logits, true_token_logits, sample, ntokens, target)
            for k, v in custom_output.items():
                logging_output[k] = v
        return loss, sample_size, logging_output

    def compute_loss(self, model, net_output, sample, reduce=True):
        lprobs = model.get_normalized_probs(net_output, log_probs=True)
        lprobs = lprobs.view(-1, lprobs.size(-1))
        target = model.get_targets(sample, net_output).view(-1)
        loss = F.nll_loss(
            lprobs,
            target,
            ignore_index=self.padding_idx,
            reduction='sum' if reduce else 'none',
        )
        return loss, loss

    @staticmethod
    def aggregate_logging_outputs(logging_outputs):
        """Aggregate logging outputs from data parallel training."""
        loss_sum = sum(log.get('loss', 0) for log in logging_outputs)
        ntokens = sum(log.get('ntokens', 0) for log in logging_outputs)
        nsentences = sum(log.get('nsentences', 0) for log in logging_outputs)
        sample_size = sum(log.get('sample_size', 0) for log in logging_outputs)
        agg_output = {
            'loss': loss_sum / sample_size / math.log(2) if sample_size > 0 else 0.,
            'ntokens': ntokens,
            'nsentences': nsentences,
            'sample_size': sample_size,
        }
        from fairseq.custom.metrics import TrainingMetrics
        custom_output = TrainingMetrics.aggregate_and_normalize(logging_outputs)
        for k, v in custom_output.items():
            agg_output[k] = v

        if sample_size != ntokens:
            agg_output['nll_loss'] = loss_sum /ntokens / math.log(2) if ntokens > 0 else 0.
        return agg_output

File Path: custom/candidate_penalty_ce_loss.py
Content:
# Copyright (c) Facebook, Inc. and its affiliates.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
#

import math
import torch.nn.functional as F
import torch

from fairseq import utils

from fairseq.criterions import FairseqCriterion, register_criterion
from fairseq.custom.metrics import TrainingMetrics


@register_criterion('candidate_penalty_cross_entropy')
class CandidatePenaltyCrossEntropyCriterion(FairseqCriterion):
    """Applies a (1-p(x_nt)) loss to each negative target ('candidate') x_nt."""

    def __init__(self, args, task):
        super().__init__(args, task)
        self.rank_alpha = args.rank_alpha
        self.candidate_type = args.candidate_type

    def forward(self, model, sample, reduce=True, compute_custom_metrics=True):
        net_output = model(**sample['net_input'])
        target = model.get_targets(sample, net_output)
        nsentences = target.size(0)
        target = target.view(-1)

        # -- mle loss
        lprobs = model.get_normalized_probs(net_output, log_probs=True)
        lprobs = lprobs.view(-1, lprobs.size(-1))
        true_token_lprobs = F.nll_loss(
            lprobs,
            target,
            ignore_index=self.padding_idx,
            reduction='none',
        )
        mle_loss = true_token_lprobs.sum()

        # -- custom loss
        # Maximize (1 - p(x_nt)) for negative target tokens x_nt (equivalently minimize -log(1-p(x_nt)))

        # - form negative targets
        with torch.no_grad():
            # E.g. DABCC | D | EFFGD => {A,B,C} are negative targets.
            if self.candidate_type == 'prev_context':
                # Make 'the triangle'.
                ctx_cands = target.unsqueeze(0).expand(target.size(0), target.size(0))
                ctx_cands_ = (ctx_cands.tril(-1) + self.padding_idx)
                ctx_cands_ = ctx_cands_ * ctx_cands_.triu()
                ctx_cands = ctx_cands.tril(-1) + ctx_cands_

                # Don't include the target for that timestep as a negative target.
                ctx_cands = ctx_cands.masked_fill(ctx_cands == target.unsqueeze(1), self.padding_idx)
                negative_targets = torch.zeros_like(lprobs).scatter_(1, ctx_cands, 1)
            else:
                raise NotImplementedError('candidate type %s' % self.candidate_type)

        # - compute loss
        one_minus_probs = torch.clamp((1.0 - lprobs.exp()), min=1e-5)

        custom_loss = -torch.log(one_minus_probs)*negative_targets
        custom_loss = custom_loss.sum()

        loss = mle_loss + self.rank_alpha * custom_loss

        # -- metrics
        logits = net_output[0].view(-1, net_output[0].size(-1))
        true_token_logits = -F.nll_loss(
            logits,
            target,
            ignore_index=self.padding_idx,
            reduction='none',
        )

        orig = utils.strip_pad(target, self.padding_idx)
        ntokens = orig.numel()
        sample_size = sample['target'].size(0) if self.args.sentence_avg else ntokens

        logging_output = {
            'custom_loss': utils.item(custom_loss.data),
            'loss': utils.item(mle_loss.data),
            'ntokens': ntokens,
            'nsentences': nsentences,
            'sample_size': sample_size,
        }
        if compute_custom_metrics:
            custom_output = TrainingMetrics.ranking_metrics(logits, true_token_logits, sample, ntokens, target)
            for k, v in custom_output.items():
                logging_output[k] = v

        return loss, sample_size, logging_output

    @staticmethod
    def aggregate_logging_outputs(logging_outputs):
        """Aggregate logging outputs from data parallel training."""
        loss_sum = sum(log.get('loss', 0) for log in logging_outputs)
        custom_loss_sum = sum(log.get('custom_loss', 0) for log in logging_outputs)
        ntokens = sum(log.get('ntokens', 0) for log in logging_outputs)
        nsentences = sum(log.get('nsentences', 0) for log in logging_outputs)
        sample_size = sum(log.get('sample_size', 0) for log in logging_outputs)

        agg_output = {
            'loss': loss_sum / sample_size / math.log(2) if sample_size > 0 else 0.,
            'custom_loss': custom_loss_sum / sample_size / math.log(2) if sample_size > 0 else 0.,
            'ntokens': ntokens,
            'nsentences': nsentences,
            'sample_size': sample_size,
        }

        from fairseq.custom.metrics import TrainingMetrics
        custom_output = TrainingMetrics.aggregate_and_normalize(logging_outputs)
        for k, v in custom_output.items():
            agg_output[k] = v

        if sample_size != ntokens:
            agg_output['nll_loss'] = loss_sum / ntokens / math.log(2) if ntokens > 0 else 0.
        return agg_output

File Path: custom/evaluate_utils.py
Content:
# Copyright (c) Facebook, Inc. and its affiliates.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
#
import torch
from fairseq import checkpoint_utils, options, progress_bar, tasks, utils
from fairseq.meters import StopwatchMeter
from fairseq.custom.metrics import Metrics, TrainingMetrics
from tqdm import tqdm
import torch.nn.functional as F
import math

def load(args, task=None, itr=None, generator=None, log=False):
    """Returns task, model, generator, and dataset iterator for the given `args`."""
    assert args.path is not None, '--path required for generation!'
    import random
    random.seed(42)
    torch.manual_seed(42)
    utils.import_user_module(args)
    if log:
        print(args)

    use_cuda = torch.cuda.is_available() and not args.cpu

    # Load dataset splits
    if task is None:
        task = tasks.setup_task(args)
        task.load_dataset(args.gen_subset)

    # Load ensemble
    if log:
        print('| loading model(s) from {}'.format(args.path))
    models, _model_args = checkpoint_utils.load_model_ensemble(
        args.path.split(':'),
        arg_overrides=eval(args.model_overrides),
        task=task,
    )

    # Optimize ensemble for generation
    for model in models:
        model.make_generation_fast_(
            beamable_mm_beam_size=None if args.no_beamable_mm else args.beam,
            need_attn=args.print_alignment,
        )
        if args.fp16:
            model.half()
        if use_cuda:
            model.cuda()
    model = models[0]

    if itr is None:
        # Load dataset (possibly sharded)
        itr = task.get_batch_iterator(
            dataset=task.dataset(args.gen_subset),
            max_tokens=args.max_tokens,
            max_sentences=args.max_sentences,
            max_positions=args.tokens_per_sample,
            ignore_invalid_inputs=args.skip_invalid_size_inputs_valid_test,
            required_batch_size_multiple=args.required_batch_size_multiple,
            num_shards=args.num_shards,
            shard_id=args.shard_id,
            num_workers=args.num_workers,
        ).next_epoch_itr(shuffle=False)

    # Get model step
    step = torch.load(args.path)['optimizer_history'][-1]['num_updates']

    if generator is None:
        # Initialize generator
        generator = task.build_generator(args)
    return task, model, generator, itr, step


def generate_completions(model, generator, fairseq_generator, itr, eval_prefix_length, eval_completion_length, topk, topp, num_samples, beam_size, include_prefix=True):
    completions = []
    completion_metrics = Metrics()
    actual_metrics = Metrics()
    for n, sample in enumerate(tqdm(itr)):
        input_sequence = sample['net_input']['src_tokens']
        prefix_batch = batch_input_sequence_by_prefix_length(input_sequence, eval_prefix_length)
        prefix_batch = prefix_batch.cuda()
        if input_sequence.size(1) < eval_prefix_length:
            continue
        if beam_size > 1:
            assert topk == 1, 'with greedy topk must be 1'
            assert topp == 0.0, 'with greedy topp must be 0'
            sample['net_input']['src_tokens'] = prefix_batch
            res = fairseq_generator.generate([model], sample, prefix_batch, bos_token=0)  # prefix is there in preds!
            pred_completion = [res[i][0]['tokens'][eval_prefix_length:-1].cpu().tolist() for i in range(len(res))]
        elif beam_size == 1:
            pred_completion = generator.generate_completion(model, prefix_batch, eval_completion_length, topk, topp)
            pred_completion = pred_completion.cpu().tolist()
        completion_metrics.update(pred_completion)
        actual_metrics.update(input_sequence)

        if include_prefix:
            prefix_batch = prefix_batch.cpu().tolist()
            pred_completion = [prefix + completion for
                               prefix, completion in zip(prefix_batch, pred_completion)]
        completions.extend(pred_completion)

        if n == num_samples:
            break

    completion_metrics = completion_metrics.report('generated')
    actual_metrics = actual_metrics.report('actual')
    return completions, completion_metrics, actual_metrics


def batch_input_sequence_by_prefix_length(input_sequence, prefix_length):
    seq_len = input_sequence.size(1)
    # Discard tokens if the sequence length is not divisible by the prefix length.
    new_seq_len = (seq_len//prefix_length)*prefix_length
    input_sequence = input_sequence[:, :new_seq_len]
    batch = input_sequence.view(-1, prefix_length).contiguous()
    return batch


@torch.no_grad()
def eval_single_token_prediction(model, itr, dictionary, singletoken_topp=0.0, singletoken_topk=1):
    predicted_tokens = []
    target_tokens = []

    mle_loss_sum = 0
    num_samples_sum = 0
    wrong_mass_sum = 0

    logging_outputs = []

    for n, sample in tqdm(enumerate(itr)):
        sample = utils.move_to_cuda(sample)
        net_output = model(**sample['net_input'])
        logits = net_output[0][0]
        logits[:, dictionary.pad()] = -1e19
        predicted_tokens.append(logits.argmax(1).tolist())
        target = sample['target'].view(-1)
        target_tokens.append(target.tolist())

        # -- mle loss
        lprobs = model.get_normalized_probs(net_output, log_probs=True)
        lprobs = lprobs.view(-1, lprobs.size(-1))
        true_token_lprobs = F.nll_loss(
            lprobs,
            target,
            ignore_index=dictionary.pad_index,
            reduction='none',
        )
        true_token_logits = -F.nll_loss(
            logits,
            target,
            ignore_index=dictionary.pad_index,
            reduction='none',
        )
        mle_loss = true_token_lprobs.sum()
        orig = utils.strip_pad(target, dictionary.pad_index)
        ntokens = orig.numel()

        mle_loss_sum += mle_loss.item()
        num_samples_sum += ntokens

        logging_output = TrainingMetrics.ranking_metrics(logits, true_token_logits, sample, ntokens, target, topk=singletoken_topk, topp=singletoken_topp)

        negative_targets = (logits > true_token_logits[:, None]).float()
        wrong_mass_sum += (negative_targets * (F.softmax(logits, dim=1))).sum()

        logging_outputs.append(logging_output)

    ppl = math.pow(2, mle_loss_sum / num_samples_sum / math.log(2))
    custom_metrics = TrainingMetrics.aggregate_and_normalize(logging_outputs)
    custom_metrics['ppl'] = ppl
    avg_wrong_mass = wrong_mass_sum / num_samples_sum
    custom_metrics['avg_wrong_mass'] = avg_wrong_mass.item()
    return predicted_tokens, target_tokens, custom_metrics
File Path: custom/gpt2/run_gpt2.py
Content:
"""
Adapted from https://github.com/facebookresearch/unlikelihood_training/

Modiefied on Thu Oct 1 13:16:22 2020

@author: Xiang Lin
"""
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
#
import argparse
import logging
import json
import os
import re
import random

import torch
import torch.nn.functional as F
import numpy as np

from pytorch_transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW, WarmupLinearSchedule, WEIGHTS_NAME, CONFIG_NAME
from torch.utils.data import TensorDataset, SequentialSampler, DataLoader, RandomSampler

from fairseq.custom.metrics import TrainingMetrics, Metrics, ngram_metrics
from fairseq.custom.baseline_cross_entropy import CrossEntropyCriterionWCustomMetrics
from fairseq.custom.sequence_penalty_loss import SequencePenaltyCriterion
from fairseq.custom.evaluate_utils import batch_input_sequence_by_prefix_length

from collections import defaultdict
from tqdm import tqdm, trange
from pprint import pprint

RETOK = re.compile(r'\w+|[^\w\s]|\n', re.UNICODE)

logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',
                    datefmt='%m/%d/%Y %H:%M:%S',
                    level=logging.INFO)
logger = logging.getLogger(__name__)


def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):
    """ Filter a distribution of logits using top-k and/or nucleus (top-p) filtering
        Args:
            logits: logits distribution shape (vocabulary size)
            top_k > 0: keep only top k tokens with highest probability (top-k filtering).
            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).
                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)
        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317
    """
    assert logits.size(0) == 1  # batch size 1 for now - could be updated for more but the code would be less clear
    logits = logits.squeeze(0)
    top_k = min(top_k, logits.size(-1))  # Safety check
    if top_k > 0:
        # Remove all tokens with a probability less than the last token of the top-k
        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]
        logits[indices_to_remove] = filter_value

    if top_p > 0.0:
        sorted_logits, sorted_indices = torch.sort(logits, descending=True)
        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)

        # Remove tokens with cumulative probability above the threshold
        sorted_indices_to_remove = cumulative_probs > top_p
        # Shift the indices to the right to keep also the first token above the threshold
        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
        sorted_indices_to_remove[..., 0] = 0

        indices_to_remove = sorted_indices[sorted_indices_to_remove]
        logits[indices_to_remove] = filter_value
    return logits


def get_datasets(dataset_paths, max_len=1536):
    """Args:
        dataset_paths: {'train': str, 'valid': str, 'test': str}
    """
    datasets = {}

    for split, fname in dataset_paths.items():
        tensor = torch.load(fname)
        right_bound = (tensor.size(0) // (max_len+1)) * (max_len + 1)
        dataset = TensorDataset(tensor[:right_bound].view(-1, (max_len+1)))
        datasets[split] = dataset

    return datasets


def sample_sequence(model, prefix_batch, prefix_length, continuation_length, top_k, top_p):
    continuation_logits = []
    context = prefix_batch
    assert context.size(1) == prefix_length

    prev = context
    output = context
    past = None
    for i in range(continuation_length):
        logits, past = model(prev, past=past)
        logits = logits[:, -1, :]
        if top_k == 1 and top_p == 0:
            prev = logits.argmax(dim=1, keepdim=True)
        else:
            filtered_logits = top_k_top_p_filtering(logits, top_k=top_k, top_p=top_p)
            prev = F.softmax(filtered_logits, dim=-1).multinomial(num_samples=1)

        continuation_logits.append(logits)
        output = torch.cat((output, prev), dim=1)

    continuation_logits = torch.stack(continuation_logits, 1)
    return output, continuation_logits


def getNovelMask(target, vocab_size):
    b,l = target.size()
    zeros = torch.zeros(b,l,vocab_size).to(target.device)
    ones = torch.ones(b,l,vocab_size).to(target.device)

    target_index = target.unsqueeze(1).expand(b,l,l).transpose(-2,-1).triu().transpose(-2,-1)
    matrix = zeros.scatter_add_(2, target_index, ones)
    matrix[:,:,0] = 0
    summ_true = torch.tensor(range(1,l+1)).unsqueeze(0).float().to(target.device)
    summ_now = torch.sum(matrix,dim=-1)
    diff = summ_true - summ_now
    matrix[:,:,0] = diff
    matrix = torch.cat((torch.zeros(b,1,vocab_size).to(target.device),matrix[:,:-1,:]),1)
    novel_mask = matrix < 1.

    return novel_mask



def sg_loss(model, batch, args):
    longer_sample = batch[0].cuda()
    inp = longer_sample[:, :args.train_batch_size]
    model_output = model(inp)
    target = longer_sample[:, 1:]
    logits = model_output[0]

    # ScaleGrad
    ##########################################################
    probs = F.softmax(logits,dim=-1) 
    # Obtaining the masks for novel tokens
    novel_mask = getNovelMask(target[0].unsqueeze(0),logits.size(-1))
    rep_mask = ~novel_mask

    new_probs = probs * novel_mask * args.gamma + probs * rep_mask + 1e-8
    new_probs = F.normalize(new_probs,p=1,dim=-1)
    lprobs = torch.log(new_probs)
    ##########################################################


    assert lprobs.size(0) == 1, 'We work on flat sequences'
    loss = F.nll_loss(lprobs[0], target[0], reduction='sum')
    true_token_logits = -F.nll_loss(logits[0], target[0], reduction='none')
    ntokens = inp.numel()

    logging_output = TrainingMetrics.ranking_metrics(logits[0], true_token_logits, None, ntokens, target[0])
    logging_output['loss'] = loss.item()
    logging_output['normalizer'] = ntokens
    logging_output['sample_size'] = ntokens
    logging_output['ntokens'] = ntokens

    loss = loss / ntokens
    return loss, logging_output



def ngram_repeat_mask(xs, n):
    mask = torch.zeros_like(xs)
    for i, x in enumerate(xs):
        seen = set()
        xl = x.tolist()
        for j in range(len(x)-n):
            ng = tuple(xl[j:j+n])
            if ng in seen:
                mask[i, j:j+n] = 1
            seen.add(ng)
    return mask


def tokenize(text):
    # ref: https://github.com/facebookresearch/ParlAI/blob/4da3ec0bdcf1db2c3a5bd5723d1275c32a891192/parlai/core/dict.py#L451
    return RETOK.findall(text)


def get_text_continuation(bpe_completion, tokenizer, args):
    completion = tokenizer.decode(bpe_completion)
    bpe_prefix, bpe_continuation = bpe_completion[:args.prefix_length], bpe_completion[args.prefix_length:]
    prefix = tokenizer.decode(bpe_prefix)

    if prefix in completion:
        continuation = completion.replace(prefix, '')
    else:
        prefix_ = ' '.join(prefix.split(' ')[:-2])
        continuation = completion.replace(prefix_, '')

    continuation_tokens = tokenize(continuation)
    return continuation_tokens


def save_completion_metrics(bpe_metrics, word_metrics, text_completions, config, uniq, args):
    outfile = os.path.join(args.output_dir,
                           'completion__{model}__spl_{split}__topk_{topk}__topp_{topp}__pfl_{pfl}__cnl_{cnl}'.format(
                               model=args.model_name,
                               split=args.eval_split,
                               topk=args.top_k,
                               topp=args.top_p,
                               pfl=args.prefix_length,
                               cnl=args.continuation_length
                           ))
    json.dump({'bpe_metrics': bpe_metrics,
               'word_metrics': word_metrics,
               'uniq':uniq,
               'config': config,
               'completions': text_completions}, open(outfile + '.json', 'w'))
    print("%s metrics written to %s" % (args.mode, outfile + '.json'))


def save_singletoken_metrics(metrics, config, args, best=False, train_iter=None):
    output_dir = args.output_dir if not best else os.path.join(args.output_dir, 'best')
    outfile = os.path.join(output_dir,
                           'singletoken__{model}__spl_{split}__bsz_{bsz}{iter}.json'.format(
                               model=args.model_name,
                               split=args.eval_split,
                               bsz=args.batch_size_singletoken,
                               iter='_%d' % train_iter if train_iter is not None else '',
                           ))

    json.dump({'metrics': metrics,
               'config': config}, open(outfile, 'w'))
    print("%s metrics written to %s" % (args.mode, outfile))


def eval_singletoken(model, args, dataset_paths, train_iter=None):
    datasets = get_datasets(dataset_paths, max_len=args.batch_size_singletoken)
    eval_sampler = SequentialSampler(datasets[args.eval_split])
    eval_dataloader = DataLoader(datasets[args.eval_split], sampler=eval_sampler, batch_size=1)

    model.eval()

    logging_outputs = []
    predicted_tokens = []
    target_tokens = []
    with torch.no_grad():
        for i, batch in tqdm(enumerate(eval_dataloader), desc="Evaluating", total=len(eval_dataloader)):
            longer_sample = batch[0].cuda()
            inp = longer_sample[:, :args.batch_size_singletoken]
            model_output = model(inp)
            target = longer_sample[:, 1:]
            logits = model_output[0]
            lprobs = F.log_softmax(logits, dim=-1)
            assert lprobs.size(0) == 1, 'We work on flat sequences'
            loss = F.nll_loss(lprobs[0], target[0], reduction='sum')
            true_token_logits = -F.nll_loss(logits[0], target[0], reduction='none')

            pred = lprobs.argmax(dim=-1).view(-1).tolist()
            predicted_tokens.extend(pred)
            ntokens = inp.numel()

            logging_output = TrainingMetrics.ranking_metrics(logits[0], true_token_logits, None, ntokens, target[0])
            logging_output['loss'] = loss.item()
            logging_output['normalizer'] = ntokens
            logging_output['sample_size'] = ntokens
            logging_output['ntokens'] = ntokens
            logging_outputs.append(logging_output)

            # for human uniq
            target_tokens.extend(target.view(-1).tolist())

    logging_average = CrossEntropyCriterionWCustomMetrics.aggregate_logging_outputs(logging_outputs)
    logging_average['ppl'] = 2 ** logging_average['loss']
    logging_average['uniq'] = len(set(predicted_tokens))
    logging_average['human_uniq'] = len(set(target_tokens))

    save_singletoken_metrics(logging_average, model.config.to_dict(), args, train_iter=train_iter)
    return logging_average


def main():
    parser = argparse.ArgumentParser(description='openGPT-2 analysis')

    parser.add_argument('--mode', choices=['train', 'eval-singletoken', 'eval-completion', 'eval-both'], default='eval-singletoken')
    parser.add_argument('--eval-split', choices=['train', 'valid', 'test'])
    parser.add_argument('--model-name', choices=['gpt2', 'gpt2-medium', 'gpt2-large'], default='gpt2-medium')
    parser.add_argument('--model-load-dir', type=str, default=None)
    parser.add_argument('--seed', type=int, default=42)
    parser.add_argument('--data-base', type=str)
    parser.add_argument('--num-train-epochs', type=int, default=1)
    parser.add_argument('--batch-size-singletoken', type=int, default=1024)
    parser.add_argument('--batch-size-completion', type=int, default=300)
    parser.add_argument("--output-dir", default=None, type=str, required=True,
                        help="The output directory where the model predictions and checkpoints will be written.")

    # eval-completion
    parser.add_argument('--prefix-length', type=int, default=50)
    parser.add_argument('--continuation-length', type=int, default=100)
    parser.add_argument('--top-k', type=int, default=1)
    parser.add_argument('--top-p', type=float, default=0.0)

    # custom training
    # parser.add_argument('--sequence-tune-rate', type=float, default=0.5)
    parser.add_argument('--train-batch-size', type=int, default=300)
    parser.add_argument('--report-metrics-every', type=int, default=10)
    parser.add_argument('--save-every', type=int, default=1000)
    # parser.add_argument('--sequence-ngram-n', type=int, default=4)
    parser.add_argument('--train-n-steps', type=int, default=10000)
    parser.add_argument('--validate-every', type=int, default=10000)
    parser.add_argument('--gamma', type=float, default=1.)

    # training loop
    parser.add_argument("--adam-epsilon", default=1e-8, type=float,
                        help="Epsilon for Adam optimizer.")
    parser.add_argument('--max-grad-norm', type=int, default=1)
    parser.add_argument("--max-steps", default=-1, type=int,
                        help="If > 0: set total number of training \
                            steps to perform. Override num_train_epochs.")
    parser.add_argument('--gradient-accumulation-steps', type=int, default=1,
                        help="Number of updates steps to accumulate before\
                            performing a backward/update pass.")
    parser.add_argument('--learning-rate', type=float, default=6.25e-5)
    parser.add_argument("--warmup-steps", default=0, type=int,
                        help="Linear warmup over warmup_steps.")
    parser.add_argument('--lr-schedule', type=str, default='warmup_linear')
    parser.add_argument('--weight-decay', type=float, default=0.01)
    parser.add_argument('--lm-coef', type=float, default=0.9)

    args = parser.parse_args()
    print(args)

    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    torch.cuda.manual_seed_all(args.seed)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    n_gpu = torch.cuda.device_count()
    logger.info("device: {}, n_gpu {}".format(device, n_gpu))

    if not os.path.exists(args.output_dir):
        os.makedirs(args.output_dir)

    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

    dataset_paths = {
        'train': os.path.join(args.data_base, 'train_tokens_bpe_gpt2.pt'),
        'valid': os.path.join(args.data_base, 'valid_tokens_bpe_gpt2.pt'),
        'test': os.path.join(args.data_base, 'test_tokens_bpe_gpt2.pt'),
    }

    if args.model_load_dir:
        model = GPT2LMHeadModel.from_pretrained(args.model_load_dir)
    else:
        model = GPT2LMHeadModel.from_pretrained(args.model_name)
    model.to(device)

    if args.mode == 'eval-singletoken' or args.mode == 'eval-both':
        eval_singletoken(model, args, dataset_paths)

    if args.mode == 'eval-completion' or args.mode == 'eval-both':
        datasets = get_datasets(dataset_paths, max_len=args.batch_size_completion)
        eval_sampler = SequentialSampler(datasets[args.eval_split])
        eval_dataloader = DataLoader(datasets[args.eval_split], sampler=eval_sampler, batch_size=1)

        model.eval()

        with torch.no_grad():
            all_text_completions = []

            bpe_ngram_metrics = Metrics(pad=-1)
            word_ngram_metrics = Metrics(pad=-1)
            allwords = []

            for i, batch in tqdm(enumerate(eval_dataloader), desc="Evaluating", total=len(eval_dataloader)):
                input_sequence = batch[0].cuda()
                if input_sequence.size(1) < args.prefix_length:
                    continue

                # Predict the completions.
                batch = batch_input_sequence_by_prefix_length(input_sequence, args.prefix_length)
                bpe_completions, _ = sample_sequence(model, batch, args.prefix_length, args.continuation_length, args.top_k, args.top_p)
                bpe_completions = bpe_completions.tolist()

                # Extract continuations from the predicted completions.
                bpe_continuations = []
                text_continuations = []

                # Calulating uniq words in continuation
                

                for bpe_completion in bpe_completions:
                    bpe_continuations.append(bpe_completion[args.prefix_length:])
                    text_continuations.append(get_text_continuation(bpe_completion, tokenizer, args))
                    all_text_completions.append(tokenizer.decode(bpe_completion))
                    allwords.extend(tokenizer.decode(bpe_completion[args.prefix_length:]).split(' '))
                # Only keep continuations with at least one 4-gram
                # (A short continuation may occur due to predicted whitespace, then tokenizing, despite being
                #  normal length in BPE tokens).
                text_continuations = [c for c in text_continuations if len(c) > 3]

                # Update metrics with this batch of continuations.
                bpe_ngram_metrics.update(bpe_continuations)
                word_ngram_metrics.update(text_continuations)

                # update the no of unique words used
                nouniqtokens = len(set(allwords))

                # Save the (possibly intermediate) metrics.
                save_completion_metrics(bpe_metrics=bpe_ngram_metrics.report('bpe_%s' % args.eval_split),
                                        word_metrics=word_ngram_metrics.report('word_%s' % args.eval_split),
                                        text_completions=all_text_completions,
                                        config=model.config.to_dict(),
                                        uniq=nouniqtokens,
                                        args=args)

    if args.mode == 'train':
        if not os.path.exists(os.path.join(args.output_dir, 'best')):
            os.makedirs(os.path.join(args.output_dir, 'best'))

        token_loss = sg_loss
        datasets = get_datasets(dataset_paths, max_len=args.train_batch_size)
        train_sampler = RandomSampler(datasets['train'])
        train_seq_dataloader = DataLoader(datasets['train'], sampler=train_sampler, batch_size=1)

        # Setup optimizer
        if args.max_steps > 0:
            t_total = args.max_steps
            args.num_train_epochs = args.max_steps // (len(train_seq_dataloader) // args.gradient_accumulation_steps) + 1
        else:
            t_total = len(train_seq_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs

        param_optimizer = list(model.named_parameters())
        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']
        optimizer_grouped_parameters = [
            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': args.weight_decay},
            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}
            ]
        optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)
        scheduler = WarmupLinearSchedule(optimizer, warmup_steps=args.warmup_steps, t_total=t_total)

        total_steps = 0
        best_ppl = 1e20
        for _ in trange(args.num_train_epochs, desc="Epoch"):
            logging_outputs = []
            epoch_loss = 0
            epoch_steps = 0
            tqdm_bar = tqdm(train_seq_dataloader, desc="Training", total=args.train_n_steps)
            for step, batch in enumerate(tqdm_bar):
                optimizer.zero_grad()

 
                loss, batch_metrics = token_loss(model, batch, args)

                loss.backward()
                optimizer.step()
                scheduler.step()
                epoch_loss += loss.item()
                epoch_steps += 1
                total_steps += 1
                tqdm_bar.desc = "Training loss: {:.2e} lr: {:.2e}".format(epoch_loss/epoch_steps, scheduler.get_lr()[0])

                logging_outputs.append(batch_metrics)

                if epoch_steps % args.report_metrics_every == 0:
                    logging_average = CrossEntropyCriterionWCustomMetrics.aggregate_logging_outputs(logging_outputs)
                    temp = SequencePenaltyCriterion.aggregate_logging_outputs(logging_outputs)
                    for k, v in temp.items():
                        logging_average[k] = v
                    logging_average['ppl'] = 2 ** logging_average['loss']
                    print(logging_average)
                    logging_outputs = []

                if step == args.train_n_steps:
                    break

                if epoch_steps % args.save_every == 0:
                    model_to_save = model.module if hasattr(model, 'module') else model
                    output_model_file = os.path.join(args.output_dir, WEIGHTS_NAME)
                    output_config_file = os.path.join(args.output_dir, CONFIG_NAME)
                    torch.save(model_to_save.state_dict(), output_model_file)
                    model_to_save.config.to_json_file(output_config_file)
                    tokenizer.save_vocabulary(args.output_dir)

                if total_steps % args.validate_every == 0:
                    print("Validating...")
                    validation_outputs = eval_singletoken(model, args, dataset_paths, train_iter=total_steps)
                    if validation_outputs['ppl'] < best_ppl:
                        best_ppl = validation_outputs['ppl']
                        model_to_save = model.module if hasattr(model, 'module') else model
                        output_model_file = os.path.join(args.output_dir, 'best', WEIGHTS_NAME)
                        output_config_file = os.path.join(args.output_dir, 'best', CONFIG_NAME)
                        torch.save(model_to_save.state_dict(), output_model_file)
                        model_to_save.config.to_json_file(output_config_file)
                        tokenizer.save_vocabulary(os.path.join(args.output_dir, 'best'))
                        save_singletoken_metrics(validation_outputs, model.config.to_dict(), args,
                                                 train_iter=total_steps, best=True)


if __name__ == '__main__':
    main()

File Path: custom/language_modeling_with_generation.py
Content:
# Copyright (c) Facebook, Inc. and its affiliates.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
#

from fairseq.tasks import register_task
from fairseq.tasks.language_modeling import LanguageModelingTask
from fairseq.custom.sequence_penalty_loss import SequencePenaltyCriterion
import torch


@register_task('language_modeling_with_generation')
class LanguageModelingWithGenerationTask(LanguageModelingTask):
    """
    Train a language model, with generation-based evaluation.
    See `LanguageModelingTask` for args.
    """
    def __init__(self, args, dictionary, output_dictionary=None, targets=None):
        super().__init__(args, dictionary, output_dictionary=output_dictionary, targets=targets)
        self._train_step = 1
        self._compute_metrics_interval = args.compute_metrics_interval
        self._sequence_level_train_rate = args.sequence_level_train_rate

        if self._sequence_level_train_rate > 0.0:
            self.sequence_criterion = self.build_sequence_criterion(args)
        self.generator = self.build_generator(args)

    def build_sequence_criterion(self, args):
        return SequencePenaltyCriterion(args, self)

    def start_epoch(self):
        self._train_step = 1

    @staticmethod
    def add_args(parser):
        parser.add_argument('--compute-metrics-interval', type=int, default=250,
                            help='compute custom metrics in the criterion once every `compute-metrics-interval` batches')
        parser.add_argument('--sequence-level-train-rate', type=float, default=0.0,
                            help='proportion of training steps to perform sequence level training')
        # - candidate_penalty
        parser.add_argument('--candidate-type', choices=['prev_context'],
                      default='prev_context')
        parser.add_argument('--rank-alpha', type=float)
        # - sequence
        parser.add_argument('--sequence-ngram-n', type=int, default=1)
        parser.add_argument('--sequence-prefix-length', type=int, default=16)
        parser.add_argument('--sequence-completion-length', type=int, default=48)
        parser.add_argument('--sequence-candidate-type', choices=['repeat', 'random'], default='repeat')
        parser.add_argument('--mask-p', type=float, default=0.5)

        # fmt: off
        LanguageModelingTask.add_args(parser)

    def build_generator(self, args):
        from fairseq.custom.sequence_generator import SequenceGenerator
        return SequenceGenerator(
            self.target_dictionary,
            temperature=1.0,
        )

    def aggregate_logging_outputs(self, logging_outputs, criterion):
        agg_output = criterion.__class__.aggregate_logging_outputs(logging_outputs)
        if self._sequence_level_train_rate > 0.0:
            seq_agg_output = self.sequence_criterion.__class__.aggregate_logging_outputs(logging_outputs)
            for k, v in seq_agg_output.items():
                agg_output[k] = v
        return agg_output

    def train_step(self, sample, model, criterion, optimizer, ignore_grad=False):
        model.train()

        do_mle_step=True
        # -- sequence level training
        if torch.rand(1).item() < self._sequence_level_train_rate:
            # check if current minibatch has at least one legal prefix, if not, do CE loss
            if sample['net_input']['src_tokens'].size(1) >= self.sequence_criterion.sequence_prefix_length:
                do_mle_step = False

                loss, sample_size, logging_output = self.sequence_criterion(model, sample,
                                                                            generator=self.generator)
                if ignore_grad:
                    loss *= 0
                optimizer.backward(loss)

        # -- normal training
        if do_mle_step:
            compute_custom_metrics = self._train_step % self._compute_metrics_interval == 0
            loss, sample_size, logging_output = criterion(model, sample, compute_custom_metrics=compute_custom_metrics)
            if ignore_grad:
                loss *= 0
            optimizer.backward(loss)

            # only track this for normal training steps, since sequence training always computes it own metrics.
            self._train_step += 1
        return loss, sample_size, logging_output

    def train_step_with_counts(self, sample, model, criterion, optimizer, ignore_grad=False):
        model.train()
        compute_custom_metrics = self._train_step % self._compute_metrics_interval == 0
        loss, sample_size, logging_output = criterion(model, sample, compute_custom_metrics=compute_custom_metrics)
        assert 'best_tokens' in logging_output, 'best greedy tokens should be returned'
        best_tokens = logging_output['best_tokens']
        logging_output.pop('best_tokens')
        if ignore_grad:
            loss *= 0
        optimizer.backward(loss)
        self._train_step += 1
        return loss, sample_size, logging_output, best_tokens

File Path: custom/metrics.py
Content:
# Copyright (c) Facebook, Inc. and its affiliates.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
#

import numpy as np
import torch
import torch.nn.functional as F

from collections import defaultdict, Counter
from fairseq.custom.sequence_generator import SequenceGenerator
from fairseq.custom.sequence_generator import top_k_logits

from fairseq import utils
from nltk import ngrams


class TrainingMetrics(object):
    REPEAT_CONTEXT_LENGTHS = [16, 32, 128, 512]
    METRIC_NAMES = ['target_rank', 'median_target_rank',
                    'hits_at_1', 'hits_at_10']
    for l in REPEAT_CONTEXT_LENGTHS:
        METRIC_NAMES.extend([
            'repeat_at_1/%d' % l,
            'wrong_repeat_at_1/%d' % l,
            'human_repeat_at_1/%d' % l])

    @staticmethod
    def ranking_metrics(logits, true_token_logits, sample, ntokens, targets, topk=1, topp=0.0):
        """Compute summed metrics on a batch."""
        negative_targets = (logits > true_token_logits[:, None]).float()
        negative_targets_count = negative_targets.sum(dim=1)

        target_rank = negative_targets_count.sum()
        median_target_rank = negative_targets_count.median()
        hits_at_1 = (negative_targets_count == 0).sum()
        hits_at_10 = (negative_targets_count < 10).sum()

        logging_output = {
            'target_rank': utils.item(target_rank.data),
            'hits_at_1': utils.item(hits_at_1.data),
            'hits_at_10': utils.item(hits_at_10.data),
            'median_target_rank': utils.item(median_target_rank),  # NOTE: different normalization since it's not a sum
            'normalizer': ntokens
        }

        for l in TrainingMetrics.REPEAT_CONTEXT_LENGTHS:
            total_repeat_at_1, total_wrong_repeat_at_1, total_human_repeat_at_1 = \
                TrainingMetrics.repeat_at_1(logits, targets, context_length=l)

            temp = {'repeat_at_1/%d' % l: utils.item(total_repeat_at_1.data),
                    'wrong_repeat_at_1/%d' % l: utils.item(total_wrong_repeat_at_1.data),
                    'human_repeat_at_1/%d' % l: utils.item(total_human_repeat_at_1.data)
                    }
            for k in temp:
                logging_output[k] = temp[k]

        if topk > 1:
            filtered_topk = top_k_logits(logits, topk)
            softmax_topk = F.softmax(filtered_topk, dim=1)
            true_target_topk_probs = torch.gather(softmax_topk, index=targets[:, None], dim=1).sum()
            logging_output['true_topk_{}_prob'.format(topk)] = true_target_topk_probs.item()
            sum_topk_repeated_probs = 0
            sum_topk_wrepeated_probs = 0
            true_token_zeroed_topk_probs = softmax_topk.clone().scatter_(1, targets[:, None], 0)
            for timestep in range(1, targets.size(0)):
                prev_context = targets[max(0, timestep-128):timestep]
                sum_topk_repeated_probs += torch.gather(softmax_topk[timestep], index=prev_context.unique(), dim=0).sum().item()
                sum_topk_wrepeated_probs += torch.gather(true_token_zeroed_topk_probs[timestep], index=prev_context.unique(), dim=0).sum().item()
            logging_output['repeat_topk_{}'.format(topk)] = sum_topk_repeated_probs
            logging_output['wrepeat_topk_{}'.format(topk)] = sum_topk_wrepeated_probs
            logging_output['nextunique_topk_{}'.format(topk)] = softmax_topk.multinomial(1).view(-1).tolist()

        if topp > 0.0:
            trimmed_topp = SequenceGenerator._sample_topp(SequenceGenerator, F.softmax(logits, dim=1), topp)
            target_mask = (trimmed_topp[1] - targets[:, None].expand(-1, trimmed_topp[1].size(1))) == 0
            true_target_topp_probs = torch.masked_select(trimmed_topp[0], target_mask).sum()
            logging_output['true_topp_{}_prob'.format(topp)] = true_target_topp_probs.item()
            sum_topp_repeated_probs = 0
            sum_topp_wrepeated_probs = 0
            true_token_zeroed_topp_probs = torch.masked_fill(trimmed_topp[0], target_mask, 0)
            for timestep in range(1, targets.size(0)):
                prev_context = targets[max(0, timestep-128):timestep]
                topp_mask = (trimmed_topp[1][timestep][:, None] == prev_context[None, :]).sum(1).nonzero()
                sum_topp_repeated_probs += torch.gather(trimmed_topp[0][timestep], index=topp_mask.view(-1), dim=0).sum().item()
                sum_topp_wrepeated_probs += torch.gather(true_token_zeroed_topp_probs[timestep], index=topp_mask.view(-1), dim=0).sum().item()
            logging_output['repeat_topp_{}'.format(topp)] = sum_topp_repeated_probs
            logging_output['wrepeat_topp_{}'.format(topp)] = sum_topp_wrepeated_probs
            logging_output['nextunique_topp_{}'.format(topp)] = torch.gather(trimmed_topp[1], index=trimmed_topp[0].multinomial(1), dim=1).view(-1).tolist()

        return logging_output

    @staticmethod
    def repeat_at_1(logits, targets, context_length):
        with torch.no_grad():
            predictions = logits.argmax(1)

            targets = targets.unsqueeze(0)
            T = targets.size(1)
            assert logits.size(0) == T

            # T x T where prev_targets[t, :] = [y_1,...,y_t-1, -1, -1,..., -1]
            prev_targets = targets.expand(T, T).tril().masked_fill_(torch.ones_like(targets.expand(T, T)).byte().triu().bool(), -1)

            # each row t is [-1, ..., -1, y_{t-k-1}, ..., y_{t-1}, -1, ..., -1] where k is context length
            prev_targets = prev_targets.masked_fill_(torch.ones_like(targets.expand(T, T)).byte().tril(-(context_length+1)).bool(), -1)

            repeat_at_1 = (predictions[:, None] == prev_targets)
            has_repeat_at_1 = repeat_at_1.sum(1).gt(0)
            total_repeat_at_1 = has_repeat_at_1.sum()

            is_incorrect = (predictions != targets.view(-1)).view(-1, 1)
            total_wrong_repeat_at_1 = ((repeat_at_1 * is_incorrect).sum(1).gt(0)).sum()

            total_human_repeat_at_1 = (prev_targets == targets.view(T, 1)).sum(1).gt(0).sum()

        return total_repeat_at_1, total_wrong_repeat_at_1, total_human_repeat_at_1

    @staticmethod
    def aggregate_and_normalize(logging_outputs):
        agg_output = {}
        normalizer = sum(log.get('normalizer', 0) for log in logging_outputs)
        if normalizer == 0:
            return agg_output
        for name in TrainingMetrics.METRIC_NAMES:
            # 'mean of medians' special case
            if name == 'median_target_rank':
                agg_output[name] = np.mean([log[name] for log in logging_outputs if name in log])
                continue
            metric_sum = sum(log.get(name, 0) for log in logging_outputs)
            metric = metric_sum / normalizer
            agg_output[name] = metric

        # topk and top-p metrics
        keys = set()
        for log in logging_outputs:
            for k in log:
                if 'true_topk' in k or 'true_topp' in k or 'true_full_prob' in k or 'repeat_top' in k:
                    keys.add(k)
        for k in keys:
            metric_sum = sum(log.get(k, 0) for log in logging_outputs)
            metric = metric_sum / normalizer
            agg_output[k] = metric

        unique_top_keys = set()
        for log in logging_outputs:
            for k in log:
                if 'nextunique' in k:
                    unique_top_keys.add(k)
        for k in unique_top_keys:
            unique_list_of_lists = [log.get(k, []) for log in logging_outputs]
            unique_flat_list = []
            for _sublist in unique_list_of_lists:
                unique_flat_list.extend(_sublist)

            unique_metric = len(set(unique_flat_list))
            agg_output[k] = unique_metric

        return agg_output


class Metrics(object):
    def __init__(self, pad=1):
        self._metrics_list = defaultdict(list)
        self._pad = pad

    def reset(self):
        self._metrics_list = defaultdict(list)

    def update(self, batch_of_token_lists):
        if isinstance(batch_of_token_lists, torch.Tensor):
            batch_of_token_lists = batch_of_token_lists.clone().cpu().tolist()

        for token_list in batch_of_token_lists:
            for k, v in ngram_metrics(token_list, pad=self._pad).items():
                self._metrics_list[k].append(v)

    def report(self, kind='train', round_level=4):
        metrics = {}
        # Normalize list-metrics by taking the mean.
        for k, vs in self._metrics_list.items():
            metrics['%s/%s' % (kind, k)] = round(np.mean(vs), round_level)
        return metrics


def ngram_metrics(token_list, pad=1):
    if pad in token_list:
        token_list = token_list[:token_list.index(pad)]  # remove possible padding
    stats = defaultdict(float)
    for n in range(1, 5):
        ngs = [ng for ng in ngrams(token_list, n)]
        counter = Counter([ng for ng in ngrams(token_list, n)])
        stats['pct_repeat_%dgrams' % n] = 1.0 - len(counter)/len(ngs)
    return stats

File Path: custom/report_metrics.py
Content:
# Copyright (c) Facebook, Inc. and its affiliates.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
#

from fairseq.custom.metrics import Metrics
import argparse
import pickle
import os
from tqdm import tqdm
from glob import glob
import pandas as pd


def merge_dicts(*dict_args):
    result = {}
    for dictionary in dict_args:
        result.update(dictionary)
    return result


def process_files(files):
    _dict = {}
    for i, filename in tqdm(enumerate(files), total=len(files)):
        key, metrics = process_file(filename)
        if key in _dict:
            _dict[key] = merge_dicts(_dict[key], metrics)
        else:
            _dict[key] = metrics

    return _dict


def process_file(filename):
    filetype = os.path.basename(filename).split('__')[0]
    modelname = os.path.basename(filename).split('__')[1]
    if filetype not in ['metrics', 'completions', 'single_token_predictions', 'targets']:
        raise Exception

    key_dict, metrics = get_metric(filename, filetype)
    key_dict['model_name'] = modelname
    key_tuple = tuple(key_dict.items())
    return key_tuple, metrics


def get_metric(filename, filetype):
    # Get all keys from filename.
    key_value_list = '.'.join(os.path.basename(filename).split('.')[:-1]).split('__')[2:]
    setting_dict = {}
    for kv in key_value_list:
        k_v = kv.split('_')
        key = '_'.join(k_v[:-1])
        val = k_v[-1]
        try:
            _maybe_num = int(val)
        except:
            try:
                _maybe_num = float(val)
            except:
                _maybe_num = val
        setting_dict[key] = _maybe_num

    split = setting_dict['spl']
    prefix_length = setting_dict.get('pfx')
    completion_length = setting_dict.get('cmpl')

    if filetype == 'completions':
        completion_lines = open(filename, 'r').readlines()
        ngram_metrics = Metrics()
        actual_completions = []
        flat_completions = []

        for i, line in enumerate(completion_lines):
            splitted_line = line.split()
            assert len(splitted_line) == (prefix_length+completion_length)
            actual_completions.append(splitted_line[prefix_length:])
            flat_completions.extend(splitted_line[prefix_length:])

        ngram_metrics.update(actual_completions)
        num_unique_tokens_completions = len(set(flat_completions))

        result = merge_dicts(ngram_metrics.report(kind=f'{split}'), {f'{split}/num_uniq_compl': num_unique_tokens_completions})

    if filetype == 'targets':
        targets_ngram_metrics = Metrics()
        targets_completions = []  # Slice targets to have same length as completions.
        targets_flat_completions = []
        targets_lines = open(filename, 'r').readlines()

        for line in targets_lines:
            splitted_line = line.split()
            targets_flat_completions.extend(splitted_line)
            segmented_lines = [splitted_line[i*completion_length:i*completion_length+completion_length]
                               for i in range(len(splitted_line) // completion_length)]
            targets_completions.extend(segmented_lines)

        targets_ngram_metrics.update(targets_completions)
        num_unique_target = len(set(targets_flat_completions))

        result = merge_dicts(targets_ngram_metrics.report(kind=f'{split}_human'), {f'{split}/num_uniq_target': num_unique_target})

    if filetype == 'single_token_predictions':
        singlepred_flat = []
        single_prediction_lines = open(filename, 'r').readlines()
        for line in single_prediction_lines:
            singlepred_flat.extend(line.split())

        num_unique_singlepred = len(set(singlepred_flat))

        result = {f'{split}/num_uniq_singletok': num_unique_singlepred}

    if filetype == 'metrics':
        result = pickle.load(open(filename, 'rb'))

    return setting_dict, result


def print_metrics(resulting_dict, mode='pp'):
    if mode == 'pp':
        output = ''
        for k, v in resulting_dict.items():
            if isinstance(v, str):
                output += '{}:\t\t\t{}\n'.format(k,v)
            else:
                output += '{}:\t\t\t{:.{prec}f}\n'.format(k,v,prec=3)
        return output
    elif mode == 'dict':
        return resulting_dict
    elif mode == 'csv':
        keys_list = [] 
        val_list = []
        for k,v in resulting_dict.items():
            keys_list.append(k)
            if isinstance(v, str):
                val_list.append('{}'.format(v))
            else:
                val_list.append('{:.{prec}f}'.format(v,prec=3))
        return ','.join(keys_list), ','.join(val_list)


def find_tuple(tuple_first_val, key):
    for tup in key:
        if tuple_first_val == tup[0]:
            return tup[1]
    return None


def get_dataframe_for_model(_dict):
    big_list = []
    for key_tuples, value_metrics in _dict.items():
        split = find_tuple('spl', key_tuples)
        beam_size = find_tuple('bms', key_tuples)
        beam_block = find_tuple('bnb', key_tuples)
        model_name = find_tuple('model_name', key_tuples)
        topk = find_tuple('tpk', key_tuples)
        topp = find_tuple('tpp', key_tuples)
        
        metrics_to_grab = [f'{split}/pct_repeat_1grams', f'{split}/pct_repeat_4grams', f'{split}/num_uniq_compl', f'{split}/ppl', f'{split}/hits_at_1', f'{split}/repeat_at_1/128', f'{split}/wrong_repeat_at_1/128', f'{split}/num_uniq_singletok']
        per_model_list = [model_name, f'{beam_size}', f'{beam_block}', f'{topk}', f'{topp}', f'{split}']+[value_metrics[m] for m in metrics_to_grab]
        big_list.append(per_model_list)
        
    header = ['model_name', 'beam size', 'beam block', 'topk', 'topp', 'split', 'seq-rep-1', 'seq-rep-4', 'uniq-seq', 'ppl', 'acc', 'rep', 'wrep', 'uniq']
    df = pd.DataFrame(big_list, columns=header)
    
    return df


def main():
    pd.set_option("display.precision", 3)
    parser = argparse.ArgumentParser(description='output postprocessor')

    parser.add_argument('--eval-dirs', nargs='+')
    parser.add_argument('--model-names', nargs='+')

    script_args = parser.parse_args()

    model_to_files = {}

    for model_name in tqdm(script_args.model_names):
        paths = []
        for eval_dir in script_args.eval_dirs:
            paths.extend(glob(os.path.join(eval_dir, '*__{}*__spl_*__*'.format(model_name))))
        model_to_files[model_name] = paths

    model_to_metric_dicts = {}
    for name, list_of_filenames in model_to_files.items():
        model_to_metric_dicts[name] = process_files(list_of_filenames)

    model_to_dataframes = {name: get_dataframe_for_model(_name_dict)
                           for name, _name_dict in model_to_metric_dicts.items()}

    for mname, df in model_to_dataframes.items():
        print('MODEL: {}'.format(mname)+'\n')
        print(df)
        print('\n\n')


if __name__ == '__main__':
    main()

File Path: custom/sequence_generator.py
Content:
# Copyright (c) Facebook, Inc. and its affiliates.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
#

import math

import torch
import torch.nn.functional as F


class SequenceGenerator(object):
    def __init__(self, tgt_dict, temperature=1.):
        self.pad = tgt_dict.pad()
        self.unk = tgt_dict.unk()
        self.eos = tgt_dict.eos()
        self.vocab_size = len(tgt_dict)
        self.temperature = temperature

    def generate_completion_greedy_training(self, model, prefix_tokens, completion_length):
        model.eval()
        pred_toks = []
        context = prefix_tokens
        states = {}
        all_lprobs = []

        # First go over the context.
        for context_step in range(1, context.size(1)):
            _context = context[:, :context_step]
            _ = self._forward_one(model, _context, incremental_states=states, return_logits=True)

        for tstep in range(completion_length):
            lprobs, attn_t = self._forward_one(model, context, incremental_states=states)
            pred_tok = lprobs.argmax(dim=1, keepdim=True)
            pred_toks.append(pred_tok)
            context = torch.cat((context, pred_tok), 1)
            all_lprobs.append(lprobs)

        pred_toks = torch.cat(pred_toks, 1)
        all_lprobs = torch.stack(all_lprobs, 1)
        return pred_toks, all_lprobs

    @torch.no_grad()
    def generate_completion(self, model, prefix_tokens, completion_length, topk, topp):
        """topk: <1 sampling, 1 greedy, >1 top-k sampling."""
        model.eval()
        pred_toks = []
        context = prefix_tokens
        states = {}

        # First go over the context.
        for context_step in range(1, context.size(1)):
            _context = context[:, :context_step]
            _ = self._forward_one(model, _context, incremental_states=states, return_logits=True)

        for tstep in range(completion_length):
            logits, attn_t = self._forward_one(model, context, incremental_states=states, return_logits=True)
            pred_tok = self._topk_decode(logits, topk, topp)
            pred_toks.append(pred_tok)
            context = torch.cat((context, pred_tok), 1)
        pred_toks = torch.cat(pred_toks, 1)
        return pred_toks

    def _sample_topp(self, probs, sampling_topp):
        """Sample among the smallest set of elements whose cumulative probability mass exceeds p.
        See `"The Curious Case of Neural Text Degeneration"
        (Holtzman et al., 2019) <https://arxiv.org/abs/1904.09751>`_.
        Args:
            probs: (bsz x input_beam_size x vocab_size)  IK: here we dont have beam ! so bsz x vocab_size
                the model's log-probabilities over the vocabulary at the current step
        Return: A tuple of (trimed_probs, truncated_indices) where:
            trimed_probs: (bsz x input_beam_size x ?)
                the model's probabilities over the elements selected to sample from. The
                width of the third dimension is determined by top-P.
            truncated_indices: (bsz x input_beam_size x ?)
                the indices of the chosen elements.
        """
        # sort the last dimension (vocab dimension) in descending order
        sorted_probs, sorted_indices = probs.sort(descending=True)

        # compute a mask to indicate the words to be included in the top-P set.
        cumsum_probs = sorted_probs.cumsum(dim=1)
        mask = cumsum_probs.lt(sampling_topp)

        # note that mask was computed by 'lt'. One more word needs to be included
        # so that the cumulative probability mass can exceed p.
        cumsum_mask = mask.cumsum(dim=1)
        last_included = cumsum_mask[:, -1:]
        last_included.clamp_(0, mask.size()[1] - 1)
        mask = mask.scatter_(1, last_included, 1)

        # truncate unnecessary dims.
        max_dim = last_included.max()
        truncated_mask = mask[:, :max_dim + 1]
        truncated_probs = sorted_probs[:, :max_dim + 1]
        truncated_indices = sorted_indices[:, :max_dim + 1]

        # trim the words that are not in top-P by setting their probabilities
        # to 0, so that they would not be sampled later.
        trim_mask = 1 - truncated_mask
        trimed_probs = truncated_probs.masked_fill_(trim_mask, 0)
        return trimed_probs, truncated_indices

    def _topk_decode(self, logits, topk, topp):
        """WARNING!!! This can modify the `self.pad` position of `logits`."""
        if topk == 1 and topp == 0:  # greedy
            logits[:, self.pad] = -math.inf  # as in fairseq code
            pred_tok = logits.argmax(dim=1, keepdim=True)

        else:
            if topk > 1:
                logits[:, self.pad] = -1e10  # never select pad
                logits = top_k_logits(logits, topk)
                pred_tok = torch.softmax(logits, -1).multinomial(1)
            else:
                assert topp > 0.0
                filtered_probs, bookkeep_idx = self._sample_topp(torch.softmax(logits, 1), sampling_topp=topp)
                selected = filtered_probs.multinomial(1)
                pred_tok = torch.gather(bookkeep_idx, index=selected, dim=1)

        return pred_tok

    def _forward_one(self, model, tokens, incremental_states=None, temperature=1., return_attn=False, return_logits=False, **decoder_kwargs):
        if incremental_states is not None:
            decoder_out = list(model.decoder(tokens, None, incremental_state=incremental_states, return_attn=return_attn, **decoder_kwargs))
        else:
            decoder_out = list(model.decoder(tokens, None, return_attn=return_attn, **decoder_kwargs))
        decoder_out[0] = decoder_out[0][:, -1:, :]
        if temperature != 1.:
            decoder_out[0].div_(temperature)
        attn = decoder_out[1]
        if type(attn) is dict:
            attn = attn['attn']
        if attn is not None:
            if type(attn) is dict:
                attn = attn['attn']
            attn = attn[:, :, -1, :]  # B x L x t
        if return_logits:
            logits_t = decoder_out[0][:, -1, :]
            return logits_t, attn
        log_probs = model.get_normalized_probs(decoder_out, log_probs=True)
        log_probs = log_probs[:, -1, :]
        return log_probs, attn


def top_k_logits(logits, k):
    """Masks everything but the k top entries as -infinity (1e10).
    From: https://github.com/huggingface/pytorch-pretrained-BERT"""
    if k == 0:
        return logits
    else:
        values = torch.topk(logits, k)[0]
        batch_mins = values[:, -1].view(-1, 1).expand_as(logits)
        return torch.where(logits < batch_mins, torch.ones_like(logits) * -1e5, logits)
File Path: custom/sequence_penalty_loss.py
Content:
# Copyright (c) Facebook, Inc. and its affiliates.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
#

import math
import torch

from collections import defaultdict
from fairseq import utils

from fairseq.criterions import FairseqCriterion, register_criterion
from fairseq.custom.evaluate_utils import batch_input_sequence_by_prefix_length
from fairseq.custom.metrics import ngram_metrics


@register_criterion('sequence_penalty')
class SequencePenaltyCriterion(FairseqCriterion):
    def __init__(self, args, task):
        super().__init__(args, task)
        self.sequence_ngram_n = args.sequence_ngram_n
        self.sequence_prefix_length = args.sequence_prefix_length
        self.sequence_completion_length = args.sequence_completion_length
        self.sequence_candidate_type = args.sequence_candidate_type
        self.mask_p = args.mask_p

    def forward(self, model, sample, reduce=True, generator=None):
        seq_len = sample['net_input']['src_tokens'].size(1)

        # make total number of tokens equal to the sequence length (for memory purposes)
        n_batches = seq_len // (self.sequence_prefix_length + self.sequence_completion_length)
        batch = batch_input_sequence_by_prefix_length(sample['net_input']['src_tokens'],
                                                      prefix_length=self.sequence_prefix_length)
        batch = batch[:n_batches]

        pred_toks, lprobs = generator.generate_completion_greedy_training(model, batch,
                                                                          completion_length=self.sequence_completion_length)
        if self.sequence_candidate_type == 'repeat':
            mask = ngram_repeat_mask(pred_toks, self.sequence_ngram_n).type_as(lprobs)
        elif self.sequence_candidate_type == 'random':
            mask = torch.bernoulli(torch.zeros_like(pred_toks, dtype=torch.float).fill_(self.mask_p))

        pred_lprobs = lprobs.view(-1, lprobs.size(2)).gather(1, pred_toks.view(-1, 1))
        one_minus_probs = torch.clamp((1.0 - pred_lprobs.exp()), min=1e-20).view(pred_toks.size(0), pred_toks.size(1))
        loss = -torch.log(one_minus_probs)*mask
        loss = loss.sum()

        ntokens = pred_toks.numel()  # number of output tokens (tokens in completions)
        nsentences = batch.size(0)
        sample_size = ntokens
        logging_output = {
            'seq_loss': utils.item(loss.data),
            'seq_ntokens': ntokens,
            'seq_nsentences': nsentences,
            'seq_repeat_mask': utils.item(mask.sum().data),
            'seq_sample_size': sample_size,
        }

        # Sum each statistic, which will be normalized by the number of sentences in `aggregate_logging_outputs`.
        stats = defaultdict(float)
        for tok_list in pred_toks.cpu().tolist():
            ms = ngram_metrics(tok_list)
            for k, v in ms.items():
                stats[k] += v
        for k, v in stats.items():
            logging_output[k] = v

        return loss, sample_size, logging_output

    @staticmethod
    def aggregate_logging_outputs(logging_outputs):
        """Aggregate logging outputs from data parallel training."""
        loss_sum = sum(log.get('seq_loss', 0) for log in logging_outputs)
        ntokens = sum(log.get('seq_ntokens', 0) for log in logging_outputs)
        nsentences = sum(log.get('seq_nsentences', 0) for log in logging_outputs)
        sample_size = sum(log.get('seq_sample_size', 0) for log in logging_outputs)
        repeat_mask = sum(log.get('seq_repeat_mask', 0) for log in logging_outputs)

        agg_output = {
            'seq_loss': loss_sum / max(sample_size, 1.0) / math.log(2),
            'seq_ntokens': ntokens,
            'seq_nsentences': nsentences,
            'seq_sample_size': sample_size,
            'seq_repeat_mask': repeat_mask / sample_size if sample_size > 0 else 0
        }

        for n in range(1, 5):
            key = 'pct_repeat_%dgrams' % n
            ngram_repeats = sum(log.get(key, 0) for log in logging_outputs)
            # Normalize by the number of sentences since this is the sum of per-sentence metrics.
            agg_output['seq_' + key] = ngram_repeats /nsentences if nsentences > 0 else 0

        if sample_size != ntokens:
            agg_output['seq_nll_loss'] = loss_sum / max(ntokens, 1.0) / math.log(2)
        return agg_output


def ngram_repeat_mask(xs, n):
    mask = torch.zeros_like(xs)
    for i, x in enumerate(xs):
        seen = set()
        xl = x.tolist()
        for j in range(len(x)-n):
            ng = tuple(xl[j:j+n])
            if ng in seen:
                mask[i, j:j+n] = 1
            seen.add(ng)
    return mask


File Path: custom/transformer_arch.py
Content:
# Copyright (c) Facebook, Inc. and its affiliates.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
#

from fairseq.models import (
    FairseqLanguageModel,
    register_model,
    register_model_architecture,
)
from fairseq.models.transformer_lm import transformer_lm_big


@register_model_architecture('transformer_lm', 'transformer_lm_ul')
def transformer_lm_baevski_wiki103(args):
    args.decoder_layers = getattr(args, 'decoder_layers', 16)
    args.decoder_attention_heads = getattr(args, 'decoder_attention_heads', 8)
    args.dropout = getattr(args, 'dropout', 0.1)
    args.attention_dropout = getattr(args, 'attention_dropout', 0.0)
    args.activation_dropout = getattr(args, 'activation_dropout', 0.1)
    args.no_decoder_final_norm = getattr(args, 'no_decoder_final_norm', False)
    args.tie_adaptive_proj = getattr(args, 'tie_adaptive_proj', True)
    transformer_lm_big(args)

Output:
{
    "experimental_code": "def getNovelMask(target, vocab_size):\n    b,l = target.size()\n    zeros = torch.zeros(b,l,vocab_size).to(target.device)\n    ones = torch.ones(b,l,vocab_size).to(target.device)\n\n    target_index = target.unsqueeze(1).expand(b,l,l).transpose(-2,-1).triu().transpose(-2,-1)\n    matrix = zeros.scatter_add_(2, target_index, ones)\n    matrix[:,:,0] = 0\n    summ_true = torch.tensor(range(1,l+1)).unsqueeze(0).float().to(target.device)\n    summ_now = torch.sum(matrix,dim=-1)\n    diff = summ_true - summ_now\n    matrix[:,:,0] = diff\n    matrix = torch.cat((torch.zeros(b,1,vocab_size).to(target.device),matrix[:,:-1,:]),1)\n    novel_mask = matrix < 1.\n\n    return novel_mask\n\n\ndef sg_loss(model, batch, args):\n    longer_sample = batch[0].cuda()\n    inp = longer_sample[:, :args.train_batch_size]\n    model_output = model(inp)\n    target = longer_sample[:, 1:]\n    logits = model_output[0]\n\n    # ScaleGrad\n    ##########################################################\n    probs = F.softmax(logits,dim=-1) \n    # Obtaining the masks for novel tokens\n    novel_mask = getNovelMask(target[0].unsqueeze(0),logits.size(-1))\n    rep_mask = ~novel_mask\n\n    new_probs = probs * novel_mask * args.gamma + probs * rep_mask + 1e-8\n    new_probs = F.normalize(new_probs,p=1,dim=-1)\n    lprobs = torch.log(new_probs)\n    ##########################################################\n\n\n    assert lprobs.size(0) == 1, 'We work on flat sequences'\n    loss = F.nll_loss(lprobs[0], target[0], reduction='sum')\n    true_token_logits = -F.nll_loss(logits[0], target[0], reduction='none')\n    ntokens = inp.numel()\n\n    logging_output = TrainingMetrics.ranking_metrics(logits[0], true_token_logits, None, ntokens, target[0])\n    logging_output['loss'] = loss.item()\n    logging_output['normalizer'] = ntokens\n    logging_output['sample_size'] = ntokens\n    logging_output['ntokens'] = ntokens\n\n    loss = loss / ntokens\n    return loss, logging_output",
    "experimental_info": "The ScaleGrad method's implementation (sg_loss) uses a custom 'novel_mask' and a hyperparameter `gamma` to re-normalize probabilities. The `getNovelMask` function dynamically identifies novel tokens within the ground-truth sequence. The main experimental settings for training and evaluation are defined as follows:\n\n**Training Parameters:**\n- `mode`: 'train' (for training ScaleGrad)\n- `model-name`: The GPT-2 model used, e.g., 'gpt2-medium'\n- `seed`: Random seed (default: 42)\n- `data-base`: Base directory for dataset files\n- `num-train-epochs`: Number of training epochs (default: 1)\n- `train-batch-size`: Batch size for training (default: 300)\n- `report-metrics-every`: Frequency to report metrics (default: 10 steps)\n- `save-every`: Frequency to save model checkpoints (default: 1000 steps)\n- `train-n-steps`: Number of training steps (default: 10000)\n- `validate-every`: Frequency for validation (default: 10000 steps)\n- `gamma`: ScaleGrad specific hyperparameter (type: float, default: 1.0). This value is used to scale down probabilities of novel tokens.\n- `adam-epsilon`: Epsilon for Adam optimizer (default: 1e-8)\n- `max-grad-norm`: Maximum gradient norm (default: 1)\n- `max-steps`: Total number of training steps (overrides `num_train_epochs` if > 0, default: -1)\n- `gradient-accumulation-steps`: Number of steps to accumulate gradients (default: 1)\n- `learning-rate`: Optimizer learning rate (default: 6.25e-5)\n- `warmup-steps`: Linear warmup steps (default: 0)\n- `lr-schedule`: Learning rate schedule (default: 'warmup_linear')\n- `weight-decay`: Weight decay for optimizer (default: 0.01)\n- `lm-coef`: Language model coefficient (default: 0.9)\n\n**Evaluation Parameters (for single-token prediction and completion tasks):**\n- `mode`: 'eval-singletoken', 'eval-completion', or 'eval-both'\n- `eval-split`: Dataset split for evaluation ('train', 'valid', 'test')\n- `model-load-dir`: Directory to load a pre-trained model (default: None)\n- `batch-size-singletoken`: Batch size for single-token prediction evaluation (default: 1024)\n- `batch-size-completion`: Batch size for completion evaluation (default: 300)\n- `prefix-length`: Length of the prefix for text completion (default: 50)\n- `continuation-length`: Length of the generated continuation (default: 100)\n- `top-k`: Top-k sampling parameter (default: 1, implying greedy if top_p is 0)\n- `top-p`: Top-p (nucleus) sampling parameter (default: 0.0, implying no nucleus sampling)"
}
