
Input:
You are a researcher with expertise in engineering in the field of machine learning.

# Instructions
- The content described in “Repository Content” corresponds to the GitHub repository of the method described in “Method.”
- Please extract the following two pieces of information from “Repository Content”:
    - experimental_code：Extract the implementation sections that are directly related to the method described in “Method.”
    - experimental_info：Extract and output the experimental settings related to the method described in “Method.”

# Method
The researchers pre-trained over 30 Transformer LMs from scratch with varied model (300M to 32B parameters) and data sizes (33B to 3T tokens), while keeping the data corpus, tokenization, and model architecture (similar to LLaMA with grouped-query attention and rotary position embedding) fixed. They evaluated their downstream performance on 12 diverse English and Chinese datasets, covering tasks like QA, NLI, reading comprehension, coreference resolution, examination, and math word problems, using few-shot, zero-shot, and few-shot CoT prompting. The generality of observations was further validated by analyzing public LLaMA and Pythia models. To address concerns about discontinuous metrics, performance on emergent tasks was also evaluated using continuous metrics like CorrectChoiceProb and Brier Score.

# Repository Content
File Path: app/src/__init__.py
Content:

File Path: app/src/artifacts/__init__.py
Content:

File Path: app/src/artifacts/downloaders/__init__.py
Content:
from .books_downloader import BooksDownloader
from .openwebtext_downloader import OpenWebTextDownloader
from .wikipedia_downloader import WikipediaDownloader
from .ccnet_downloader import CCNetDownloader

File Path: app/src/artifacts/downloaders/books_downloader.py
Content:
from datasets import load_dataset
from pathlib import Path
import random
from tqdm import tqdm

from utilities.io import Writer
from utilities.text.util import generate_paragraphs


class BooksDownloader:
    r""" Loads the RedPajama Books dataset from HuggingFace Datasets and saves
    it to disk """

    dataset_name = "books"
    output_fp = "books/en-books.jsonl.gz"

    def __init__(
            self, lang, out_dir, overwrite, cache_dir, max_samples,
            max_paragraphs_per_sample=200, max_samples_per_book=500
    ):
        self._lang = lang
        self._out_dir = out_dir
        self._overwrite = overwrite
        self._cache_dir = cache_dir
        self._max_samples = max_samples
        self._max_paragraphs_per_sample = max_paragraphs_per_sample
        self._max_samples_per_book = max_samples_per_book
        self._filepath = None

    def __str__(self):
        return f"{self.__class__.__name__}(lang={self._lang})"

    @property
    def filepath(self):
        return self._filepath

    def __generate_chunks(self, text: str):
        if self._max_paragraphs_per_sample is None:
            yield text
            return

        n_samples = 0
        buffer = []
        buffer_size = random.randint(1, self._max_paragraphs_per_sample)
        for par in generate_paragraphs(text):
            buffer.append(par)
            if len(buffer) >= buffer_size:
                yield "\n".join(buffer)

                buffer_size = random.randint(
                    1, self._max_paragraphs_per_sample
                )
                buffer = []
                n_samples += 1

                if n_samples >= self._max_samples_per_book > 0:
                    break

    def run(self, logger):
        if self._lang != "en":
            logger.info(f"{str(self)} Skipping {self._lang}")
            return

        self._filepath = Path(self._out_dir) / self.output_fp
        logger.info(f"{str(self)} Output file: {self._filepath}")
        logger.info(f"{str(self)} max_samples: {self._max_samples}")

        if self._filepath.exists():
            if not self._overwrite:
                raise FileExistsError(f"File {self._filepath} already exists.")
            else:
                self._filepath.unlink()
                logger.info(f"{str(self)} Deleted {self._filepath}")

        out_uri = "file://" + str(self._filepath)
        writer = Writer(uri=out_uri, schema=[("text", str)])

        logger.info(f"{str(self)} Download start.")
        pbar = tqdm(desc="writing progress", total=self._max_samples)
        flush_every = 5_000

        n_docs = 0
        for book in load_dataset(
                "togethercomputer/RedPajama-Data-1T", name="book",
                cache_dir=self._cache_dir,
                split="train", streaming=True
        ):
            for chunk in self.__generate_chunks(book["text"]):
                n_docs += 1
                if n_docs > self._max_samples > 0:
                    break

                writer.write(
                    data_obj={"text": chunk},
                    flush=n_docs % flush_every == 0
                )
                pbar.update(1)

            else:
                continue
            break

        pbar.close()
        writer.close()
        logger.info(f"{str(self)} Download finished; num_samples={n_docs - 1}")

File Path: app/src/artifacts/downloaders/ccnet_downloader.py
Content:
from collections import defaultdict
import subprocess
from pathlib import Path
import re
import functools
from tqdm import tqdm
import multiprocessing as mp
from multiprocessing.pool import Pool
from urllib.parse import urlparse
import os

from utilities.io import Reader, Writer
from utilities.io.s3 import init_client


class CCNetDownloader(object):
    r"""
    This class downloads / loads ccnet data and writes it to a jsonl file.
    """

    dataset_name = "ccnet"

    # extension of the cc input files
    cc_ext = ".json.gz"

    def __init__(
            self,
            artifacts_dir,
            cc_input,
            cc_input_base_uri,
            lang, num_samples, max_workers,
            endpoint_url
    ):
        # write args to class variables
        self._lang = lang
        self._num_samples = num_samples
        self._cc_input = cc_input
        self._cc_input_base_uri = cc_input_base_uri
        self._endpoint_url = endpoint_url

        # parallel readers
        if max_workers is not None:
            self._parallel_readers = max_workers
        else:
            self._parallel_readers = mp.cpu_count() - 2

        # build output path
        self._output_fp = Path(artifacts_dir) / "datasets" / \
                          self._lang / "ccnet" / "ccnet.jsonl"
        self._output_fp.parent.mkdir(parents=True, exist_ok=True)

    def __str__(self):
        return f"{self.__class__.__name__}({self._lang})"

    @property
    def filepath(self):
        return self._output_fp

    def __ccnet_file_filter(self, fp: str) -> bool:
        r""" function to filter commoncrawl  input files. """
        # we only keep files in the target language
        if not Path(fp).name.startswith(f"{self._lang}_"):
            return False

        # check extension
        if not fp.endswith(self.cc_ext):
            return False

        return True

    def run(self, logger):
        if not Path(self._cc_input).exists():
            raise ValueError(
                f"Listings file {self._cc_input} does not exist"
            )

        # read the listings file and return the relative paths listed in the
        # file.
        logger.info(f"{str(self)} Start loading input listings...")
        with open(self._cc_input) as f:
            input_listings = list(map(
                lambda _fp: os.path.join(self._cc_input_base_uri, _fp),
                filter(self.__ccnet_file_filter, map(str.strip, f.readlines()))
            ))

        # partition cc input by snapshot id in order to ensure that we have a
        # balanced number of samples per snapshot. This is to avoid bias due
        # to distribution shifts over time.
        logger.info(f"{str(self)} Partitioning inputs by snapshot...")
        snapsh_re = re.compile(r'\b\d{4}-\d{2}\b')
        inputs_by_snapsh = defaultdict(list)
        for listing in input_listings:
            if (dump_id := snapsh_re.search(listing).group()) is None:
                continue
            inputs_by_snapsh[dump_id].append(listing)

        samples_per_snapshot = max(
            1, self._num_samples // len(inputs_by_snapsh)
        )

        # kick off processes
        manager = mp.Manager()
        data_queue = manager.Queue(maxsize=128 * self._parallel_readers)

        # writer
        writer_proc = mp.Process(
            target=self._writer_worker, args=(data_queue,)
        )
        writer_proc.start()

        logger.info(f"{str(self)} Start loading {self._num_samples} samples "
                    f"from {len(inputs_by_snapsh)} snapshots")

        with Pool(processes=self._parallel_readers) as pool:
            counts_per_snapsh = pool.starmap(
                functools.partial(self._load_snapshot, data_queue=data_queue),
                [
                    (snpsh_id, snpsh_files, samples_per_snapshot)
                    for snpsh_id, snpsh_files in inputs_by_snapsh.items()
                ]
            )

        total_samples = 0
        for counts, snapshot_id in counts_per_snapsh:
            logger.info(f"{str(self)} Snapshot {snapshot_id}: "
                        f"loaded {counts} samples.")
            total_samples += counts

        logger.info(f"{str(self)} Total: loaded {total_samples} samples.")
        logger.info(f"{str(self)} Shuffling...")
        subprocess.run(["shuf", self._output_fp, "-o", self._output_fp])
        logger.info(f"{str(self)} Done. Output: {self._output_fp}")

        # send kill signal to writer
        data_queue.put_nowait(None)
        writer_proc.join()
        manager.shutdown()

    def _load_snapshot(
            self, snapshot_id, input_uris, num_samples, data_queue: mp.Queue,
    ):
        # partition input files into head, middle and tail
        head_uris = list(filter(lambda _u: "_head" in _u, input_uris))
        middle_uris = list(filter(lambda _u: "_middle" in _u, input_uris))
        tail_uris = list(filter(lambda _u: "_tail" in _u, input_uris))

        # compute number of samples to load from each bucket
        samples_per_bucket = {
            "head": int(num_samples * 0.1),
            "middle": int(num_samples * 0.2),
            "tail": int(num_samples * 0.7)
        }

        if urlparse(self._cc_input_base_uri).scheme == "s3":
            s3_client = init_client(
                endpoint_url=self._endpoint_url,
                signature_version="s3v4",
                aws_access_key_id=os.environ.get("AWS_ACCESS_KEY_ID"),
                aws_secret_access_key=os.environ.get("AWS_SECRET_ACCESS_KEY")
            )
        else:
            s3_client = None

        reader = Reader(
            schema=[("raw_content", str), ("language", str)],
            s3_client=s3_client
        )

        total_samples = 0

        for bucket, bucket_list in zip(
                ["head", "middle", "tail"], [head_uris, middle_uris, tail_uris]
        ):
            samples_retrieved = 0
            target_samples = samples_per_bucket[bucket]

            for uri in bucket_list:

                samples_to_retrieve = target_samples - samples_retrieved
                if samples_to_retrieve <= 0:
                    break

                for idx, record in reader.read(
                        uri=uri, max_samples=samples_to_retrieve
                ):
                    data_queue.put({
                        "text": record.raw_content,
                        "lang": record.language,
                        "source": uri
                    })

                    samples_retrieved += 1
                    total_samples += 1

        return total_samples, snapshot_id

    def _writer_worker(self, data_queue: mp.Queue):

        writer = Writer(
            uri="file://" + str(self._output_fp),
            schema=[("text", str), ("lang", str), ("source", str)]
        )

        flush_every = 10_000

        pbar = tqdm(desc="writing progress")

        num_recs = 0
        while True:
            data = data_queue.get()

            if data is None:
                break

            num_recs += 1
            writer.write(data, flush=num_recs % flush_every == 0)
            pbar.update(1)

        pbar.close()
        writer.close()

File Path: app/src/artifacts/downloaders/openwebtext_downloader.py
Content:
from datasets import load_dataset
from pathlib import Path
from tqdm import tqdm

from utilities.io import Writer


class OpenWebTextDownloader:
    r""" Loads the Openwebtext dataset from HuggingFace Datasets and saves it
    to disk """

    dataset_name = "openwebtext"
    output_fp = "openwebtext/en-openwebtext.jsonl.gz"

    def __init__(self, lang, out_dir, overwrite, cache_dir, max_samples):
        self._lang = lang
        self._out_dir = out_dir
        self._overwrite = overwrite
        self._cache_dir = cache_dir
        self._max_samples = max_samples
        self._filepath = None

    def __str__(self):
        return f"{self.__class__.__name__}(lang={self._lang})"

    @property
    def filepath(self):
        return self._filepath

    def run(self, logger):
        if self._lang != "en":
            logger.info(f"{str(self)} Skipping {self._lang}")
            return

        self._filepath = Path(self._out_dir) / self.output_fp
        logger.info(f"{str(self)} Output file: {self._filepath}")
        logger.info(f"{str(self)} max_samples: {self._max_samples}")

        if self._filepath.exists():
            if not self._overwrite:
                raise FileExistsError(f"File {self._filepath} already exists.")
            else:
                self._filepath.unlink()
                logger.info(f"{str(self)} Deleted {self._filepath}")

        out_uri = "file://" + str(self._filepath)
        writer = Writer(uri=out_uri, schema=[("text", str)])

        logger.info(f"{str(self)} Download start.")
        pbar = tqdm(desc="writing progress")
        flush_every = 10_000

        n_docs = 0
        for record in load_dataset(
                "openwebtext", cache_dir=self._cache_dir, split="train",
                streaming=True
        ):
            n_docs += 1
            if n_docs > self._max_samples > 0:
                break
            writer.write(
                data_obj={"text": record["text"]},
                flush=n_docs % flush_every == 0
            )
            pbar.update(1)

        pbar.close()
        writer.close()
        logger.info(f"{str(self)} Download finished; num_samples={n_docs - 1}")

File Path: app/src/artifacts/downloaders/wikipedia_downloader.py
Content:
from datasets import load_dataset
from pathlib import Path
from tqdm import tqdm

from utilities.io import Writer


class WikipediaDownloader:
    r""" Loads the Wikipedia dataset from HuggingFace Datasets and saves it to
    disk """

    dataset_name = "wikipedia"
    output_pattern = "wikipedia/{lang}-wikipedia.jsonl.gz"

    def __init__(self, lang, out_dir, overwrite, cache_dir, max_samples):
        self._lang = lang
        self._out_dir = out_dir
        self._overwrite = overwrite
        self._cache_dir = cache_dir
        self._max_samples = max_samples
        self._filepath = None

    def __str__(self):
        return f"{self.__class__.__name__}({self._lang})"

    @property
    def filepath(self):
        return self._filepath

    def run(self, logger):
        output_fn = self.output_pattern.format(lang=self._lang)
        self._filepath = Path(self._out_dir) / output_fn

        logger.info(f"{str(self)} Output file: {self._filepath}")
        logger.info(f"{str(self)} max_samples: {self._max_samples}")

        if self._filepath.exists():
            if not self._overwrite:
                raise FileExistsError(f"File {self._filepath} already exists.")
            else:
                self._filepath.unlink()
                logger.info(f"{str(self)} Deleted {self._filepath}")

        out_uri = "file://" + str(self._filepath)
        writer = Writer(uri=out_uri, schema=[("text", str)])

        logger.info(f"{str(self)} Download start.")
        pbar = tqdm(desc="writing progress")
        flush_every = 10_000

        try:
            # try to load wikipedia data from preprocessed huggingface dataset
            ds_iterator = load_dataset(
                "wikipedia", f"20220301.{self._lang}", streaming=True,
                split="train"
            )
            logger.info(f"{str(self)} Load {self._lang}-wiki from 20220301")
        except Exception as _:
            # if that fails, load from original huggingface dataset and process
            ds_iterator = load_dataset(
                "wikipedia", language=self._lang, date="20230801",
                cache_dir=self._cache_dir, beam_runner="DirectRunner",
                split="train"
            )
            logger.info(f"{str(self)} Load {self._lang}-wiki from 20230801")

        n_docs = 0
        for record in ds_iterator:
            n_docs += 1
            if n_docs > self._max_samples > 0:
                break
            writer.write(
                data_obj={"text": record["text"]},
                flush=n_docs % flush_every == 0
            )
            pbar.update(1)

        pbar.close()
        writer.close()
        logger.info(f"{str(self)} Download finished; num_samples={n_docs - 1}")

File Path: app/src/artifacts/ft_trainer.py
Content:
from pathlib import Path
from tqdm import tqdm
import fasttext
import subprocess

from core.document import Document
from core.quality_signals.utils.classifiers import \
    preprocess_quality_classifier
from core.constants import CCNET_LABEL
from utilities.io import Reader


class FastTextTrainer:
    # cc label
    cc_label = CCNET_LABEL

    # output file naming convention
    output_fmt = "{dataset}.model.bin"

    def __init__(
            self, artifacts_dir, ccnet_data, target_data, target_name,
            samples_per_class, lang
    ):

        # write args to class variables
        self._ccnet_data = ccnet_data
        self._target_data = target_data
        self._samples_per_class = samples_per_class
        self._lang = lang
        self._target_label = f"__label__{target_name}"

        # build output directory
        out_dir = Path(artifacts_dir) / "classifiers" / self._lang
        out_dir.mkdir(parents=True, exist_ok=True)
        self._output = out_dir / self.output_fmt.format(dataset=target_name)
        self._train_data = out_dir / f"{target_name}.data.train"

    def run(self, logger):
        log_prefix = f"{self.__class__.__name__}(" \
                     f"lang={self._lang}, ccdata={self._ccnet_data}, " \
                     f"target_data={self._target_data}, " \
                     f"target_label={self._target_label})"

        train_data_fh = open(self._train_data, "w")

        logger.info(f"{log_prefix} Start building fasttext classifier")

        # write target data
        samples_per_slice = self._samples_per_class // len(self._target_data)
        total_target_samples = 0

        for target_data_fp in self._target_data:
            reader = Reader(schema=[("text", str)])
            total_target_samples += self.__write_train_chunk(
                uri="file://" + str(target_data_fp),
                reader=reader,
                writer=train_data_fh,
                max_samples=samples_per_slice,
                target_label=self._target_label
            )

        logger.info(f"{log_prefix} Number of target "
                    f"samples found: {total_target_samples}")

        # write ccnet data
        reader = Reader(schema=[("text", str)])
        ccnet_samples = self.__write_train_chunk(
            uri="file://" + str(self._ccnet_data),
            reader=reader,
            writer=train_data_fh,
            max_samples=total_target_samples,
            target_label=self.cc_label
        )
        train_data_fh.close()
        logger.info(f"{log_prefix} Total ccnet samples: {ccnet_samples}")

        # shuffle train data
        logger.info(f"{log_prefix} Shuffling train data")
        subprocess.run(
            ["shuf", "-o", str(self._train_data), str(self._train_data)]
        )

        # train fasttext classifier
        model = fasttext.train_supervised(
            input=str(self._train_data), verbose=2
        )
        model.save_model(str(self._output))
        logger.info(f"{log_prefix} Saved model to {self._output}")

    @staticmethod
    def __write_train_chunk(
            uri, reader: Reader, writer, max_samples, target_label
    ):
        num_samples = 0

        for record in tqdm(
                reader.read(uri, max_samples=max_samples, return_idx=False),
                total=max_samples
        ):
            doc = Document(record.text, domain=None)
            text = preprocess_quality_classifier(document=doc)
            writer.write(f"{target_label} {text}\n")
            num_samples += 1

        writer.flush()

        return num_samples

File Path: app/src/artifacts/hash_dist.py
Content:
import functools
import multiprocessing as mp
from multiprocessing.pool import Pool
import numpy as np
from pathlib import Path
from tqdm import tqdm

from core.document import Document
from utilities.io import Reader


def _compute_hash_features(text: str, buckets: int):
    r""" Compute the hash features for a given text """
    # compute hash features directly in the document class
    # for consistency
    document = Document(
        content=text, domain=None, precompute_ngrams=False,
        precompute_hash_features=True, dsir_buckets=buckets
    )
    return document.hash_features, document.num_raw_words


class HashDist:
    # output file naming convention
    output_file_fmt_counts = "{dataset}.{lang}.{buckets}.counts.npy"
    output_file_fmt_lambda = "{dataset}.{lang}.lambda.npy"

    def __init__(
            self, artifacts_dir, num_samples, buckets, max_workers, logger
    ):
        self._artifacts_dir = artifacts_dir
        self._num_samples = num_samples
        self._buckets = buckets
        self._max_workers = max_workers
        self._logger = logger

    def run(self, lang, datafile, dataset):
        log_prefix = f"{self.__class__.__name__}(" \
                     f"lang={lang}, datafile={datafile}, dataset={dataset})"
        datafile = str(Path(datafile).absolute())

        out_dir = Path(self._artifacts_dir) / "dsir" / f"{lang}"
        out_dir.mkdir(parents=True, exist_ok=True)
        out_fp_dist = out_dir / self.output_file_fmt_counts.format(
            dataset=dataset, lang=lang, buckets=self._buckets
        )
        out_fp_lambda = out_dir / self.output_file_fmt_lambda.format(
            dataset=dataset, lang=lang
        )
        self._logger.info(
            f"{log_prefix} Start dsir computation for {lang}-{dataset}"
        )
        self._logger.info(f"{log_prefix} Reading data from {datafile}")
        self._logger.info(f"{log_prefix} Write distribution to {out_fp_dist}")
        self._logger.info(f"{log_prefix} Write lambda to {out_fp_lambda}")

        if self._max_workers is not None:
            if self._max_workers < 0:
                raise ValueError("max_workers must be >= 0")
            max_proc = min(self._max_workers, mp.cpu_count() - 1)
        else:
            max_proc = mp.cpu_count() - 1

        self._logger.info(f"{log_prefix} Using {max_proc} processes")
        reader = Reader(schema=[("text", str)])

        def _wrap_reader():
            r""" wrap reader so that it can be used with multiprocessing.
            Otherwise, pickling of records fails. """
            for record in reader.read(
                    uri="file://" + datafile,
                    max_samples=self._num_samples,
                    return_idx=False
            ):
                yield record.text

        global_dist = np.zeros(self._buckets, dtype=np.int64)

        # MLE estimator for lambda of Poisson distribution
        lambda_mle = 0
        num_samples = 0

        with Pool(max_proc) as pool:
            for dist, dlen in tqdm(
                    pool.imap_unordered(
                        functools.partial(
                            _compute_hash_features,
                            buckets=self._buckets
                        ),
                        _wrap_reader()
                    ),
                    total=self._num_samples,
                    desc=f"Reading {datafile}"
            ):
                global_dist += dist
                lambda_mle += dlen
                num_samples += 1

        # save lambda
        np.save(file=str(out_fp_lambda), arr=lambda_mle / num_samples)
        self._logger.info(f"{log_prefix} Saved lambda to {out_fp_lambda}")

        # save distribution
        np.save(file=str(out_fp_dist), arr=global_dist)
        self._logger.info(f"{log_prefix} Saved distribution to {out_fp_dist}")

        self._logger.info(f"{log_prefix} Finished dsir for {lang}-{dataset}.")

File Path: app/src/artifacts/update_resources.py
Content:
import argparse
from collections import defaultdict
import itertools
import json
from pathlib import Path
import time
import urllib.request
import requests
import tarfile
from typing import Dict, List, Tuple

_UT1_BLACKLIST_URL = "http://dsi.ut-capitole.fr" \
                     "/blacklists/download/blacklists.tar.gz"
_LDNOOBW_URL = "https://raw.githubusercontent.com/LDNOOBW/List-of-Dirty-" \
               "Naughty-Obscene-and-Otherwise-Bad-Words/master/{lang}"


def _build_category_index(raw_categories) -> Dict[Tuple[str], int]:
    r""" Build a mapping with a list of categories, corresponding to a unique
    combination of categories, to a category ID.
    """
    categories = sorted(raw_categories)

    category_index = {}
    for i, category in enumerate(itertools.chain.from_iterable(
            itertools.combinations(categories, r) for r in
            range(1, len(categories) + 1)
    )):
        category_index[tuple(str(s) for s in sorted(category))] = i

    return category_index


def _domain_to_category_list_mapping(bad_urls_dir, raw_categories):
    domain_to_category = defaultdict(list)

    for category_dir in (bad_urls_dir / "blacklists").iterdir():
        if not category_dir.is_dir():
            continue

        category = category_dir.name

        if category not in raw_categories:
            continue

        with open(category_dir / "domains", "r") as f:
            for dom in map(str.strip, f.readlines()):
                domain_to_category[dom].append(category)

    # postprocess
    domain_to_category = {
        dom: tuple(str(s) for s in sorted(set(categories)))
        for dom, categories in domain_to_category.items()
    }

    return domain_to_category


def create_bad_urls_index(artifacts_dir: Path, raw_categories: List[str]):
    r""" update the URL blacklists from the University of Toulouse:

    @param artifacts_dir: (Path) The path to the resources directory
    @param raw_categories: (List[str]) The domain categories

    """

    ut1_blacklist_dir = artifacts_dir / "bad_urls"

    if not ut1_blacklist_dir.exists():
        ut1_blacklist_dir.mkdir(parents=True)

    print(f"fetching UT1 blacklist from {_UT1_BLACKLIST_URL}...")

    with urllib.request.urlopen(_UT1_BLACKLIST_URL) as response:
        with tarfile.open(fileobj=response, mode="r|gz") as tar:
            tar.extractall(path=ut1_blacklist_dir)

    with open(ut1_blacklist_dir / "_FETCH_TIMESTAMP", "w") as f:
        f.write(str(int(time.time())))

    print(f"raw UT1 list fetched.")

    category_index = _build_category_index(raw_categories)

    # convert the raw UT1 blacklist to a domain -> category_id mapping where
    # a category corresponds to any combination of raw categories.
    domain_to_category_list = _domain_to_category_list_mapping(
        ut1_blacklist_dir, raw_categories
    )

    domain_to_category_id = {
        dom: category_index[categories]
        for dom, categories in domain_to_category_list.items()
    }

    with open(ut1_blacklist_dir / "domain_to_category_id.json", "w") as f:
        json.dump(domain_to_category_id, f)

    # save the category index as int -> category mapping
    category_index = {
        i: categories for categories, i in category_index.items()
    }
    with open(ut1_blacklist_dir / "category_index.json", "w") as f:
        json.dump(category_index, f)


def create_bad_words_list(artifacts_dir: Path, lang: str):
    r""" Fetch the LDNOOBW word list

    Args:
        artifacts_dir (Path): The path to the resources directory
        lang (str): The language to fetch the word list for
    """

    ldnoobw_dir = artifacts_dir / "bad_words"

    if not ldnoobw_dir.exists():
        ldnoobw_dir.mkdir(parents=True)

    word_list_fp = ldnoobw_dir / f"{lang}.txt"
    url = _LDNOOBW_URL.format(lang=lang)

    print(f"fetching bad words list from {url}...")

    response = requests.get(url)
    if response.status_code != 200:
        raise Exception(f"{response.status_code} -- {url}.")

    data = response.content.decode('utf-8')

    with open(ldnoobw_dir / f"_{lang}_FETCH_TIMESTAMP", "w") as f:
        f.write(str(int(time.time())))

    data = set(w for w in data.splitlines() if w is not None)

    with open(word_list_fp, 'w') as f:
        f.write('\n'.join(data))

    print(f"bad words list ({lang}) updated.")


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--langs", type=str, nargs="+")
    parser.add_argument("--artifacts_dir", type=str)
    parser.add_argument("--block_categories", type=str, nargs="+")
    args = parser.parse_args()

    artifacts_dir = Path(args.artifacts_dir)
    artifacts_dir.mkdir(parents=True, exist_ok=True)

    # fetch ut1 blacklist
    create_bad_urls_index(artifacts_dir=artifacts_dir,
                          raw_categories=args.block_categories)

    # fetch ldnoobw
    langs = set(args.langs)
    for lang in langs:
        try:
            create_bad_words_list(lang=lang, artifacts_dir=artifacts_dir)
        except Exception as e:
            print(f"Failed to fetch LDNOOBW {lang}: {e}")


if __name__ == '__main__':
    main()

File Path: app/src/artifacts/utils/__init__.py
Content:

File Path: app/src/artifacts/utils/data_utils.py
Content:
import random
import string

# text normalization: lowercasing and removing punctuation
TRANSLATION_TABLE = str.maketrans(
    string.ascii_lowercase + string.ascii_uppercase + "\n",
    string.ascii_lowercase * 2 + " ",
    string.punctuation
)


def normalize_text(text: str, max_words: int = -1):
    r""" Normalize text by lowercasing and removing punctuation; if max words
    is larger than 0, then a random but contiguous span of max_words is
    selected from the text.

    Args:
        text: text to normalize
        max_words: maximum number of words to keep in text

    Returns:
        normalized text
    """
    text = text.translate(TRANSLATION_TABLE)
    text = text.split()
    num_words = len(text)

    # pick a random span inside the text if it is too long
    if num_words > max_words > 0:
        start = random.randint(0, num_words - max_words)
        text = text[start:start + max_words]

    return " ".join(text)

File Path: app/src/artifacts/utils/logging_utils.py
Content:
import logging
from logging.handlers import QueueHandler
import multiprocessing as mp

__all__ = [
    "worker_logger_configurer",
    "listener_logger_configurer",
    "LOG_FMT"
]

LOG_FMT = '[%(asctime)s]::(PID %(process)d)::%(levelname)-2s::%(message)s'


def worker_logger_configurer(queue: mp.Queue, level=logging.DEBUG):
    root = logging.getLogger()

    if not root.hasHandlers():
        h = logging.handlers.QueueHandler(queue)
        root.addHandler(h)

    root.setLevel(level)


def listener_logger_configurer(logfile, level=logging.DEBUG):
    root = logging.getLogger()
    formatter = logging.Formatter(LOG_FMT)

    # write to log file
    if logfile is not None:
        if not logfile.parent.exists():
            logfile.parent.mkdir(parents=True)
        file_handler = logging.FileHandler(logfile)
        file_handler.setFormatter(formatter)
        root.addHandler(file_handler)

    # write to stdout
    stream_handler = logging.StreamHandler()
    stream_handler.setFormatter(formatter)
    root.addHandler(stream_handler)

    root.setLevel(level)

File Path: app/src/bloomfilter.py
Content:
import argparse
import random

import boto3
import concurrent.futures
from dataclasses import dataclass
from datetime import datetime as dt
import gzip
import io
import logging
import msgspec
import os
from pathlib import Path
import polars as pl
import progiter
import pyarrow as pa
import pybloomfilter
import re
from typing import Tuple
from urllib.parse import urlparse
from typing import Dict, List

from utilities.logging import configure_logger
from utilities.io import ParquetBatchWriter


@dataclass
class ReadStatus:
    is_success: bool
    msg: str
    uri: str


class Deduper:
    r""" Bloom filter for exact deduplication of ccnet shards. Based on
    document contents. """
    __slots__ = (
        "_args", "_logger", "_job_id", "_input_base_uri", "_scheme",
        "_output_fp", "_out_schema", "_bloom_fp"
    )

    # regex to extract filepaths from source file listings
    input_patterns = [
        re.compile(r".*/[a-z]{2}_middle\.json\.gz"),
        re.compile(r".*/[a-z]{2}_head\.json\.gz")
    ]

    output_pattern = "duplicates-{timestamp}-{snapshot}.parquet"

    def __init__(self):
        self._job_id = dt.now().strftime("%Y%m%d_%H%M%S")
        self._args = self.__parse_arguments()

        # set random seed
        random.seed(self._args.seed)

        # parse args
        self._input_base_uri = self._args.input_base_uri
        self._scheme = urlparse(self._input_base_uri).scheme

        # init logging
        logfile = Path(self._args.output_dir) / "logs" / f"{self._job_id}.log"
        configure_logger(logfile=logfile, level=logging.INFO, stream=False)
        self._logger = logging.getLogger()

        # output writer
        self._output_fp = Path(self._args.output_dir) / "duplicates.parquet"
        self._out_schema = pa.schema([
            ("shard_id", pa.string()),
            ("doc_id", pa.string()),
            ("digest", pa.string())
        ])

        # log setup
        for attr in [
            "listings", "input_base_uri", "output_dir", "parallel_readers",
            "capacity", "error_rate", "seed", "max_inputs", "batch_size"
        ]:
            self._logger.info(f"{attr}: {getattr(self._args, attr)}")

    def __parse_arguments(self) -> argparse.Namespace:

        if self.__doc__ is not None:
            description = " - " + self.__doc__
        else:
            description = self.__class__.__name__

        parser = argparse.ArgumentParser(
            prog=self.__class__.__name__, description=description
        )

        # io
        parser.add_argument(
            "--listings", type=str, default=None,
            help="Path to a file containing paths to ccnet shards; needs to "
                 "match with the input_base_uri argument."
        )
        parser.add_argument(
            "--input_base_uri", type=str, default=None,
            help="base uri of the input files."
        )
        parser.add_argument(
            "--output_dir", type=str, default=None,
            help="directory where the output will be stored."
        )
        parser.add_argument(
            "--s3_profile", type=str, default="default",
            help="profile name of the s3 client."
        )
        parser.add_argument(
            "--endpoint_url", type=str, default=None,
            help="S3 bucket endpoint url."
        )
        parser.add_argument(
            "--parallel_readers", type=int, default=1,
            help="number of parallel reader processes. Defaults to 1."
        )
        parser.add_argument(
            "--max_inputs", type=int, default=None,
            help="maximum number of inputs to process. For debugging."
        )
        parser.add_argument(
            "--batch_size", type=int, default=None,
            help="number of listings to be processed per process."
        )

        parser.add_argument(
            "--seed", type=int, default=42,
            help="random seed."
        )

        # dedup params
        parser.add_argument(
            "--capacity", type=int, default=None,
            help="Capacity of the bloom filter. This is the maximum number of "
                 "unique documents that can be stored in the filter while "
                 "keeping the error rate under `error_rate`."
        )
        parser.add_argument(
            "--error_rate", type=float, default=0.01,
            help="false positive probability that will hold given that "
                 "'capacity' is not exceeded. Defaults to 0.001"
        )
        args = parser.parse_args()

        return args

    def __init_client(self):
        if self._scheme != "s3":
            return None

        session = boto3.Session(profile_name=self._args.s3_profile)
        return session.client(
            service_name='s3',
            endpoint_url=self._args.endpoint_url,
            config=boto3.session.Config(
                signature_version="s3v4",
                retries={'max_attempts': 5, 'mode': 'standard'}
            )
        )

    def __filter_listings(self, obj_key: str):
        for pat in self.input_patterns:
            if pat.search(obj_key) is not None:
                return True

        return False

    def __parse_listings(self):
        # build input uris
        with open(self._args.listings, "r") as f:
            uris = list(
                map(lambda ls: os.path.join(self._input_base_uri, ls.strip()),
                    filter(self.__filter_listings, f.readlines()))
            )

        return uris

    @staticmethod
    def __load_from_s3(uri, client):
        try:
            streaming_body = client.get_object(
                Bucket=uri.netloc, Key=uri.path.lstrip("/")
            )["Body"]
            buffer = io.BytesIO(streaming_body.read())
            msg = f"__S3_URI_READ_SUCCESS__ success reading {uri.path}"
            is_success = True
        except Exception as e:
            msg = (
                f"__S3_URI_READ_ERROR__ failed reading {uri.path}: "
                f"caught exception {e.__class__.__name__}: {e}"
            )
            buffer = None
            is_success = False

        return is_success, msg, buffer

    @staticmethod
    def __load_from_disk(uri):
        try:
            with open(uri.path, "rb") as f:
                buffer = io.BytesIO(f.read())
                msg = f"__DISK_URI_READ_SUCCESS__ success reading {uri.path}"
                is_success = True
        except Exception as e:
            msg = (
                f"__DISK_URI_READ_ERROR__ failed reading {uri.path}: "
                f"caught exception {e.__class__.__name__}: {e}"
            )
            buffer = None
            is_success = False

        return is_success, msg, buffer

    def _load_file(self, uri, client) -> Tuple[ReadStatus, io.BytesIO]:
        if uri.scheme == "s3":
            is_success, msg, buffer = self.__load_from_s3(uri, client)
        elif uri.scheme == "file":
            is_success, msg, buffer = self.__load_from_disk(uri)
        else:
            raise ValueError(f"Unknown scheme {uri.scheme}")

        read_status = ReadStatus(
            is_success=is_success, msg=msg, uri=uri.geturl()
        )
        return read_status, buffer

    def _load_and_parse_inputs(
            self, input_chunk
    ) -> Dict[str, Tuple[ReadStatus, List[Dict]]]:
        # build msgspec decoder
        decoder = msgspec.json.Decoder(
            type=msgspec.defstruct(name="Record", fields=[("digest", str)])
        )

        client = self.__init_client()
        data = {}

        for uri in input_chunk:
            read_status, buffer = self._load_file(
                uri=urlparse(uri), client=client
            )

            if not read_status.is_success:
                data[uri] = (read_status, [])
                continue

            shard_id = read_status.uri.replace(
                self._input_base_uri, ""
            ).lstrip("/")

            uri_data = []

            try:
                with gzip.open(buffer, "rb") as f:
                    for idx, obj in enumerate(f):
                        rec = decoder.decode(obj)
                        digest = str(getattr(rec, "digest")).replace(
                            "sha1:", ""
                        )
                        uri_data.append({
                            "shard_id": shard_id,
                            "doc_id": f"{shard_id}/{idx}",
                            "digest": digest
                        })
            except Exception as e:
                uri_data = []
                read_status.msg = (
                    f"__S3_URI_DECODE_ERROR__ failed decoding {uri}: "
                    f"caught exception {e.__class__.__name__}: {e}"
                )
                read_status.is_success = False

            data[uri] = (read_status, uri_data)

        del buffer

        return data

    def __parallel_run(self, input_uris):
        # shuffle input uris
        random.shuffle(input_uris)

        if self._args.max_inputs is not None:
            self._logger.info(f"Limiting inputs to {self._args.max_inputs}")
            input_uris = input_uris[:self._args.max_inputs]

        # divide input uris into snapshots
        snapsh_re = re.compile(r'\b\d{4}-\d{2}\b')
        snapshots = {}
        for uri in input_uris:
            snapshot = snapsh_re.search(uri).group(0)
            if snapshot not in snapshots:
                snapshots[snapshot] = [uri]
            else:
                snapshots[snapshot].append(uri)

        snapshot_ids_sorted = sorted(snapshots.keys(), reverse=True)

        # init bloomfilter
        bloomfilter = pybloomfilter.BloomFilter(
            capacity=self._args.capacity,
            error_rate=self._args.error_rate
        )

        self._logger.info(f"Filter capacity: {bloomfilter.capacity}")
        self._logger.info(f"Filter error rate: {bloomfilter.error_rate}")
        self._logger.info(f"Filter hash seeds: {bloomfilter.hash_seeds}")

        num_docs, num_dupes = 0, 0

        # progress bars
        pman = progiter.ProgressManager(backend="rich")
        total_progress = pman.progiter(
            total=self._args.capacity, postfix_str="Duplicates: --"
        )
        download_progress = pman.progiter(
            total=len(input_uris), desc="Download"
        )

        num_failed_uri = 0
        num_succ_uri = 0

        for snapsh_id in snapshot_ids_sorted:
            uri_list = snapshots[snapsh_id]
            random.shuffle(uri_list)

            uri_list_partitioned = [
                uri_list[i:i + self._args.batch_size]
                for i in range(0, len(uri_list), self._args.batch_size)
            ]

            self._logger.info(f"__SNAPSHOT_START__ {snapsh_id}")

            # output writer
            timestamp = dt.now().strftime("%Y%m%d_%H%M%S")
            output_fp = (
                    Path(self._args.output_dir) /
                    self.output_pattern.format(
                        timestamp=timestamp, snapshot=snapsh_id
                    )
            )
            out_writer = ParquetBatchWriter(
                output_fp=output_fp, schema=self._out_schema
            )

            try:
                with concurrent.futures.ProcessPoolExecutor(
                        max_workers=self._args.parallel_readers
                ) as executor:
                    futures = {
                        executor.submit(
                            self._load_and_parse_inputs, input_chunk
                        ): i
                        for i, input_chunk in enumerate(uri_list_partitioned)
                    }

                    for future in concurrent.futures.as_completed(futures):
                        data_chunks = future.result()
                        del futures[future]
                        download_progress.step(len(data_chunks))

                        for (
                                uri, (read_status, uri_data)
                        ) in data_chunks.items():

                            if not read_status.is_success:
                                self._logger.error(read_status.msg)
                                num_failed_uri += 1
                                continue

                            num_succ_uri += 1
                            download_progress.set_postfix_str(
                                f"success: {num_succ_uri} "
                                f"({num_failed_uri} failed)"
                            )

                            self._logger.info(read_status.msg)

                            for record in uri_data:
                                digest = record["digest"]

                                if bloomfilter.add(digest):
                                    out_writer.update_batch(obj=record)
                                    num_dupes += 1

                                num_docs += 1
                                total_progress.step(1)

                                if num_docs % (1024 ** 2) == 0:
                                    out_writer.write_batch()

                            dupe_prop = round(100 * num_dupes / num_docs, 2)
                            total_progress.set_postfix_str(
                                f"Duplicates: {num_dupes} / {num_docs}"
                                f" ({dupe_prop:.2f}%)"
                            )

            except KeyboardInterrupt:
                self._logger.info("Keyboard interrupt. Stopping.")
                executor.shutdown(wait=False, cancel_futures=True)
                out_writer.close()
                break
            except Exception as e:
                self._logger.error(
                    f"Caught exception {e.__class__.__name__}: {e}"
                )
                executor.shutdown(wait=False, cancel_futures=True)
                out_writer.close()
                self._logger.info(f"__SNAPSHOT_FAIL__ {snapsh_id}")
                continue

            out_writer.close()
            self._logger.info(f"__SNAPSHOT_FINISH__ {snapsh_id}")

        pman.stop()
        bloomfilter.close()

        self._logger.info(f"Filtering complete.")

    def run(self):
        start_time = dt.now()
        print(f"start @ {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        self.__parallel_run(input_uris=self.__parse_listings())
        end_time = dt.now()
        print(f"end @ {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        end_str = f"Total time: {end_time - start_time}"
        print(end_str)
        self._logger.info(end_str)

    def __result_summary(self):
        dump_reg = "(\d{4}-\d{2})\/"
        # read duplicates
        query = (
            pl.scan_parquet(self._output_fp)
            .with_columns(
                pl.col("shard_id").str.extract(dump_reg, 1).alias("snapshot")
            )
            .group_by("snapshot")
            .agg(pl.count())
        )

        stats = query.collect()

        with pl.Config(fmt_str_lengths=1000, tbl_rows=100):
            print(stats)


if __name__ == '__main__':
    deduper = Deduper()
    deduper.run()

File Path: app/src/core/__init__.py
Content:

File Path: app/src/core/constants.py
Content:
PRECISION = 8
CCNET_LABEL = "__label__cc"

File Path: app/src/core/data_types.py
Content:
from dataclasses import dataclass
from msgspec import Struct

from typing import List, Tuple, Optional, Dict
from typing_extensions import TypeAlias

ScoreType: TypeAlias = Tuple[int, int, Optional[float]]
SignalType: TypeAlias = List[ScoreType]


@dataclass
class TextSlice:
    text: str
    start: int
    end: int

    def __len__(self):
        return len(self.text)


class InputSpec(Struct):
    raw_content: str
    url: str
    nlines: int
    original_nlines: int
    source_domain: str
    length: int
    original_length: int
    language: str
    language_score: float
    perplexity: float
    bucket: str
    digest: str
    cc_segment: str
    date_download: str


class OutputSpec(Struct):
    id: str
    id_int: int
    metadata: Dict[str, str]
    quality_signals: Dict[str, List[Tuple[int, int, Optional[float]]]]




File Path: app/src/core/document.py
Content:
from nltk.tokenize import WordPunctTokenizer
import re
from typing import Optional, Tuple, Callable

from utilities.text import normalize, form_ngrams
from core.data_types import TextSlice
from core.quality_signals.utils.dsir import hash_feature

_word_tokenizer = WordPunctTokenizer()


def _compute_ngrams(text_seq, n):
    return tuple(form_ngrams(iter(text_seq), n))


def split_paragraphs(
        text: str, normalizer: Callable[[str], str], remove_empty: bool = True
) -> Tuple[TextSlice]:
    """
    This function is adapted from dolma: https://github.com/allenai/dolma

    Split a string into paragraphs. A paragraph is defined as a sequence of
    zero or more characters, followed by a newline character, or a sequence
     of one or more characters, followed by the end of the string.
    """
    text_slices = tuple(
        TextSlice(normalizer(text[match.start():match.end()]), match.start(),
                  match.end())
        for match in re.finditer(r"([^\n]*\n|[^\n]+$)", text)
    )

    if remove_empty is True:
        text_slices = tuple(
            text_slice for text_slice in text_slices if text_slice[0].strip()
        )

    return text_slices


class Document:
    __slots__ = (
        "_raw_content", "_normalized_content", "_raw_lines",
        "_normalized_lines", "_raw_words", "_normalized_words",
        "_num_raw_words", "_num_normalized_words", "_domain", "_raw_2grams",
        "_raw_3grams", "_norm_2grams", "_norm_3grams", "_norm_4grams",
        "_hash_features"
    )

    def __init__(
            self, content: str, domain: Optional[str],
            precompute_ngrams: bool = False,
            precompute_hash_features: bool = False,
            dsir_buckets: Optional[int] = None
    ):
        self._raw_content = content
        self._domain = domain

        # the normalized content: lowercased and punctuation removed
        self._normalized_content = normalize(content)

        # the lines of the document (split by newline)
        self._raw_lines: Tuple[TextSlice] = split_paragraphs(
            text=content, normalizer=lambda x: x, remove_empty=False
        )

        # the lines of the document (split by newline), normalized
        self._normalized_lines: Tuple[TextSlice] = split_paragraphs(
            text=content, normalizer=normalize, remove_empty=False
        )

        # the words of the document after normalization
        self._raw_words = tuple(_word_tokenizer.tokenize(self._raw_content))

        # the normalized words of the document (split by whitespace)
        self._normalized_words = tuple(self._normalized_content.split())

        # get number of words before and after normalization
        self._num_raw_words = len(self._raw_words)
        self._num_normalized_words = len(self._normalized_words)

        # precompute ngrams
        if precompute_ngrams:
            # raw grams
            self._raw_2grams = _compute_ngrams(self._raw_words, 2)
            self._raw_3grams = _compute_ngrams(self._raw_words, 3)

            # normalized grams
            self._norm_2grams = _compute_ngrams(self._normalized_words, 2)
            self._norm_3grams = _compute_ngrams(self._normalized_words, 3)
            self._norm_4grams = _compute_ngrams(self._normalized_words, 4)
        else:
            self._raw_2grams = None
            self._raw_3grams = None
            self._norm_2grams = None
            self._norm_3grams = None
            self._norm_4grams = None

        # precomupte hash features
        if precompute_hash_features:
            bigrams = self._raw_2grams or _compute_ngrams(self._raw_words, 2)
            self._hash_features = hash_feature(
                unigrams=self._raw_words,
                bigrams=bigrams,
                buckets=dsir_buckets
            )
        else:
            self._hash_features = None

    def __len__(self):
        return len(self._raw_content)

    @property
    def raw_content(self):
        return self._raw_content

    @property
    def normalized_content(self):
        return self._normalized_content

    @property
    def raw_lines(self):
        return self._raw_lines

    @property
    def normalized_lines(self):
        return self._normalized_lines

    @property
    def raw_words(self):
        return self._raw_words

    @property
    def normalized_words(self):
        return self._normalized_words

    @property
    def num_raw_words(self):
        return self._num_raw_words

    @property
    def num_normalized_words(self):
        return self._num_normalized_words

    @property
    def domain(self):
        return self._domain

    @property
    def raw_1grams(self):
        return self._raw_words

    @property
    def raw_2grams(self):
        return self._raw_2grams

    @property
    def raw_3grams(self):
        return self._raw_3grams

    @property
    def norm_1grams(self):
        return self._normalized_words

    @property
    def norm_2grams(self):
        return self._norm_2grams

    @property
    def norm_3grams(self):
        return self._norm_3grams

    @property
    def norm_4grams(self):
        return self._norm_4grams

    @property
    def hash_features(self):
        return self._hash_features

File Path: app/src/core/exceptions.py
Content:
class S3ReadError(Exception):
    def __init__(self, message):
        super().__init__(message)


class S3WriteError(Exception):
    def __init__(self, message):
        super().__init__(message)


class LocalReadError(Exception):
    def __init__(self, message):
        super().__init__(message)


class UnknownReadError(Exception):
    def __init__(self, message):
        super().__init__(message)

File Path: app/src/core/quality_signals/__init__.py
Content:

File Path: app/src/core/quality_signals/base.py
Content:
from core.document import Document
from core.data_types import SignalType


class RPSBase:
    r""" Base class for RP signal functions. Each child class must implement
    the __call__ method. The __call__ method takes a document as input and
    returns a score. """
    DATA_TYPE = SignalType

    RPS_PREFIX: str = "RPS_"

    __slots__ = ["__field_name"]

    def __init__(self, *args, **kwargs):  # noqa
        # make sure all classes start with RPS_; this is to ensure that
        # the get_rule_based_signals function works correctly when new signal
        # functions are added
        assert self.__class__.__name__.startswith(self.RPS_PREFIX), \
            f"Name of signal function must" \
            f" start with {self.RPS_PREFIX}; got {self.__class__.__name__}"

        self.__field_name = self.__class__.__name__.lower()

    def __call__(self, document: Document):
        raise NotImplementedError

    @property
    def field_name(self):
        return self.__field_name

File Path: app/src/core/quality_signals/classifiers.py
Content:
import sys
from typing import List, Tuple, Type
import fasttext

from core.constants import PRECISION, CCNET_LABEL
from core.quality_signals.base import RPSBase
from core.document import Document
from core.data_types import SignalType
from core.quality_signals.utils.classifiers import \
    preprocess_quality_classifier
from utilities.register.registry_utils import *

__all__ = [
    "register_classifier_callables", "classifier_schema"
]


def classifier_schema() -> List[Tuple[str, Type]]:
    r""" Returns a list of signal names and their data types """
    return signal_schema(module=sys.modules[__name__])


def register_classifier_callables(
        wikiref_model: str,
        palm_model: str,
        wikipedia_model: str
) -> List[RPSBase]:
    r""" Returns a list of signal functions (i.e., RPSBase instances) that
    are used to extract content signals from a document.

    Args:
        wikiref_model: A fasttext model trained on Wikipedia references.
        palm_model: A fasttext model trained on ccnet vs
            {books, openwebtext, wikipedia}.
        wikipedia_model: A fasttext model trained on Wikipedia articles.

    Returns:
        A list of signal function class instances.
    """
    return list(map(
        lambda cls: cls(
            wikiref_model=wikiref_model,
            palm_model=palm_model,
            wikipedia_model=wikipedia_model,
        ),
        get_callables_from_module(module=sys.modules[__name__])
    ))


class BaseMLSignal(RPSBase):
    __slots__ = "_ft_model"

    def __init__(self, ft_model_file: str):
        super(BaseMLSignal, self).__init__()
        if ft_model_file is None:
            self._ft_model = None
        else:
            self._ft_model = fasttext.load_model(str(ft_model_file))

    def __call__(self, document: Document) -> SignalType:
        if self._ft_model is None:
            return [(0, len(document), None)]

        if len(document.raw_content) == 0:
            return [(0, len(document), None)]

        text = preprocess_quality_classifier(document=document)
        pred = self._ft_model.predict(text=text)

        (pred_label, pred_prob) = pred
        pred_label = pred_label[0]
        pred_prob = pred_prob[0]

        if pred_label == CCNET_LABEL:
            high_quality_score = 1 - pred_prob
        else:
            high_quality_score = pred_prob

        score = round(float(high_quality_score), PRECISION)
        return [(0, len(document), score)]


class RPS_Doc_ML_Wikiref_Score(BaseMLSignal):  # noqa
    r""" Fasttext classifier prediction for the document being a Wikipedia
    reference. This is the same fasttext model as in the RedPajama-1T
    dataset."""
    __slots__ = ()

    def __init__(self, wikiref_model: str, *args, **kwargs):  # noqa
        super(RPS_Doc_ML_Wikiref_Score, self).__init__(
            ft_model_file=wikiref_model
        )


class RPS_Doc_ML_Palm_Score(BaseMLSignal):  # noqa
    r""" Fasttext classifier prediction for the document being a Wikipedia
    article, OpenWebText sample or a RedPajama-V1 book."""
    __slots__ = ()

    def __init__(self, palm_model: str, *args, **kwargs):  # noqa
        super(RPS_Doc_ML_Palm_Score, self).__init__(
            ft_model_file=palm_model
        )


class RPS_Doc_ML_Wikipedia_Score(BaseMLSignal):  # noqa
    r""" Fasttext classifier prediction for the document being a Wikipedia
    article."""
    __slots__ = ()

    def __init__(self, wikipedia_model: str, *args, **kwargs):  # noqa
        super(RPS_Doc_ML_Wikipedia_Score, self).__init__(
            ft_model_file=wikipedia_model
        )

File Path: app/src/core/quality_signals/content.py
Content:
import re
import sys
import operator
from pathlib import Path
from typing import List, Tuple, Type

from core.constants import PRECISION
from core.quality_signals.base import RPSBase
from core.quality_signals.utils.stop_words import get_stop_words
from core.document import Document
from core.data_types import SignalType
from core.quality_signals.utils.content import \
    load_bad_words, load_bad_urls_index
from utilities.register.registry_utils import *
from utilities.text import form_ngrams

__all__ = ["register_content_callables", "content_schema"]


def content_schema() -> List[Tuple[str, Type]]:
    r""" Returns a list of signal names and their data types """
    return signal_schema(module=sys.modules[__name__])


def register_content_callables(
        language: str, bad_urls_dir: str, bad_words_dir: str
) -> List[RPSBase]:
    r""" Returns a list of signal functions (i.e., RPSBase instances) that
    are used to extract content signals from a document.

    Args:
        language: The language of the document.
        bad_urls_dir: directory containing the UT1 blacklist.
        bad_words_dir: directory containing the LDNOOBW blacklist.

    Returns:
        A list of signal function class instances.
    """
    return list(map(
        lambda cls: cls(
            language=language,
            bad_urls_dir=bad_urls_dir,
            bad_words_dir=bad_words_dir
        ),
        get_callables_from_module(module=sys.modules[__name__])
    ))


class RPS_Doc_LDNOOBW_Words(RPSBase):  # noqa
    r""" The number of sequences of words that are contained in the
    List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words blocklist. The
    blocklist is obtained from
    https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words
    """
    __slots__ = ["_block_words", "_gram_vals"]

    def __init__(
            self, bad_words_dir: str, language: str, *args, **kwargs  # noqa
    ):
        super(RPS_Doc_LDNOOBW_Words, self).__init__()
        self._block_words = load_bad_words(
            bad_words_dir=Path(bad_words_dir), lang=language
        )

        # cache the number of words in each block list entry
        self._gram_vals = set(map(
            lambda w: 1 + operator.countOf(w, " "), self._block_words
        ))

    def __call__(self, document: Document) -> SignalType:
        if len(document.normalized_content) == 0:
            return [(0, len(document), .0)]

        num_dirty = 0

        # for each ngram value, count the number of ngrams in the document
        # which are also in the block words list
        for n in self._gram_vals:
            if n == 1:
                num_dirty += sum(
                    1 for _ in filter(
                        lambda w: w in self._block_words,
                        document.normalized_words
                    )
                )
                continue

            num_dirty += sum(
                1 for _ in filter(
                    lambda t: " ".join(t) in self._block_words,
                    # try to fetch the cached ngrams, otherwise compute them
                    # on the fly
                    getattr(document, f"norm_{n}grams", None)
                    or
                    form_ngrams(iter(document.normalized_words), n)
                )
            )

        score = float(num_dirty)
        return [(0, len(document), score)]


class RPS_Doc_Lorem_Ipsum(RPSBase):  # noqa
    r""" The ratio between the number of occurences of 'lorem ipsum'
    and the number of characters in the text after normalization. Text is
    normalized by lowercasing and removing punctuation. """
    SEARCH_TEXT = "lorem ipsum"
    SEARCH_REGEX = re.compile(r"lorem ipsum", re.IGNORECASE)

    __slots__ = ()

    def __call__(self, document: Document) -> SignalType:
        if len(document.normalized_content) == 0:
            return [(0, len(document), 0.0)]

        if self.SEARCH_TEXT not in document.normalized_content:
            return [(0, len(document), .0)]

        num_occurences = len(self.SEARCH_REGEX.findall(
            document.normalized_content
        ))

        score = float(num_occurences) / len(document.normalized_content)
        score = round(score, PRECISION)

        return [(0, len(document), score)]


class RPS_Doc_Curly_Bracket(RPSBase):  # noqa
    r""" The ratio between the number of occurences of '{' or '}' and the
    number of characters in the raw text. """
    SEARCH_TEXT = ("{", "}")
    __slots__ = ()

    def __call__(self, document: Document) -> SignalType:
        if len(document.raw_content) == 0:
            return [(0, len(document), .0)]

        if all(map(lambda x: x not in document.raw_content, self.SEARCH_TEXT)):
            return [(0, len(document), .0)]

        num_occurences = sum(
            map(lambda x: operator.countOf(document.raw_content, x),
                self.SEARCH_TEXT)
        )

        score = float(num_occurences) / len(document.raw_content)
        score = round(score, PRECISION)

        return [(0, len(document), score)]


class RPS_Doc_UT1_Blacklist(RPSBase):  # noqa
    r""" An categorical id of the list of categories of the domain of the
    document. Categories are obtained from the UT1 blacklist.
    """
    __slots__ = ["_ut1_mapping"]

    def __init__(self, bad_urls_dir: str, *args, **kwargs):  # noqa
        super(RPS_Doc_UT1_Blacklist, self).__init__()
        self._ut1_mapping = load_bad_urls_index(Path(bad_urls_dir))

    def __call__(self, document: Document) -> SignalType:
        score: int = self._ut1_mapping.get(document.domain, None)
        return [(0, len(document), score)]


class RPS_Doc_Stop_Word_Fraction(RPSBase):  # noqa
    r""" The ratio between the number of stop words and the number of words in
    the document. """
    __slots__ = ["_stop_words"]

    def __init__(self, language: str, *args, **kwargs):  # noqa
        super(RPS_Doc_Stop_Word_Fraction, self).__init__()
        self._stop_words = get_stop_words(language)

    def __call__(self, document: Document) -> SignalType:
        if len(document.normalized_words) == 0:
            return [(0, len(document), .0)]

        num_stop_words = sum(
            map(lambda w: w in self._stop_words, document.raw_words)
        )

        score = float(num_stop_words) / document.num_raw_words
        score = round(score, PRECISION)

        return [(0, len(document), score)]

File Path: app/src/core/quality_signals/importance_weights.py
Content:
import numpy as np
import scipy.stats as stats
import sys
from typing import List, Tuple, Type, Optional
from pathlib import Path

from core.constants import PRECISION
from core.quality_signals.base import RPSBase
from core.quality_signals.utils.dsir import hash_feature
from core.document import Document
from core.data_types import SignalType

from utilities.register.registry_utils import *
from utilities.text import form_ngrams

__all__ = [
    "register_importance_weights_callables",
    "importance_weights_schema"
]


def importance_weights_schema() -> List[Tuple[str, Type]]:
    r""" Returns a list of signal names and their data types """
    return signal_schema(module=sys.modules[__name__])


def register_importance_weights_callables(
        source_fps: Optional[Tuple[str]],
        wiki_fps: Optional[Tuple[str]],
        openwebtext_fps: Optional[Tuple[str]],
        books_fps: Optional[Tuple[str]],
        language: str
) -> List[RPSBase]:
    r""" Returns a list of signal functions (i.e., RPSBase instances) that
    are used to extract content signals from a document.

    Returns:
        A list of signal function class instances.
    """
    return list(map(
        lambda cls: cls(
            language=language,
            source_fps=source_fps,
            wiki_fps=wiki_fps,
            openwebtext_fps=openwebtext_fps,
            books_fps=books_fps
        ),
        get_callables_from_module(module=sys.modules[__name__])
    ))


class Base_Importance(RPSBase):  # noqa
    r""" Base class for functions which return the log ratio of the likelihood
    of the document's features with respect to the target domain
    versus the source domain. """

    __slots__ = (
        "_log_diff_dist", "_feature_dim", "_target_lambda",
        "_source_lambda", "_length_correction"
    )

    def __init__(
            self,
            target_fps: Tuple[str, str],
            source_fps: Tuple[str, str],
            language: str,
            length_correction: bool = False
    ):
        super(Base_Importance, self).__init__()
        self._length_correction = length_correction

        if target_fps is None or source_fps is None:
            self._log_diff_dist = None
            self._feature_dim = None
            return

        target_count_fp, target_lambbda_fp = target_fps
        source_count_fp, source_lambda_fp = source_fps

        assert language == Path(target_count_fp).stem.split(".")[1], \
            f"Language mismatch between {target_count_fp} and {language}"

        assert language == Path(source_count_fp).stem.split(".")[1], \
            f"Language mismatch between {target_count_fp} and {language}"

        # load hash counts
        target_counts = np.load(target_count_fp)
        target_dist = target_counts / target_counts.sum()
        source_counts = np.load(source_count_fp)
        source_dist = source_counts / source_counts.sum()

        if length_correction:
            self._target_lambda = np.load(target_lambbda_fp)
            self._source_lambda = np.load(source_lambda_fp)
        else:
            self._target_lambda = None
            self._source_lambda = None

        # compute log diff dist
        self._feature_dim = target_counts.shape[0]
        self._log_diff_dist = np.array(
            np.log(target_dist + 1e-8) - np.log(source_dist + 1e-8)
        )

    def __call__(self, document: Document) -> SignalType:
        if self._log_diff_dist is None:
            return [(0, len(document), None)]

        doc_len = len(document)

        if doc_len == 0:
            return [(0, doc_len, None)]

        # try to fetch cached features, if not compute them
        features = (
            document.hash_features
            if document.hash_features is not None
            else
            hash_feature(
                unigrams=document.raw_words,
                # fetch cached bigrams, otherwise comptue them
                bigrams=(
                        document.raw_2grams
                        or
                        tuple(form_ngrams(iter(document.raw_words), 2))
                ),
                buckets=self._feature_dim
            )
        )

        logratio = np.inner(features, self._log_diff_dist)
        score = float(logratio)

        if not self._length_correction:
            score = round(score, PRECISION)
            return [(0, doc_len, score)]

        # correct for the length assuming a Poisson distribution
        return self.__add_length_penalty(score, doc_len)

    def __add_length_penalty(self, score, doc_len):
        # correct for the length assuming a Poisson distribution
        len_prob_source = stats.poisson.pmf(doc_len, self._source_lambda)
        len_prob_target = stats.poisson.pmf(doc_len, self._target_lambda)

        len_correction = np.log(len_prob_target + 1e-8) - \
                         np.log(len_prob_source + 1e-8)

        score += float(len_correction)
        score = round(score, PRECISION)
        return [(0, doc_len, score)]


class RPS_Doc_Wikipedia_Importance(Base_Importance):  # noqa
    r""" Given a bag of {1,2}-wordgram model trained on Wikipedia articles p,
    and a model trained on the source domain q. This is the logarithm of the
    ratio p(doc)/q(doc). If length_correction is enabled, then the length of
    score is adjusted by adding the term log(p_poisson(len) / q_poisson(len))
    to the final score.
    """
    __slots__ = ()

    def __init__(
            self,
            wiki_fps: Tuple[str, str],
            source_fps: Tuple[str, str],
            language: str,
            *args, **kwargs  # noqa
    ):
        super(RPS_Doc_Wikipedia_Importance, self).__init__(
            target_fps=wiki_fps,
            source_fps=source_fps,
            language=language,
            length_correction=False
        )


class RPS_Doc_Wikipedia_Importance_Length_Correction(  # noqa
    Base_Importance
):
    r""" Given a bag of {1,2}-wordgram model trained on Wikipedia articles p,
    and a model trained on the source domain q. This is the logarithm of the
    ratio p(doc)/q(doc). If length_correction is enabled, then the length of
    score is adjusted by adding the term log(p_poisson(len) / q_poisson(len))
    to the final score. Corrects for length by adding a length penalty term.
    """
    __slots__ = ()

    def __init__(
            self,
            wiki_fps: Tuple[str, str],
            source_fps: Tuple[str, str],
            language: str,
            *args, **kwargs  # noqa
    ):
        super(RPS_Doc_Wikipedia_Importance_Length_Correction,
              self).__init__(
            target_fps=wiki_fps,
            source_fps=source_fps,
            language=language,
            length_correction=True
        )


class RPS_Doc_Books_Importance(Base_Importance):  # noqa
    r""" Given a bag of {1,2}-wordgram model trained on Books p,
    and a model trained on the source domain q. This is the logarithm of the
    ratio p(doc)/q(doc). If length_correction is enabled, then the length of
    score is adjusted by adding the term log(p_poisson(len) / q_poisson(len))
    to the final score.
    """
    __slots__ = ()

    def __init__(
            self,
            books_fps: Tuple[str, str],
            source_fps: Tuple[str, str],
            language: str,
            *args, **kwargs  # noqa
    ):
        super(RPS_Doc_Books_Importance, self).__init__(
            target_fps=books_fps,
            source_fps=source_fps,
            language=language,
            length_correction=False
        )


class RPS_Doc_Books_Importance_Length_Correction(  # noqa
    Base_Importance
):  # noqa
    r""" Given a bag of {1,2}-wordgram model trained on Books p,
    and a model trained on the source domain q. This is the logarithm of the
    ratio p(doc)/q(doc). If length_correction is enabled, then the length of
    score is adjusted by adding the term log(p_poisson(len) / q_poisson(len))
    to the final score. Corrects for length by adding a length penalty term.
    """
    __slots__ = ()

    def __init__(
            self,
            books_fps: Tuple[str, str],
            source_fps: Tuple[str, str],
            language: str,
            *args, **kwargs  # noqa
    ):
        super(RPS_Doc_Books_Importance_Length_Correction, self).__init__(
            target_fps=books_fps,
            source_fps=source_fps,
            language=language,
            length_correction=True
        )


class RPS_Doc_OpenWebText_Importance(Base_Importance):  # noqa
    r""" Given a bag of {1,2}-wordgram model trained on OpenWebText p,
    and a model trained on the source domain q. This is the logarithm of the
    ratio p(doc)/q(doc). If length_correction is enabled, then the length of
    score is adjusted by adding the term log(p_poisson(len) / q_poisson(len))
    to the final score.
    """
    __slots__ = ()

    def __init__(
            self,
            openwebtext_fps: Tuple[str, str],
            source_fps: Tuple[str, str],
            language: str,
            *args, **kwargs  # noqa
    ):
        super(RPS_Doc_OpenWebText_Importance, self).__init__(
            target_fps=openwebtext_fps,
            source_fps=source_fps,
            language=language,
            length_correction=False
        )


class RPS_Doc_OpenWebText_Importance_Length_Correction(  # noqa
    Base_Importance):  # noqa
    r""" Given a bag of {1,2}-wordgram model trained on OpenWebText p,
    and a model trained on the source domain q. This is the logarithm of the
    ratio p(doc)/q(doc). If length_correction is enabled, then the length of
    score is adjusted by adding the term log(p_poisson(len) / q_poisson(len))
    to the final score. Corrects for length by adding a length penalty term.
    """
    __slots__ = ()

    def __init__(
            self,
            openwebtext_fps: Tuple[str, str],
            source_fps: Tuple[str, str],
            language: str,
            *args, **kwargs  # noqa
    ):
        super(
            RPS_Doc_OpenWebText_Importance_Length_Correction, self
        ).__init__(
            target_fps=openwebtext_fps,
            source_fps=source_fps,
            language=language,
            length_correction=True
        )

File Path: app/src/core/quality_signals/lines.py
Content:
import sys
from typing import List, Tuple, Type

from core.constants import PRECISION
from core.quality_signals.base import RPSBase
from core.data_types import SignalType, ScoreType, TextSlice
from core.document import Document
from utilities.register.registry_utils import *

__all__ = [
    "register_lines_callables", "lines_schema"
]


def lines_schema() -> List[Tuple[str, Type]]:
    r""" Returns a list of signal names and their data types """
    return signal_schema(module=sys.modules[__name__])


def register_lines_callables() -> List[RPSBase]:
    r""" Returns a list of signal functions (i.e., RPSBase instances) that
    are used to extract line signals from a document.

    Returns:
        A list of signal function class instances.
    """
    return list(map(
        lambda cls: cls(),
        get_callables_from_module(module=sys.modules[__name__])
    ))


class RPS_Lines_Javascript_Counts(RPSBase):  # noqa
    r""" The number of occurences of the word "javascript" in each line. """
    SEARCH_TEXT = "javascript"
    __slots__ = ()

    def _process_line(self, text_slice: TextSlice) -> ScoreType:
        if len(text_slice.text) == 0:
            return tuple((text_slice.start, text_slice.end, 0.0))

        score = float(sum(
            1 for w in text_slice.text.split() if w == self.SEARCH_TEXT
        ))

        return tuple((text_slice.start, text_slice.end, score))

    def __call__(self, document: Document) -> SignalType:
        return list(map(self._process_line, document.normalized_lines))


class RPS_Lines_Ending_With_Terminal_Punctution_Mark(RPSBase):  # noqa
    r""" A list of integers indicating whether (1) or not (0) a line ends with
    a terminal punctuation mark. A terminal punctation mark is defined as
    one of the following: ".", "!", "?", "”" """
    TERMINAL_PUNCTUATION_MARKS = (".", "!", "?", "”")
    __slots__ = ()

    def _process_line(self, text_slice: TextSlice) -> ScoreType:
        score = text_slice.text.rstrip().endswith(
            self.TERMINAL_PUNCTUATION_MARKS
        )
        score = float(score)
        return tuple((text_slice.start, text_slice.end, score))

    def __call__(self, document: Document) -> SignalType:
        return list(map(self._process_line, document.raw_lines))


class RPS_Lines_Num_Words(RPSBase):  # noqa
    r""" The number of words in each line. This is computed based on the
    normalied text. Normalization is done by lowercasing the text and
    removing punctuation."""
    __slots__ = ()

    def _process_line(self, text_slice: TextSlice) -> ScoreType:  # noqa
        score = len(text_slice.text.split())
        return tuple((text_slice.start, text_slice.end, score))

    def __call__(self, document: Document) -> SignalType:
        return list(map(self._process_line, document.normalized_lines))


class RPS_Lines_Uppercase_Letter_Fraction(RPSBase):  # noqa
    r""" The ratio between number of uppercase letters and total number of
    characters in each line. This is based on the raw text. """
    __slots__ = ()

    def _process_line(self, text_slice: TextSlice) -> ScoreType:  # noqa
        if len(text_slice) == 0:
            return tuple((text_slice.start, text_slice.end, 0.0))

        score = sum(map(str.isupper, text_slice.text)) / len(text_slice)
        score = round(score, PRECISION)
        return tuple((text_slice.start, text_slice.end, score))

    def __call__(self, document: Document) -> SignalType:
        return list(map(self._process_line, document.raw_lines))


class RPS_Lines_Numerical_Chars_Fraction(RPSBase):  # noqa
    r""" The ratio between number of numerical characters and total number of
    characters in each line. This is based on text after lowercasing and
    removing punctuation."""
    __slots__ = ()

    def _process_line(self, text_slice: TextSlice) -> ScoreType:  # noqa
        if len(text_slice) == 0:
            return tuple((text_slice.start, text_slice.end, 0.0))

        score = sum(map(str.isnumeric, text_slice.text)) / len(text_slice)
        score = round(score, PRECISION)
        return tuple((text_slice.start, text_slice.end, score))

    def __call__(self, document: Document) -> SignalType:
        return list(map(self._process_line, document.normalized_lines))


class RPS_Lines_Start_With_Bulletpoint(RPSBase):  # noqa
    r""" Whether the lines that start with a bullet point symbol. The
    following set of unicodes are considered a bullet point:
    \u2022 (bullet point), \u2023 (triangular bullet point), \u25B6 (black
    right pointing triangle), \u25C0 (black left pointing triangle),
    \u25E6 (white bullet point), \u25A0 (black square), \u25A1 (white
    square), \u25AA (black small square), \u25AB (white small square),
    \u2013 (en dash)."""
    BULLET_POINT_SYMBOLS = (
        "\u2022",  # bullet point
        "\u2023",  # triangular bullet point
        "\u25B6",  # black right pointing triangle
        "\u25C0",  # black left pointing triangle
        "\u25E6",  # white bullet point
        "\u25A0",  # black square
        "\u25A1",  # white square
        "\u25AA",  # black small square
        "\u25AB",  # white small square
        "\u2013",  # en dash
    )

    __slots__ = ()

    def _process_line(self, text_slice: TextSlice) -> ScoreType:  # noqa
        score = text_slice.text.lstrip().startswith(self.BULLET_POINT_SYMBOLS)
        score = float(score)
        return tuple((text_slice.start, text_slice.end, score))

    def __call__(self, document: Document) -> SignalType:
        num_lines = len(document.raw_lines)

        if num_lines == 0:
            return [(0, len(document), None)]

        return list(map(self._process_line, document.raw_lines))

File Path: app/src/core/quality_signals/natural_language.py
Content:
from collections import Counter
import math
import re
import sys
from typing import List, Tuple, Type

from core.constants import PRECISION
from core.data_types import SignalType
from core.quality_signals.base import RPSBase
from core.document import Document
from utilities.register.registry_utils import *

__all__ = [
    "register_natural_language_callables",
    "natural_language_schema"
]


def natural_language_schema() -> List[Tuple[str, Type]]:
    r""" Returns a list of signal names and their data types """
    return signal_schema(module=sys.modules[__name__])


def register_natural_language_callables() -> List[RPSBase]:
    r""" Returns a list of signal functions (i.e., RPSBase instances) that
    are used to extract natural language signals from a document.

    Returns:
        A list of signal function class instances.
    """
    return list(map(
        lambda cls: cls(),
        get_callables_from_module(module=sys.modules[__name__])
    ))


class RPS_Doc_Num_Sentences(RPSBase):  # noqa
    r""" The number of sentences in the content. This is calculated using
    the regex r'\b[^.!?]+[.!?]*' """
    SENT_PATTERN = re.compile(r'\b[^.!?]+[.!?]*', flags=re.UNICODE)

    __slots__ = ()

    def __call__(self, document: Document) -> SignalType:
        r""" count the number of sentences in the content using regex"""
        score = float(len(self.SENT_PATTERN.findall(document.raw_content)))
        return [(0, len(document), score)]


class RPS_Doc_Word_Count(RPSBase):  # noqa
    r""" The number of words in the content after normalization. """
    __slots__ = ()

    def __call__(self, document: Document) -> SignalType:
        return [(0, len(document), document.num_normalized_words)]


class RPS_Doc_Mean_Word_Length(RPSBase):  # noqa
    r""" The mean length of words in the content normalization. """
    __slots__ = ()

    def __call__(self, document: Document) -> SignalType:
        if document.num_normalized_words == 0:
            return [(0, len(document), None)]

        num_chars = float(sum(map(len, document.normalized_words)))
        score = num_chars / document.num_normalized_words
        score = round(score, PRECISION)
        return [(0, len(document), score)]


class RPS_Doc_Symbol_To_Word_Ratio(RPSBase):  # noqa
    r""" The ratio of symbols to words in the content. This is analogous to
    the signal used in Gopher. Symbols are defined "#", "...", and "…". """
    SYMBOLS = ("#", "...", "…")

    __slots__ = ()

    def __call__(self, document: Document) -> SignalType:
        num_words = document.num_raw_words

        if num_words == 0:
            return [(0, len(document), None)]

        # count the number of symbols in the content
        num_symbols = float(sum(
            document.raw_content.count(x) for x in self.SYMBOLS
        ))

        score = num_symbols / num_words
        score = round(score, PRECISION)
        return [(0, len(document), score)]


class RPS_Doc_Frac_Lines_End_With_Ellipsis(RPSBase):  # noqa
    r""" The fraction of lines that end with an ellipsis, where an ellipsis
    is defined as either "..." or "…". """
    ELLIPSIS_SYMBOLS = ("...", "…")

    __slots__ = ()

    def __call__(self, document: Document) -> SignalType:
        num_lines = len(document.raw_lines)

        if num_lines == 0:
            return [(0, len(document), None)]

        total_ellipsis_lines = float(sum(
            text_slice.text.rstrip().endswith(self.ELLIPSIS_SYMBOLS)
            for text_slice in document.raw_lines
        ))

        score = total_ellipsis_lines / num_lines
        score = round(score, PRECISION)
        return [(0, len(document), score)]


class RPS_Doc_Frac_No_Alph_Words(RPSBase):  # noqa
    r""" The fraction of words that contain no alphabetical character.
    This is based on the raw content. """
    ALPH_REGEX = re.compile(r"[a-zA-Z]")

    __slots__ = ()

    def __call__(self, document: Document) -> SignalType:
        num_words = document.num_raw_words

        if num_words == 0:
            return [(0, len(document), None)]

        num_words_with_alpha = float(sum(
            int(self.ALPH_REGEX.search(word) is not None)
            for word in document.raw_words
        ))

        score = 1.0 - num_words_with_alpha / num_words
        score = round(score, PRECISION)
        return [(0, len(document), score)]


class RPS_Doc_Frac_Unique_Words(RPSBase):  # noqa
    r""" The fraction of unique words in the content. This is also known as
    the degeneracy of a text sample. Calculated based on the normalized
    content. """
    __slots__ = ()

    def __call__(self, document: Document) -> SignalType:
        num_words = document.num_normalized_words

        if num_words == 0:
            return [(0, len(document), None)]

        score = float(len(set(document.normalized_words))) / num_words
        score = round(score, PRECISION)
        return [(0, len(document), score)]


class RPS_Doc_Unigram_Entropy(RPSBase):  # noqa
    r""" The entropy of the unigram distribution of the
    content. This measures the diversity of the content and is computed
    using sum(-x / total * log(x / total)) where the sum is taken over
    over counts of unique words in the noramlized (punctuation removed,
    lowercased) content."""
    __slots__ = ()

    def __call__(self, document: Document) -> SignalType:
        if len(document.normalized_words) == 0:
            return [(0, len(document), None)]

        # count the number of times each word appears in the content
        counter = Counter(document.normalized_words)

        # calculate the entropy of the unigram distribution
        total = sum(counter.values())
        entropy = sum(map(
            lambda x: -x / total * math.log(x / total) if x > 0 else 0.0,
            counter.values()
        ))

        score = round(entropy, PRECISION)
        return [(0, len(document), score)]


class RPS_Doc_Frac_All_Caps_Words(RPSBase):  # noqa
    r""" The fraction of words in the content that only conist of uppercase
    letters. This is based on the raw content."""
    __slots__ = ()

    def __call__(self, document: Document) -> SignalType:
        num_words = document.num_raw_words

        if num_words == 0:
            return [(0, len(document), None)]

        score = float(sum(map(str.isupper, document.raw_words))) / num_words
        score = round(score, PRECISION)
        return [(0, len(document), score)]

File Path: app/src/core/quality_signals/repetitions.py
Content:
from collections import Counter
import numpy as np
import sys
from typing import List, Tuple, Type

from core.constants import PRECISION
from core.quality_signals.base import RPSBase
from core.document import Document
from core.data_types import SignalType
from utilities.register.registry_utils import *
from utilities.text import form_ngrams

__all__ = [
    "register_repetitions_callables",
    "repetitions_schema"
]


def repetitions_schema() -> List[Tuple[str, Type]]:
    r""" Returns a list of signal names and their data types """
    return signal_schema(module=sys.modules[__name__])


def register_repetitions_callables() -> List[RPSBase]:
    r""" Returns a list of signal functions (i.e., RPSBase instances) that
    are used to extract repetition related signals from a document.

    Returns:
        A list of signal function class instances.
    """
    return list(map(
        lambda cls: cls(),
        get_callables_from_module(module=sys.modules[__name__])
    ))


class Base_RPS_Frac_Chars_In_Top_NGram(RPSBase):  # noqa
    r""" Base class for calculating the fraction of characters in the
    top N-gram. This operates on the lower-cased, punctation removed
    content."""
    NGRAM_SIZE: int = None

    __slots__ = []

    def __call__(self, document: Document) -> SignalType:
        if self.NGRAM_SIZE is None:
            raise NotImplementedError(
                "NGRAM_SIZE must be set in the subclass"
            )

        # get the most common ngram
        most_common_ngram = Counter(
            # fetch the ngrams from the document if they exist, otherwise
            # compute them
            getattr(document, f"norm_{self.NGRAM_SIZE}grams", None)
            or
            form_ngrams(iter(document.normalized_words), self.NGRAM_SIZE)
        ).most_common(1)

        if len(most_common_ngram) == 0:
            return [(0, len(document), 0.0)]

        ngram, count = most_common_ngram[0]

        if count <= 1:
            return [(0, len(document), 0.0)]

        total_chars = sum(len(w) for w in document.normalized_words)
        score = sum(len(w) for w in ngram) * count / total_chars
        score = round(score, PRECISION)
        return [(0, len(document), score)]


class RPS_Doc_Frac_Chars_Top_2gram(Base_RPS_Frac_Chars_In_Top_NGram):  # noqa
    r""" The fraction of characters in the top word Bigram. Operates on the
    lower-cased, punctation removed content."""
    NGRAM_SIZE = 2
    __slots__ = []


class RPS_Doc_Frac_Chars_Top_3gram(Base_RPS_Frac_Chars_In_Top_NGram):  # noqa
    r""" The fraction of characters in the top word Trigram. Operates on the
    lower-cased, punctation removed content."""
    NGRAM_SIZE = 3
    __slots__ = []


class RPS_Doc_Frac_Chars_Top_4gram(Base_RPS_Frac_Chars_In_Top_NGram):  # noqa
    r""" The fraction of characters in the top word 4gram. Operates on the
    lower-cased, punctation removed content."""
    NGRAM_SIZE = 4
    __slots__ = []


class Base_RPS_Frac_Chars_In_Dupe_NGrams(RPSBase):  # noqa
    r""" Base class for calculating the fraction of characters in
    duplicate word N-grams. This operates on the lower-cased, punctation
    removed content. The function also ensures that characters in overlapping
    ngrams are only counted once."""
    NGRAM_SIZE: int = None
    __slots__ = []

    def __call__(self, document: Document) -> SignalType:
        if self.NGRAM_SIZE is None:
            raise NotImplementedError(
                "NGRAM_SIZE must be set in the subclass"
            )

        if len(document.normalized_words) < self.NGRAM_SIZE:
            return [(0, len(document), 0.0)]

        # fetch the ngrams from the document if they exist, otherwise
        # compute them
        doc_n_grams = (
                getattr(document, f"norm_{self.NGRAM_SIZE}grams", None)
                or
                tuple(form_ngrams(
                    iter(document.normalized_words), self.NGRAM_SIZE
                ))
        )

        # keep only ngrams which occur at least twice
        ngram_dupes = {
            ngram for ngram, count in Counter(doc_n_grams).items() if count > 1
        }

        duplicated_grams = np.zeros(len(document.normalized_words), dtype=int)

        i = 0
        for ngram in doc_n_grams:
            if ngram in ngram_dupes:
                duplicated_grams[i: i + self.NGRAM_SIZE] = 1

            i += 1

        word_lengths = np.array(list(map(len, document.normalized_words)))
        chars_duped = np.sum(word_lengths * duplicated_grams)
        total_chars = np.sum(word_lengths)

        if total_chars == 0:
            return [(0, len(document), 0.0)]

        score = float(chars_duped / total_chars)
        score = round(score, PRECISION)
        return [(0, len(document), score)]


class RPS_Doc_Frac_Chars_Dupe_5Grams(  # noqa
    Base_RPS_Frac_Chars_In_Dupe_NGrams
):
    r""" The fraction of characters in duplicate word 5grams. This operates on
    the lower-cased, punctation removed content. It is also ensured that
    characters in overlapping ngrams are only counted once. """
    NGRAM_SIZE = 5
    __slots__ = []


class RPS_Doc_Frac_Chars_Dupe_6Grams(  # noqa
    Base_RPS_Frac_Chars_In_Dupe_NGrams
):
    r""" The fraction of characters in duplicate word 6grams. This operates on
    the lower-cased, punctation removed content. It is also ensured that
    characters in overlapping ngrams are only counted once. """
    NGRAM_SIZE = 6
    __slots__ = []


class RPS_Doc_Frac_Chars_Dupe_7Grams(  # noqa
    Base_RPS_Frac_Chars_In_Dupe_NGrams
):
    r""" The fraction of characters in duplicate word 7grams. This operates on
    the lower-cased, punctation removed content. It is also ensured that
    characters in overlapping ngrams are only counted once. """
    NGRAM_SIZE = 7
    __slots__ = []


class RPS_Doc_Frac_Chars_Dupe_8Grams(  # noqa
    Base_RPS_Frac_Chars_In_Dupe_NGrams
):
    r""" The fraction of characters in duplicate word 8grams. This operates on
    the lower-cased, punctation removed content. It is also ensured that
    characters in overlapping ngrams are only counted once. """
    NGRAM_SIZE = 8
    __slots__ = []


class RPS_Doc_Frac_Chars_Dupe_9Grams(  # noqa
    Base_RPS_Frac_Chars_In_Dupe_NGrams
):
    r""" The fraction of characters in duplicate word 9grams. This operates on
    the lower-cased, punctation removed content. It is also ensured that
    characters in overlapping ngrams are only counted once. """
    NGRAM_SIZE = 9
    __slots__ = []


class RPS_Doc_Frac_Chars_Dupe_10Grams(  # noqa
    Base_RPS_Frac_Chars_In_Dupe_NGrams
):
    r""" The fraction of characters in duplicate word 10grams. This operates on
    the lower-cased, punctation removed content. It is also ensured that
    characters in overlapping ngrams are only counted once. """
    NGRAM_SIZE = 10
    __slots__ = []

File Path: app/src/core/quality_signals/utils/__init__.py
Content:

File Path: app/src/core/quality_signals/utils/classifiers.py
Content:
from core.document import Document


def preprocess_quality_classifier(document: Document):
    r""" Preprocesses a document for quality classification. This function
    removes all newlines and trailing whitespaces from the document.

    Args:
        document: A document.

    Returns:
        A string.
    """
    # remove newlines and trailing and leading whitespaces
    return " ".join(document.raw_content.splitlines()).strip()

File Path: app/src/core/quality_signals/utils/content.py
Content:
import json
from pathlib import Path
from typing import Dict, Set

_DEFAULT_LANGS = ("en", "fr", "it", "es", "de")


def load_bad_urls_index(bad_urls_dir: Path) -> Dict[str, int]:
    with open(bad_urls_dir / "domain_to_category_id.json", "r") as f:
        domain_to_category_id = json.load(f)
    return domain_to_category_id


def load_bad_words(bad_words_dir: Path, lang: str) -> Set[str]:
    r""" load the LDNOOBW word list for a given language

    Source:
        https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words

    Args:
        bad_words_dir (Path): The path to the resources directory where the
            list is stored
        lang (str): The language for which to fetch the word list

    Returns:
        A set of words
    """
    if lang not in _DEFAULT_LANGS:
        return set()

    ldnoobw_fp = bad_words_dir / f"{lang}.txt"

    if not ldnoobw_fp.exists():
        raise FileNotFoundError(f"LDNOOBW word list {ldnoobw_fp} not found!")

    with open(ldnoobw_fp, 'r') as f:
        data = set(ln.strip() for ln in f.readlines())

    return data

File Path: app/src/core/quality_signals/utils/dsir.py
Content:
import numpy as np
from typing import Tuple


def compute_hash(ngram: str, buckets: int):
    return int(abs(hash(ngram)) % buckets)


def hash_feature(
        unigrams: Tuple[str], bigrams: Tuple[str], buckets: int
) -> np.ndarray:
    counts = np.zeros(buckets, dtype=np.int64)

    for unigram in unigrams:
        counts[compute_hash(unigram, buckets=buckets)] += 1

    for bigram in bigrams:
        counts[compute_hash(bigram, buckets=buckets)] += 1

    return counts

File Path: app/src/core/quality_signals/utils/stop_words.py
Content:
"""
The stop words in this file are taken from https://github.com/6/stopwords-json
"""

from typing import Set

__all__ = ["get_stop_words"]


def get_stop_words(lang) -> Set[str]:
    return stop_words[lang]


stop_words = {
    "bg": {"а", "автентичен", "аз", "ако", "ала", "бе", "без", "беше",
           "би", "бивш", "бивша", "бившо", "бил", "била", "били", "било",
           "благодаря", "близо", "бъдат", "бъде", "бяха", "в", "вас",
           "ваш", "ваша", "вероятно", "вече", "взема", "ви", "вие",
           "винаги", "внимава", "време", "все", "всеки", "всички",
           "всичко", "всяка", "във", "въпреки", "върху", "г", "ги",
           "главен", "главна", "главно", "глас", "го", "година",
           "години", "годишен", "д", "да", "дали", "два", "двама",
           "двамата", "две", "двете", "ден", "днес", "дни", "до",
           "добра", "добре", "добро", "добър", "докато", "докога",
           "дори", "досега", "доста", "друг", "друга", "други", "е",
           "евтин", "едва", "един", "една", "еднаква", "еднакви",
           "еднакъв", "едно", "екип", "ето", "живот", "за", "забавям",
           "зад", "заедно", "заради", "засега", "заспал", "затова",
           "защо", "защото", "и", "из", "или", "им", "има", "имат",
           "иска", "й", "каза", "как", "каква", "какво", "както",
           "какъв", "като", "кога", "когато", "което", "които", "кой",
           "който", "колко", "която", "къде", "където", "към", "лесен",
           "лесно", "ли", "лош", "м", "май", "малко", "ме", "между",
           "мек", "мен", "месец", "ми", "много", "мнозина", "мога",
           "могат", "може", "мокър", "моля", "момента", "му", "н", "на",
           "над", "назад", "най", "направи", "напред", "например", "нас",
           "не", "него", "нещо", "нея", "ни", "ние", "никой", "нито",
           "нищо", "но", "нов", "нова", "нови", "новина", "някои",
           "някой", "няколко", "няма", "обаче", "около", "освен",
           "особено", "от", "отгоре", "отново", "още", "пак", "по",
           "повече", "повечето", "под", "поне", "поради", "после",
           "почти", "прави", "пред", "преди", "през", "при", "пък",
           "първата", "първи", "първо", "пъти", "равен", "равна", "с",
           "са", "сам", "само", "се", "сега", "си", "син", "скоро",
           "след", "следващ", "сме", "смях", "според", "сред", "срещу",
           "сте", "съм", "със", "също", "т", "т.н.", "тази", "така",
           "такива", "такъв", "там", "твой", "те", "тези", "ти", "то",
           "това", "тогава", "този", "той", "толкова", "точно", "три",
           "трябва", "тук", "тъй", "тя", "тях", "у", "утре", "харесва",
           "хиляди", "ч", "часа", "че", "често", "чрез", "ще", "щом",
           "юмрук", "я", "як"},
    "de": {"Ernst", "Ordnung", "Schluss", "a", "ab", "aber", "ach", "acht",
           "achte", "achten", "achter", "achtes", "ag", "alle", "allein",
           "allem", "allen", "aller", "allerdings", "alles", "allgemeinen",
           "als", "also", "am", "an", "andere", "anderen", "andern", "anders",
           "au", "auch", "auf", "aus", "ausser", "ausserdem", "außer",
           "außerdem", "b", "bald", "bei", "beide", "beiden", "beim",
           "beispiel", "bekannt", "bereits", "besonders", "besser", "besten",
           "bin", "bis", "bisher", "bist", "c", "d", "d.h", "da", "dabei",
           "dadurch", "dafür", "dagegen", "daher", "dahin", "dahinter",
           "damals", "damit", "danach", "daneben", "dank", "dann", "daran",
           "darauf", "daraus", "darf", "darfst", "darin", "darum", "darunter",
           "darüber", "das", "dasein", "daselbst", "dass", "dasselbe", "davon",
           "davor", "dazu", "dazwischen", "daß", "dein", "deine", "deinem",
           "deiner", "dem", "dementsprechend", "demgegenüber", "demgemäss",
           "demgemäß", "demselben", "demzufolge", "den", "denen", "denn",
           "denselben", "der", "deren", "derjenige", "derjenigen", "dermassen",
           "dermaßen", "derselbe", "derselben", "des", "deshalb", "desselben",
           "dessen", "deswegen", "dich", "die", "diejenige", "diejenigen",
           "dies", "diese", "dieselbe", "dieselben", "diesem", "diesen",
           "dieser", "dieses", "dir", "doch", "dort", "drei", "drin", "dritte",
           "dritten", "dritter", "drittes", "du", "durch", "durchaus",
           "durfte", "durften", "dürfen", "dürft", "e", "eben", "ebenso",
           "ehrlich", "ei", "ei,", "eigen", "eigene", "eigenen", "eigener",
           "eigenes", "ein", "einander", "eine", "einem", "einen", "einer",
           "eines", "einige", "einigen", "einiger", "einiges", "einmal",
           "eins", "elf", "en", "ende", "endlich", "entweder", "er", "erst",
           "erste", "ersten", "erster", "erstes", "es", "etwa", "etwas",
           "euch", "euer", "eure", "f", "folgende", "früher", "fünf", "fünfte",
           "fünften", "fünfter", "fünftes", "für", "g", "gab", "ganz", "ganze",
           "ganzen", "ganzer", "ganzes", "gar", "gedurft", "gegen",
           "gegenüber", "gehabt", "gehen", "geht", "gekannt", "gekonnt",
           "gemacht", "gemocht", "gemusst", "genug", "gerade", "gern",
           "gesagt", "geschweige", "gewesen", "gewollt", "geworden", "gibt",
           "ging", "gleich", "gott", "gross", "grosse", "grossen", "grosser",
           "grosses", "groß", "große", "großen", "großer", "großes", "gut",
           "gute", "guter", "gutes", "h", "habe", "haben", "habt", "hast",
           "hat", "hatte", "hatten", "hattest", "hattet", "heisst", "her",
           "heute", "hier", "hin", "hinter", "hoch", "hätte", "hätten", "i",
           "ich", "ihm", "ihn", "ihnen", "ihr", "ihre", "ihrem", "ihren",
           "ihrer", "ihres", "im", "immer", "in", "indem", "infolgedessen",
           "ins", "irgend", "ist", "j", "ja", "jahr", "jahre", "jahren", "je",
           "jede", "jedem", "jeden", "jeder", "jedermann", "jedermanns",
           "jedes", "jedoch", "jemand", "jemandem", "jemanden", "jene",
           "jenem", "jenen", "jener", "jenes", "jetzt", "k", "kam", "kann",
           "kannst", "kaum", "kein", "keine", "keinem", "keinen", "keiner",
           "kleine", "kleinen", "kleiner", "kleines", "kommen", "kommt",
           "konnte", "konnten", "kurz", "können", "könnt", "könnte", "l",
           "lang", "lange", "leicht", "leide", "lieber", "los", "m", "machen",
           "macht", "machte", "mag", "magst", "mahn", "mal", "man", "manche",
           "manchem", "manchen", "mancher", "manches", "mann", "mehr", "mein",
           "meine", "meinem", "meinen", "meiner", "meines", "mensch",
           "menschen", "mich", "mir", "mit", "mittel", "mochte", "mochten",
           "morgen", "muss", "musst", "musste", "mussten", "muß", "mußt",
           "möchte", "mögen", "möglich", "mögt", "müssen", "müsst", "müßt",
           "n", "na", "nach", "nachdem", "nahm", "natürlich", "neben", "nein",
           "neue", "neuen", "neun", "neunte", "neunten", "neunter", "neuntes",
           "nicht", "nichts", "nie", "niemand", "niemandem", "niemanden",
           "noch", "nun", "nur", "o", "ob", "oben", "oder", "offen", "oft",
           "ohne", "p", "q", "r", "recht", "rechte", "rechten", "rechter",
           "rechtes", "richtig", "rund", "s", "sa", "sache", "sagt", "sagte",
           "sah", "satt", "schlecht", "schon", "sechs", "sechste", "sechsten",
           "sechster", "sechstes", "sehr", "sei", "seid", "seien", "sein",
           "seine", "seinem", "seinen", "seiner", "seines", "seit", "seitdem",
           "selbst", "sich", "sie", "sieben", "siebente", "siebenten",
           "siebenter", "siebentes", "sind", "so", "solang", "solche",
           "solchem", "solchen", "solcher", "solches", "soll", "sollen",
           "sollst", "sollt", "sollte", "sollten", "sondern", "sonst",
           "soweit", "sowie", "später", "startseite", "statt", "steht",
           "suche", "t", "tag", "tage", "tagen", "tat", "teil", "tel", "tritt",
           "trotzdem", "tun", "u", "uhr", "um", "und", "und?", "uns", "unser",
           "unsere", "unserer", "unter", "v", "vergangenen", "viel", "viele",
           "vielem", "vielen", "vielleicht", "vier", "vierte", "vierten",
           "vierter", "viertes", "vom", "von", "vor", "w", "wahr?", "wann",
           "war", "waren", "wart", "warum", "was", "wegen", "weil", "weit",
           "weiter", "weitere", "weiteren", "weiteres", "welche", "welchem",
           "welchen", "welcher", "welches", "wem", "wen", "wenig", "wenige",
           "weniger", "weniges", "wenigstens", "wenn", "wer", "werde",
           "werden", "werdet", "weshalb", "wessen", "wie", "wieder", "wieso",
           "will", "willst", "wir", "wird", "wirklich", "wirst", "wissen",
           "wo", "wohl", "wollen", "wollt", "wollte", "wollten", "worden",
           "wurde", "wurden", "während", "währenddem", "währenddessen", "wäre",
           "würde", "würden", "x", "y", "z", "z.b", "zehn", "zehnte",
           "zehnten", "zehnter", "zehntes", "zeit", "zu", "zuerst", "zugleich",
           "zum", "zunächst", "zur", "zurück", "zusammen", "zwanzig", "zwar",
           "zwei", "zweite", "zweiten", "zweiter", "zweites", "zwischen",
           "zwölf", "über", "überhaupt", "übrigens"},
    "en": {"a", "a's", "able", "about", "above", "according", "accordingly",
           "across", "actually", "after", "afterwards", "again", "against",
           "ain't", "all", "allow", "allows", "almost", "alone", "along",
           "already", "also", "although", "always", "am", "among", "amongst",
           "an", "and", "another", "any", "anybody", "anyhow", "anyone",
           "anything", "anyway", "anyways", "anywhere", "apart", "appear",
           "appreciate", "appropriate", "are", "aren't", "around", "as",
           "aside", "ask", "asking", "associated", "at", "available", "away",
           "awfully", "b", "be", "became", "because", "become", "becomes",
           "becoming", "been", "before", "beforehand", "behind", "being",
           "believe", "below", "beside", "besides", "best", "better",
           "between", "beyond", "both", "brief", "but", "by", "c", "c'mon",
           "c's", "came", "can", "can't", "cannot", "cant", "cause", "causes",
           "certain", "certainly", "changes", "clearly", "co", "com", "come",
           "comes", "concerning", "consequently", "consider", "considering",
           "contain", "containing", "contains", "corresponding", "could",
           "couldn't", "course", "currently", "d", "definitely", "described",
           "despite", "did", "didn't", "different", "do", "does", "doesn't",
           "doing", "don't", "done", "down", "downwards", "during", "e",
           "each", "edu", "eg", "eight", "either", "else", "elsewhere",
           "enough", "entirely", "especially", "et", "etc", "even", "ever",
           "every", "everybody", "everyone", "everything", "everywhere", "ex",
           "exactly", "example", "except", "f", "far", "few", "fifth", "first",
           "five", "followed", "following", "follows", "for", "former",
           "formerly", "forth", "four", "from", "further", "furthermore", "g",
           "get", "gets", "getting", "given", "gives", "go", "goes", "going",
           "gone", "got", "gotten", "greetings", "h", "had", "hadn't",
           "happens", "hardly", "has", "hasn't", "have", "haven't", "having",
           "he", "he's", "hello", "help", "hence", "her", "here", "here's",
           "hereafter", "hereby", "herein", "hereupon", "hers", "herself",
           "hi", "him", "himself", "his", "hither", "hopefully", "how",
           "howbeit", "however", "i", "i'd", "i'll", "i'm", "i've", "ie", "if",
           "ignored", "immediate", "in", "inasmuch", "inc", "indeed",
           "indicate", "indicated", "indicates", "inner", "insofar", "instead",
           "into", "inward", "is", "isn't", "it", "it'd", "it'll", "it's",
           "its", "itself", "j", "just", "k", "keep", "keeps", "kept", "know",
           "known", "knows", "l", "last", "lately", "later", "latter",
           "latterly", "least", "less", "lest", "let", "let's", "like",
           "liked", "likely", "little", "look", "looking", "looks", "ltd", "m",
           "mainly", "many", "may", "maybe", "me", "mean", "meanwhile",
           "merely", "might", "more", "moreover", "most", "mostly", "much",
           "must", "my", "myself", "n", "name", "namely", "nd", "near",
           "nearly", "necessary", "need", "needs", "neither", "never",
           "nevertheless", "new", "next", "nine", "no", "nobody", "non",
           "none", "noone", "nor", "normally", "not", "nothing", "novel",
           "now", "nowhere", "o", "obviously", "of", "off", "often", "oh",
           "ok", "okay", "old", "on", "once", "one", "ones", "only", "onto",
           "or", "other", "others", "otherwise", "ought", "our", "ours",
           "ourselves", "out", "outside", "over", "overall", "own", "p",
           "particular", "particularly", "per", "perhaps", "placed", "please",
           "plus", "possible", "presumably", "probably", "provides", "q",
           "que", "quite", "qv", "r", "rather", "rd", "re", "really",
           "reasonably", "regarding", "regardless", "regards", "relatively",
           "respectively", "right", "s", "said", "same", "saw", "say",
           "saying", "says", "second", "secondly", "see", "seeing", "seem",
           "seemed", "seeming", "seems", "seen", "self", "selves", "sensible",
           "sent", "serious", "seriously", "seven", "several", "shall", "she",
           "should", "shouldn't", "since", "six", "so", "some", "somebody",
           "somehow", "someone", "something", "sometime", "sometimes",
           "somewhat", "somewhere", "soon", "sorry", "specified", "specify",
           "specifying", "still", "sub", "such", "sup", "sure", "t", "t's",
           "take", "taken", "tell", "tends", "th", "than", "thank", "thanks",
           "thanx", "that", "that's", "thats", "the", "their", "theirs",
           "them", "themselves", "then", "thence", "there", "there's",
           "thereafter", "thereby", "therefore", "therein", "theres",
           "thereupon", "these", "they", "they'd", "they'll", "they're",
           "they've", "think", "third", "this", "thorough", "thoroughly",
           "those", "though", "three", "through", "throughout", "thru", "thus",
           "to", "together", "too", "took", "toward", "towards", "tried",
           "tries", "truly", "try", "trying", "twice", "two", "u", "un",
           "under", "unfortunately", "unless", "unlikely", "until", "unto",
           "up", "upon", "us", "use", "used", "useful", "uses", "using",
           "usually", "uucp", "v", "value", "various", "very", "via", "viz",
           "vs", "w", "want", "wants", "was", "wasn't", "way", "we", "we'd",
           "we'll", "we're", "we've", "welcome", "well", "went", "were",
           "weren't", "what", "what's", "whatever", "when", "whence",
           "whenever", "where", "where's", "whereafter", "whereas", "whereby",
           "wherein", "whereupon", "wherever", "whether", "which", "while",
           "whither", "who", "who's", "whoever", "whole", "whom", "whose",
           "why", "will", "willing", "wish", "with", "within", "without",
           "won't", "wonder", "would", "wouldn't", "x", "y", "yes", "yet",
           "you", "you'd", "you'll", "you're", "you've", "your", "yours",
           "yourself", "yourselves", "z", "zero"},
    "es": {"a", "actualmente", "acuerdo", "adelante", "ademas", "además",
           "adrede", "afirmó", "agregó", "ahi", "ahora", "ahí", "al", "algo",
           "alguna", "algunas", "alguno", "algunos", "algún", "alli", "allí",
           "alrededor", "ambos", "ampleamos", "antano", "antaño", "ante",
           "anterior", "antes", "apenas", "aproximadamente", "aquel",
           "aquella", "aquellas", "aquello", "aquellos", "aqui", "aquél",
           "aquélla", "aquéllas", "aquéllos", "aquí", "arriba", "arribaabajo",
           "aseguró", "asi", "así", "atras", "aun", "aunque", "ayer", "añadió",
           "aún", "b", "bajo", "bastante", "bien", "breve", "buen", "buena",
           "buenas", "bueno", "buenos", "c", "cada", "casi", "cerca", "cierta",
           "ciertas", "cierto", "ciertos", "cinco", "claro", "comentó", "como",
           "con", "conmigo", "conocer", "conseguimos", "conseguir",
           "considera", "consideró", "consigo", "consigue", "consiguen",
           "consigues", "contigo", "contra", "cosas", "creo", "cual", "cuales",
           "cualquier", "cuando", "cuanta", "cuantas", "cuanto", "cuantos",
           "cuatro", "cuenta", "cuál", "cuáles", "cuándo", "cuánta", "cuántas",
           "cuánto", "cuántos", "cómo", "d", "da", "dado", "dan", "dar", "de",
           "debajo", "debe", "deben", "debido", "decir", "dejó", "del",
           "delante", "demasiado", "demás", "dentro", "deprisa", "desde",
           "despacio", "despues", "después", "detras", "detrás", "dia", "dias",
           "dice", "dicen", "dicho", "dieron", "diferente", "diferentes",
           "dijeron", "dijo", "dio", "donde", "dos", "durante", "día", "días",
           "dónde", "e", "ejemplo", "el", "ella", "ellas", "ello", "ellos",
           "embargo", "empleais", "emplean", "emplear", "empleas", "empleo",
           "en", "encima", "encuentra", "enfrente", "enseguida", "entonces",
           "entre", "era", "eramos", "eran", "eras", "eres", "es", "esa",
           "esas", "ese", "eso", "esos", "esta", "estaba", "estaban", "estado",
           "estados", "estais", "estamos", "estan", "estar", "estará", "estas",
           "este", "esto", "estos", "estoy", "estuvo", "está", "están", "ex",
           "excepto", "existe", "existen", "explicó", "expresó", "f", "fin",
           "final", "fue", "fuera", "fueron", "fui", "fuimos", "g", "general",
           "gran", "grandes", "gueno", "h", "ha", "haber", "habia", "habla",
           "hablan", "habrá", "había", "habían", "hace", "haceis", "hacemos",
           "hacen", "hacer", "hacerlo", "haces", "hacia", "haciendo", "hago",
           "han", "hasta", "hay", "haya", "he", "hecho", "hemos", "hicieron",
           "hizo", "horas", "hoy", "hubo", "i", "igual", "incluso", "indicó",
           "informo", "informó", "intenta", "intentais", "intentamos",
           "intentan", "intentar", "intentas", "intento", "ir", "j", "junto",
           "k", "l", "la", "lado", "largo", "las", "le", "lejos", "les",
           "llegó", "lleva", "llevar", "lo", "los", "luego", "lugar", "m",
           "mal", "manera", "manifestó", "mas", "mayor", "me", "mediante",
           "medio", "mejor", "mencionó", "menos", "menudo", "mi", "mia",
           "mias", "mientras", "mio", "mios", "mis", "misma", "mismas",
           "mismo", "mismos", "modo", "momento", "mucha", "muchas", "mucho",
           "muchos", "muy", "más", "mí", "mía", "mías", "mío", "míos", "n",
           "nada", "nadie", "ni", "ninguna", "ningunas", "ninguno", "ningunos",
           "ningún", "no", "nos", "nosotras", "nosotros", "nuestra",
           "nuestras", "nuestro", "nuestros", "nueva", "nuevas", "nuevo",
           "nuevos", "nunca", "o", "ocho", "os", "otra", "otras", "otro",
           "otros", "p", "pais", "para", "parece", "parte", "partir", "pasada",
           "pasado", "paìs", "peor", "pero", "pesar", "poca", "pocas", "poco",
           "pocos", "podeis", "podemos", "poder", "podria", "podriais",
           "podriamos", "podrian", "podrias", "podrá", "podrán", "podría",
           "podrían", "poner", "por", "porque", "posible", "primer", "primera",
           "primero", "primeros", "principalmente", "pronto", "propia",
           "propias", "propio", "propios", "proximo", "próximo", "próximos",
           "pudo", "pueda", "puede", "pueden", "puedo", "pues", "q", "qeu",
           "que", "quedó", "queremos", "quien", "quienes", "quiere", "quiza",
           "quizas", "quizá", "quizás", "quién", "quiénes", "qué", "r",
           "raras", "realizado", "realizar", "realizó", "repente", "respecto",
           "s", "sabe", "sabeis", "sabemos", "saben", "saber", "sabes",
           "salvo", "se", "sea", "sean", "segun", "segunda", "segundo",
           "según", "seis", "ser", "sera", "será", "serán", "sería", "señaló",
           "si", "sido", "siempre", "siendo", "siete", "sigue", "siguiente",
           "sin", "sino", "sobre", "sois", "sola", "solamente", "solas",
           "solo", "solos", "somos", "son", "soy", "soyos", "su", "supuesto",
           "sus", "suya", "suyas", "suyo", "sé", "sí", "sólo", "t", "tal",
           "tambien", "también", "tampoco", "tan", "tanto", "tarde", "te",
           "temprano", "tendrá", "tendrán", "teneis", "tenemos", "tener",
           "tenga", "tengo", "tenido", "tenía", "tercera", "ti", "tiempo",
           "tiene", "tienen", "toda", "todas", "todavia", "todavía", "todo",
           "todos", "total", "trabaja", "trabajais", "trabajamos", "trabajan",
           "trabajar", "trabajas", "trabajo", "tras", "trata", "través",
           "tres", "tu", "tus", "tuvo", "tuya", "tuyas", "tuyo", "tuyos", "tú",
           "u", "ultimo", "un", "una", "unas", "uno", "unos", "usa", "usais",
           "usamos", "usan", "usar", "usas", "uso", "usted", "ustedes", "v",
           "va", "vais", "valor", "vamos", "van", "varias", "varios", "vaya",
           "veces", "ver", "verdad", "verdadera", "verdadero", "vez",
           "vosotras", "vosotros", "voy", "vuestra", "vuestras", "vuestro",
           "vuestros", "w", "x", "y", "ya", "yo", "z", "él", "ésa", "ésas",
           "ése", "ésos", "ésta", "éstas", "éste", "éstos", "última",
           "últimas", "último", "últimos"},
    "fi": {"aiemmin", "aika", "aikaa", "aikaan", "aikaisemmin", "aikaisin",
           "aikajen", "aikana", "aikoina", "aikoo", "aikovat", "aina",
           "ainakaan", "ainakin", "ainoa", "ainoat", "aiomme", "aion",
           "aiotte", "aist", "aivan", "ajan", "alas", "alemmas", "alkuisin",
           "alkuun", "alla", "alle", "aloitamme", "aloitan", "aloitat",
           "aloitatte", "aloitattivat", "aloitettava", "aloitettevaksi",
           "aloitettu", "aloitimme", "aloitin", "aloitit", "aloititte",
           "aloittaa", "aloittamatta", "aloitti", "aloittivat", "alta",
           "aluksi", "alussa", "alusta", "annettavaksi", "annetteva",
           "annettu", "ansiosta", "antaa", "antamatta", "antoi", "aoua", "apu",
           "asia", "asiaa", "asian", "asiasta", "asiat", "asioiden",
           "asioihin", "asioita", "asti", "avuksi", "avulla", "avun", "avutta",
           "edelle", "edelleen", "edellä", "edeltä", "edemmäs", "edes",
           "edessä", "edestä", "ehkä", "ei", "eikä", "eilen", "eivät", "eli",
           "ellei", "elleivät", "ellemme", "ellen", "ellet", "ellette", "emme",
           "en", "enemmän", "eniten", "ennen", "ensi", "ensimmäinen",
           "ensimmäiseksi", "ensimmäisen", "ensimmäisenä", "ensimmäiset",
           "ensimmäisiksi", "ensimmäisinä", "ensimmäisiä", "ensimmäistä",
           "ensin", "entinen", "entisen", "entisiä", "entisten", "entistä",
           "enää", "eri", "erittäin", "erityisesti", "eräiden", "eräs",
           "eräät", "esi", "esiin", "esillä", "esimerkiksi", "et", "eteen",
           "etenkin", "etessa", "ette", "ettei", "että", "haikki", "halua",
           "haluaa", "haluamatta", "haluamme", "haluan", "haluat", "haluatte",
           "haluavat", "halunnut", "halusi", "halusimme", "halusin", "halusit",
           "halusitte", "halusivat", "halutessa", "haluton", "he", "hei",
           "heidän", "heihin", "heille", "heiltä", "heissä", "heistä", "heitä",
           "helposti", "heti", "hetkellä", "hieman", "hitaasti", "hoikein",
           "huolimatta", "huomenna", "hyvien", "hyviin", "hyviksi", "hyville",
           "hyviltä", "hyvin", "hyvinä", "hyvissä", "hyvistä", "hyviä", "hyvä",
           "hyvät", "hyvää", "hän", "häneen", "hänelle", "hänellä", "häneltä",
           "hänen", "hänessä", "hänestä", "hänet", "ihan", "ilman",
           "ilmeisesti", "itse", "itsensä", "itseään", "ja", "jo", "johon",
           "joiden", "joihin", "joiksi", "joilla", "joille", "joilta",
           "joissa", "joista", "joita", "joka", "jokainen", "jokin", "joko",
           "joku", "jolla", "jolle", "jolloin", "jolta", "jompikumpi", "jonka",
           "jonkin", "jonne", "joo", "jopa", "jos", "joskus", "jossa", "josta",
           "jota", "jotain", "joten", "jotenkin", "jotenkuten", "jotka",
           "jotta", "jouduimme", "jouduin", "jouduit", "jouduitte", "joudumme",
           "joudun", "joudutte", "joukkoon", "joukossa", "joukosta", "joutua",
           "joutui", "joutuivat", "joutumaan", "joutuu", "joutuvat", "juuri",
           "jälkeen", "jälleen", "jää", "kahdeksan", "kahdeksannen",
           "kahdella", "kahdelle", "kahdelta", "kahden", "kahdessa",
           "kahdesta", "kahta", "kahteen", "kai", "kaiken", "kaikille",
           "kaikilta", "kaikkea", "kaikki", "kaikkia", "kaikkiaan",
           "kaikkialla", "kaikkialle", "kaikkialta", "kaikkien", "kaikkin",
           "kaksi", "kannalta", "kannattaa", "kanssa", "kanssaan", "kanssamme",
           "kanssani", "kanssanne", "kanssasi", "kauan", "kauemmas", "kaukana",
           "kautta", "kehen", "keiden", "keihin", "keiksi", "keille", "keillä",
           "keiltä", "keinä", "keissä", "keistä", "keitten", "keittä", "keitä",
           "keneen", "keneksi", "kenelle", "kenellä", "keneltä", "kenen",
           "kenenä", "kenessä", "kenestä", "kenet", "kenettä", "kennessästä",
           "kenties", "kerran", "kerta", "kertaa", "keskellä", "kesken",
           "keskimäärin", "ketkä", "ketä", "kiitos", "kohti", "koko",
           "kokonaan", "kolmas", "kolme", "kolmen", "kolmesti", "koska",
           "koskaan", "kovin", "kuin", "kuinka", "kuinkan", "kuitenkaan",
           "kuitenkin", "kuka", "kukaan", "kukin", "kukka", "kumpainen",
           "kumpainenkaan", "kumpi", "kumpikaan", "kumpikin", "kun", "kuten",
           "kuuden", "kuusi", "kuutta", "kylliksi", "kyllä", "kymmenen",
           "kyse", "liian", "liki", "lisäksi", "lisää", "lla", "luo", "luona",
           "lähekkäin", "lähelle", "lähellä", "läheltä", "lähemmäs", "lähes",
           "lähinnä", "lähtien", "läpi", "mahdollisimman", "mahdollista", "me",
           "meidän", "meille", "meillä", "melkein", "melko", "menee", "meneet",
           "menemme", "menen", "menet", "menette", "menevät", "meni",
           "menimme", "menin", "menit", "menivät", "mennessä", "mennyt",
           "menossa", "mihin", "mikin", "miksi", "mikä", "mikäli", "mikään",
           "milloin", "milloinkan", "minne", "minun", "minut", "minä", "missä",
           "mistä", "miten", "mitä", "mitään", "moi", "molemmat", "mones",
           "monesti", "monet", "moni", "moniaalla", "moniaalle", "moniaalta",
           "monta", "muassa", "muiden", "muita", "muka", "mukaan", "mukaansa",
           "mukana", "mutta", "muu", "muualla", "muualle", "muualta",
           "muuanne", "muulloin", "muun", "muut", "muuta", "muutama",
           "muutaman", "muuten", "myöhemmin", "myös", "myöskin", "myöskään",
           "myötä", "ne", "neljä", "neljän", "neljää", "niiden", "niin",
           "niistä", "niitä", "noin", "nopeammin", "nopeasti", "nopeiten",
           "nro", "nuo", "nyt", "näiden", "näin", "näissä", "näissähin",
           "näissälle", "näissältä", "näissästä", "näitä", "nämä", "ohi",
           "oikea", "oikealla", "oikein", "ole", "olemme", "olen", "olet",
           "olette", "oleva", "olevan", "olevat", "oli", "olimme", "olin",
           "olisi", "olisimme", "olisin", "olisit", "olisitte", "olisivat",
           "olit", "olitte", "olivat", "olla", "olleet", "olli", "ollut",
           "oma", "omaa", "omaan", "omaksi", "omalle", "omalta", "oman",
           "omassa", "omat", "omia", "omien", "omiin", "omiksi", "omille",
           "omilta", "omissa", "omista", "on", "onkin", "onko", "ovat",
           "paikoittain", "paitsi", "pakosti", "paljon", "paremmin", "parempi",
           "parhaillaan", "parhaiten", "perusteella", "peräti", "pian",
           "pieneen", "pieneksi", "pienelle", "pienellä", "pieneltä",
           "pienempi", "pienestä", "pieni", "pienin", "puolesta", "puolestaan",
           "päälle", "runsaasti", "saakka", "sadam", "sama", "samaa", "samaan",
           "samalla", "samallalta", "samallassa", "samallasta", "saman",
           "samat", "samoin", "sata", "sataa", "satojen", "se", "seitsemän",
           "sekä", "sen", "seuraavat", "siellä", "sieltä", "siihen", "siinä",
           "siis", "siitä", "sijaan", "siksi", "silloin", "sillä", "silti",
           "sinne", "sinua", "sinulle", "sinulta", "sinun", "sinussa",
           "sinusta", "sinut", "sinä", "sisäkkäin", "sisällä", "siten",
           "sitten", "sitä", "ssa", "sta", "suoraan", "suuntaan", "suuren",
           "suuret", "suuri", "suuria", "suurin", "suurten", "taa", "taas",
           "taemmas", "tahansa", "tai", "takaa", "takaisin", "takana", "takia",
           "tapauksessa", "tarpeeksi", "tavalla", "tavoitteena", "te",
           "tietysti", "todella", "toinen", "toisaalla", "toisaalle",
           "toisaalta", "toiseen", "toiseksi", "toisella", "toiselle",
           "toiselta", "toisemme", "toisen", "toisensa", "toisessa",
           "toisesta", "toista", "toistaiseksi", "toki", "tosin", "tuhannen",
           "tuhat", "tule", "tulee", "tulemme", "tulen", "tulet", "tulette",
           "tulevat", "tulimme", "tulin", "tulisi", "tulisimme", "tulisin",
           "tulisit", "tulisitte", "tulisivat", "tulit", "tulitte", "tulivat",
           "tulla", "tulleet", "tullut", "tuntuu", "tuo", "tuolla", "tuolloin",
           "tuolta", "tuonne", "tuskin", "tykö", "tähän", "tällä", "tällöin",
           "tämä", "tämän", "tänne", "tänä", "tänään", "tässä", "tästä",
           "täten", "tätä", "täysin", "täytyvät", "täytyy", "täällä", "täältä",
           "ulkopuolella", "usea", "useasti", "useimmiten", "usein", "useita",
           "uudeksi", "uudelleen", "uuden", "uudet", "uusi", "uusia", "uusien",
           "uusinta", "uuteen", "uutta", "vaan", "vahemmän", "vai",
           "vaiheessa", "vaikea", "vaikean", "vaikeat", "vaikeilla",
           "vaikeille", "vaikeilta", "vaikeissa", "vaikeista", "vaikka",
           "vain", "varmasti", "varsin", "varsinkin", "varten", "vasen",
           "vasenmalla", "vasta", "vastaan", "vastakkain", "vastan", "verran",
           "vielä", "vierekkäin", "vieressä", "vieri", "viiden", "viime",
           "viimeinen", "viimeisen", "viimeksi", "viisi", "voi", "voidaan",
           "voimme", "voin", "voisi", "voit", "voitte", "voivat", "vuoden",
           "vuoksi", "vuosi", "vuosien", "vuosina", "vuotta", "vähemmän",
           "vähintään", "vähiten", "vähän", "välillä", "yhdeksän", "yhden",
           "yhdessä", "yhteen", "yhteensä", "yhteydessä", "yhteyteen", "yhtä",
           "yhtäälle", "yhtäällä", "yhtäältä", "yhtään", "yhä", "yksi",
           "yksin", "yksittäin", "yleensä", "ylemmäs", "yli", "ylös", "ympäri",
           "älköön", "älä"},
    "fr": {"a", "abord", "absolument", "afin", "ah", "ai", "aie", "ailleurs",
           "ainsi", "ait", "allaient", "allo", "allons", "allô", "alors",
           "anterieur", "anterieure", "anterieures", "apres", "après", "as",
           "assez", "attendu", "au", "aucun", "aucune", "aujourd",
           "aujourd'hui", "aupres", "auquel", "aura", "auraient", "aurait",
           "auront", "aussi", "autre", "autrefois", "autrement", "autres",
           "autrui", "aux", "auxquelles", "auxquels", "avaient", "avais",
           "avait", "avant", "avec", "avoir", "avons", "ayant", "b", "bah",
           "bas", "basee", "bat", "beau", "beaucoup", "bien", "bigre", "boum",
           "bravo", "brrr", "c", "car", "ce", "ceci", "cela", "celle",
           "celle-ci", "celle-là", "celles", "celles-ci", "celles-là", "celui",
           "celui-ci", "celui-là", "cent", "cependant", "certain", "certaine",
           "certaines", "certains", "certes", "ces", "cet", "cette", "ceux",
           "ceux-ci", "ceux-là", "chacun", "chacune", "chaque", "cher",
           "chers", "chez", "chiche", "chut", "chère", "chères", "ci", "cinq",
           "cinquantaine", "cinquante", "cinquantième", "cinquième", "clac",
           "clic", "combien", "comme", "comment", "comparable", "comparables",
           "compris", "concernant", "contre", "couic", "crac", "d", "da",
           "dans", "de", "debout", "dedans", "dehors", "deja", "delà",
           "depuis", "dernier", "derniere", "derriere", "derrière", "des",
           "desormais", "desquelles", "desquels", "dessous", "dessus", "deux",
           "deuxième", "deuxièmement", "devant", "devers", "devra",
           "different", "differentes", "differents", "différent", "différente",
           "différentes", "différents", "dire", "directe", "directement",
           "dit", "dite", "dits", "divers", "diverse", "diverses", "dix",
           "dix-huit", "dix-neuf", "dix-sept", "dixième", "doit", "doivent",
           "donc", "dont", "douze", "douzième", "dring", "du", "duquel",
           "durant", "dès", "désormais", "e", "effet", "egale", "egalement",
           "egales", "eh", "elle", "elle-même", "elles", "elles-mêmes", "en",
           "encore", "enfin", "entre", "envers", "environ", "es", "est", "et",
           "etant", "etc", "etre", "eu", "euh", "eux", "eux-mêmes",
           "exactement", "excepté", "extenso", "exterieur", "f", "fais",
           "faisaient", "faisant", "fait", "façon", "feront", "fi", "flac",
           "floc", "font", "g", "gens", "h", "ha", "hein", "hem", "hep", "hi",
           "ho", "holà", "hop", "hormis", "hors", "hou", "houp", "hue", "hui",
           "huit", "huitième", "hum", "hurrah", "hé", "hélas", "i", "il",
           "ils", "importe", "j", "je", "jusqu", "jusque", "juste", "k", "l",
           "la", "laisser", "laquelle", "las", "le", "lequel", "les",
           "lesquelles", "lesquels", "leur", "leurs", "longtemps", "lors",
           "lorsque", "lui", "lui-meme", "lui-même", "là", "lès", "m", "ma",
           "maint", "maintenant", "mais", "malgre", "malgré", "maximale", "me",
           "meme", "memes", "merci", "mes", "mien", "mienne", "miennes",
           "miens", "mille", "mince", "minimale", "moi", "moi-meme",
           "moi-même", "moindres", "moins", "mon", "moyennant", "multiple",
           "multiples", "même", "mêmes", "n", "na", "naturel", "naturelle",
           "naturelles", "ne", "neanmoins", "necessaire", "necessairement",
           "neuf", "neuvième", "ni", "nombreuses", "nombreux", "non", "nos",
           "notamment", "notre", "nous", "nous-mêmes", "nouveau", "nul",
           "néanmoins", "nôtre", "nôtres", "o", "oh", "ohé", "ollé", "olé",
           "on", "ont", "onze", "onzième", "ore", "ou", "ouf", "ouias", "oust",
           "ouste", "outre", "ouvert", "ouverte", "ouverts", "o|", "où", "p",
           "paf", "pan", "par", "parce", "parfois", "parle", "parlent",
           "parler", "parmi", "parseme", "partant", "particulier",
           "particulière", "particulièrement", "pas", "passé", "pendant",
           "pense", "permet", "personne", "peu", "peut", "peuvent", "peux",
           "pff", "pfft", "pfut", "pif", "pire", "plein", "plouf", "plus",
           "plusieurs", "plutôt", "possessif", "possessifs", "possible",
           "possibles", "pouah", "pour", "pourquoi", "pourrais", "pourrait",
           "pouvait", "prealable", "precisement", "premier", "première",
           "premièrement", "pres", "probable", "probante", "procedant",
           "proche", "près", "psitt", "pu", "puis", "puisque", "pur", "pure",
           "q", "qu", "quand", "quant", "quant-à-soi", "quanta", "quarante",
           "quatorze", "quatre", "quatre-vingt", "quatrième", "quatrièmement",
           "que", "quel", "quelconque", "quelle", "quelles", "quelqu'un",
           "quelque", "quelques", "quels", "qui", "quiconque", "quinze",
           "quoi", "quoique", "r", "rare", "rarement", "rares", "relative",
           "relativement", "remarquable", "rend", "rendre", "restant", "reste",
           "restent", "restrictif", "retour", "revoici", "revoilà", "rien",
           "s", "sa", "sacrebleu", "sait", "sans", "sapristi", "sauf", "se",
           "sein", "seize", "selon", "semblable", "semblaient", "semble",
           "semblent", "sent", "sept", "septième", "sera", "seraient",
           "serait", "seront", "ses", "seul", "seule", "seulement", "si",
           "sien", "sienne", "siennes", "siens", "sinon", "six", "sixième",
           "soi", "soi-même", "soit", "soixante", "son", "sont", "sous",
           "souvent", "specifique", "specifiques", "speculatif", "stop",
           "strictement", "subtiles", "suffisant", "suffisante", "suffit",
           "suis", "suit", "suivant", "suivante", "suivantes", "suivants",
           "suivre", "superpose", "sur", "surtout", "t", "ta", "tac", "tant",
           "tardive", "te", "tel", "telle", "tellement", "telles", "tels",
           "tenant", "tend", "tenir", "tente", "tes", "tic", "tien", "tienne",
           "tiennes", "tiens", "toc", "toi", "toi-même", "ton", "touchant",
           "toujours", "tous", "tout", "toute", "toutefois", "toutes",
           "treize", "trente", "tres", "trois", "troisième", "troisièmement",
           "trop", "très", "tsoin", "tsouin", "tu", "té", "u", "un", "une",
           "unes", "uniformement", "unique", "uniques", "uns", "v", "va",
           "vais", "vas", "vers", "via", "vif", "vifs", "vingt", "vivat",
           "vive", "vives", "vlan", "voici", "voilà", "vont", "vos", "votre",
           "vous", "vous-mêmes", "vu", "vé", "vôtre", "vôtres", "w", "x", "y",
           "z", "zut", "à", "â", "ça", "ès", "étaient", "étais", "était",
           "étant", "été", "être", "ô"},
    "it": {"IE", "a", "abbastanza", "abbia", "abbiamo", "abbiano", "abbiate",
           "accidenti", "ad", "adesso", "affinche", "agl", "agli", "ahime",
           "ahimè", "ai", "al", "alcuna", "alcuni", "alcuno", "all", "alla",
           "alle", "allo", "allora", "altri", "altrimenti", "altro", "altrove",
           "altrui", "anche", "ancora", "anni", "anno", "ansa", "anticipo",
           "assai", "attesa", "attraverso", "avanti", "avemmo", "avendo",
           "avente", "aver", "avere", "averlo", "avesse", "avessero", "avessi",
           "avessimo", "aveste", "avesti", "avete", "aveva", "avevamo",
           "avevano", "avevate", "avevi", "avevo", "avrai", "avranno",
           "avrebbe", "avrebbero", "avrei", "avremmo", "avremo", "avreste",
           "avresti", "avrete", "avrà", "avrò", "avuta", "avute", "avuti",
           "avuto", "basta", "bene", "benissimo", "berlusconi", "brava",
           "bravo", "c", "casa", "caso", "cento", "certa", "certe", "certi",
           "certo", "che", "chi", "chicchessia", "chiunque", "ci", "ciascuna",
           "ciascuno", "cima", "cio", "cioe", "cioè", "circa", "citta",
           "città", "ciò", "co", "codesta", "codesti", "codesto", "cogli",
           "coi", "col", "colei", "coll", "coloro", "colui", "come", "cominci",
           "comunque", "con", "concernente", "conciliarsi", "conclusione",
           "consiglio", "contro", "cortesia", "cos", "cosa", "cosi", "così",
           "cui", "d", "da", "dagl", "dagli", "dai", "dal", "dall", "dalla",
           "dalle", "dallo", "dappertutto", "davanti", "degl", "degli", "dei",
           "del", "dell", "della", "delle", "dello", "dentro", "detto", "deve",
           "di", "dice", "dietro", "dire", "dirimpetto", "diventa",
           "diventare", "diventato", "dopo", "dov", "dove", "dovra", "dovrà",
           "dovunque", "due", "dunque", "durante", "e", "ebbe", "ebbero",
           "ebbi", "ecc", "ecco", "ed", "effettivamente", "egli", "ella",
           "entrambi", "eppure", "era", "erano", "eravamo", "eravate", "eri",
           "ero", "esempio", "esse", "essendo", "esser", "essere", "essi",
           "ex", "fa", "faccia", "facciamo", "facciano", "facciate", "faccio",
           "facemmo", "facendo", "facesse", "facessero", "facessi",
           "facessimo", "faceste", "facesti", "faceva", "facevamo", "facevano",
           "facevate", "facevi", "facevo", "fai", "fanno", "farai", "faranno",
           "fare", "farebbe", "farebbero", "farei", "faremmo", "faremo",
           "fareste", "faresti", "farete", "farà", "farò", "fatto", "favore",
           "fece", "fecero", "feci", "fin", "finalmente", "finche", "fine",
           "fino", "forse", "forza", "fosse", "fossero", "fossi", "fossimo",
           "foste", "fosti", "fra", "frattempo", "fu", "fui", "fummo", "fuori",
           "furono", "futuro", "generale", "gia", "giacche", "giorni",
           "giorno", "già", "gli", "gliela", "gliele", "glieli", "glielo",
           "gliene", "governo", "grande", "grazie", "gruppo", "ha", "haha",
           "hai", "hanno", "ho", "i", "ieri", "il", "improvviso", "in", "inc",
           "infatti", "inoltre", "insieme", "intanto", "intorno", "invece",
           "io", "l", "la", "lasciato", "lato", "lavoro", "le", "lei", "li",
           "lo", "lontano", "loro", "lui", "lungo", "luogo", "là", "ma",
           "macche", "magari", "maggior", "mai", "male", "malgrado",
           "malissimo", "mancanza", "marche", "me", "medesimo", "mediante",
           "meglio", "meno", "mentre", "mesi", "mezzo", "mi", "mia", "mie",
           "miei", "mila", "miliardi", "milioni", "minimi", "ministro", "mio",
           "modo", "molti", "moltissimo", "molto", "momento", "mondo", "mosto",
           "nazionale", "ne", "negl", "negli", "nei", "nel", "nell", "nella",
           "nelle", "nello", "nemmeno", "neppure", "nessun", "nessuna",
           "nessuno", "niente", "no", "noi", "non", "nondimeno", "nonostante",
           "nonsia", "nostra", "nostre", "nostri", "nostro", "novanta", "nove",
           "nulla", "nuovo", "o", "od", "oggi", "ogni", "ognuna", "ognuno",
           "oltre", "oppure", "ora", "ore", "osi", "ossia", "ottanta", "otto",
           "paese", "parecchi", "parecchie", "parecchio", "parte", "partendo",
           "peccato", "peggio", "per", "perche", "perchè", "perché", "percio",
           "perciò", "perfino", "pero", "persino", "persone", "però", "piedi",
           "pieno", "piglia", "piu", "piuttosto", "più", "po", "pochissimo",
           "poco", "poi", "poiche", "possa", "possedere", "posteriore",
           "posto", "potrebbe", "preferibilmente", "presa", "press", "prima",
           "primo", "principalmente", "probabilmente", "proprio", "puo",
           "pure", "purtroppo", "può", "qualche", "qualcosa", "qualcuna",
           "qualcuno", "quale", "quali", "qualunque", "quando", "quanta",
           "quante", "quanti", "quanto", "quantunque", "quasi", "quattro",
           "quel", "quella", "quelle", "quelli", "quello", "quest", "questa",
           "queste", "questi", "questo", "qui", "quindi", "realmente",
           "recente", "recentemente", "registrazione", "relativo", "riecco",
           "salvo", "sara", "sarai", "saranno", "sarebbe", "sarebbero",
           "sarei", "saremmo", "saremo", "sareste", "saresti", "sarete",
           "sarà", "sarò", "scola", "scopo", "scorso", "se", "secondo",
           "seguente", "seguito", "sei", "sembra", "sembrare", "sembrato",
           "sembri", "sempre", "senza", "sette", "si", "sia", "siamo", "siano",
           "siate", "siete", "sig", "solito", "solo", "soltanto", "sono",
           "sopra", "sotto", "spesso", "srl", "sta", "stai", "stando",
           "stanno", "starai", "staranno", "starebbe", "starebbero", "starei",
           "staremmo", "staremo", "stareste", "staresti", "starete", "starà",
           "starò", "stata", "state", "stati", "stato", "stava", "stavamo",
           "stavano", "stavate", "stavi", "stavo", "stemmo", "stessa",
           "stesse", "stessero", "stessi", "stessimo", "stesso", "steste",
           "stesti", "stette", "stettero", "stetti", "stia", "stiamo",
           "stiano", "stiate", "sto", "su", "sua", "subito", "successivamente",
           "successivo", "sue", "sugl", "sugli", "sui", "sul", "sull", "sulla",
           "sulle", "sullo", "suo", "suoi", "tale", "tali", "talvolta",
           "tanto", "te", "tempo", "ti", "titolo", "torino", "tra", "tranne",
           "tre", "trenta", "troppo", "trovato", "tu", "tua", "tue", "tuo",
           "tuoi", "tutta", "tuttavia", "tutte", "tutti", "tutto", "uguali",
           "ulteriore", "ultimo", "un", "una", "uno", "uomo", "va", "vale",
           "vari", "varia", "varie", "vario", "verso", "vi", "via", "vicino",
           "visto", "vita", "voi", "volta", "volte", "vostra", "vostre",
           "vostri", "vostro", "è"}
}

File Path: app/src/core/schema/__init__.py
Content:

File Path: app/src/core/schema/rp.py
Content:
r""" This module contains definitions of the schemas used in the project. These
are used to build msgspec writers and readers.

The schemas are defined as lists of tuples, where each tuple contains the name
and type of the field.

"""

from core.data_types import SignalType
from core.quality_signals.content import content_schema
from core.quality_signals.repetitions import repetitions_schema
from core.quality_signals.natural_language import natural_language_schema
from core.quality_signals.lines import lines_schema
from core.quality_signals.classifiers import classifier_schema
from core.quality_signals.importance_weights import importance_weights_schema

METADATA_SCHEMA = [
    ("cc_net_source", str),
    ("cc_segment", str),
    ("shard_id", str),
    ("url", str),
    ("source_domain", str),
    ("language", str),
    ("snapshot_id", str)
]

QUALITY_SIGNALS_SCHEMA = [
    ("ccnet_length", SignalType),
    ("ccnet_original_length", SignalType),
    ("ccnet_nlines", SignalType),
    ("ccnet_original_nlines", SignalType),
    ("ccnet_language_score", SignalType),
    ("ccnet_perplexity", SignalType),
    ("ccnet_bucket", SignalType),
    *content_schema(),
    *natural_language_schema(),
    *repetitions_schema(),
    *lines_schema(),
    *classifier_schema(),
    *importance_weights_schema(),
]

RP_SIGNAL_SCHEMA = [
    ("id", str),
    ("id_int", int),
    ("metadata", METADATA_SCHEMA),
    ("quality_signals", QUALITY_SIGNALS_SCHEMA)
]

File Path: app/src/core/worker.py
Content:
import sys
import fasttext
import gc
import hashlib
import logging
import logging.handlers
import multiprocessing as mp
import os
from pathlib import Path
import re
from typing import List, Dict, Callable, Optional
from urllib.parse import urlparse
import urllib3
import pyarrow as pa
import uuid

from core.document import Document
from core.quality_signals.content import register_content_callables
from core.quality_signals.lines import register_lines_callables
from core.quality_signals.natural_language import \
    register_natural_language_callables
from core.quality_signals.repetitions import register_repetitions_callables
from core.quality_signals.classifiers import register_classifier_callables
from core.quality_signals.importance_weights import \
    register_importance_weights_callables
from core.data_types import InputSpec
from core.schema.rp import RP_SIGNAL_SCHEMA
from dedupe.minhash import MinHash
from utilities.io import Reader, Writer, ParquetBatchWriter
from utilities.io.s3 import init_client
from utilities.logging.mp import configure_worker_logger

# disable warnings
fasttext.FastText.eprint = lambda x: None
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)  # noqa

_BYTE_ORDER = sys.byteorder


def _ccnet_bucket_to_int(bucket: str) -> Optional[float]:
    r""" ccnet bucket name to float mapping """
    if bucket == "head":
        return 0.0
    elif bucket == "middle":
        return 1.0
    elif bucket == "tail":
        return 2.0
    else:
        return None


class Worker:
    # output file pattern
    shard_pattern_signals = "{shard_id}.signals.json.gz"
    shard_pattern_minhash = "{shard_id}.minhash.parquet"

    # regex to extract snapshot id from uri
    snapsh_re = re.compile(r'\b\d{4}-\d{2}\b')
    uri_id_re = re.compile(r'\b\d{4}-\d{2}\b/.*')

    def __init__(
            self, language: str,
            snapshot_id: str,
            input_listings: List[str],
            input_base_uri: str,
            output_base_uri: str,
            log_dir: str,
            classifier_files: Dict[str, str],
            dsir_files: Dict[str, str],
            dsir_bucket: int,
            ldnoobw_dir: Path,
            ut1_dir: Path,
            minhash_similarities: List[float],
            minhash_ngram_size: int,
            minhash_num_permutations: int,
            monitor_queue: mp.Queue,
            logging_queue: mp.Queue,
            seed: int,
            endpoint_url: str = None,
            max_docs: int = -1,
            flush_interval=1000
    ):
        self._lang = language
        self._snapshot_id = snapshot_id
        self._input_base_uri = input_base_uri
        self._output_base_uri = output_base_uri
        self._dsir_files = dsir_files
        self._dsir_buckets = dsir_bucket
        self._flush_interval = flush_interval

        # init logger
        configure_worker_logger(logging_queue, level=logging.INFO)
        self._logger = logging.getLogger()

        # minhash setup
        self._minhash = MinHash(
            similarity_thresholds=minhash_similarities,
            ngram_size=minhash_ngram_size,
            num_permutations=minhash_num_permutations,
            seed=seed
        )

        self._logger.info(f"__MINHASH_PERM_CHECKSUM__ "
                          f"{self._minhash.checksum}")

        self._max_docs = max_docs
        self._monitor_queue = monitor_queue
        self._endpoint_url = endpoint_url

        self._job_id = str(uuid.uuid4())

        # build input paths
        self._input_uri_list = list(map(
            lambda x: os.path.join(self._input_base_uri, x),
            input_listings
        ))

        # init file to keep track of failed input files
        self._failed_input_file = os.path.join(
            log_dir, f"{language}-inputs.{self._job_id}.FAIL"
        )

        # init file to keep track of successful input files
        self._success_input_file = os.path.join(
            log_dir, f"{language}-inputs.{self._job_id}.SUCCESS"
        )

        # setup input file reader
        read_scheme = urlparse(self._input_base_uri).scheme
        if read_scheme == "s3":
            client = init_client(
                endpoint_url=self._endpoint_url,
                aws_access_key_id=os.environ.get("AWS_ACCESS_KEY_ID"),
                aws_secret_access_key=os.environ.get("AWS_SECRET_ACCESS_KEY"),
                signature_version="s3v4"
            )
        else:
            client = None

        self._reader = Reader(
            input_spec=InputSpec, threads=1, s3_client=client,
            logger=self._logger
        )

        # classifier model filepaths
        self._palm_model_file = classifier_files.get("palm")
        self._wikiref_model_file = classifier_files.get("wikiref")
        self._wikipedia_model_file = classifier_files.get("wikipedia")

        # initialize signal functions
        self._quality_signals = self.__init_quality_signals(
            ldnoobw_dir=ldnoobw_dir, ut1_dir=ut1_dir
        )

        # minhash_schema
        self._minhash_schema = pa.schema([
            ("shard_id", pa.string()),
            ("id", pa.string()),
            ("id_int", pa.uint64()),
            *[
                (
                    "signature_sim{s}".format(s=s), pa.list_(pa.binary())
                )
                for s in minhash_similarities
            ]
        ])

    @property
    def job_id(self):
        return self._job_id

    def __init_quality_signals(self, ldnoobw_dir, ut1_dir) -> List[Callable]:
        callables = []

        # initialize content signal functions
        self._logger.info(f"Registering content signals for {self._lang}..")
        callables += register_content_callables(
            language=self._lang,
            bad_urls_dir=ut1_dir,
            bad_words_dir=ldnoobw_dir
        )

        # initialize repetition removal signal functions
        self._logger.info(f"Registering repetition signals for {self._lang}..")
        callables += register_repetitions_callables()

        # initialize natural language signal functions
        self._logger.info(f"Registering natlang signals for {self._lang}..")
        callables += register_natural_language_callables()

        # initialize line signal functions
        self._logger.info(f"Registering line level signals for {self._lang}..")
        callables += register_lines_callables()

        # initialize ml heuristics signal functions
        self._logger.info(f"Registering classifier signals for {self._lang}..")
        callables += register_classifier_callables(
            wikiref_model=self._wikiref_model_file,
            palm_model=self._palm_model_file,
            wikipedia_model=self._wikipedia_model_file
        )

        # initialize importance weights signal functions
        # hacky -- first index is the counts file, second is the lambda file
        # this is set in pipeline.py
        self._logger.info(f"Registering dsir signals for {self._lang}..")
        callables += register_importance_weights_callables(
            source_fps=self._dsir_files.get("ccnet"),
            wiki_fps=self._dsir_files.get("wikipedia"),
            openwebtext_fps=self._dsir_files.get("openwebtext"),
            books_fps=self._dsir_files.get("books"),
            language=self._lang
        )

        return callables

    def __process_record(
            self, idx: int, record, uri_id: str, snapshot_id: str
    ):
        # Setup document; this precomputes ngrams and hash features
        document = Document(
            record.raw_content,
            domain=record.source_domain,
            precompute_ngrams=True,
            precompute_hash_features=True,
            dsir_buckets=self._dsir_buckets
        )

        # compute signals
        rp_v2_signals = {}
        for func in self._quality_signals:
            rp_v2_signals[func.field_name] = func(document)  # noqa

        # compute minhash signatures
        minhash_signatures = self._minhash.compute_banded_signatures(
            tokens=document.normalized_words
        )

        # compute document ids
        doc_id = f"{uri_id}/{idx}"
        doc_id_int = int.from_bytes(
            hashlib.sha1(doc_id.encode("utf-8")).digest()[:8],  # take 8 bytes
            byteorder=_BYTE_ORDER, signed=False
        )

        record_data = {
            "id": f"{uri_id}/{idx}",
            "id_int": doc_id_int,
        }

        metadata = {
            "cc_segment": record.cc_segment,
            "cc_net_source": uri_id,
            "url": record.url,
            "source_domain": record.source_domain,
            "language": record.language,
            "snapshot_id": snapshot_id
        }

        ccnet_quality_signals = {
            "ccnet_length": (
                (0, len(document), float(record.length)),
            ),
            "ccnet_original_length": (
                (0, len(document), float(record.original_length)),
            ),
            "ccnet_nlines": (
                (0, len(document), float(record.nlines)),
            ),
            "ccnet_original_nlines": (
                (0, len(document), float(record.original_nlines)),
            ),
            "ccnet_language_score": (
                (0, len(document), float(record.language_score)),
            ),
            "ccnet_perplexity": (
                (0, len(document), float(record.perplexity)),
            ),
            "ccnet_bucket": (
                (0, len(document), _ccnet_bucket_to_int(record.bucket)),
            ),
        }

        record_data["metadata"] = metadata
        record_data["quality_signals"] = {
            **ccnet_quality_signals, **rp_v2_signals
        }

        return record_data, minhash_signatures, doc_id, doc_id_int

    def __process_uri(self, docs_to_fetch: int, uri: str):
        num_docs = 0
        docs_added = 0
        snapshot_id = self.snapsh_re.search(uri).group(0)
        uri_id = self.uri_id_re.search(uri).group(0)

        # signal writer
        signal_uri = os.path.join(
            self._output_base_uri,
            self.shard_pattern_signals.format(shard_id=uri_id.split(".")[0]),
        )
        signal_writer = Writer(uri=signal_uri, schema=RP_SIGNAL_SCHEMA)
        self._logger.info(f"Initialized jsonl writer to {signal_uri}")

        # init minhash writer
        minhash_uri = os.path.join(
            self._output_base_uri,
            self.shard_pattern_minhash.format(shard_id=uri_id.split(".")[0]),
        )
        minhash_writer = ParquetBatchWriter(
            output_fp=minhash_uri, schema=self._minhash_schema
        )
        self._logger.info(f"Initialized parquet writer to {minhash_uri}")

        for idx, record in self._reader.read(
                uri=uri, max_samples=docs_to_fetch, return_idx=True
        ):
            # compute signals
            (
                record_data, minhash_signatures, doc_id, doc_id_int
            ) = self.__process_record(
                idx=idx, record=record, uri_id=uri_id, snapshot_id=snapshot_id
            )
            num_docs += 1
            docs_added += 1

            # write quality signals
            signal_writer.write(record_data)

            # record minhash signatures
            minhash_writer.update_batch(
                obj={"shard_id": uri_id, "id_int": doc_id_int, "id": doc_id,
                     **minhash_signatures}
            )

            # send to monitor
            if num_docs % self._flush_interval == 0:
                minhash_writer.write_batch()
                signal_writer.flush()
                self._monitor_queue.put({
                    "lang": self._lang, "num_docs": docs_added
                })
                docs_added = 0

        if docs_added > 0:
            self._monitor_queue.put({
                "lang": self._lang, "num_docs": docs_added
            })

        # close writers
        signal_writer.close()
        minhash_writer.close()

        gc.collect()

        return num_docs

    def run(self):
        total_docs = 0

        for i, uri in enumerate(self._input_uri_list, start=1):
            docs_to_fetch = self._max_docs - total_docs
            if docs_to_fetch <= 0 < self._max_docs:
                self._logger.info(
                    f"Reached max docs {self._max_docs} at {uri}")
                break

            # process file
            self._logger.info(
                f"Start processing {uri} ({i}/{len(self._input_uri_list)})"
            )
            try:
                docs_in_uri = self.__process_uri(docs_to_fetch, uri)
            except Exception as e:
                with open(self._failed_input_file, "a+") as f:
                    f.write(f"{uri}\n")
                self._logger.error(f"__URI_FAIL__ {uri} with exception: "
                                   f"{e.__class__.__name__}: {e} in "
                                   f"{self.__class__.__name__}.__process_uri")
                continue

            total_docs += docs_in_uri
            self._logger.info(
                f"__URI_SUCCESS__ {uri} ({i}/{len(self._input_uri_list)})"
            )

            # send signal that a uri has been completed
            self._monitor_queue.put({
                "lang": self._lang, "num_docs": None, "uri_complete": True
            })

            # keep track of completed uris
            with open(self._success_input_file, "a+") as f:
                f.write(f"{uri}\n")

        self._logger.info(f"Worker {self._job_id} Completed. "
                          f"Processed {total_docs} documents.")

        gc.collect()

        return total_docs, self._lang

File Path: app/src/dedupe/__init__.py
Content:
from .minhash import MinHash

File Path: app/src/dedupe/minhash.py
Content:
import hashlib
import numpy as np
from typing import List, Dict, Optional, Tuple

from dedupe.utils import optimal_param, generate_signature


class MinHash:
    _sig_key_pat = "signature_sim{s}"

    def __init__(
            self,
            similarity_thresholds: List[float],
            ngram_size: int,
            num_permutations: int,
            seed: int,
    ):
        self._similarity_thresholds = similarity_thresholds
        self._rng = np.random.RandomState(seed)
        self._ngram_size = ngram_size

        self._bands_rows = {
            str(s): optimal_param(threshold=s, num_perm=num_permutations)
            for s in similarity_thresholds
        }

        self._hashranges = {
            self._sig_key_pat.format(s=s): self._get_hashrange(b, r)
            for s, (b, r) in self._bands_rows.items()
        }

        # init minhash artifacts
        self.__init_minhash(num_permutations)

    def __init_minhash(self, num_permutations):
        # minhash constants
        self._max_hash = np.uint64((1 << 32) - 1)
        self._mersenne_prime = np.uint64((1 << 61) - 1)
        self._permutations = np.array(
            [
                (
                    self._rng.randint(
                        1, self._mersenne_prime, dtype=np.uint64
                    ),
                    self._rng.randint(
                        0, self._mersenne_prime, dtype=np.uint64
                    ),
                )
                for _ in range(num_permutations)
            ],
            dtype=np.uint64,
        ).T

        # compute checksum for permutations
        self._checksum = hashlib.sha256(
            self._permutations.tobytes()
        ).hexdigest()

    @staticmethod
    def _get_hashrange(b, r):
        return [(i * r, (i + 1) * r) for i in range(b)]

    @property
    def similarity_thresholds(self):
        return self._similarity_thresholds

    @property
    def checksum(self):
        return self._checksum

    def compute_banded_signatures(
            self, tokens: Tuple[str]
    ) -> Dict[str, Optional[List[bytes]]]:
        if len(tokens) < self._ngram_size:
            return {k: None for k in self._hashranges.keys()}

        # compute signature
        minhashes: np.ndarray = generate_signature(
            words_sequence=iter(tokens),
            ngram_size=self._ngram_size,
            permutations=self._permutations,
            max_hash=self._max_hash,
            mersenne_prime=self._mersenne_prime
        )

        # partition signatures into bands
        signatures = {
            sig_key: [
                bytes(minhashes[start:end].byteswap().data)
                for start, end in hashrange
            ]
            for sig_key, hashrange in self._hashranges.items()
        }

        return signatures

File Path: app/src/dedupe/utils.py
Content:
"""
To reduce dependencies, the functions in this module are adapted from
the `datasketch` library with minor modifications.
"""

from scipy.integrate import quad as integrate
import hashlib
import numpy as np
import struct
from typing import Iterable, List

from utilities.text import form_ngrams


def _false_positive_probability(threshold, b, r):
    def proba(s):
        return 1 - (1 - s ** float(r)) ** float(b)

    a, *_ = integrate(proba, 0.0, threshold)

    return a


def _false_negative_probability(threshold, b, r):
    def proba(s):
        return 1 - (1 - (1 - s ** float(r)) ** float(b))

    a, *_ = integrate(proba, threshold, 1.0)

    return a


def optimal_param(
        threshold: float,
        num_perm: int,
        false_positive_weight: float = 0.5,
        false_negative_weight: float = 0.5
):
    r"""
    Compute the optimal `MinHashLSH` parameter that minimizes the weighted sum
    of probabilities of false positive and false negative.
    """
    min_error = float("inf")
    opt = (0, 0)
    for b in range(1, num_perm + 1):
        max_r = int(num_perm / b)
        for r in range(1, max_r + 1):
            fp = _false_positive_probability(threshold, b, r)
            fn = _false_negative_probability(threshold, b, r)
            error = fp * false_positive_weight + fn * false_negative_weight
            if error < min_error:
                min_error = error
                opt = (b, r)
    return opt


def sha1_hash32(data: bytes) -> int:
    """
    A 32-bit hash function based on SHA1.

    Note:
        This implementation is copied from datasketch to avoid dependency.

    Args:
        data (bytes): the data to generate 32-bit integer hash from.

    Returns:
        int: an integer hash value that can be encoded using 32 bits.
    """
    return struct.unpack("<I", hashlib.sha1(data).digest()[:4])[0]


def generate_signature(
        words_sequence: Iterable[str],
        ngram_size: int,
        permutations: np.ndarray,
        max_hash: int,
        mersenne_prime: int
) -> np.ndarray:
    r"""
    Combined with some datasketch code to better parallelize computation.

    Note:
        This implementation is adapted from the near-dedupe implementation by
        the bigcode project.

    Parameters
    ----------
    words_sequence : str
        A sequence of (normalized) words for which to generate a signature.
    ngram_size : int
        The size of n-grams.
    permutations : np.ndarray
        The permutations for the minhash.
    max_hash: int
        The maximum value for hashes.
    mersenne_prime: int
        The mersenne prime.

    Returns
    -------
    List[np.uint32]
        The minhash signature.
    """
    num_perm = permutations.shape[-1]
    hashvalues = np.ones(num_perm, dtype=np.uint64) * max_hash
    tokens = {" ".join(t) for t in form_ngrams(words_sequence, ngram_size)}
    h_vals = np.array(
        [sha1_hash32(token.encode("utf-8")) for token in tokens],
        dtype=np.uint64
    )
    a, b = permutations
    phv = np.bitwise_and(
        ((h_vals * np.tile(a, (len(h_vals), 1)).T).T + b) % mersenne_prime,
        max_hash
    )

    # compute the minhash
    signature = np.vstack([phv, hashvalues]).min(axis=0).astype(np.uint32)

    return signature

File Path: app/src/pipeline.py
Content:
import argparse
import os
from concurrent.futures import ProcessPoolExecutor, as_completed
from collections import defaultdict
from datetime import datetime as dt
import gc
import json
import logging
import multiprocessing as mp
import numpy as np
from pathlib import Path
import random
import re
import time
from typing import Dict, List
import uuid
from urllib.parse import urlparse

from core.worker import Worker
from utilities.logging.trackers import RateTracker
from utilities.logging.mp import *


def get_timestamp():
    return dt.now().isoformat()


def monitor_progress(
        logging_queue: mp.Queue, monitor_queue: mp.Queue, languages: List[str],
        total_uri_counts: Dict[str, int]
):
    start_time = time.time()
    log_str = " | ".join([f"{lang}: {{{lang}:,}}" for lang in languages]) + \
              " | total: {total:,} | {rate:.2f} docs/s" + \
              " | {lang}: processed {uc}/{tuc} uris"

    # setup logging
    configure_worker_logger(logging_queue, level=logging.INFO)
    logger = logging.getLogger()

    total_docs = 0

    uri_counts_per_lang = {lang: 0 for lang in languages}
    doc_counts_per_lang = {k: 0 for k in languages}

    logger.info(f"Start monitoring...")

    rate_tracker = RateTracker(n=200)
    current_lang = None

    try:
        while True:

            batch_time = time.time()

            if (data := monitor_queue.get()) is None:
                break

            lang = data["lang"]
            num_docs = data["num_docs"]
            uri_complete = data.get("uri_complete", False)

            if uri_complete:
                # we received a uri complete signal -- record that one more
                # uri has been processed
                uri_counts_per_lang[lang] += 1
                continue

            if lang != current_lang:
                current_lang = lang
                rate_tracker.reset()
                logger.info(f"reset tracker for {lang}")

            doc_counts_per_lang[lang] += num_docs
            total_docs += num_docs

            rate_tracker.update(num_docs, batch_time)
            rate = rate_tracker.get_rate(time.time())

            # log stats
            logger.info(log_str.format(
                **doc_counts_per_lang, total=total_docs, rate=rate,
                lang=lang, uc=uri_counts_per_lang[lang],
                tuc=total_uri_counts[lang],
            ))

    except KeyboardInterrupt:
        logger.error(f"KeybordInterrupt. Shutting down progress monitor.")
        return

    logger.info("=" * 80)
    logger.info(f"Done. Total time {time.time() - start_time:2f}s")
    logger.info("=" * 80)
    logger.info("Document counts:")

    for lang, num_docs in doc_counts_per_lang.items():
        logger.info(f"{lang}: {num_docs:,}")

    logger.info(f"Total: {total_docs:,}")
    logger.info("=" * 80)
    logger.info(f"Progress monitor done.")


def main_logger_process(logging_queue: mp.Queue, logfile: Path):
    configure_listener_logger(logfile=logfile, level=logging.INFO)

    while True:
        message = logging_queue.get()
        if message is None:
            break
        logger = logging.getLogger(message.name)
        logger.handle(message)


class RPSignalJob:
    r""" Class for running the rp_signals pipeline """

    # descriptions for input and output arguments. This will be shown using
    # the --help flag
    input_descr = "The input must be provided as a listings file containing " \
                  "the relative paths to the data files, one per line as " \
                  "relative paths (to input_pase_uri)."

    def __init__(self):
        self._args = self.parse_arguments()
        self._job_id = str(uuid.uuid4())[:16]

        random.seed(self._args.seed)

        # convenience access to args
        self._languages = self._args.langs
        self._inputs_per_process = self._args.inputs_per_process

        # minhash
        self._minhash_ngram_size = self._args.minhash_ngram_size
        self._minhash_num_permutations = self._args.minhash_num_permutations
        self._minhash_similarities = self._args.minhash_similarities

        # artifacts
        self._artifacts_dir = Path(self._args.artifacts_dir)
        self._classifiers_dir = self._artifacts_dir / "classifiers"
        self._dsir_dir = self._artifacts_dir / "dsir"
        self._bad_words_dir = self._artifacts_dir / "bad_words"
        self._bad_urls_dir = self._artifacts_dir / "bad_urls"

        # i/o args
        self._snapshot_id = self._args.cc_snapshot_id
        self._input_listings = self.__parse_input_listings()
        self._output_base_uri = self._args.output_base_uri
        self._output_base_uri_parsed = urlparse(self._output_base_uri)
        self._log_dir = Path(
            self._output_base_uri_parsed.path
        ) / "logs" / self._snapshot_id

        # get classifier filepaths
        self._classifiers_files = self.__parse_classifiers_dir()

        # get filepaths for importance weights
        self._dsir_files = self.__parse_dsir_dir()

    def parse_arguments(self):
        if self.__doc__ is not None:
            description = " - " + self.__doc__
        else:
            description = self.__class__.__name__

        parser = argparse.ArgumentParser(
            prog=self.__class__.__name__, description=description
        )

        # input and outputs
        parser.add_argument(
            "--input", type=str, default=None, help=self.input_descr
        )
        parser.add_argument(
            "--input_base_uri", type=str,
            default=None,
            help="Base URL (prefix) used for files list in input. Used to "
                 "select the access method: s3://<path>/ or file://<path>/"
        )
        parser.add_argument(
            "--output_base_uri", type=str,
            default=None,
            help="Base URL (prefix) used for files list in output. Used to "
                 "select the access method: s3://<path>/ or file://<path>/"
        )
        parser.add_argument(
            "--filename_keep_patterns", type=str, nargs="+", default=None,
            help="list of regex patterns to match against filenames to keep"
        )
        parser.add_argument(
            "--cc_snapshot_id", type=str, default=None,
            help="id of the common crawl snapshot to process."
        )
        parser.add_argument(
            "--artifacts_dir", type=str, default=None,
            help="Path on the local filesystem to the directory containing "
                 "artifacts"
        )
        parser.add_argument(
            "--ext", type=str, default=".json.gz",
            help="File extension of input files; defaults to .json.gz"
        )
        parser.add_argument(
            "--max_docs", type=int, default=-1,
            help="maximum number of documents to process per input "
                 "file; for development purposes"
        )
        parser.add_argument(
            "--max_proc", type=int, default=None,
            help="maximum number of processes to use; default is the number "
                 "of available CPUs"
        )
        parser.add_argument(
            "--seed", type=int, default=42, help="random seed"
        )
        parser.add_argument(
            "--endpoint_url", type=str, default=None,
            help="endpoint url where the s3 bucket is exposed. "
        )
        parser.add_argument(
            "--inputs_per_process", type=int, default=20,
            help="number of inputs to process per worker"
        )
        parser.add_argument(
            "--langs", type=str, nargs="+",
            default=["en"],
            help="subset of languages for which data files are processed."
        )

        # dsir
        parser.add_argument(
            "--dsir_buckets", type=int, default=10_000,
            help="dimension of feature vector for dsir"
        )

        # minhash
        parser.add_argument(
            "--minhash_ngram_size", type=int, default=None,
            help="ngram size for minhash"
        )
        parser.add_argument(
            "--minhash_num_permutations", type=int, default=None,
            help="number of permutations for minhash"
        )
        parser.add_argument(
            "--minhash_similarities", nargs="+", default=None, type=float,
            help="json string with minhash similarities"
        )

        return parser.parse_args()

    def __parse_input_listings(self) -> Dict[str, List[str]]:
        r""" Parse the input listing """
        if self._args.input is None:
            raise ValueError("Input argument must be provided")

        if not Path(self._args.input).exists():
            raise ValueError(f"Listings {self._args.input} does not exist")

        inputs_per_language = defaultdict(list)
        fn_regexes = list(map(
            lambda p: re.compile(p), self._args.filename_keep_patterns or []
        ))

        with open(self._args.input) as f:
            for line in f.readlines():
                listing = line.strip()

                if not listing:
                    continue

                lang = Path(listing).name.split("_")[0]

                if lang not in self._languages:
                    continue

                if len(fn_regexes) > 0:
                    if not any(p.match(listing) for p in fn_regexes):
                        continue

                inputs_per_language[lang].append(listing)

        return inputs_per_language

    def __parse_classifiers_dir(self) -> Dict[str, Dict[str, str]]:
        model_files = defaultdict(dict)

        for lang in self._languages:
            model_dir = self._classifiers_dir / lang
            if not model_dir.exists():
                continue
            for model_file in model_dir.glob("*.bin"):
                domain = model_file.stem.split(".")[0]
                model_files[lang][domain] = str(model_file)

        return model_files

    def __parse_dsir_dir(self) -> Dict[str, Dict[str, List[str]]]:
        dsir_filepaths = defaultdict(dict)

        for lang in self._languages:
            dsir_dir = self._dsir_dir / lang

            if not dsir_dir.exists():
                continue

            for counts_file in dsir_dir.glob("*.counts.npy"):
                domain = counts_file.stem.split(".")[0]
                dsir_filepaths[lang][domain] = [str(counts_file)]

            for lambda_file in dsir_dir.glob("*.lambda.npy"):
                domain = lambda_file.stem.split(".")[0]
                dsir_filepaths[lang][domain].append(str(lambda_file))

        return dsir_filepaths

    def __log_run_setup(self, logger):
        logger.info(f"logging outputs to {self._log_dir}")
        logger.info(f"job_id: {self._job_id}")
        logger.info(f"PYTHONHASHSEED: {os.environ.get('PYTHONHASHSEED')}")

        # log args
        for arg, val in vars(self._args).items():
            logger.info(f"{arg}: {val}")

        # logs job fields
        logger.info(f"classifier_files: \n"
                    f"{json.dumps(self._classifiers_files, indent=4)}")
        logger.info(f"dsir_files: \n"
                    f"{json.dumps(self._dsir_files, indent=4)}")

    def run(self):
        max_proc = min(mp.cpu_count(), self._args.max_proc or np.inf)

        # get total number of uris per language
        total_uri_counts = {
            lang: len(self._input_listings[lang]) for lang in self._languages
        }

        # setup logging
        log_file = Path(self._log_dir) / f"{self._job_id}.log"
        manager = mp.Manager()

        queue_buffer_size = 128 * (self._args.max_proc or mp.cpu_count())

        # kick off logger process
        logging_queue = manager.Queue(maxsize=queue_buffer_size)
        logger_proc = mp.Process(
            target=main_logger_process, args=(logging_queue, log_file)
        )
        logger_proc.start()

        # start progress monitor
        monitor_queue = manager.Queue(maxsize=queue_buffer_size)
        monitor_proc = mp.Process(
            target=monitor_progress,
            args=(logging_queue, monitor_queue, self._languages,
                  total_uri_counts)
        )
        monitor_proc.start()

        configure_worker_logger(queue=logging_queue, level=logging.INFO)
        logger = logging.getLogger()

        # log run setup
        self.__log_run_setup(logger)
        for lang in self._languages:
            logger.info(f"{lang}: {len(self._input_listings[lang]):,} inputs")

        for lang in self._languages:
            lang_inputs = self._input_listings[lang]
            random.shuffle(lang_inputs)

            logger.info("*" * 80)
            logger.info(f"Start processing {lang}")

            chunk_size = self._args.inputs_per_process
            input_chunks = [
                lang_inputs[i * chunk_size:(i + 1) * chunk_size]
                for i in range(len(lang_inputs) // chunk_size)
            ]

            max_docs_per_chunk = self._args.max_docs // len(input_chunks)

            with ProcessPoolExecutor(max_workers=max_proc - 2) as executor:
                futures = [
                    executor.submit(
                        self._run_chunk,
                        input_listings=chunk,
                        lang=lang,
                        max_docs=max_docs_per_chunk,
                        monitor_queue=monitor_queue,
                        logging_queue=logging_queue,
                    )
                    for chunk in input_chunks
                ]

                try:
                    for future in as_completed(futures):
                        result = future.result()
                        futures.remove(future)
                        wid = result["job_id"]
                        exc = result["exception"]
                        if exc is not None:
                            logger.error(f"__WORKER_FAIL__ ({wid}) exc={exc}")
                            continue

                        logger.info(f"__WORKER_COMPLETED__ {wid} completed.")
                except KeyboardInterrupt:
                    logger.error(f"KeyboardInterrupt. Shutting down.")
                    executor.shutdown(wait=False, cancel_futures=True)
                    break

            gc.collect()

        # signal monitor to stop
        monitor_queue.put(None)
        monitor_proc.join()

        # signal logger to stop
        logging_queue.put_nowait(None)
        logger_proc.join()

        manager.shutdown()

    def _run_chunk(
            self, input_listings, lang, max_docs, monitor_queue, logging_queue
    ):

        if len(input_listings) == 0:
            return {"exception": None, "lang": lang, "job_id": None}

        proc = Worker(
            language=lang,
            snapshot_id=self._snapshot_id,
            input_listings=input_listings,
            input_base_uri=self._args.input_base_uri,
            output_base_uri=self._output_base_uri,
            log_dir=self._log_dir,
            classifier_files=self._classifiers_files.get(lang, {}),
            dsir_files=self._dsir_files.get(lang, {}),
            dsir_bucket=self._args.dsir_buckets,
            ldnoobw_dir=self._bad_words_dir,
            ut1_dir=self._bad_urls_dir,
            minhash_similarities=self._minhash_similarities,
            minhash_ngram_size=self._minhash_ngram_size,
            minhash_num_permutations=self._minhash_num_permutations,
            logging_queue=logging_queue,
            monitor_queue=monitor_queue,
            endpoint_url=self._args.endpoint_url,
            max_docs=max_docs,
            seed=self._args.seed,
            flush_interval=5000
        )

        try:
            proc.run()
            exc = None
        except Exception as e:
            exc = f"{e.__class__.__name__}: {e}"

        gc.collect()

        return {"exception": exc, "lang": lang, "job_id": proc.job_id}


if __name__ == '__main__':
    mp.set_start_method('fork')
    mp.set_executable("python")
    job = RPSignalJob()
    job.run()

File Path: app/src/prep_artifacts.py
Content:
import argparse
import logging
from datetime import datetime as dt
import os
from pathlib import Path

from artifacts.downloaders import (
    WikipediaDownloader,
    OpenWebTextDownloader,
    BooksDownloader,
    CCNetDownloader
)
from artifacts.hash_dist import HashDist
from artifacts.ft_trainer import FastTextTrainer
from utilities.logging.configure import configure_logger


def parse_arguments():
    def nullable_string(val):
        # converts empty string to None
        return None if not val else val

    parser = argparse.ArgumentParser()
    # input and outputs
    parser.add_argument(
        "--artifacts_dir", type=str, default=None,
        help="Directory where artifacts of the pipeline are stored"
    )
    parser.add_argument(
        "--cc_input", type=str, default=None,
        help="cc_net output listings"
    )
    parser.add_argument(
        "--cc_input_base_uri", type=str, default=None,
        help="Base URL (prefix) used for files list in input. Used to "
             "select the access method: s3://<path>/ or file://<path>/"
    )
    parser.add_argument(
        "--cache_dir", type=str, default=None,
        help="huggingface cache directory"
    )
    parser.add_argument(
        "--overwrite", action="store_true",
        help="Overwrite existing files"
    )
    parser.add_argument(
        "--lang", type=str, default=None
    )
    parser.add_argument(
        "--max_workers", type=int, default=None,
        help="Maximum number of workers to use"
    )
    parser.add_argument(
        "--dsir_num_samples", type=int, default=None,
        help="Number of samples to use for dsir"
    )
    parser.add_argument(
        "--dsir_feature_dim", type=int, default=None,
        help="Number of buckets to use for dsir"
    )
    parser.add_argument(
        "--classifiers_num_samples", type=int, default=None,
        help="Number of samples to use for classifiers"
    )
    parser.add_argument(
        "--endpoint_url", type=nullable_string, default=None,
        help="endpoint url where the s3 bucket is exposed."
    )

    # sampling
    parser.add_argument(
        "--max_samples_per_book", type=int, default=None,
        help="Maximum number of samples to use per book"
    )
    parser.add_argument(
        "--max_paragraphs_per_book_sample", type=int, default=None,
        help="Maximum number of paragraphs to use per book sample"
    )

    return parser.parse_args()


def main(artifacts_dir: str, cc_input: str, cc_input_base_uri: str,
         cache_dir: str, overwrite: bool, lang: str,
         max_workers: int, endpoint_url: str,
         dsir_num_samples: int, dsir_feature_dim: int,
         classifiers_num_samples: int, max_samples_per_book: int,
         max_paragraphs_per_book_sample: int
         ):
    if max_workers is None:
        max_workers = os.cpu_count() - 2
    else:
        max_workers = min(max_workers, os.cpu_count() - 2)

    # parse config
    num_samples = max(dsir_num_samples, classifiers_num_samples)

    # build output directory
    datasets_dir = Path(artifacts_dir) / "datasets" / f"{lang}"
    datasets_dir.mkdir(exist_ok=True, parents=True)
    timestamp = dt.now().strftime("%Y%m%d-%H%M%S")
    logfile = Path(artifacts_dir) / f"logs/{lang}_artifacts@{timestamp}.log"
    logfile.parent.mkdir(exist_ok=True, parents=True)
    configure_logger(logfile=logfile, level=logging.INFO)
    logger = logging.getLogger()

    logger.info(f"Start preparing artifacts for {lang}")
    logger.info(f"num_samples: {num_samples}")
    logger.info(f"PYTHONHASHSEED: {os.environ.get('PYTHONHASHSEED')}")

    # download ccnet dataset
    ccnet = CCNetDownloader(
        lang=lang, artifacts_dir=artifacts_dir, cc_input=cc_input,
        cc_input_base_uri=cc_input_base_uri, num_samples=num_samples,
        max_workers=max_workers, endpoint_url=endpoint_url
    )
    ccnet.run(logger=logger)

    # download wikipedia dataset
    wikipedia = WikipediaDownloader(
        lang=lang, out_dir=datasets_dir,
        overwrite=overwrite, cache_dir=cache_dir,
        max_samples=num_samples
    )
    wikipedia.run(logger=logger)

    # download openwebtext dataset
    openwebtext = OpenWebTextDownloader(
        lang=lang, out_dir=datasets_dir,
        overwrite=overwrite, cache_dir=cache_dir,
        max_samples=num_samples
    )
    openwebtext.run(logger=logger)

    # download books dataset
    books = BooksDownloader(
        lang=lang, out_dir=datasets_dir,
        overwrite=overwrite, cache_dir=cache_dir,
        max_samples=num_samples,
        max_paragraphs_per_sample=max_paragraphs_per_book_sample,
        max_samples_per_book=max_samples_per_book,
    )
    books.run(logger=logger)

    # compute hash distributions
    hash_dist = HashDist(
        artifacts_dir=artifacts_dir,
        num_samples=num_samples,
        buckets=dsir_feature_dim,
        max_workers=max_workers,
        logger=logger
    )

    # compute hash distribution for each dataset
    for obj in [wikipedia, openwebtext, books, ccnet]:
        fp = obj.filepath

        if fp is None:
            continue

        hash_dist.run(lang=lang, datafile=fp, dataset=obj.dataset_name)

    if lang == "en":
        # compute fasttext palm classifier
        target_name = "palm"
        target_data = [
            wikipedia.filepath, books.filepath, openwebtext.filepath
        ]
    else:
        # for non english languages, we use wikipedia as target
        target_name = f"wikipedia"
        target_data = [wikipedia.filepath]

    trainer = FastTextTrainer(
        artifacts_dir=artifacts_dir,
        ccnet_data=ccnet.filepath,
        target_data=target_data,
        target_name=target_name,
        samples_per_class=classifiers_num_samples,
        lang=lang
    )
    trainer.run(logger=logger)

    logger.info(f"Finished preparing artifacts for {lang}")


if __name__ == '__main__':
    args = parse_arguments()
    main(artifacts_dir=args.artifacts_dir,
         cc_input=args.cc_input,
         cc_input_base_uri=args.cc_input_base_uri,
         cache_dir=args.cache_dir,
         overwrite=args.overwrite,
         lang=args.lang,
         max_workers=args.max_workers,
         endpoint_url=args.endpoint_url,
         dsir_num_samples=args.dsir_num_samples,
         dsir_feature_dim=args.dsir_feature_dim,
         classifiers_num_samples=args.classifiers_num_samples,
         max_samples_per_book=args.max_samples_per_book,
         max_paragraphs_per_book_sample=args.max_paragraphs_per_book_sample
         )

File Path: app/src/run_lsh.py
Content:
import argparse
from datetime import datetime as dt
import gc
import logging
import networkit.components as nk_components
import networkit.graph as nk_graph
import numpy as np
import os
from pathlib import Path
import polars as pl
import pyarrow as pa
import pyarrow.dataset as ds
import s3fs
import time
from typing import Dict, Tuple, List
from urllib.parse import urlparse

from dedupe.utils import optimal_param

LOG_FMT = '[%(asctime)s]::(PID %(process)d)::%(levelname)-2s::%(message)s'


class LSH:
    r""" Locality Sensitive Hashing (LSH) algorithm for near deduplication. """
    __slots__ = (
        "_args", "_num_bands", "_job_id", "_logger", "_sig_key", "_schema"
    )

    # regex to extract filepaths from source file listings
    fp_pattern = r'\/(\d{4}-\d{2}\/\d{4}\/.*\.json\.gz)$'

    # signature key
    sig_key_pat = "signature_sim{s}"

    def __init__(self):
        self._job_id = dt.now().strftime("%Y%m%d_%H%M%S")
        self._args = self.__parse_arguments()

        self._sig_key = self.sig_key_pat.format(
            s=str(self._args.similarity)
        )

        # get number of bands and rows
        self._num_bands, _ = optimal_param(
            threshold=self._args.similarity, num_perm=self._args.num_perm
        )

        # init schema
        self._schema = self.__init_schema()

        # init logging
        self.__init_logger()

        # log setup
        self._logger.info("=" * 80)
        self._logger.info("LSH config:")
        for k, v in vars(self._args).items():
            self._logger.info(f"{k}: {v}")
        self._logger.info("=" * 80)

    def __init_schema(self) -> pa.Schema:
        return pa.schema([
            ("id", pa.string()),
            ("shard_id", pa.string()),
            ("id_int", pa.uint64()),
            (self._sig_key, pa.list_(pa.binary()))
        ])

    def __init_logger(self):
        self._logger = logging.getLogger(self._job_id)
        self._logger.setLevel(logging.DEBUG)

        # log to file
        logfile = Path(self._args.output_dir) / "logs" / f"{self._job_id}.log"
        if not logfile.parent.exists():
            logfile.parent.mkdir(parents=True)
        filehandler = logging.FileHandler(logfile)
        filehandler.setFormatter(logging.Formatter(LOG_FMT))
        self._logger.addHandler(filehandler)

        # log to stdout
        stream_handler = logging.StreamHandler()
        stream_handler.setFormatter(logging.Formatter(LOG_FMT))
        self._logger.addHandler(stream_handler)

    def __parse_arguments(self) -> argparse.Namespace:

        if self.__doc__ is not None:
            description = " - " + self.__doc__
        else:
            description = self.__class__.__name__

        parser = argparse.ArgumentParser(
            prog=self.__class__.__name__, description=description
        )
        parser.add_argument(
            "--listings", type=str, default=None,
            help="file containing paths to minhash parquet files. LSH will be"
                 "run on the minhashes stored in these files."
        )
        parser.add_argument(
            "--input_base_uri", type=str, default=None,
            help="base uri of the input files."
        )
        parser.add_argument(
            "--output_dir", type=str, default=None,
            help="root directory where the output will be stored."
        )
        parser.add_argument(
            "--similarity", type=float, default=None,
            help="similarity threshold for two documents to be considered near"
                 " duplicates."
        )
        parser.add_argument(
            "--num_perm", type=int, default=None,
            help="number of permutations used during minhashing."
        )
        parser.add_argument(
            "--max_docs", type=int, default=-1,
            help="maximum number of documents to process. If set to -1, all "
                 "documents will be processed."
        )

        # s3
        parser.add_argument(
            "--s3_profile", type=str, default="default",
            help="aws profile to use when connecting to s3."
        )
        parser.add_argument(
            "--endpoint_url", type=str, default=None,
            help="endpoint url of the s3 server."
        )

        args = parser.parse_args()

        return args

    def __build_dataset(self) -> pa.dataset.Dataset:
        base_uri = urlparse(self._args.input_base_uri)

        if base_uri.scheme == "file":
            return self.__buil_dataset_local(base_uri)
        elif base_uri.scheme == "s3":
            return self.__build_dataset_s3()
        else:
            raise ValueError(f"Invalid base uri: {base_uri}")

    def __buil_dataset_local(self, base_uri) -> pa.dataset.Dataset:
        root_path = Path(base_uri.path)

        # 1) get paths and build pyarrow dataset
        with open(self._args.listings, "r") as f:
            input_paths = [
                root_path / Path(line.strip()) for line in f.readlines()
            ]

        return ds.dataset(
            source=input_paths, schema=self._schema, format="parquet"
        )

    def __build_dataset_s3(self) -> pa.dataset.Dataset:
        fs = s3fs.S3FileSystem(
            profile=self._args.s3_profile,
            endpoint_url=self._args.endpoint_url
        )

        # 1) get paths and build pyarrow dataset
        with open(self._args.listings, "r") as f:
            input_paths = list(map(
                lambda ln: os.path.join(self._args.input_base_uri, ln.strip()),
                f.readlines()
            ))

        return ds.dataset(
            source=input_paths, filesystem=fs, schema=self._schema,
            format="parquet"
        )

    def run(self):
        global_start_time = time.time()

        # 1) build pyarrow dataset; this is a lazy operation pointing to a
        # collection of parquet files on disk or in an S3 bucket
        pa_dset = self.__build_dataset()

        # 2) build edges
        step_time = time.time()
        self._logger.info("Start building edges")
        edges = self.__build_edges(pa_dset=pa_dset)
        step_time = time.time() - step_time
        self._logger.info(
            f"Building edges complete. Shape={edges.shape}; Time={step_time}s"
        )

        # 3) detect components
        step_time = time.time()
        self._logger.info("Start detecting components")
        (
            components, num_nodes, reversed_mapper
        ) = self.__run_connected_components(edges=edges)
        step_time = time.time() - step_time
        self._logger.info(
            f"Connected compontents complete. Time={step_time}s"
        )

        del edges
        gc.collect()

        # 4) collect cluster ids
        step_time = time.time()
        self._logger.info("Start collecting cluster ids")
        cluster_ids = self.__get_doc_to_cluster_array(
            components=components, reversed_mapper=reversed_mapper
        )
        step_time = time.time() - step_time
        self._logger.info(f"Building doc->cluster index complete. "
                          f"Time={step_time}s")

        # 5) build cluster dataframes
        step_time = time.time()
        self._logger.info("Start building final cluster dataframes")
        cluster_dataframes = self.__build_cluster_dataframes(
            pa_dset=pa_dset, doc_to_cluster=cluster_ids
        )
        step_time = time.time() - step_time
        self._logger.info(f"Building final cluster dataframes complete. "
                          f"Time={step_time}s")

        # 6) write cluster dataframes to disk
        out_root = Path(self._args.output_dir)
        for k, v in cluster_dataframes.items():

            tag = Path(k.split(".")[0]).with_suffix(".clusters.parquet")
            if not (out_root / tag).parent.exists():
                (out_root / tag).parent.mkdir(parents=True)

            # write to disk
            v.write_parquet(out_root / tag)
            self._logger.info(f"Wrote cluster data to {out_root / tag}")

        elapsed_time = time.time() - global_start_time
        self._logger.info(f"LSH complete. Total time: {elapsed_time}s")

    def __build_edges(self, pa_dset: pa.dataset.Dataset) -> np.ndarray:

        # build polars query plan
        query = pl.scan_pyarrow_dataset(pa_dset)

        if self._args.max_docs > 0:
            query = query.head(self._args.max_docs)

        query = (
            query
            .select(
                pl.col(["id_int", self._sig_key])
            )
            .filter(
                ~pl.col(self._sig_key).is_null()
            )
            .with_columns(
                pl.Series(
                    name="band",
                    values=[list(range(self._num_bands))],
                    dtype=pl.List(pl.UInt8)
                )
            )
            .explode(self._sig_key, "band")
            .group_by(self._sig_key, "band")
            .agg(pl.col("id_int"))
            .filter(
                pl.col("id_int").list.lengths() > 1
            )
            .select(
                pl.col("id_int"),
                pl.col("id_int").list.min().alias("min_node")
            )
            .explode("id_int")
            .filter(
                pl.col("id_int") != pl.col("min_node")
            )
            .select(
                pl.concat_list(["id_int", "min_node"]).alias("edges")
            )
            .unique("edges")
        )

        self._logger.debug(f"Query Plan:\n{query.explain()}")
        self._logger.debug(f"Start running query...")
        edges = query.collect(streaming=True).to_numpy().flatten()
        self._logger.debug(f"Completed running query.")
        gc.collect()

        return edges

    @staticmethod
    def __run_connected_components(
            edges: np.ndarray
    ) -> Tuple[List[List[int]], int, Dict[int, int]]:
        # build graph from edges
        graph = nk_graph.Graph()
        node_mapper = {}

        for row in edges:
            node_id1, node_id2 = row

            if node_id1 not in node_mapper:
                node_mapper[node_id1] = graph.addNode()

            if node_id2 not in node_mapper:
                node_mapper[node_id2] = graph.addNode()

            graph.addEdge(node_mapper[node_id1], node_mapper[node_id2])

        reversed_mapper = {value: key for key, value in node_mapper.items()}

        # compute connected components
        cc = nk_components.ConnectedComponents(G=graph)
        cc.run()
        components = cc.getComponents()
        num_nodes = sum(cc.getComponentSizes().values())

        return components, num_nodes, reversed_mapper

    @staticmethod
    def __get_doc_to_cluster_array(
            components: List[List[int]], reversed_mapper: Dict[int, int]
    ) -> np.ndarray:
        def __process_comp(comp) -> np.ndarray:
            nodes = np.array(
                list(map(reversed_mapper.get, comp))
            ).reshape(-1, 1)
            cluster_id = min(map(reversed_mapper.get, comp))
            cluster_id = np.repeat(cluster_id, len(nodes)).reshape(-1, 1)
            return np.hstack((nodes, cluster_id))

        data = np.vstack(tuple(map(__process_comp, components)))

        return data

    def __build_cluster_dataframes(
            self, pa_dset: pa.dataset.Dataset, doc_to_cluster: np.ndarray
    ) -> Dict[str, pl.DataFrame]:
        cluster_df = pl.LazyFrame(
            data=doc_to_cluster,
            schema=[("id_int", pl.UInt64), ("cluster_id", pl.UInt64)]
        )

        # build polars query plan
        query = pl.scan_pyarrow_dataset(pa_dset)

        if self._args.max_docs > 0:
            query = query.head(self._args.max_docs)

        partitioned_dfs = (
            query
            .select(pl.col(["id", "id_int", "shard_id"]))
            .join(other=cluster_df, on="id_int", how="inner")
            .select(pl.col(["id", "id_int", "cluster_id", "shard_id"]))
            .collect()
        )

        with pl.Config(set_fmt_str_lengths=5000, tbl_rows=20):
            self._logger.info(
                f"First 20 rows of minhash clusters:\n\n"
                f"{partitioned_dfs.sort(by='cluster_id').head(20)}"
            )
        time.sleep(2)

        partitioned_dfs = partitioned_dfs.partition_by(by="shard_id",
                                                       as_dict=True)

        return partitioned_dfs


if __name__ == '__main__':
    job = LSH()
    job.run()

File Path: app/src/token_count.py
Content:
import argparse
import boto3
import botocore.client
import concurrent.futures
from dataclasses import dataclass
from datetime import datetime as dt
import gzip
import io
import logging
import msgspec
import os
from pathlib import Path
import progiter
import pyarrow as pa
import random
import re
from typing import Tuple, List
from tokenizers import Tokenizer
from urllib.parse import urlparse, ParseResult

from utilities.logging import configure_logger
from utilities.io.writer import ParquetBatchWriter


class InputSpec(msgspec.Struct):
    raw_content: str


@dataclass
class DlStatus:
    is_success: bool
    msg: str
    uri: str


@dataclass
class InputResult:
    is_success: bool
    msg: str
    input_id: str
    num_docs: int = 0
    num_tokens: int = 0
    token_counts: List[Tuple[int, int]] = None


TOKENIZER = Tokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")


class PostProcessor:
    listings_re = re.compile(
        r".*(\d{4}-\d{2}/\d{4}/(?:en|es|de|fr|it)_(?:tail|middle|head)).json.gz"
    )

    def __parse_arguments(self) -> argparse.Namespace:

        if self.__doc__ is not None:
            description = " - " + self.__doc__
        else:
            description = self.__class__.__name__

        parser = argparse.ArgumentParser(
            prog=self.__class__.__name__, description=description
        )

        # io
        parser.add_argument(
            "--snapshots", nargs="+", type=str, default=None,
        )
        parser.add_argument(
            "--input_base_uri", type=str, default=None,
            help="base uri of the input files."
        )
        parser.add_argument(
            "--logs_dir", type=str, default=None,
            help="directory to store logs."
        )

        parser.add_argument(
            "--s3_profile", type=str, default="default",
            help="profile name of the s3 client."
        )
        parser.add_argument(
            "--endpoint_url", type=str, default=None,
            help="S3 bucket endpoint url."
        )
        parser.add_argument(
            "--parallelism", type=int, default=1,
            help="number of parallel processes. Defaults to 1."
        )

        parser.add_argument(
            "--batch_size", type=int, default=1,
            help="batch size. Defaults to 1."
        )
        parser.add_argument(
            "--max_inputs", type=int, default=4,
            help="maximum number of inputs to process. For debugging."
        )

        parser.add_argument(
            "--debug", default=0, choices=[0, 1], type=int,
            help="runs in debug mode if set to 1."
        )
        parser.add_argument(
            "--input_listings", type=str, default="listings.txt",
            help="path to file containing input ids."
        )
        parser.add_argument(
            "--seed", type=int, default=42
        )

        args = parser.parse_args()

        return args

    def __init__(self):
        self._job_id = dt.now().strftime("%Y%m%d_%H%M%S")
        self._args = self.__parse_arguments()

        random.seed(self._args.seed)

        # i/o
        self._input_base_uri = self._args.input_base_uri
        self._logs_dir = self._args.logs_dir

    def __init_client(self):
        session = boto3.Session(profile_name=self._args.s3_profile)
        client = session.client(
            service_name='s3',
            endpoint_url=self._args.endpoint_url,
            config=boto3.session.Config(
                signature_version='s3v4',
                retries={'max_attempts': 10, 'mode': 'standard'}
            )
        )
        return session, client

    @staticmethod
    def _dload_file(uri: ParseResult, client) -> Tuple[DlStatus, io.BytesIO]:
        try:
            streaming_body = client.get_object(
                Bucket=uri.netloc, Key=uri.path.lstrip("/")
            )["Body"]
            buffer = io.BytesIO(streaming_body.read())
            msg = f"__S3_URI_READ_SUCCESS__ success reading {uri.path}"
            is_success = True
        except Exception as e:
            msg = (
                f"__S3_URI_READ_ERROR__ failed reading {uri.path}: "
                f"caught exception {e.__class__.__name__}: {e}"
            )
            buffer = None
            is_success = False

        read_status = DlStatus(is_success=is_success, msg=msg, uri=str(uri))
        return read_status, buffer

    def __load_input_ids(
            self, snapshot: str
    ) -> List[str]:

        assert self._args.input_listings is not None

        input_ids = []
        with open(self._args.input_listings, "r") as fin:
            for ln in fin.readlines():
                try:
                    ln = self.listings_re.findall(ln.strip())[0]
                except IndexError:
                    continue
                if f"{snapshot}/" not in ln:
                    continue
                input_ids.append(ln)

        return input_ids

    def _process_listings(self, input_ids: List[str]) -> List[InputResult]:
        sess, client = self.__init_client()

        # decoding and encoding
        decoder = msgspec.json.Decoder(type=InputSpec)

        results = []
        for input_id in input_ids:
            proc_res: InputResult = self._process_single_listing(
                client, input_id, decoder
            )
            results.append(proc_res)

        return results

    def _process_single_listing(
            self, client, input_id, decoder
    ) -> InputResult:
        # handle signals
        result: InputResult = self._handle_documents(
            client, input_id, decoder
        )
        if not result.is_success:
            result.msg = f"__FAIL__ {input_id} ({result.msg})"
            return result

        result.msg = f"__SUCCESS__ {input_id}"

        return result

    def _handle_documents(
            self,
            client: botocore.client.BaseClient,
            input_id: str,
            decoder
    ) -> InputResult:
        # download doc
        input_uri = urlparse(
            os.path.join(
                self._input_base_uri, f"{input_id}.json.gz"
            )
        )
        dl_status, input_buffer = self._dload_file(input_uri, client=client)

        # check if download was successful
        if not dl_status.is_success:
            return InputResult(
                is_success=False, msg=dl_status.msg, input_id=input_id
            )

        num_docs = 0
        total_tokens = 0
        token_counts = []

        try:
            with gzip.open(input_buffer, mode="rb") as in_fh:
                for idx, obj in enumerate(in_fh):
                    record = decoder.decode(obj)

                    # tokenize
                    num_tokens = len(
                        TOKENIZER.encode(record.raw_content).tokens
                    )
                    token_counts.append((idx, num_tokens))

                    total_tokens += num_tokens
                    num_docs += 1

        except Exception as e:
            msg = (
                f"__DECODE_ENCODE_FAIL__ {input_id}: "
                f"caught exception {e.__class__.__name__}: {e}"
            )
            return InputResult(is_success=False, msg=msg, input_id=input_id)

        return InputResult(
            is_success=True,
            msg="",
            input_id=input_id,
            num_docs=num_docs,
            num_tokens=total_tokens,
            token_counts=token_counts
        )

    def run(self):
        # init logging
        logfile = Path(self._logs_dir) / f"{self._job_id}.log"
        configure_logger(logfile=logfile, level=logging.INFO, stream=False)
        logger = logging.getLogger()

        # log configs
        for attr in (
                "snapshots", "input_base_uri", "batch_size",
                "parallelism", "max_inputs", "debug", "input_listings", "seed"
        ):
            logger.info(f"__CONFIG__ {attr}: {getattr(self._args, attr)}")

        for snapshot in self._args.snapshots:
            logger.info(f"__START_SNAPSHOT__ {snapshot}")
            try:
                self.run_snapshot(snapshot, logger=logger)
            except KeyboardInterrupt:
                break
            logger.info(f"__END_SNAPSHOT__ {snapshot}")

    def run_snapshot(self, snapshot_id, logger):
        # load input file ids
        input_ids = self.__load_input_ids(snapshot_id)
        msg = (
            f"__INPUT_LISTINGS_LOADED__ "
            f"found {len(input_ids)} input files in {snapshot_id}"
        )
        logger.info(msg)
        random.shuffle(input_ids)

        if self._args.max_inputs is not None:
            input_ids = input_ids[:self._args.max_inputs]

        input_ids_batches = [
            input_ids[i:i + self._args.batch_size]
            for i in range(0, len(input_ids), self._args.batch_size)
        ]

        # init output writer
        out_fp = Path(self._logs_dir) / f"{snapshot_id}_counts.parquet"
        out_schema = pa.schema([
            ("input_id", pa.string()),
            ("doc_id", pa.string()),
            ("snapshot_id", pa.string()),
            ("num_tokens", pa.int64())
        ])

        pq_writer = ParquetBatchWriter(output_fp=out_fp, schema=out_schema)

        if self._args.debug:
            self.__debug_run(
                input_ids_batches, logger=logger, snapshot_id=snapshot_id,
                pq_writer=pq_writer
            )
        else:
            self.__parallel_run(
                input_ids_batches, logger=logger, snapshot_id=snapshot_id,
                pq_writer=pq_writer
            )

        pq_writer.close()

    def __debug_run(
            self,
            input_ids_batches: List[List[str]],
            logger: logging.Logger,
            snapshot_id: str,
            pq_writer: ParquetBatchWriter
    ):
        num_docs = 0
        num_succ = 0
        num_fail = 0
        total_tokens = 0

        # progress bar
        total_inputs = sum(map(len, input_ids_batches))
        pman = progiter.ProgressManager(backend="rich")
        pbar = pman.progiter(
            total=total_inputs,
            desc=f"Processing {snapshot_id}",
            backend="rich"
        )

        for batch in input_ids_batches:
            inputs_results: List[InputResult] = self._process_listings(batch)

            for proc_res in inputs_results:
                if proc_res.is_success:
                    num_succ += 1
                    num_docs += proc_res.num_docs
                    total_tokens += proc_res.num_tokens
                else:
                    num_fail += 1

                logger.info(proc_res.msg)

                pbar.step(1)
                pbar.set_postfix_str(
                    f"total_inputs: {num_succ:,} ({num_fail:,} fail); "
                    f"num_docs: {num_docs:,} -- "
                    f"num_tokens: {total_tokens:,}"
                )

                if not proc_res.is_success:
                    continue

                for idx, num_tokens in proc_res.token_counts:
                    pq_writer.update_batch({
                        "input_id": proc_res.input_id,
                        "doc_id": f"{proc_res.input_id}.json.gz/{idx}",
                        "snapshot_id": snapshot_id,
                        "num_tokens": num_tokens,
                    })

            pq_writer.write_batch()

        pman.stop()

        # log summary
        logger.info(
            f"__PROCESSING_COMPLETE__\n*******************\n"
            f"num_inputs_success: {num_succ:,}\n"
            f"num_inputs_failed: {num_fail:,}\n"
            f"num_docs: {num_docs:,}\n"
            f"num_tokens: {total_tokens:,}"
        )

    def __parallel_run(
            self,
            input_ids_batches: List[List[str]],
            logger: logging.Logger,
            snapshot_id: str,
            pq_writer: ParquetBatchWriter
    ):
        num_docs = 0
        num_succ = 0
        num_fail = 0
        total_tokens = 0

        # progress bar
        total_inputs = sum(map(len, input_ids_batches))
        pman = progiter.ProgressManager(backend="rich")
        pbar = pman.progiter(
            total=total_inputs,
            desc=f"Processing {snapshot_id}",
            backend="rich"
        )

        # process listings
        try:
            with concurrent.futures.ProcessPoolExecutor(
                    max_workers=self._args.parallelism
            ) as executor:
                futures = {
                    executor.submit(
                        self._process_listings,
                        input_ids=batch,
                    ): batch
                    for batch in input_ids_batches
                }

                for future in concurrent.futures.as_completed(futures):
                    proc_results: List[InputResult] = future.result()
                    del futures[future]

                    for proc_res in proc_results:
                        if proc_res.is_success:
                            num_succ += 1
                            num_docs += proc_res.num_docs
                            total_tokens += proc_res.num_tokens
                        else:
                            num_fail += 1

                        logger.info(proc_res.msg)

                        pbar.step(1)
                        pbar.set_postfix_str(
                            f"total_inputs: {num_succ:,} ({num_fail:,} fail); "
                            f"num_docs: {num_docs:,} -- "
                            f"num_tokens: {total_tokens:,}"
                        )

                        if not proc_res.is_success:
                            continue

                        for idx, num_tokens in proc_res.token_counts:
                            pq_writer.update_batch({
                                "input_id": proc_res.input_id,
                                "doc_id": f"{proc_res.input_id}.json.gz/{idx}",
                                "snapshot_id": snapshot_id,
                                "num_tokens": num_tokens,
                            })

                    pq_writer.write_batch()

        except KeyboardInterrupt:
            logger.info("KeyboardInterrupt caught. Terminating...")
            pman.stop()
            executor.shutdown(wait=False, cancel_futures=True)
            pq_writer.close()
            raise KeyboardInterrupt

        pman.stop()

        # log summary
        logger.info(
            f"__PROCESSING_COMPLETE__\n*******************\n"
            f"num_inputs_success: {num_succ:,}\n"
            f"num_inputs_failed: {num_fail:,}\n"
            f"num_docs: {num_docs:,}\n"
            f"num_tokens: {total_tokens:,}"
        )


if __name__ == '__main__':
    pp = PostProcessor()
    pp.run()

File Path: app/src/utilities/__init__.py
Content:

File Path: app/src/utilities/io/__init__.py
Content:
from .writer import Writer, ParquetBatchWriter
from .reader import Reader

File Path: app/src/utilities/io/reader.py
Content:
import io
import msgspec
import boto3
import gzip
from urllib.parse import urlparse, ParseResult
import pathlib
from typing import Optional, Type, List, Tuple
import xopen

from core.data_types import InputSpec
from core.exceptions import *


class Reader:
    r""" Read plain jsonl, jsonl.zst and jsonl.gz files using msgspec """

    def __init__(
            self,
            schema: List[Tuple[str, type]] = None,
            input_spec: Type[msgspec.Struct] = InputSpec,
            s3_client: Optional[boto3.client] = None,
            threads: int = 1,
            logger=None
    ):
        self._client = s3_client
        self._threads = threads

        if logger is None:
            self._print = print
        else:
            self._print = logger.error

        # msgspec decoder
        if schema is not None:
            input_type = msgspec.defstruct(name="Record", fields=schema)
        else:
            input_type = input_spec

        self._obj_decoder = msgspec.json.Decoder(type=input_type)

        self._total_consumed = 0

    def read(self, uri: str, max_samples: Optional[int] = -1,
             return_idx: bool = True):
        n_samples = 0

        try:
            with self.__get_filehandle(uri) as fh:
                for idx, obj in enumerate(fh):
                    try:
                        record = self._obj_decoder.decode(obj)
                        if return_idx:
                            yield idx, record
                        else:
                            yield record
                    except Exception as e:
                        self._print(f"__SAMPLE_READ_ERROR__ {uri}/{idx}: "
                                    f"{e.__class__.__name__}: {e}")
                        continue

                    n_samples += 1

                    if n_samples >= max_samples > 0:
                        break
        except S3ReadError as e:
            raise e
        except LocalReadError:
            raise e
        except Exception as e:
            raise UnknownReadError(f"unknown __URI_READ_ERROR__ {uri}: "
                                   f"{e.__class__.__name__}: {e}")

    def __get_filehandle(self, uri: str):
        uri = urlparse(uri)

        if uri.scheme == "s3":
            return self.__get_s3_filehandle(uri)

        if uri.scheme == "file":
            return self.__get_local_filehandle(uri)

        raise ValueError(f"Invalid uri: {uri}; must be of the form "
                         f"s3://<bucket>/<key> or file://<path>")

    def __get_s3_filehandle(self, uri: ParseResult):
        assert self._client is not None, "S3 client not initialized"

        try:
            streaming_body = self._client.get_object(
                Bucket=uri.netloc, Key=uri.path.lstrip("/")
            )["Body"]
            buffer = io.BytesIO(streaming_body.read())
        except Exception as e:
            raise S3ReadError(
                f"__S3_URI_READ_ERROR__ failed reading {uri.path}: "
                f"caught exception {e.__class__.__name__}: {e}"
            )

        return gzip.open(buffer, mode="rb")

    def __get_local_filehandle(self, uri: ParseResult):
        fp = pathlib.Path(uri.path)

        try:
            if fp.suffix == ".gz":
                return xopen.xopen(fp, mode="rb", threads=self._threads)

            if fp.suffix == ".jsonl":
                return open(fp, mode="rb")
        except Exception as e:
            raise LocalReadError(
                f"__LOCAL_URI_READ_ERROR__ failed reading {uri.path}: "
                f"caught exception {e.__class__.__name__}: {e}"
            )

        raise ValueError(f"File type of {fp} not supported.")

File Path: app/src/utilities/io/s3.py
Content:
import boto3
from botocore import UNSIGNED


def init_client(
        endpoint_url: str,
        aws_access_key_id: str = None,
        aws_secret_access_key: str = None,
        signature_version: str = UNSIGNED
):
    return boto3.client(
        service_name='s3',
        aws_access_key_id=aws_access_key_id,
        aws_secret_access_key=aws_secret_access_key,
        endpoint_url=endpoint_url,
        config=boto3.session.Config(
            signature_version=signature_version,
            retries={
                'max_attempts': 5,  # this is the default in standard mode
                'mode': 'standard'
            }
        )
    )

File Path: app/src/utilities/io/writer.py
Content:
import pathlib
import msgspec
import gzip
import pyarrow as pa
import pyarrow.parquet as pq
from urllib.parse import urlparse
import boto3

from typing import Type, Any, Dict, List, Tuple, Optional


class Writer:
    def __init__(
            self,
            uri: str,
            schema: List[Tuple[str, type]],
            s3_client: Optional[boto3.client] = None
    ):
        self._client = s3_client
        uri = urlparse(uri)

        if uri.scheme == "s3":
            raise NotImplementedError("streaming to S3 not supported yet")

        elif uri.scheme == "file":
            fp = pathlib.Path(uri.path)

            if not fp.parent.exists():
                fp.parent.mkdir(parents=True, exist_ok=True)

            if fp.suffix == ".gz":
                self._filehandle = gzip.open(fp, mode="wb")
            elif fp.suffix == ".jsonl":
                self._filehandle = open(fp, mode="wb")
            else:
                raise ValueError(f"File type of {fp} not supported.")
        else:
            raise ValueError(f"Invalid uri: {uri}; must be of the form "
                             f"s3://<bucket>/<key> or file://<path>")

        # encode records using msgspec
        self._encoder = msgspec.json.Encoder()
        self._buffer = bytearray(64)

        # define record struct
        self._record: Type[msgspec.Struct] = msgspec.defstruct(
            name="Record", fields=schema
        )

    def write(self, data_obj: Dict[str, Any], flush: bool = False):
        self._encoder.encode_into(self._record(**data_obj), self._buffer)
        self._buffer.extend(b"\n")
        self._filehandle.write(self._buffer)

        if flush:
            self.flush()

    def close(self):
        self.flush()
        self._filehandle.close()

    def flush(self):
        self._filehandle.flush()
        self._buffer.clear()


class ParquetBatchWriter:

    def __init__(self, output_fp, schema: pa.Schema):
        self._schema = schema
        self._writer = pq.ParquetWriter(output_fp, self._schema)
        self.__init_batch()

    def close(self):
        if len(self._batch[self._schema.names[0]]) > 0:
            self.write_batch()
        self._writer.close()

    def update_batch(self, obj: Dict[str, Any]):
        for col in self._schema.names:
            self._batch[col].append(obj[col])

    def write_batch(self):
        self._writer.write_batch(batch=pa.record_batch(
            data=[
                pa.array(self._batch[field.name], type=field.type)
                for field in self._schema
            ],
            schema=self._schema
        ))
        self.__init_batch()

    def __init_batch(self):
        self._batch = {col: [] for col in self._schema.names}

File Path: app/src/utilities/logging/__init__.py
Content:
from .format import LOG_FMT
from .configure import configure_logger

File Path: app/src/utilities/logging/configure.py
Content:
import logging
from pathlib import Path
from typing import Optional

from .format import LOG_FMT

__all__ = [
    "configure_logger",
]


def configure_logger(
        logfile: Optional[Path] = None, level: int = logging.DEBUG,
        stream: bool = True
):
    root = logging.getLogger()
    formatter = logging.Formatter(LOG_FMT)

    # write to log file
    if logfile is not None:
        if not logfile.parent.exists():
            logfile.parent.mkdir(parents=True)
        file_handler = logging.FileHandler(logfile)
        file_handler.setFormatter(formatter)
        root.addHandler(file_handler)

    # write to stdout
    if stream:
        stream_handler = logging.StreamHandler()
        stream_handler.setFormatter(formatter)
        root.addHandler(stream_handler)

    root.setLevel(level)

File Path: app/src/utilities/logging/format.py
Content:
LOG_FMT = '[%(asctime)s]::(PID %(process)d)::%(levelname)-2s::%(message)s'

File Path: app/src/utilities/logging/mp.py
Content:
import logging
from logging.handlers import QueueHandler
import multiprocessing as mp
from pathlib import Path
from typing import Optional

from .format import LOG_FMT

__all__ = [
    "configure_worker_logger",
    "configure_listener_logger",
]


def configure_worker_logger(
        queue: Optional[mp.Queue] = None, level: int = logging.DEBUG
):
    root = logging.getLogger()

    if not root.hasHandlers() and queue is not None:
        h = logging.handlers.QueueHandler(queue)
        root.addHandler(h)

    root.setLevel(level)


def configure_listener_logger(
        logfile: Optional[Path] = None, level: int = logging.DEBUG
):
    root = logging.getLogger()
    formatter = logging.Formatter(LOG_FMT)

    # write to log file
    if logfile is not None:
        if not logfile.parent.exists():
            logfile.parent.mkdir(parents=True)
        file_handler = logging.FileHandler(logfile)
        file_handler.setFormatter(formatter)
        root.addHandler(file_handler)

    # write to stdout
    stream_handler = logging.StreamHandler()
    stream_handler.setFormatter(formatter)
    root.addHandler(stream_handler)

    root.setLevel(level)

File Path: app/src/utilities/logging/trackers.py
Content:
__all__ = ["RateTracker"]


class RateTracker:
    def __init__(self, n=200):
        self._start_time_tracker = []
        self._counts_tracker = []
        self._n = n

    def update(self, count, start_time):
        if len(self._start_time_tracker) >= self._n:
            self._start_time_tracker.pop(0)
            self._counts_tracker.pop(0)

        self._start_time_tracker.append(start_time)
        self._counts_tracker.append(count)

    def get_rate(self, current_time: float):
        if len(self._start_time_tracker) == 0:
            return 0

        if current_time - self._start_time_tracker[0] < 1e-6:
            return 0

        start_time = self._start_time_tracker[0]
        pages = sum(self._counts_tracker)
        return pages / (current_time - start_time)

    def reset(self):
        self._start_time_tracker = []
        self._counts_tracker = []

File Path: app/src/utilities/register/__init__.py
Content:

File Path: app/src/utilities/register/registry_utils.py
Content:
import inspect
from typing import Tuple, List, Type

from core.quality_signals.base import RPSBase

__all__ = [
    "get_callables_from_module", "signal_schema"
]

_SIG_PREF = RPSBase.RPS_PREFIX


def get_callables_from_module(module: object) -> List[Type[RPSBase]]:
    r""" Returns a list of signal class references that are defined in the
    module.

    Args:
        module: The module to search for signal classes, obtained via
            `sys.modules[__name__]`.

    Returns:
        A list of signal class references.
    """

    def _sig_func_predicate(mem: object):
        return inspect.isclass(mem) and mem.__name__.startswith(_SIG_PREF)

    return [cls for _, cls in inspect.getmembers(module, _sig_func_predicate)]


def signal_schema(module: object) -> List[Tuple[str, Type]]:
    r""" Returns a list of signal names and their data types, defining the
    schema for signals. """
    return list(map(
        lambda cls: (cls.__name__.lower(), cls.DATA_TYPE),
        get_callables_from_module(module=module)
    ))

File Path: app/src/utilities/text/__init__.py
Content:
from .ngrams import form_ngrams
from .normalization import normalize
from .util import generate_paragraphs

File Path: app/src/utilities/text/ngrams.py
Content:
def form_ngrams(sequence, n):
    history = []
    # build the first ngram, yielding only when we have a full ngram
    while n > 1:
        try:
            next_item = next(sequence)
        except StopIteration:
            # no more data, terminate the generator
            return
        history.append(next_item)
        n -= 1

    # yield each ngram we have, then add the next item and repeat
    for item in sequence:
        history.append(item)
        yield tuple(history)
        del history[0]

File Path: app/src/utilities/text/normalization.py
Content:
import re
import string
import unicodedata

TRANSLATION_TABLE_PUNCTUATION = str.maketrans("", "", string.punctuation)


def normalize(
        text: str,
        remove_punct: bool = True,
        lowercase: bool = True,
        nfd_unicode: bool = True,
        white_space: bool = True
) -> str:
    """ Normalize the text by lowercasing and removing punctuation. """
    # remove punctuation
    if remove_punct:
        text = text.translate(TRANSLATION_TABLE_PUNCTUATION)

    # lowercase
    if lowercase:
        text = text.lower()

    if white_space:
        text = text.strip()
        text = re.sub(r"\s+", " ", text)

    # NFD unicode normalization
    if nfd_unicode:
        text = unicodedata.normalize("NFD", text)

    return text

File Path: app/src/utilities/text/util.py
Content:
import re


def generate_paragraphs(text: str, remove_empty: bool = True):
    for match in re.finditer(r"([^\n]*\n|[^\n]+$)", text):
        text_slice = text[match.start():match.end()]

        if remove_empty and not text_slice.strip():
            continue

        yield text_slice

Output:
{
    "experimental_code": "File Path: app/src/artifacts/downloaders/books_downloader.py\nContent:\nclass BooksDownloader:\n    dataset_name = \"books\"\n    output_fp = \"books/en-books.jsonl.gz\"\n    def __init__(\n            self, lang, out_dir, overwrite, cache_dir, max_samples,\n            max_paragraphs_per_sample=200, max_samples_per_book=500\n    ):\n        self._lang = lang\n        self._out_dir = out_dir\n        self._overwrite = overwrite\n        self._cache_dir = cache_dir\n        self._max_samples = max_samples\n        self._max_paragraphs_per_sample = max_paragraphs_per_sample\n        self._max_samples_per_book = max_samples_per_book\n        self._filepath = None\n\n    def run(self, logger):\n        # ... (setup file paths and writer)\n        for book in load_dataset(\n                \"togethercomputer/RedPajama-Data-1T\", name=\"book\",\n                cache_dir=self._cache_dir,\n                split=\"train\", streaming=True\n        ):\n            for chunk in self.__generate_chunks(book[\"text\"]):\n                n_docs += 1\n                if n_docs > self._max_samples > 0:\n                    break\n                writer.write(data_obj={\"text\": chunk}, flush=n_docs % flush_every == 0)\n                pbar.update(1)\n            else:\n                continue\n            break\n        pbar.close()\n        writer.close()\n\nFile Path: app/src/core/worker.py\nContent:\nclass Worker:\n    # ... (init and properties)\n    def __init_quality_signals(self, ldnoobw_dir, ut1_dir) -> List[Callable]:\n        callables = []\n        callables += register_content_callables(\n            language=self._lang, bad_urls_dir=ut1_dir, bad_words_dir=ldnoobw_dir\n        )\n        callables += register_repetitions_callables()\n        callables += register_natural_language_callables()\n        callables += register_lines_callables()\n        callables += register_classifier_callables(\n            wikiref_model=self._wikiref_model_file,\n            palm_model=self._palm_model_file,\n            wikipedia_model=self._wikipedia_model_file\n        )\n        callables += register_importance_weights_callables(\n            source_fps=self._dsir_files.get(\"ccnet\"), wiki_fps=self._dsir_files.get(\"wikipedia\"),\n            openwebtext_fps=self._dsir_files.get(\"openwebtext\"), books_fps=self._dsir_files.get(\"books\"),\n            language=self._lang\n        )\n        return callables\n\n    def __process_record(\n            self, idx: int, record, uri_id: str, snapshot_id: str\n    ):\n        document = Document(\n            record.raw_content,\n            domain=record.source_domain,\n            precompute_ngrams=True,\n            precompute_hash_features=True,\n            dsir_buckets=self._dsir_buckets\n        )\n        rp_v2_signals = {}\n        for func in self._quality_signals:\n            rp_v2_signals[func.field_name] = func(document)\n\n        minhash_signatures = self._minhash.compute_banded_signatures(tokens=document.normalized_words)\n        # ... (document ID, metadata, CCNet quality signals construction)\n        record_data[\"metadata\"] = metadata\n        record_data[\"quality_signals\"] = {**ccnet_quality_signals, **rp_v2_signals}\n        return record_data, minhash_signatures, doc_id, doc_id_int\n\nFile Path: app/src/dedupe/minhash.py\nContent:\nclass MinHash:\n    # ... (init and properties)\n    def compute_banded_signatures(\n            self, tokens: Tuple[str]\n    ) -> Dict[str, Optional[List[bytes]]]:\n        if len(tokens) < self._ngram_size:\n            return {k: None for k in self._hashranges.keys()}\n        minhashes: np.ndarray = generate_signature(\n            words_sequence=iter(tokens),\n            ngram_size=self._ngram_size,\n            permutations=self._permutations,\n            max_hash=self._max_hash,\n            mersenne_prime=self._mersenne_prime\n        )\n        signatures = {\n            sig_key: [bytes(minhashes[start:end].byteswap().data) for start, end in hashrange]\n            for sig_key, hashrange in self._hashranges.items()\n        }\n        return signatures\n\nFile Path: app/src/bloomfilter.py\nContent:\nclass Deduper:\n    # ... (init and methods)\n    def __parallel_run(self, input_uris):\n        bloomfilter = pybloomfilter.BloomFilter(capacity=self._args.capacity, error_rate=self._args.error_rate)\n        # ... (loop through snapshots and data chunks)\n        for (uri, (read_status, uri_data)) in data_chunks.items():\n            # ... (error handling and logging)\n            for record in uri_data:\n                digest = record[\"digest\"]\n                if bloomfilter.add(digest):\n                    out_writer.update_batch(obj=record)\n                    num_dupes += 1\n                num_docs += 1\n                total_progress.step(1)\n\nFile Path: app/src/run_lsh.py\nContent:\nclass LSH:\n    # ... (init)\n    def __build_edges(self, pa_dset: pa.dataset.Dataset) -> np.ndarray:\n        query = pl.scan_pyarrow_dataset(pa_dset)\n        if self._args.max_docs > 0:\n            query = query.head(self._args.max_docs)\n        query = (\n            query\n            .select(pl.col([\"id_int\", self._sig_key]))\n            .filter(~pl.col(self._sig_key).is_null())\n            .with_columns(pl.Series(name=\"band\", values=[list(range(self._num_bands))], dtype=pl.List(pl.UInt8)))\n            .explode(self._sig_key, \"band\")\n            .group_by(self._sig_key, \"band\")\n            .agg(pl.col(\"id_int\"))\n            .filter(pl.col(\"id_int\").list.lengths() > 1)\n            .select(pl.col(\"id_int\"), pl.col(\"id_int\").list.min().alias(\"min_node\"))\n            .explode(\"id_int\")\n            .filter(pl.col(\"id_int\") != pl.col(\"min_node\"))\n            .select(pl.concat_list([\"id_int\", \"min_node\"]).alias(\"edges\"))\n            .unique(\"edges\")\n        )\n        edges = query.collect(streaming=True).to_numpy().flatten()\n        return edges\n\nFile Path: app/src/token_count.py\nContent:\nTOKENIZER = Tokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\nclass PostProcessor:\n    # ... (init and methods)\n    def _handle_documents(\n            self,\n            client: botocore.client.BaseClient,\n            input_id: str,\n            decoder\n    ) -> InputResult:\n        # ... (download doc and error handling)\n        num_docs = 0\n        total_tokens = 0\n        token_counts = []\n        try:\n            with gzip.open(input_buffer, mode=\"rb\") as in_fh:\n                for idx, obj in enumerate(in_fh):\n                    record = decoder.decode(obj)\n                    num_tokens = len(TOKENIZER.encode(record.raw_content).tokens)\n                    token_counts.append((idx, num_tokens))\n                    total_tokens += num_tokens\n                    num_docs += 1\n        except Exception as e:\n            # ... (error handling)\n            pass\n        return InputResult(is_success=True, msg=\"\", input_id=input_id, num_docs=num_docs, num_tokens=total_tokens, token_counts=token_counts)\n",
    "experimental_info": "The provided repository content describes the data preparation pipeline for pre-training large language models, rather than the LM training or evaluation code itself. The experimental settings for data preparation are as follows:\n-   **Data Sources:** Raw text data is acquired from multiple datasets:\n    -   RedPajama Books (from HuggingFace `togethercomputer/RedPajama-Data-1T`, name=\"book\"). Books are chunked into samples with a maximum of 200 paragraphs per sample and 500 samples per book.\n    -   OpenWebText (from HuggingFace `openwebtext`).\n    -   Wikipedia (from HuggingFace `wikipedia`, attempting `20220301.{lang}` or falling back to `20230801` date for specified language).\n    -   CCNet (Common Crawl) data is processed by language, partitioned by snapshot ID, and split into 'head', 'middle', and 'tail' segments (10%, 20%, 70% of samples respectively) to avoid bias from distribution shifts.\n-   **Languages Supported:** English (`\"en\"`) is a primary focus, but the pipeline is designed to support multiple languages (e.g., \"es\", \"de\", \"fr\", \"it\" are explicitly referenced in stop word lists).\n-   **Tokenization for Counting:** The `token_count.py` script utilizes the `mistralai/Mistral-7B-v0.1` tokenizer for tokenizing raw content and counting tokens.\n-   **Data Quality Signals:** A comprehensive suite of quality signals is applied to documents:\n    -   **ML Classifiers:** FastText models are used to predict quality scores for documents, including models trained on Wikipedia references, a \"Palm\" model (trained on CCNet vs. Books, OpenWebText, Wikipedia), and a Wikipedia article classifier.\n    -   **Content-based Signals:** Include counting occurrences of words from the List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words (LDNOOBW), \"lorem ipsum\" phrases, curly bracket characters, and categorizing domains based on the UT1 URL blacklist. It also computes the fraction of stop words.\n    -   **Natural Language Signals:** Quantify features like the number of sentences, word count, mean word length, symbol-to-word ratio, fraction of lines ending with an ellipsis, fraction of words with no alphabetical characters, fraction of unique words, unigram entropy, and fraction of all-caps words.\n    -   **Repetition Signals:** Measure the fraction of characters in the top 2-gram, 3-gram, and 4-gram, as well as the fraction of characters in duplicate 5-gram to 10-gram sequences (ensuring non-overlapping counts).\n    -   **Importance Weights (DSIR):** Compute the log ratio of the likelihood of document features (using 1-gram and 2-gram hash features with 10,000 buckets by default) with respect to target domains (Wikipedia, Books, OpenWebText) versus a source domain (CCNet), with an optional Poisson length correction.\n-   **Deduplication Settings:**\n    -   **Exact Deduplication:** Performed using a Bloom Filter with configurable capacity and an error rate (defaulting to 0.01). It processes documents by `sha1` digest, extracted from `ccnet` data, for exact matches within snapshots.\n    -   **Near Deduplication (MinHash LSH):** Uses MinHash signatures with configurable `ngram_size`, `num_permutations`, and `similarity_thresholds` (e.g., 0.8) to identify near-duplicate documents through Locality Sensitive Hashing (LSH). The `optimal_param` utility function is used to determine the number of bands and rows for LSH from the similarity threshold and number of permutations.\n-   **Parallel Processing:** The overall pipeline (`pipeline.py`) orchestrates workers in parallel using `multiprocessing` to process input listings in chunks, with configurable maximum processes and inputs per process."
}
