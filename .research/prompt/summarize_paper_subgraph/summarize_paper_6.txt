
Input:

You are an expert research assistant responsible for summarizing a research paper that will serve as the foundation (Research A) for further exploration and integration.

Your task is to generate a structured summary of the given research paper with a focus on:
- **Technical Contributions**: Identify the main research problem and key findings.
- **Methodology**: Describe the techniques, models, or algorithms used.
- **Experimental Setup**: Outline the datasets, benchmarks, and validation methods.
- **Limitations**: Highlight any weaknesses, constraints, or assumptions.
- **Future Research Directions**: Suggest possible extensions or new areas for research.

Below is the full text of the research paper:

```
Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation Xiang Lin 1 Simeng Han 1 Shaﬁq Joty 1 2 Abstract Advanced large-scale neural language models have led to signiﬁcant success in many language generation tasks. However, the most commonly used training objective, Maximum Likelihood Es- timation (MLE), has been shown problematic, where the trained model prefers using dull and repetitive phrases. In this work, we introduce ScaleGrad, a modiﬁcation straight to the gradient of the loss function, to remedy the degeneration issue of the standard MLE objective. By directly maneuvering the gradient information, ScaleGrad makes the model learn to use novel tokens. Empir- ical results show the effectiveness of our method not only in open-ended generation, but also in directed generation tasks. With the simplicity in architecture, our method can serve as a general training objective that is applicable to most of the neural text generation tasks. 1. Introduction Text generation has been one of the most important research problems in natural language processing (NLP). Thanks to the advances in neural architectures, models are now capa- ble of generating texts that are of better quality than before (Brown et al., 2020). However, despite the countless ef- forts that have been made to improve neural architectures, models trained with the standard Maximum Likelihood Es- timation (MLE) objective are known to prefer generating dull and highly repetitive texts. For instance, in open-ended generation tasks, such as story continuation or open dia- logue generation, it has been observed that even with large pre-trained models like GPT-2 (Radford et al., 2019), high frequency tokens largely dominate the generation (Welleck et al., 2020; Holtzman et al., 2020). Similar observation has been reported in directed generationtasks such as summa- 1Nanyang Technological University, Singapore 2Salesforce Research Asia, Singapore. Correspondence to: Xiang Lin <linx0057@e.ntu.edu.sg>. Proceedings of the38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s). rization (See et al., 2017), image captioning (Melas-Kyriazi et al., 2018; Wang & Chan, 2019) and machine translation (Tu et al., 2016; Stahlberg & Byrne, 2019). The methods proposed to solve the degeneration issues with neural text generation can be primarily categorized into two groups: (i) training based methods, which include incorpo- rating auxiliary losses (See et al., 2017; Welleck et al., 2020; Li et al., 2020) and coverage vector (See et al., 2017; Tu et al., 2016); (ii) decoding based methods, such as stochastic beam search (Kool et al., 2019), top-ksampling (Fan et al., 2018), nucleus or top-psampling (Holtzman et al., 2020), and inverse probability weighting (Zhang et al., 2021). Though decoding based methods, in particular nucleus and top-k sampling, perform well in practice in open-ended generation tasks, signiﬁcantly reducing the degeneration problem, they do not address the fundamental modeling is- sue that the token-level probabilities produced by the neural model are problematic (Welleck et al., 2020). In addition, our experiments demonstrate that sampling methods also fail to generate high-quality texts in directed generation tasks such as abstractive text summarization. In this work, based on the known observation that the text generation models trained with MLE objective tend to gener- ate repetitive tokens or phrases, we introduce a novel method called ScaleGrad for neural text generation training, by di- rectly maneuvering the gradients to make the model learn to use novel tokens during training. Our method lies in the training based group, which aims to address the fundamen- tal modeling problem, that is, the token-level distribution predicted by the generation model. In a concurrent work, Wang et al. (2020) introduce a tem- perature scaling approach called Contextual Temperature to improve general language modeling. In this approach, the temperature value in the softmax function is parameterized by a neural network that is jointly trained with the main model. Though the objective of their work is not explicitly related to text degeneration, their analysis shows tempera- ture scaling essentially changes the gradient updates that each token receives during training, which further motivates our work. We conduct experiments with different neural architectures arXiv:2106.07207v1  [cs.CL]  14 Jun 2021Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation including LSTM (Hochreiter & Schmidhuber, 1997) and Transformer (Vaswani et al., 2017) across different tasks in opened-ended and directed text generation. Through extensive analysis we demonstrate that ScaleGrad consis- tently improves the generation quality according to both human evaluation and automatic metrics. Compared to other training based methods, ScaleGrad is architecturally simpler and easier to ﬁt into current neural models ( §3.2), while possessing a wide applicability to different text gener- ation tasks (§4.2 and §5.2). The source code is available at https://github.com/shawnlimn/ScaleGrad. 2. Background 2.1. Neural text generation The NLP tasks involving text generation can be broadly categorized into two types: directed generationand open- ended generation(Holtzman et al., 2020). In the former case, the output text can be seen as a constrained transformation of the input. Examples include text summarization, machine translation, and image captioning. In the latter case, the input context only provides a certain degree of constraints such that the model is allowed to generate the following texts with a considerable degree of freedom. Story/text continuation and dialogue generation fall in this category. Neural models frame text generation tasks as some form of conditional language modeling, which is typically trained to maximize the log likelihood (equivalently, minimize the negative log likelihood) of the training data. The Maximum Likelihood Estimationor MLE objective for an input-output pair (x,y) can be expressed as follows. LMLE = − T∑ t=1 log Pθ(yt|y<t,x) (1) where θdenotes model parameters, T is the length of the output sequence y, and x is the task-speciﬁc input condition, e.g., source document in summarization, image in image captioning, conversation history in dialogue generation and ∅in text continuation. Teacher Forcing(Williams & Zipser, 1989), where current step’s target token is passed as the next input to the decoder rather than the predicted token, is usually used to train the models for faster convergence. Degeneration Degeneration has been a key problem in neural text generation models for open-ended tasks, where the model generates texts that are repetitive, overly generic (dull), incoherent and gibberish. It can happen at different levels of granularity – token, phrase, sentence and paragraph. The problem has not been mitigated even with large-scale pre-trained models like GPT-2 Large (Radford et al., 2019; Holtzman et al., 2020). Degeneration has also been observed in directed generation tasks even though the output in these tasks is conﬁned by the input. For instance, in text summa- rization, most of the advanced models such as BertSum (Liu & Lapata, 2019), BART (Lewis et al., 2020) and ProphetNet (Qi et al., 2020) make use of tri-gram blocking (Paulus et al., 2018) within beam search to remove duplicate trigrams during decoding, which improves the generation quality in terms of the automatic metric. This implies that even with involvement of large-scale pre-trained models, degeneration still exists. Similar issues have been reported in machine translation (Koehn & Knowles, 2017; Stahlberg & Byrne, 2019), image-description generation (Melas-Kyriazi et al., 2018; Wang & Chan, 2019) and next utterance generation in conversations (Jiang et al., 2020). 2.2. Combating neural text degeneration Out of the methods proposed to tackle neural text degenera- tion, top-ksampling (Fan et al., 2018) and nucleus sampling (Holtzman et al., 2020) stand out as representatives of de- coding based methods and unlikelihood training (Welleck et al., 2020) as a representative training based method. Dur- ing each decoding step, nucleus and top- k sampling use different functions to ﬁlter the candidate tokens, thus refor- malizing the probability distribution. Then they sample the next token from the new distribution instead of maximiz- ing the actual likelihood. Randomness brought by these sampling methods reduces duplicate tokens in the output. However, decoding strategy solely does not solve the under- lying modeling problem with MLE training, as pointed out by Welleck et al. (2020). Our analysis in §5.2 also reveals that sampling methods fail to generate high-quality texts in directed generation tasks. To address the issue with MLE, Welleck et al. (2020) pro- pose the neural unlikelihood (UL) training method. During training, at each decoding step t, UL adds an auxiliary loss to the original cross entropy loss as follows. Lt UL = −log Pθ(yt|y<t)   MLE −α ∑ c∈Ct log(1 −Pθ(c|y<t))   UL (2) where α is a hyper-parameter and Ct is the set of nega- tive tokens at decoding step t, which is constructed by previous context tokens that are not the current token, Ct = {y1,...,y t−1}\yt. The auxiliary UL loss decreases the total loss based on the “unlikely” probabilities of nega- tive tokens, thus implicitly reducing the probability assigned to the repetitive tokens. UL training targets at improving the underlying modeling problem, which accords with our goal. Therefore, we mainly compare our method with UL training.1 We discuss how our method is different from UL training from the gradient perspective in §3.3. 1Welleck et al. (2020) also propose a sequence-level UL. Since our work focuses on token-level modeling, we compare with their token-level UL training in this work.Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation 3. Methodology: learning to use novel tokens Training a text generation model with MLE objective treats each token in the gold (ground truth) sequence equally. It has been shown that with this approach, the model exhibits the tendency to generate repetitive tokens/phrases during inference (Welleck et al., 2020; Holtzman et al., 2020). To mitigate this degeneration problem, we argue that the model should focus more on learning to use novel tokens, rather than treating all the tokens in a sequence equally. Our main idea is to maintain a dynamic list of novel tokens at each decoding step during training and encourage the model to learn to use tokens from this list for generation. Formally, let y = (y1,...,y t,...,y T) be the ground-truth (target) token sequence that the model is learning to gener- ate in an auto-regressive manner, one token at a time. At time step t, we deﬁne the token ˜yt i in the vocabulary V as a novel token, if ˜yt i has not been generated before, i.e., ˜yt i /∈ {y1,...,y t−1}. By the deﬁnition, we have a dynamic set of novel tokens St novel ⊆V at each decoding step tin train- ing, which shrinks over time as new tokens are observed in the ground-truth sequence (see Appendix B for an illus- tration). Note that the set of non-novel tokens at each step (i.e., V\St novel) is equivalent to the set of negative tokensCt in UL (Eq. 2) except that it may contain the current target token yt, if it was observed before. To encourage the model to focus on learning to use novel tokens, we propose an architecturally-simple yet effective method that can ﬁt into most of the auto-regressive generation models. Our method, requiring no carefully-designed components, is derived di- rectly from the gradient analysis of the loss function. 3.1. Gradient analysis for MLE training Let us ﬁrst consider the gradient analysis of the model trained with MLE. Let ot ∈R|V|denote the pre-softmax scores ( i.e., logits) over the vocabulary at time step t, where ot i is the score for the token with index i. Simi- larly, let pt k = [softmax(ot)]k represent the probability of the ground truth token with index kin the vocabulary. The partial derivative of the MLE objective (Eq. 1) at time stept with respect to the logit ot i can be shown as (omitting tand ‘MLE’ subscript for simplicity): ∇oi L= ∂L ∂pk ·∂pk ∂oi = pi −1 (i= k) (3) where pi = [softmax(o)]i and 1 (·) is the Indicator function (derivation is given in Appendix A). Speciﬁcally, the gradi- ent of the lossw.r.t.the ground truth token logitok is (pk−1) and for any other token logit oi is pi. As the gradient-based optimization proceeds, the gradient converges to ϵ, a num- ber that is close enough to 0. Another interpretation is that the gradient of the loss is supposed to be close to 0 around a (local) minimum. Therefore, to reach the minimum, or to make the gradient close to 0, the model would try to increase the probability of ground truth token pk and reduce the probability of non-ground truth token pi in the MLE training. From Eq. 3, it is clear that the gradient that every tokenoi ∈ V receives is directly related to its generation probability pi. Therefore, we hypothesize that directly manipulating the generation probabilities of tokens, thereby controlling their gradients, can help us achieve our goal, which is to train the model so that it is encouraged to use novel tokens. 3.2. Our method: ScaleGrad To encourage the model to learn to use novel tokens for generation, we can control the gradient to force the model to either increase the probability of novel tokens or decrease the probability for non-novel tokens. Based on this basic idea, we propose an effective training method keeping it in the simplest form. Speciﬁcally, at each decoding step of training, we re-normalize the softmax output (the probabil- ity distribution over the vocabulary) in a way such that the model is informed of the current set of novel tokens and encouraged to use them. Assuming that pt is the softmax output at step tand St novel is the corresponding set of novel tokens at that step, we re-compute the probability distribu- tion as follows (again omitting tfor notational simplicity): ˜pi =    γ·pi ∑|Snovel| j=1 γ·pj + ∑|V′| j=1 pj , if i∈Snovel pi ∑|Snovel| j=1 γ·pj + ∑|V′| j=1 pj , otherwise (4) where V′= V \St novel is the non-novel tokens set at step t and γ ∈(0,1) is the only hyper-parameter in our method that controls to what degree we want to encourage the model to focus on novel tokens; a smaller value of γincurs more aggressive push for using novel tokens. The effect of the above change is that we directly re-scale the generation probability (after re-normalization) of the tokens. For i ∈Snovel, the effective probability becomes ˜pi = λi·pi with λi ∈(0,1), and for i /∈Snovel, the effective probability becomes ˜pi = αi·pi with αi >1.2 Since λi·pi and αi ·pi are new re-normalized probabilities, they both are naturally bounded in [0,1]. Consequently, assuming that the ground truth token is indexed at k, the modiﬁed loss function at step tfor our proposed method becomes: LSG = − |V|∑ i=1 1 (i= k) [ 1 (i∈Snovel) log(λi ·pi) + 1 (i /∈Snovel) log(αi ·pi) ] (5) 2Note that λi and αi are functions of pi rather than constant numbers. E.g., λi = γ/(∑|Snovel| j=1 γ·pj + ∑|V′| j=1 pj).Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation Figure 1: Illustration of the gradient norm for ScaleGrad and MLE. T-N denotes the Target (ground truth) - Novel token, T-NN denotes the Target - Non-Novel token, NT-N denotes the Non-Target - Novel token and NT-NN denotes the Non-Target - Non-Novel token. The gradient for each token has been changed to3: ∇oi L= ˜pi −1 (i= k) =    λi ·pi −1, if i= kand i∈Snovel αi ·pi −1, if i= kand i /∈Snovel λi ·pi, if i̸= kand i∈Snovel αi ·pi, if i̸= kand i /∈Snovel (6) We now discuss why this re-scaling encourages the model to use novel tokens. As mentioned, during training with each gradient update the model tries to decrease the gradient norm to 0 to reach a local minimum. First, for a ground truth token (i.e., i = k), if it is also a novel token, the gradient norm |λi·pi−1|is pushed away from0 so that the model has to learn to increase the probability pi further to reduce the gradient norm; if it is not a novel token,|αi·pi−1|is pushed slightly closer to 0, which still makes the model learn to predict the ground truth but with a relatively lower strength. For non-ground truth tokens (i.e., i ̸= k), when it is not a novel token, |αi ·pi|increases the gradient norm so that the model learns to assign much lower probability pi to reduce it. Similarly, when the token is novel but not a ground truth token, the resulting gradient norm |λi ·pi|becomes smaller, for which the model only moderately learns to decrease the probability pi to reduce the norm further. To give more insights, Figure 1 plots the gradient norm for a toy example with two tokens, one of which is a novel token (i.e., |V|= 2, |Snovel|= 1). The dash lines represent the gradient information for MLE training, i.e., |pi−1 (i= k)|. We can see how ScaleGrad scales the gradient dynamically 3Derivation is given in Appendix A. for different types of tokens. For instance, for a target token belonging to the novel token set (T-N), its gradient norm has been scaled to a larger value compared to MLE, rendering that the model needs to learn to assign even higher probabil- ity to the target token to decrease the gradient. Similarly, for a non-target token that is not a novel token (NT-NN ), the scaled gradient makes the model assign even lower probabil- ity to the token in order to decrease the gradient. Moreover, the monotonic relation between the probability and the gra- dient norm guarantees that the model still learns to predict target tokens and reject non-target tokens, but in more dy- namic degrees of strength. 3.3. Comparison with unlikelihood training We now analyze UL from the perspective of its gradients and compare with ours. The gradient of the UL loss (Eq. 2) with a single negative token (i.e., |Ct|= 1) is: ∇oi L= mi ·pi −1 (i= k) =    (1 −α pneg 1 −pneg )pi −1, if i= k (1 −α pneg 1 −pneg )pi, if i̸= kand i̸= ineg (1 +α)pi, if i̸= kand i= ineg (7) where pi = [softmax(o)]i, pneg is the probability of the negative-candidate token with index ineg, and 1 (i = k) is the indicator function with kbeing the index of the ground truth or target token (see the original paper for derivation). We know that as the gradient-based optimization progresses, the gradient norm decreases and converges to near 0 (§3.1- §3.2). To generate a ground truth token, the model must learn to assign the highest probability to it. In other words, the probability assigned by the model to a ground truth token (i.e., pk) should always increase as the training pro- gresses, or equivalently the norm |∇ok L|should decrease (monotonic relation). If this is not the case, the model may not learn to predict the ground truth tokens correctly, which in turn hurts the generation quality. Based on the gradients (Eq. 7), we can identify one case where UL may provide such undesired property. Since the ground truth is always by deﬁnition a non-negative token in UL (i.e., i = k ̸= ineg), the gradient norm from Eq. 7 is |∇ok L| = |µk ·pk −1|where µk = (1 −α pneg 1−pneg ). We see that when pneg > 1 α+1 (e.g., when α = 1 and pneg >0.5), µk becomes negative, having the gradient norm |∇ok L|= ⏐⏐−|µk|·pk −1 ⏐⏐= |µk|·pk + 1. In this case, the training procedure will decrease pk to reduce |∇ok L|, which contradicts with the optimization principle. Thus, UL may become less effective in such special cases (subject to the choice of the value of αand pneg). Appendix C further clariﬁes this issue using the same notation as the original pa- per (Welleck et al., 2020). In contrast, the gradient analysisStraight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation Table 1: Results for open-ended generation tasks on the Wikitext-103 testset. ppl, uniq and Rep/l are computed at BPE-level and the rest are at word-level. The “ ↑” denotes higher value for better performance and “ ↓” is the opposite. Number marked with * are estimated based on the testset. The results are averaged over 3 runs with different random seeds. Full results with standard deviation are reported in Appendix F.1. Language Modeling Auto Completion Models ppl ↓ uniq↑ Rep/16↓ Rep/32↓ Rep/128↓ Rep-1↓ Rep-2↓ Rep-3↓ uniq-w↑ MLE 13.241 12.54k 0.234 0.380 0.619 0.661 0.500 0.424 16.83k UL (α= 1.0) 16.062 13.18k 0.212 0.341 0.558 0.559 0.363 0.291 19.11k SG (γ= 0.2) 14.203 13.61k 0.197 0.317 0.522 0.443 0.215 0.143 22.25k Human - 18.27k 0.177 0.285 0.480 0.382* 0.096* 0.037* 27.55k* in Eq. 6 shows that ScaleGrad does not have such properties in learning to predict ground truth tokens. 4. Experiments We showcase the performance of ScaleGrad in both open- ended and directed generation tasks. To verify the effec- tiveness of our approach, for all the experiments below, we use exactly the same hyper-parameters (except for method- speciﬁc ones) and setup as the corresponding baseline un- less stated otherwise. All the experimental details, such as model hyper-parameters, training and dataset settings regarding the reproducibility can be found in Appendix G. For qualitative assessments, we show examples of gener- ated texts in Table 6 and more in Appendix K. For both open-ended and directed generation tasks, in order to model different regularization strengths imposed by the methods, we choose α∈{0.5,1.0,1.5}for unlikelihood training and γ ∈{0.2,0.5,0.8}for ScaleGrad.4 The ﬁnal models are chosen based on their performance on the corresponding development sets. 4.1. Open-ended generation Setup We consider language modeling and text auto- completion, where we compare the performance of the model trained with ScaleGrad against the models trained with MLE and unlikelihood (UL) training introduced lately to mitigate degeneration in open-ended tasks. We follow the same setup as (Welleck et al., 2020). Speciﬁcally, we ﬁne-tune the pre-trained GPT-2 (Radford et al., 2019) on Wikitext-103 (Merity et al., 2017) with a maximum se- quence length of 300 tokens. Each model is trained for a maximum of 35k iterations and evaluated based on the perplexity on the validation set after every 1k iterations. We report language modeling results on the testset for each model selected according to the perplexity on the validation 4α= 1.0 is recommended by Welleck et al. (2020), which can be seen as applying unlikelihood loss with a moderate strength. We use α= 0.5 and 1.5 to evaluate for weak and strong strengths. set. The same saved models are also used for text auto- completion, where 50 BPE (Sennrich et al., 2016) tokens (from testset) are given as preﬁx and the models are to gen- erate the continuation of 100 next tokens. To evaluate the modeling capability exclusively, following Welleck et al. (2020), we apply greedy decoding in all our experiments in this section. Later, in §5.1, we analyze how our method performs with different decoding methods. In language modeling, we measure the generation quality by the standard perplexity (ppl), and Rep/land ‘uniq’ measures of token-level distribution as (Welleck et al., 2020). Rep/l measures the number of times that a token from the previous ltokens is repeated, when generating the following token; in our case, l∈{16,32,128}. The ‘uniq’ is deﬁned as the number of unique next-token predictions on a test/validation set. For auto-completion, we report the repetition ratios of n- gram words (Rep-n) as well as the number of unique words (uniq-w) that are used during generation on the testset. Results We present our main results on the testset in Ta- ble 1. The results with different hyper-parameters for both methods on the validation set are reported in Appendix F.1 and in §5.3. From Table 1, we notice that in language mod- eling, the model trained with ScaleGrad (SG) yields a token distribution that is much closer to human, while maintain- ing a lower perplexity. Compared to the UL baseline, SG achieves 1%, 2%, 4% lower repetitions in Rep/16, Rep/32 and Rep/128, respectively, while having 11% lower perplex- ity. It also uses more unique tokens compared to others (e.g., 3% more compared to UL training). Overall, our method signiﬁcantly improves the token-level distribution and keeps a high generation quality. In auto-completion, from the quantitative perspective, SG produces texts with much fewer repetitive n-grams compared to MLE and UL. It uses nearly 5.5k more unique words compared to the MLE baseline, which agrees with the purpose of making the model learn to use novel tokens in training. Human evaluation We have conducted a user study to verify the quality of generated texts. The study is conductedStraight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation Table 2: Results for open-ended generations on PTB testset. ppl, uniq and Rep/l are computed at BPE-level and the rest are at word-level. The “↑” denotes higher value for better performance and “↓” is the opposite. Numbers marked with * are estimated based on the testset. Language Modeling Auto Completion Models ppl ↓ uniq↑ Rep/16↓ Rep/32↓ Rep/128↓ Rep-1↓ Rep-2↓ Rep-3↓ uniq-w↑ PTB MLE 33.952 5.60k 0.157 0.292 0.530 0.652 0.493 0.424 6.46k UL (α= 1.0) 41.232 5.96k 0.139 0.260 0.476 0.533 0.333 0.259 7.60k SG (γ= 0.2) 40.731 6.15k 0.126 0.231 0.426 0.417 0.198 0.131 8.42k Human - 8.84k 0.118 0.222 0.421 0.362* 0.089* 0.033* 11.32k* Table 3: Human evaluation results for auto-completion. % Agr. is the percentage agreement and AC1 denotes Gwet’s AC1/gamma coefﬁcient. Winners are marked in bold. Win Rate % Agr. AC1 SG vs MLE 84.0% 84.0% 0.78 SG vs UL 70.5% 79.0% 0.64 for two pairs of systems (SG vs. UL, SG vs. MLE). For each pair, we randomly choose the same 100 preﬁxes for the systems to produce their own continuations and ask two native speakers of English to judge which text is the better continuation of the given preﬁx in terms of their relevance to the preﬁx, grammaticality and readability. More details about the study can be found in Appendix D. From the results in Table 3, we can observe that the texts produced by the models trained with ScaleGrad (SG) are preferred by the human users in most of the cases,i.e., 84.0% and 70.5%, respectively. We also compute the percentage agreement and chance-correlated Gwet’s AC1/gamma co- efﬁcient (Gwet, 2008) as inter-user agreement to verify the reliability of the study (details in Appendix D). We see that the agreements are substantial in both measures. Generalizability To further verify the generalizability (i.e., different datasets and domains) of our method, apart from WikiText-103 (Merity et al., 2017), we evaluate the models on two other datasets: Penn TreeBank or PTB (Mar- cus et al., 1993) and IMDB (Maas et al., 2011). In particu- lar, after ﬁne-tuning GPT-2 with different training strategies (MLE, SG and UL) on WikiText-103 training data, we test the language modeling and auto-completion performance on the other two datasets. For PTB, we use the standard testset. As for IMDB, we randomly sample 500 movie reviews from the dataset. In Table 2, we show the experimental results on the PTB test- set, from which we can see that SG consistently improves over the MLE baseline in degeneration while possessing an acceptable increase in perplexity, and it outperforms UL consistently. Additionally, we present the results on IMDB movies review in Table 12 in Appendix F.2, where we ob- serve similar performance trending as in the experiment on PTB testset. From the two experiments, we can draw the conclusion that our method, SG, is capable of gener- alizing well to different datasets and domains. Examples of generated text for auto completion task can be found in Appendix K. 4.2. Directed generation For directed generation, we consider two tasks: image para- graph captioning and text summarization. Table 4: Results for image paragraph captioning on the Visual Genome testset. Models CIDEr MLE w/o 3-block 10.51 UL w/o 3-block (α=0.5) 14.65 SG w/o 3-block (γ=0.5) 19.42 MLE w/ 3-block 22.77 UL w/ 3-block (α=0.5) 22.25 SG w/ 3-block (γ=0.5) 24.62 4.2.1. I MAGE PARAGRAPH CAPTIONING Setup We use the captioning model proposed by Melas- Kyriazi et al. (2018) as the baseline, which comprises a CNN encoder that is pre-trained for object detection and a 1-layer LSTM decoder. The models are trained and evaluated on the paragraph captioning dataset, Visual Genome (Krause et al., 2017). We train the model with SG and compare it to the ones trained with MLE and UL. The performance is measured by CIDEr (Vedantam et al., 2015), which com- putes TF-IDF weighted n-gram overlaps between the model generated captions and the reference captions. We follow Melas-Kyriazi et al. (2018) to apply greedy inference since beam search did not yield any further gain.Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation Results Table 4 shows the CIDEr scores for different train- ing methods on Visual Genome testset with and without tri-gram blocking (Paulus et al., 2018) during inference. Without tri-gram blocking, MLE produces texts that are full of repetitive phrases (see Appendix K for examples), which leads to a low CIDEr score. When UL or SG is incorporated, the performance has been notably improved from 10.51 to 14.65 and 19.42, respectively. When tri-gram blocking is applied, our method is still capable of yielding 1.85 point improvement. This is because SG further improves the token-level degeneration on top of tri-gram blocking. In contrast, the model trained with UL has a slightly worse CIDEr score compared to the MLE baseline. We analyze n-gram level degeneration further in §5.2. Table 5: Experimental results for text summarization on CNN/DM and NYT50 testsets. R-1, R-2 and R-L stand for F1-based ROUGE-1, ROUGE-2 and ROUGE-L.WMD-1 denotes 1-gram MoverScore. Models R-1 R-2 R-L WMD-1 CNN/DM BertSum w/ MLE 41.87 19.42 38.93 19.89 BertSum w/ UL (α= 0.5) 42.03 19.36 39.09 20.21 BertSum w/ SG (γ= 0.8) 42.19 19.53 39.25 20.23 NYT50 BertSum w/ MLE 48.73 31.00 45.23 28.73 BertSum w/ UL (α= 0.5) 48.54 30.73 44.99 28.50 BertSum w/ SG (γ= 0.8) 49.29 31.30 45.78 29.14 4.2.2. A BSTRACTIVE TEXT SUMMARIZATION Setup We use the abstractive summarization model Bert- Sum (Liu & Lapata, 2019) as our baseline, which adopts a Transformer architecture to take advantage of pre-trained BERT (Devlin et al., 2019) as the encoder. At the ﬁrst stage, the encoder is trained with an extractive summarization objective (binary classiﬁcation for sentence selection). At the second stage, it initializes the decoder randomly and (re)trains the entire encoder-decoder model with an abstrac- tive (or generative) objective. For our experiments, we take the encoder that was trained at the ﬁrst stage and train the entire (abstractive) model with different training methods (MLE, UL and SG) using the default training setup on two benchmark datasets: CNN/DM (Hermann et al., 2015; Nal- lapati et al., 2016) and NYT50 (Durrett et al., 2016). During inference, length normalization (Wu et al., 2016), tri-gram blocking and beam search (beam size = 5) are used as in (Liu & Lapata, 2019). We evaluate performance of the models with the standard F1-based ROUGE (Lin, 2004) scores (R-1, R-2, R-L) and a model-based evaluation MoverScore (Zhao et al., 2019), which computes the Word Mover Distance (WMD) between the reference summary and generated summary based on the representations from BERT. We report 1-gram MoverScore (WMD-1), which has been proven to have higher correlation with human than other metrics (Zhao et al., 2019). Results From Table 5, we notice that on CNN/DM, the model trained with SG outperforms the models trained with MLE and UL when measured by ROUGE. In WMD-1, UL yields similar performance as ours. Both SG and UL further improve over the MLE baseline. The improvements imply that token-level degeneration may still exist even when tri- gram blocking is applied. On NYT-50, UL underperforms MLE, while our method improves in all measures. In §3.3, we discussed a possible reason behind UL’s underperfor- mance from a gradient perspective. 5. Analysis of ScaleGrad After comparing with UL and MLE on both directed and open-ended generation tasks, we now analyze ScaleGrad from different perspectives to gain more insights. 5.1. Open-ended generation Compatibility with decoding strategies One advantage of SG training is that it is compatible with decoding-based methods. One can choose different decoding strategies based on the speciﬁc needs. Table 7 provides the results of different decoding strategies used along with SG training for text auto-completion (results for other variations and baselines are in Appendix H). We observe that beam search, even with larger beam size, is not effective in mitigating the degeneration issue, which accords with the observation in (Holtzman et al., 2020). As expected, stochastic decoding, top-kand nucleus (top-p) sampling, help to further reduce repetition. This sets good examples of combining training and decoding strategies for the task in hand. Auto completion with different decoding lengths From a practice point of view, we analyze how SG performs in text generation with varied decoding lengths. In Figure 2, we show the Rep-1 of generated text from the auto completion task with the constraints in different decoding (continuation) lengths. We see that compared to MLE counterpart, SG yields consistent improvements on Rep-1, or token-level de- generation, regardless the different decoding lengths, which further veriﬁes the effectiveness and generalizability of our method. 5.2. Directed generation Comparison with stochastic decoding Although top-p and top-ksampling have been proven successful in open- ended generation, they have not been tested in directed gen- eration tasks. In order to see if they could lead to the same improvements as SG training, we conduct additional exper-Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation Table 6: An example of generated continuations for the same preﬁx. Preﬁx: You’s patriotism, and Mei’s reﬂections on the quotidian are a few examples. More broadly, Du Fu’s work in transforming the lushi from mere word play into “a vehicle for serious poetic utterance” MLE: is a major contribution to the genre. The ﬁrst of the three major works of the genre, was written in the early 1920s, and was the ﬁrst of the three to be published in English. It was published in the United States in the United States, and was translated into French in the 1930s. The work was the ﬁrst of the three UL: is a major inﬂuence on the modern Chinese novel. The modern Chinese novel is often characterized as being “literary in nature”, but it is not limited to literary genres. The modern Chinese novel has been described as “a literary genre that has been inﬂuenced by the Western literary tradition”. The modern Chinese novel has been described as “a literary genre that has been inﬂuenced by the Western literary tradition” SG: is a powerful example of how the Chinese literary tradition can be used to explore themes of nationalism and self determination. In addition to his own poetry, Du Fu wrote several works of prose. The ﬁrst was a collection of short stories called “The Story of the Three Sisters”, which was published in 1885. It was followed by a series of novels, including the ﬁrst two volumes of the Song dynasty epic poem “The Three Sisters” Table 7: Results of different decoding strategies with Scale- Grad training for auto-completion on WikiText-103 testset. Approaches Rep-1 Rep-2 Rep-3 uniq-w SG+Greedy Search 0.443 0.215 0.143 22.25k SG+Beam Search (b= 6) 0.453 0.250 0.171 8.32k SG+Top-p(p= 0.3) 0.356 0.107 0.049 30.48k SG+Top-k(k= 40) 0.254 0.039 0.012 39.50k Figure 2: Rep-1 in auto completion with different decoding lengths. All the numbers are computed based on the results from 3 runs with different random seeds. iments with the BertSum summmarization model, whose underlying language model is more mature due to the in- volvement of BERT, compared to the image paragraph cap- tioning model. For the interested readers, we also provide the results of stochastic decoding on image paragraph cap- tioning in Appendix I. Table 8 shows the performance for stochastic decoding in BertSum trained with MLE. Since ROUGE-1 measures the exact 1-gram overlaps between reference and generated summaries, it may not be sufﬁcient to evaluate the perfor- mance of stochastic decoding methods, which may generate more diverse output while conveying the same meaning. Therefore, we also report the MoverScore that is capable of considering the semantic similarity rather than just n-gram overlaps. Both the ROUGE and MoverScore in Table 8 lead to the conclusion that stochastic decoding methods sig- niﬁcantly lower the performance compared to the standard beam search. This implies that they may not be a good ﬁt for directed generation tasks. In contrast, SG possesses a wider applicability in mitigating degeneration issues as shown earlier in Table 5. Table 8: Summarization results (F1-based ROUGE-1 and MoverScore) for stochastic decoding on NYT50 testset. Models ROUGE-1 WMD-1 Top-p(p=0.3) 45.44 24.61 Top-p(p=0.9) 42.33 21.67 Top-k(k=40) 41.23 20.70 Top-k(k=100) 40.86 20.38 Baseline 48.73 28.73 N-gram degeneration To investigate further how SG minimizes degeneration and helps to improve the perfor- mance in automatic measures, we compute the n-gram repe- tition ratios of the outputs from the image captioning model (Melas-Kyriazi et al., 2018) and report the numbers in Ta- ble 9.5 Compared to human, the MLE baseline has signif- icantly higher repetitions, thus having the lowest CIDEr score (Table 4). With SG, the model yields a much better repetition ratio, which explains the notable performance boost in CIDEr. Tri-gram blocking resolves the issue of 3- or higher n-gram degeneration in a hard-coded way, improv- ing CIDEr signiﬁcantly. However, the token and 2-gram repetitions still remain high and improvable in MLE with tri-gram blocking. When both tri-gram blocking and SG 5Since Melas-Kyriazi et al. (2018) used a soft tri-gram blocking, some of the duplicate tri-grams still remain.Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation (a) Perplexity  (b) Rep/l  (c) # of unique tokens Figure 3: Hyper-parameter (γ) sensitivity in the language modeling task on Wikitext-103 development set (best viewed in color). Rep/lis computed as the average of Rep/16, Rep/32 and Rep/128. UL-1.0 and UL-0.5 represent unlikelihood training with α=1.0 and 0.5, respectively. α= 1.5 is not included as it incurs signiﬁcantly higher perplexity compared to others. Individual Rep/lresults can be found in Appendix J. are applied, the generated texts have the lowest and most human-like repetitions. Table 9: Degeneration analysis for image paragraph caption- ing with/without tri-gram blocking. Numbers in bold are closest to human. Models Rep-1 Rep-2 Rep-3 MLE 0.723 0.587 0.530 SG 0.500 0.270 0.195 MLE w/ 3-block 0.575 0.271 0.094 SG w/ 3-block 0.440 0.146 0.037 Human 0.421 0.123 0.042 5.3. Hyper-parameter sensitivity Towards better usage and understanding of ScaleGrad, we show how the key metrics in language modeling change with the hyper-parameter γ in Figure 3.6 As discussed, a smaller value ofγincurs a stronger push to use novel tokens, giving higher perplexity and more unique tokens. Having a low perplexity and a low repetition ratio could be seen as a trade-off between general generation quality and diversity. However, we observe that when UL achieves similar per- formance in Rep/lwith SG, i.e., when γ = 0.5, α = 0.5 and γ = 0.3, α= 1.0 (ﬁg. 3b), it exhibits much higher per- plexity compared to SG with a difference of 1.35 and 2.58, respectively (ﬁg. 3a). Similarly, when both methods have similar performance on perplexity, i.e., when γ = 0.2 and α= 0.5 (ﬁg. 3a), SG yields 3.82% lower in Rep/l(ﬁg. 3b) and uses 1.11k more unique tokens (ﬁg. 3c). In summary, 6Note that for our main results in §4, we only search hyper- parameters from 3 chosen values. More numbers of γin Figure 3 is intended to show the hyper-parameter sensitivity of ScaleGrad. One should not regard this as unfair comparison where different numbers of hyper-parameter are explored for different methods. SG is able to reduce the degeneration without detracting much from the generation quality. In general, γin ScaleGrad can be chosen based on the per- formance of the baseline model. If the baseline produces many repetitive tokens/phrases (e.g., image paragraph cap- tioning experiments), a smaller value of γshould be used. Conversely, in tasks with less degeneration (e.g., summariza- tion experiments), a larger γcan be used to further improve the unigram and bigram level degeneration without affecting the perplexity much. 6. Conclusion We have introduced a novel training method, called Scale- Grad, directly modifying the gradient of the standard MLE objective to remedy the text degeneration issues. The im- provement veriﬁed by both automatic metrics and human evaluation against the baselines in extensive experiments across different tasks in open-ended and directed generation and different architectures ( i.e., LSTM and Transformer) demonstrate the effectiveness and generalizability of our method. Further analysis shows that ScaleGrad yields token distributions that are much closer to human-written texts compared to the baselines. Our method brings a good alter- native to current training strategies for language generation. In future, we plan to extend the idea in two directions. First, we would like to repurpose the deﬁnition of the set of the tokens that ScaleGrad operates on (i.e., the novel token set) to enable the model to realize other objectives, e.g., Zhao et al. (2021) has successfully adapted ScaleGrad to prevent early endpointing for online automatic speech recognition. Second, we would like to investigate a mechanism to dynam- ically adjust the hyper-parameter γ in the decoding steps such that the model could learn with different degrees of strength depending on the context.Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation References Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877–1901. Curran Asso- ciates, Inc., 2020. URL https://proceedings. neurips.cc/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper. pdf. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT: Pre-training of deep bidirectional transformers for lan- guage understanding. In Proceedings of the 2019 Con- ference of the North American Chapter of the Associa- tion for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, June 2019. Asso- ciation for Computational Linguistics. doi: 10.18653/ v1/N19-1423. URL https://www.aclweb.org/ anthology/N19-1423. Durrett, G., Berg-Kirkpatrick, T., and Klein, D. Learning- based single-document summarization with compression and anaphoricity constraints. In Proceedings of the 54th Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers), pp. 1998–2008, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1188. URL https: //www.aclweb.org/anthology/P16-1188. Fan, A., Lewis, M., and Dauphin, Y . Hierarchical neu- ral story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguis- tics (Volume 1: Long Papers), pp. 889–898, Melbourne, Australia, July 2018. Association for Computational Lin- guistics. doi: 10.18653/v1/P18-1082. URL https: //www.aclweb.org/anthology/P18-1082. Gwet, K. L. Computing inter-rater reliability and its vari- ance in the presence of high agreement. British Journal of Mathematical and Statistical Psychology, 61(1):29–48, 2008. Hermann, K. M., Kocisky, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., and Blunsom, P. Teaching ma- chines to read and comprehend. In Cortes, C., Lawrence, N. D., Lee, D. D., Sugiyama, M., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 28, pp. 1693–1701. Curran Associates, Inc., 2015. Hochreiter, S. and Schmidhuber, J. Long short-term mem- ory. Neural Comput., 9(8):1735–1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL https://doi.org/10.1162/neco.1997. 9.8.1735. Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y . The curious case of neural text degeneration. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum? id=rygGQyrFvH. Jiang, S., Wolf, T., Monz, C., and de Rijke, M. Tldr: Token loss dynamic reweighting for reducing repeti- tive utterance generation. arXiv, 2020. URL https: //arxiv.org/abs/2003.11963. Koehn, P. and Knowles, R. Six challenges for neural ma- chine translation. In Proceedings of the First Work- shop on Neural Machine Translation, pp. 28–39, Van- couver, August 2017. Association for Computational Lin- guistics. doi: 10.18653/v1/W17-3204. URL https: //www.aclweb.org/anthology/W17-3204. Kool, W., Van Hoof, H., and Welling, M. Stochastic beams and where to ﬁnd them: The Gumbel-top-k trick for sampling sequences without replacement. In Chaud- huri, K. and Salakhutdinov, R. (eds.), Proceedings of the 36th International Conference on Machine Learn- ing, volume 97 of Proceedings of Machine Learning Re- search, pp. 3499–3508, Long Beach, California, USA, 09– 15 Jun 2019. PMLR. URL http://proceedings. mlr.press/v97/kool19a.html. Krause, J., Johnson, J., Krishna, R., and Fei-Fei, L. A hierarchical approach for generating descriptive image paragraphs. In Computer Vision and Patterm Recognition (CVPR), 2017. Lewis, M., Liu, Y ., Goyal, N., Ghazvininejad, M., Mo- hamed, A., Levy, O., Stoyanov, V ., and Zettlemoyer, L. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and com- prehension. In Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics, pp. 7871–7880, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020. acl-main.703. URL https://www.aclweb.org/ anthology/2020.acl-main.703. Li, M., Roller, S., Kulikov, I., Welleck, S., Boureau, Y .- L., Cho, K., and Weston, J. Don’t say that! mak- ing inconsistent dialogue unlikely with unlikelihood training. In Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics, pp. 4715–4728, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation acl-main.428. URL https://www.aclweb.org/ anthology/2020.acl-main.428. Lin, C.-Y . Rouge: A package for automatic eval- uation of summaries. In Proc. ACL workshop on Text Summarization Branches Out , pp. 10, 2004. URL http://research.microsoft.com/ ˜cyl/download/papers/WAS2004.pdf. Liu, Y . and Lapata, M. Text summarization with pre- trained encoders. In Proceedings of the 2019 Confer- ence on Empirical Methods in Natural Language Pro- cessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 3730–3740, Hong Kong, China, November 2019. As- sociation for Computational Linguistics. doi: 10.18653/ v1/D19-1387. URL https://www.aclweb.org/ anthology/D19-1387. Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y ., and Potts, C. Learning word vectors for sen- timent analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguis- tics: Human Language Technologies, pp. 142–150, Port- land, Oregon, USA, June 2011. Association for Com- putational Linguistics. URL http://www.aclweb. org/anthology/P11-1015. Marcus, M. P., Santorini, B., and Marcinkiewicz, M. A. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2): 313–330, 1993. URL https://www.aclweb.org/ anthology/J93-2004. Melas-Kyriazi, L., Rush, A., and Han, G. Training for diversity in image paragraph captioning. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 757–761, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1084. URL https: //www.aclweb.org/anthology/D18-1084. Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models. In ICLR, 2017. URL https: //openreview.net/pdf?id=Byj72udxe. Nallapati, R., Zhou, B., dos Santos, C., Gu `I‡lc ¸ehre, C ¸., and Xiang, B. Abstractive text summarization using sequence-to-sequence RNNs and beyond. In Proceed- ings of The 20th SIGNLL Conference on Computational Natural Language Learning, pp. 280–290, Berlin, Ger- many, August 2016. Association for Computational Lin- guistics. doi: 10.18653/v1/K16-1028. URL https: //www.aclweb.org/anthology/K16-1028. Paulus, R., Xiong, C., and Socher, R. A deep rein- forced model for abstractive summarization. In ICLR, 2018. URL https://openreview.net/pdf? id=HkAClQgA-. Qi, W., Yan, Y ., Gong, Y ., Liu, D., Duan, N., Chen, J., Zhang, R., and Zhou, M. ProphetNet: Pre- dicting future n-gram for sequence-to-SequencePre- training. In Findings of the Association for Computa- tional Linguistics: EMNLP 2020, pp. 2401–2410, On- line, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.ﬁndings-emnlp.217. URL https://www.aclweb.org/anthology/ 2020.findings-emnlp.217. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners. Open-AI Blog, 2019. See, A., Liu, P. J., and Manning, C. D. Get to the point: Summarization with pointer-generator networks. In Pro- ceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1073–1083, Vancouver, Canada, July 2017. Asso- ciation for Computational Linguistics. doi: 10.18653/ v1/P17-1099. URL https://www.aclweb.org/ anthology/P17-1099. Sennrich, R., Haddow, B., and Birch, A. Neural machine translation of rare words with subword units. In Pro- ceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1715–1725, Berlin, Germany, August 2016. Asso- ciation for Computational Linguistics. doi: 10.18653/ v1/P16-1162. URL https://www.aclweb.org/ anthology/P16-1162. Stahlberg, F. and Byrne, B. On NMT search errors and model errors: Cat got your tongue? In Proceedings of the 2019 Conference on Empirical Methods in Natu- ral Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP- IJCNLP), pp. 3356–3362, Hong Kong, China, Novem- ber 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1331. URL https://www. aclweb.org/anthology/D19-1331. Tu, Z., Lu, Z., Liu, Y ., Liu, X., and Li, H. Model- ing coverage for neural machine translation. In Pro- ceedings of the 54th Annual Meeting of the Associa- tion for Computational Linguistics (Volume 1: Long Pa- pers), pp. 76–85, Berlin, Germany, August 2016. Asso- ciation for Computational Linguistics. doi: 10.18653/ v1/P16-1008. URL https://www.aclweb.org/ anthology/P16-1008. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Atten- tion is all you need. In Guyon, I., Luxburg, U. V ., Bengio,Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation S., Wallach, H., Fergus, R., Vishwanathan, S., and Gar- nett, R. (eds.), Advances in Neural Information Process- ing Systems 30, pp. 5998–6008. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/ 7181-attention-is-all-you-need.pdf . Vedantam, R., Lawrence Zitnick, C., and Parikh, D. Cider: Consensus-based image description evaluation. In Pro- ceedings of the IEEE conference on computer vision and pattern recognition, pp. 4566–4575, 2015. Wang, P.-H., Hsieh, S.-I., Chang, S.-C., Chen, Y .-T., Pan, J.-Y ., Wei, W., and Juan, D.-C. Contextual temperature for language modeling. arXiv, 2020. URL https:// arxiv.org/abs/2012.13575. Wang, Q. and Chan, A. B. Describing like humans: On diversity in image captioning. In 2019 IEEE/CVF Con- ference on Computer Vision and Pattern Recognition (CVPR), pp. 4190–4198, 2019. Welleck, S., Kulikov, I., Roller, S., Dinan, E., Cho, K., and Weston, J. Neural text generation with unlikelihood train- ing. In International Conference on Learning Represen- tations, 2020. URL https://openreview.net/ forum?id=SJeYe0NtvH. Williams, R. J. and Zipser, D. A learning algorithm for con- tinually running fully recurrent neural networks. Neural Computation, 1(2):270–280, 1989. Wu, Y ., Schuster, M., Chen, Z., Le, Q. V ., Norouzi, M., Macherey, W., Krikun, M., Cao, Y ., Gao, Q., Macherey, K., Klingner, J., Shah, A., Johnson, M., Liu, X., Kaiser, L., Gouws, S., Kato, Y ., Kudo, T., Kazawa, H., Stevens, K., Kurian, G., Patil, N., Wang, W., Young, C., Smith, J., Riesa, J., Rudnick, A., Vinyals, O., Corrado, G., Hughes, M., and Dean, J. Google’s neural machine translation system: Bridging the gap between human and machine translation. CoRR, abs/1609.08144, 2016. URL http: //arxiv.org/abs/1609.08144. Zhang, X., Sun, M., Liu, J., and Li, X. Improving diversity of neural text generation via inverse probability weight- ing. arXiv, 2021. URL https://arxiv.org/abs/ 2103.07649. Zhao, W., Peyrard, M., Liu, F., Gao, Y ., Meyer, C. M., and Eger, S. MoverScore: Text generation evaluating with contextualized embeddings and earth mover dis- tance. In Proceedings of the 2019 Conference on Empir- ical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 563–578, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1053. URL https: //www.aclweb.org/anthology/D19-1053. Zhao, Y ., Ni, C., Leung, C.-C., Joty, S., Chng, E. S., and Ma, B. Preventing early endpointing for online auto- matic speech recognition. In ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Sig- nal Processing (ICASSP), pp. 6813–6817, 2021. doi: 10.1109/ICASSP39728.2021.9413613.Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation A. Derivations Derivation of the gradient of loss w.r.t. logit We follow the same notation as in the main paper. At time step t, as- suming that the pre-softmax scores (i.e., logits) are denoted as ot over the vocabulary V, where ot i denotes the score for the token with index iin the vocabulary. Similarly, we have pt i = [softmax(ot)]i. Let kdenote the index of the ground truth token at step t. The cross entropy loss at step tis given as (we omit tfor notational simplicity): L= − ∑ i yilog pi (8) where yi = 1 if i = k, otherwise yi = 0. Thus the loss function can be rewritten as: L= −log pk = −log( eok ∑ jeoj ) = log( ∑ j eoj ) −ok (9) Therefore, we can derive the partial derivative of the loss w.r.t.the logit oi as follows. ∇oi L= ∇oi log( ∑ j eoj ) −∇oi ok = 1∑ jeoj ·∇oi ( ∑ j eoj ) −1 (i= k) = eoi ∑ jeoj −1 (i= k) = pi −1 (i= k) (10) Derivation of the gradient of ScaleGrad w.r.t. logit We ﬁrst denote Snovel as the novel token set at current time step and V′= V \Snovel. Suppose the current target token belongs to the novel token set, i.e., k ∈Snovel. The scaling equation for target token can be rewritten into the function of logits as follows. ˜pk = γ·pk γ∑ j∈Snovel pj + ∑ j∈V′ pj = γ· eok∑ m∈V eom γ∑ j∈Snovel eoj∑ m∈V eom + ∑ j∈V′ eoj∑ m∈V eom = γ·eok γ∑ j∈Snovel eoj + ∑ j∈V′ eoj (11) For the notational simplicity, we notate a = (γ∑ j∈Snovel eoj + ∑ j∈V′ eoj ). The loss function can be rewritten accordingly as: L= −log(˜pk) = −log γ·eok a = loga−log(γ·eok ) (12) We thus have the gradient of the SG loss w.r.t.the logit (oi) as follows: ∇oi L= ∇oi log a−∇oi log(γ·eok ) = 1 a ·∇oi a− 1 γ·eok ·∇oi (γ·eok ) = 1 a ·(γ·eoi 1 (i∈Snovel) +eoi 1 (i∈V′)) −1 (i= k) =    γ·eok a −1, if i= kand i∈Snovel γ·eoi a , if i̸= kand i∈Snovel eok a −1, if i= kand i /∈Snovel eoi a , if i̸= kand i /∈Snovel =    λi ·pk −1, if i= kand i∈Snovel λi ·pi, if i̸= kand i∈Snovel αi ·pk −1, if i= kand i /∈Snovel αi ·pi, if i̸= kand i /∈Snovel (13) Similarly, it is easy to derive the same results when current target token does not belong to the novel token set. B. Novel token set illustration Figure 4 shows an example of how the novel token set changes when the model is learning to predict the sentence “people who are interested ..”. At beginning, the novel token set Snovel is equivalent to the vocabulary V. The size of the novel token set shrinks as the decoding proceeds. C. Undesired property of UL training We use the same notation as Welleck et al. (2020) to explain the undesired UL property. From their paper (page 4): With a single negative candidate, the (negative) gradient is: ∇La = x∗−m⊙p, where m=    (1 −α pneg 1 −pneg ) if i̸= ineg (1 +α) if i= ineg (14) where x∗ ∈ {0,1}V is a one-hot ground-truth vector, m ∈ RV, p = pθ(·|x<t), and pneg is the probability of the negative candidate at index ineg. As the paper says (page 5): “.... At the ground-truth token index i∗, the unlikelihood gra- dient is positive, increasing the ground-truth token’s prob-Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation Figure 4: An illustration of how the novel token set changes as decoding proceeds for the sentence “people who are interested ...”. The words marked in purple are the target words that the model is learning to predict at each decoding step. ability with a magnitude that grows with pneg. Conversely, at the negative candidate index ineg the gradient is negative. At all other token indices i /∈{i∗,ineg}, the gradient moves from negative to positive as pneg increases. For instance, with α= 1.0 the gradient increases the probability of each token xi when the model assigns high probability to the negative candidate (pneg >0.5). ” We notice that at the ground-truth token index i∗, with α= 1.0 and pneg >0.5, the gradient norm is |∇La|= 1 +|m|· p∗. The model will therefore decrease p∗to reduce |∇La|, which is against our optimization principle. D. Human evaluation details We conduct the human evaluation for two pairs of systems i.e., SG vs. MLE and SG vs. UL. For each pair, the models generate their own continuations based on the same 100 randomly chosen preﬁxes. Two native speakers of English are then asked to evaluate the generated texts independently. During the study, users are instructed to judge which gen- erated text is a better continuation of the preﬁx based on the overall quality (e.g., readability, relevance to the preﬁx, grammar, and ﬂuency). The Win Rate in Table 3 is calculated as the total number of times that two users prefer the texts produced by the winner divided by the total number of cases in the evaluation (2 ×100 = 200). To get a reliable human study, we also compute the percentage agreement and the chance correlated measure, Gwet’s AC1/gamma coefﬁcient (Gwet, 2008) as the inter-rater agreement. Gwet’s AC1/gamma coefﬁcient overcomes the issue where traditional measures, such as Cohen’s Kappa, are not robust to skewed distributions of rankings. Figure 5 shows the interface for human evaluation study. E. Hyper-parameter search domain for directed generation During decoding, we apply length normalization follow- ing previous works. For the hyper-parameter in length normalization (beam search decoding), we use β ∈ {0.0,0.5,1.0,1.5,2.0}for text summarization and β ∈ {0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0}for image paragraph captioning. F. Experimental results on open-ended generation F.1. Full experimental results on WikiText-103 We present the full experimental results on WikiText-103 (Merity et al., 2017) test set for open-ended generations in Table 10. All the numbers are averaged over 3 runs with different random seeds and shown together with standard deviations. In addition, we provide the full results w.r.t.different hyper- parameters for UL and SG on the WikiText-103 validation set in Table 11. F.2. Open-ended generations results on IMDB dataset Table 12 shows the open-ended generation results on movie revies from IMDB dataset.Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation Figure 5: Human evaluation interface G. Experimental details In this section, we present the details of the datasets used in our experiments as well as the necessary experimental setup. All the experiments were conducted with a single GPU on our machine (CPU: Intel(R) Xeon(R) Gold 6240 CPU @ 2.60GHz; GPU: NVIDIA RTX 2080Ti). For each task in our experiments, we use the same model architecture and train it with different objectives (i.e., MLE, ScaleGrad and unlikelihood). The hyper-parameters that are used for different training objectives in the same task are exactly same, except for the ones described in Appendix E. We list the key hyper-parameters in this section. G.1. Open-ended generation Dataset The WikiText-103 (Merity et al., 2017) is a col- lection of over 100 million tokens extracted from the set of veriﬁed Good and Featured articles on Wikipedia. The training, validation and test sets contain 104m, 218k and 245k tokens, respectively. Experiments For all the experiments, we use the same setup and the same hyper-parameters as listed in Table 13, except for the method-speciﬁc hyper-parameters. We load the GPT-2 medium and ﬁne-tune it on WikiText-103 with a maximum of 35k iterations and select the model based on the validation perplexity. G.2. Summarization Dataset We use CNN/DM (Hermann et al., 2015; Nalla- pati et al., 2016) and NYT50 (Durrett et al., 2016) in our experiments for text summarization. Table 14 shows the dataset statistics in details. Experiments The models are taken from (Liu & Lapata, 2019) and we train the models for the abstractive summa- rization with MLE, unlikelihood training and ScaleGrad on CNN/DM and NYT50. We list the hyper-parameters that we used in Table 15. G.3. Image paragraph generation Dataset We use the image paragraph captioning corpus Visual Genome dataset, introduced by Krause et al. (2017). The dataset contains 14,575 training, 2,487 validation, and 2,489 testing images. The average length of description paragraph is 67.50 tokens.Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation Table 10: Results for open-ended generations on the Wikitext-103 testset. ppl, uniq and Rep/l are computed at BPE-level and the rest are at word-level. The “↑” denotes higher value for better performance and “↓” is the opposite. Number marked with * are estimated based on the testset. Language Modeling Auto Completion Models ppl ↓ uniq↑ Rep/16↓ Rep/32↓ Rep/128↓ Rep-1↓ Rep-2↓ Rep-3↓ uniq-w↑ MLE 13.24±2e−4 12.54k±4e−3 0.234±5e−6 0.380±8e−6 0.619±7e−6 0.661±1e−5 0.500±3e−5 0.424±7e−5 16.83k±1e−1 UL (α= 1.0) 16.06±2e−2 13.18k±6e−3 0.212±1e−6 0.341±1e−7 0.558±9e−6 0.559±6e−5 0.363±2e−4 0.291±3e−4 19.11k±7e−2 SG (γ= 0.2) 14.20 ±2e−2 13.61k±2e−3 0.197±6e−7 0.317±1e−6 0.522±4e−6 0.443±9e−7 0.215±2e−6 0.143±4e−6 22.25k±2e−2 Human - 18.27k 0.177 0.285 0.480 0.382* 0.096* 0.037* 27.55k* Table 11: Results for open-ended generation tasks on the Wikitext-103 validation set. ppl, uniq and Rep/l are computed at BPE-level and the rest are at word-level. The “↑” denotes higher value for better performance and “↓” is the opposite. Number marked with * are estimated based on the testset. The results are averaged over 3 runs with different random seeds. Language Modeling Auto Completion Models ppl ↓ uniq ↑ Rep/16 ↓ Rep/32 ↓ Rep/128 ↓ Rep-1 ↓ Rep-2 ↓ Rep-3 ↓ uniq-w ↑ MLE 13.17 12.52k 0.236 0.384 0.621 0.665 0.510 0.428 16.71k UL(α= 0.5) 14.91 12.45k 0.217 0.350 0.579 0.601 0.424 0.348 18.02k UL(α= 1.0) 16.52 12.77k 0.210 0.336 0.552 0.551 0.359 0.289 19.14k UL(α= 1.5) 19.63 13.41k 0.201 0.315 0.523 0.489 0.267 0.205 22.00k SG(γ= 0.2) 14.43 13.73k 0.195 0.316 0.518 0.451 0.237 0.175 22.29k SG(γ= 0.5) 13.53 13.25k 0.218 0.352 0.576 0.561 0.389 0.331 19.13k SG(γ= 0.8) 13.27 12.79k 0.229 0.369 0.603 0.625 0.443 0.365 17.59k Human – 17.68k 0.173 0.278 0.470 0.376 0.097 0.032 27.63k Experiments We follow the same experimental setup as in (Melas-Kyriazi et al., 2018). We train the model with different objectives and choose the model for testing based on the validation loss. During generation, tri-gram blocking and length-normalization are applied. Hyper-parameters that are used in our experiments are listed in Table 16. H. Experimental results of different decoding strategies for auto-completion. Table 17 shows the results for the auto-completion task when we train the model with ScaleGrad and infer with different decoding strategies. I. Stochastic decoding for image paragraph captioning We apply different stochastic decoding strategies for the MLE baseline on image paragraph captioning and report the results in Table 18. The experimental results demonstrate that stochastic decoding strategies do not work well in di- rected generation tasks, which is consitent with our ﬁndings in summarizaiton experiments. J. Hyper-parameter sensitivity To fully present the sensitivity of Rep/ l to the hyper- parameter, we further show how the Rep/ l (i.e., l=16, 32 and 128) change with γin Figure 6. K. Examples In the following, we show the examples of generated texts in three tasks: auto-completion (Table 19 and Table 20), image paragraph captioning (Table 21 and Table 22) and text summarization (Table 23, Table 24, Table 25 and Table 26). In addition, Table 27 and Table 28 show the example of auto completion on PTB testset and movie reviews from IMDB dataset.Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation Table 12: Results for open-ended generations on movie reviews from IMDB dataset. ppl, uniq and Rep/l are computed at BPE-level and the rest are at word-level. Numbers marked with * are estimated based on the movie reviews from IMDB. Language Modeling Auto Completion Models ppl uniq Rep/16 Rep/32 Rep/128 Rep-1 Rep-2 Rep-3 uniq-w MLE 100.764 7.48k 0.153 0.254 0.449 0.662 0.499 0.429 7.70k UL (α= 1.0) 108.334 8.09k 0.123 0.205 0.373 0.545 0.346 0.274 9.31k SG (γ= 0.2) 110.451 8.14k 0.114 0.187 0.344 0.383 0.142 0.081 10.42k Human - 14.49k 0.118 0.208 0.378 0.329* 0.084* 0.009* *19.11k (a) Rep/16  (b) Rep/32  (c) Rep/128 Figure 6: Hyper-parameter (γ) sensitivity in the language modeling task on Wikitext-103 development set. Table 13: Hyper-parameters for open-ended generation. M denotes the model-speciﬁc hyper-parameters. lr0 is initial learning rate. Models lr 0 M batch MLE 2 ×10−5 – 300 UL 2 ×10−5 0.5/1.0/1.5 300 ScaleGrad 2 ×10−5 0.2/0.5/0.8 300 Table 14: Dataset statistics for summarization. Dataset Training Size Validation Size Test Size CNN/DM 287,227 13,368 11,490 NYT50 96,834 4,000 3,452 Table 15: Hyper-parameter lists for text summarization. M denotes the model-speciﬁc hyper-parameters. lrBERT 0 and lrdec 0 stand for initial learning rate for BERT and Trans- former decoder. βis the hyper-parameter in length normal- ization. Models lr BERT0 lrdec0 M batch β Beam Size CNN/DM MLE 0.002 0.2 – 140 1.0 5 UL 0.002 0.2 0.5 140 2.0 5 ScaleGrad 0.002 0.2 0.8 140 1.5 5 NYT50 MLE 0.002 0.2 – 140 1.5 5 UL 0.002 0.2 0.5 140 2.0 5 ScaleGrad 0.002 0.2 0.8 140 1.5 5 Table 16: Hyper-parameter lists for image paragraph cap- tioning. M denotes the model-speciﬁc hyper-parameters. lr0 is initial learning rate. Models lr 0 M batch β(w/o & w/ 3-blocking) MLE 5×10−4 – 10 0.0/0.2 UL 5×10−4 0.5 10 0.0/0.3 ScaleGrad 5×10−4 0.5 10 0.6/0.6Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation Table 17: Results of different decoding strategies for auto- completion. Approaches ppl Rep-1 Rep-2 Rep-3 uniq-w ScaleGrad Greedy Search (γ= 0.2) 14.20 0.443 0.215 0.144 22.25k Beam Search (b = 3) 14.20 0.422 0.210 0.134 8.75k Beam Search (b = 6) 14.20 0.453 0.250 0.171 8.32k Beam Search (b = 10) 14.20 0.489 0.298 0.214 8.00k Top-p(p = 0.3) 14.20 0.356 0.107 0.049 30.48k Top-p(p = 0.9) 14.20 0.217 0.027 0.008 52.76k Top-k(k = 40) 14.20 0.254 0.039 0.012 39.50k Top-k(k = 100) 14.20 0.234 0.031 0.010 44.27k UL Greedy Search (α= 1.0) 16.06 0.559 0.363 0.291 19.11k Beam Search (b= 6) 16.06 0.577 0.418 0.325 7.49k Top-p(p= 0.3) 16.06 0.444 0.176 0.070 24.45k Top-k(k= 40) 16.06 0.336 0.067 0.021 31.89k MLE MLE 13.24 0.661 0.500 0.424 16.83k Beam Search (b= 6) 13.24 0.697 0.566 0.463 6.11k Top-p(p= 0.3) 13.24 0.558 0.210 0.116 20.13k Top-k(k= 40) 13.24 0.485 0.154 0.076 24.26k Human – 0.382 0.096 0.037 27.55k Table 18: Image paragraph captioning results for stochastic decoding on Visual Genome testset. Models CIDEr Top-p(p=0.3) 19.54 Top-p(p=0.9) 18.67 Top-k(k=40) 18.73 Top-k(k=100) 18.05 MLE w/ 3-block 22.77Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation Table 19: Example 1 of generated continuations for different training objectives. Preﬁx Robert Boulter is an English ﬁlm, television and theatre actor. He had a guest starring role on the television series The Bill in 2000. This was followed by a starring MLE role in the ﬁlm The Last of the Mohicans in 2002. He has appeared in the television series The X Factor, The X Factor Australia, The X Factor New Zealand, The X Factor South Africa, The X Factor New Zealand, The X Factor South Africa, The X Factor New Zealand, The X Factor New Zealand, The X Factor New Zealand, The X Factor New Zealand, The X Factor New Zealand, The X Factor New Zealand, The X Factor New Zealand, The X Factor New Zealand Unlikelihood role in the ﬁlm The Last of the Mohicans, which was released in 2002. He also appeared in the television series The X Factor, where he played the role of a DJ. In 2005, he starred in the ﬁlm The Last of the Mohicans, which was released in 2006. He also appeared in the television series The X Factor, where he played the role of a DJ. In 2008, he starred in the television series The X Factor, where he played the role ScaleGrad role in the BBC’s comedy series The Ofﬁce in 2002. In 2003, he appeared in the BBC’s crime drama series The Yard, playing a police ofﬁcer who is caught up in a murder investigation. He also appeared in the BBC’s crime drama series The X Factor, playing a man who is accused of murdering his wife. In 2004, he starred in the BBC’s comedy series The Ofﬁce, playing a ﬁctional version of himself. He has appeared in several Table 20: Example 2 of generated continuations for different training objectives. Preﬁx You’s patriotism, and Mei’s reﬂections on the quotidian are a few examples. More broadly, Du Fu’s work in transforming the lushi from mere word play into “a vehicle for serious poetic utterance” MLE is a major contribution to the genre. The ﬁrst of the three major works of the genre, was written in the early 1920s, and was the ﬁrst of the three to be published in English. It was published in the United States in the United States, and was translated into French in the 1930s. The work was the ﬁrst of the three Unlikelihood is a major inﬂuence on the modern Chinese novel. The modern Chinese novel is often characterized as being “literary in nature”, but it is not limited to literary genres. The modern Chinese novel has been described as “a literary genre that has been inﬂuenced by the Western literary tradition”. The modern Chinese novel has been described as “a literary genre that has been inﬂuenced by the Western literary tradition” ScaleGrad is a powerful example of how the Chinese literary tradition can be used to explore themes of nationalism and self determination. In addition to his own poetry, Du Fu wrote several works of prose. The ﬁrst was a collection of short stories called “The Story of the Three Sisters”, which was published in 1885. It was followed by a series of novels, including the ﬁrst two volumes of the Song dynasty epic poem “The Three Sisters”Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation Table 21: Example 1 of generated paragraphs for different training objective on Visual Genome testset. Reference Three adult horses are in a ﬁeld and a small horse. A horse color light brown has the neck low and is eating grass. This horse has the mane and the tail color tan. Two dark brown horses also are eating grass from the ground. The baby horse has a white spot on the front and is tiny. On back there is a wall of stones. MLE w/o tri-gram blocking There are two horses in a ﬁeld. The horses are brown and white. The horses are brown and white. The horses are brown and white. The horses are brown and white. The horses are brown and white. The horses are brown and white. The horses are brown. The horses are brown. The horses are brown. The horses are brown. The horses are brown. The horses are brown. The horses are brown. The horses are brown. The horses are brown . The horses are brown. The horses are brown. The horses are brown. The horses are brown. The horses are brown. The horses are brown. MLE w/ tri-gram blocking There are two horses in a ﬁeld. The horses are brown and white. The horse on the right is brown and brown. The horse is brown. The other horse is white. The grass is green and green. The sky is very and green. There are trees in the background. The trees are green and healthy. The horses are black. The sun is shining brightly. ScaleGrad w/ tri-gram blocking Two horses are standing in a ﬁeld. The ﬁeld is covered with green grass and weeds. The horse is brown and white. The horses are brown and white. There is a large tree behind the horses. The sky is blue and clear. There are many trees in the background. The grass is green. The sun is shining brightly. The sky is mostly cloudy. Table 22: Example 2 of generated paragraphs for different training objectives on Visual Genome testset. Reference There is a white and yellow train with red doors next to a red and yellow train. They are on top of the railroad tracks. There is a brick walkway next to the trains. MLE w/o tri-gram blocking A train is on the tracks. The train is yellow and yellow. The train is red and white . The train is red and white. The train is red and white . The train is on the train. The train is yellow and yellow. The train is blue and white . The train is on the tracks. The train is yellow and yellow . The train is blue and white. The train is blue and white. The train is on the tracks . The train is yellow. The train is blue. The train is blue. The train is blue. The train is blue. The train is blue. The train is blue. The train is blue. The train is blue. The train is blue. The train is blue. MLE w/ tri-gram blocking A train is on the tracks. The train is yellow and yellow. The train has a yellow stripe on the front. The front of the train is red. The sky is blue. There are many windows on the train. There is a train on the platform. The platform is made of metal. The is a platform on the side of the train. The are many tracks on the train. There are wires on the ground. There is a building behind the train tracks. There is a large building behind the train. ScaleGrad w/ tri-gram blocking A train is on the tracks. There are two sets of tracks next to the train. The train is white and yellow. There is a large white building behind the trains.Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation Table 23: Example 1 of summaries generated by the model trained with different training objective on CNN/DM testset. Reference report by dick marty, lead investigator for council of europe, gives bleak description of secret prisons run by cia in poland and romania from 2003 to 2006. marty says he relied heavily on information from cia agents. critics in poland and romania attack marty’s use of anonymous sources. MLE swiss sen dick marty reports that central intelligence agency operates secret prisons run by american in poland and romania from 2003 to 2006. says prison were operated exclusively by americans in poland and. Unlikelihood swiss sen dick marty reports that secret prisons run by central intelligence agency in eastern europe, with information he says is gleaned from anonymous intelligence agents. report is prepared by swiss senator investigating cia operations for council of europe, 46 - nation rights group. scathing report says prison were operated exclusively by americans in poland and romania from 2003 to 2006. ScaleGrad dick marty, swiss senator investigating cia operations for council of europe, gives bleak description of secret prisons run by central intelligence agency in eastern europe, with information he says is gleaned from anonymous intelligence agents. report says prisons were operated exclusively by americans in poland and romania from 2003 to 2006. Table 24: Example 2 of summaries generated by the model trained with different training objective on CNN/DM testset. Reference middleton wrote about eating whale for a national newspaper travel story. she described eating it at the #123-a-night juvet landscape hotel. whale and dolphin conservation group said the news was ‘disappointing’. the wdc said ms.middleton should have gone whale watching instead. MLE the 31-year-old has been slammed for ‘promoting the cruel and unnecessary whaling industry’. the 31-year-old boasted of eating minke whale in norway, to the horror of environmental campaigners who say she is helping promote an ‘unimaginably cruel’ trade. the hunting and importation of whale meat is banned in britain , as it is in most parts of the world. Unlikelihood the 31-year-old boasted of eating minke whale at juvet landscape hotel in norway. the 31-year-old has been slammed for ‘promoting the cruel and unnecessary whaling industry ’¡q¿environmental campaigners say she is helping promote an ‘ unimaginably cruel ’ trade. ScaleGrad duchess of cambridge’s brother-in-law has led a personal crusade against poaching and protecting wildlife. pippa middleton boasted of eating minke whale in norway, conservation group said she is helping promote ‘promoting the cruel and unnecessary whaling industry’.Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation Table 25: Example 1 of summaries generated by the model trained with different training objective on NYT50 testset. Reference protesters angry over bribery scandal involving state-run oil company petrobras. brazilian president dilma rousseff also is struggling with an economic downturn. MLE protesters are calling for president dilma rousseff to be impeached. rousseff’s supporters call for the president to be impeachment¡q¿they say there hasn’t been any evidence she was involved in the corruption scandal. Unlikelihood protesters are calling for president dilma rousseff to be impeached. there is a number of issues at play. one of the biggest is an investigation into a multimillion-dollar kickback scheme at the state-run oil company petrobras. ScaleGrad president dilma rousseff is facing an economic downturn and a massive bribery scandal. rousseff has defended her right to protest and acknowledged the need to clean up corruption at petrobras. Table 26: Example 2 of summaries generated by the model trained with different training objective on NYT50 testset. Reference wollemi pine, which ﬂourished 200 million years ago, is available to gardeners and can be grown indoors or outdoors. thought to extinct, it was discovered in 1994 in wollemi national park in australia. botanists took cuttings from wild colony of trees and began propagating baby pines and sending them to botanic gardens around world. MLE new york city plant - o - ramachan pine, which grew among dinosaur 200 million years ago, is named after dr david noble, who rappelled into deep gorge in blue mountains of wollemi national park, ten miles west of sydney, australia. photos. map. pine was discovered until 1994, when bush delay and national parks ofﬁcer david noble tried rappelled into woods of wollemi, which he had never seen before. scientists had evidence of wollemi pine’s existence, but only from fossils. Unlikelihood nicholas d kristof op-ed column on plight of wollemi pine, one of oldest on earth who grew among dinosaur 200 million years ago. notes that it was not discovered until 1994, when bush rescue ofﬁcer david noble rappelled into deep gorge in blue mountains of sydney, australia, and came back up with branch of tall tree that he never saw before. ScaleGrad nicholas d kristof op-ed column on new species of wollemi pine, which grew among dinosaur 200 million years ago and was discovered until 1994, when bush delay and national parks ofﬁcer david noble rappelled into deep gorge in blue mountains of wollemi national park, 120 miles west of sydney, australia, and discovered new species, wollemia nobilis, named after noble.Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation Table 27: Example of generated continuations for different training objectives on PTB testset. Preﬁx last resort who were criticized after the 1987 crash – once again couldn’t handle the selling pressure. Big investment banks refused to step up to the plate to support the beleaguered ﬂoor traders by buying big blocks of stock, traders say. Heavy selling MLE pressure was also felt by the Wall Street banks. The Wall Street banks were forced to sell more stock to hedge funds and other investors, and to buy more stock to hedge against losses. The Wall Street banks were forced to sell more stock to hedge funds and other investors, and to buy more stock to hedge against losses. The Wall Street banks were forced to sell more stock to hedge funds and other investors, and to buy more stock to hedge against losses. Unlikelihood pressure was also felt by the banks themselves. In 1987, the New York Federal Reserve Bank, which had been the largest bank in the United States since the 1930s, was forced to sell $1.5 billion worth of stock. The bank’s stock price fell to $0.001, and it was forced to close its New York branch. The bank’s stock price fell to$0.001, and it was forced to close its New ScaleGrad by the big banks led to a sharp drop in prices, and the market collapsed. In 1992, the Federal Reserve Bank of New York issued a report on the ﬁnancial crisis, which concluded that the Fed’s actions had not been sufﬁcient to prevent the collapse of the housing market. The report recommended that the Fed increase its balance sheet by $1.5 trillion (equivalent to about $2.5 trillion today), but this was not implemented. Table 28: Example of generated continuations for different training objectives on movie reviews from IMDB dataset. Preﬁx One of the other reviewers has mentioned that after watching just 1 Oz episode you’ll be hooked. They are right, as this is exactly what happened with me. The ﬁrst thing that struck me about Oz was its brutality and unﬂinching scenes of violence. MLE I was not expecting anything like that in a show about a family. I was expecting a show about a family, but I was expecting something more. I was expecting a show about a family, but I was expecting something more. I was expecting a show about a family, but I was expecting something more. I was expecting a show about a family, but I was expecting something more. I was expecting a show about a family, but I was expecting something more. I was expecting a show Unlikelihood I was immediately hooked. The violence was so graphic that I was completely taken aback by how it was handled. I was also struck by the fact that the show was so well-written that it was almost impossible to not be entertained. The second thing that struck me about Oz was its ability to make me laugh. I was so taken aback by the violence that I couldn ’t help but laugh. The violence was so graphic that I was completely taken aback by how it ScaleGrad It was also very funny, but not funny enough to make me laugh out loud. The second thing that struck me was the fact that the show’s characters were so well developed that they were able to carry their own weight without being annoying or annoying. In its original broadcast, “Oz” was watched by 9.5 million viewers, according to Nielsen ratings. It was the highest rated
```

## **Instructions:**
1. Analyze the paper based on the categories listed below.
2. Your response **must be a valid JSON object** that can be directly parsed using `json.loads()`.
3. Do not include any extra text, explanations, or formatting outside of the JSON object.
4. **If a field has no available information, set its value to `"Not mentioned"` instead of leaving it empty.**
5. Ensure that the JSON format is correct, including the use of **double quotes (`"`) for all keys and values.**
## **Output Format (JSON)**:
```json
{
    "main_contributions": "<Concise description of the main research problem and contributions>",
    "methodology": "<Brief explanation of the key techniques, models, or algorithms>",
    "experimental_setup": "<Description of datasets, benchmarks, and validation methods>",
    "limitations": "<Summary of weaknesses, constraints, or assumptions>",
    "future_research_directions": "<Potential areas for extending this research>"
}
```
Output:
{
    "main_contributions": "The paper introduces ScaleGrad, a novel training method that directly modifies the gradient of the standard Maximum Likelihood Estimation (MLE) objective to address the prevalent text degeneration issue in neural text generation. ScaleGrad encourages the model to learn and utilize novel tokens by dynamically re-normalizing softmax outputs during training. Empirical results demonstrate ScaleGrad's effectiveness in reducing repetition and increasing diversity in both open-ended (language modeling, text auto-completion) and directed (image paragraph captioning, abstractive text summarization) generation tasks, outperforming standard MLE and Unlikelihood training. Its architectural simplicity and wide applicability make it a general training objective.",
    "methodology": "ScaleGrad's core idea is to encourage models to use novel tokens by directly manipulating gradient information. During training, at each decoding step, a dynamic set of 'novel tokens' is maintained, comprising tokens not yet observed in the ground-truth sequence up to that point. The method re-normalizes the softmax probability distribution over the vocabulary using a hyper-parameter gamma (0 < gamma < 1). For novel tokens, their probabilities are scaled down by a factor involving gamma, while for non-novel tokens, they are effectively scaled up. This manipulation directly influences the gradients received by each token's logit: pushing the ground-truth novel token's probability higher and significantly reducing the probability of non-ground truth, non-novel tokens. The paper provides a detailed gradient analysis, showing how this re-scaling encourages novel token usage and also contrasts ScaleGrad's gradient behavior with that of Unlikelihood training, highlighting a potential issue in UL where it might decrease ground-truth token probability under certain conditions, which ScaleGrad avoids.",
    "experimental_setup": "The method was evaluated across various tasks and architectures. For **open-ended generation**, GPT-2 was fine-tuned on Wikitext-103 for language modeling and text auto-completion, with additional tests on Penn TreeBank (PTB) and IMDB datasets. Evaluation metrics included perplexity (ppl), unique token count (uniq), token-level repetition rates (Rep/l for l=16, 32, 128), n-gram word repetition ratios (Rep-n), and unique word count (uniq-w). Greedy decoding was primarily used, with an analysis of compatibility with stochastic decoding (top-k, top-p) and beam search. A human evaluation involving two native English speakers judged text quality. For **directed generation**, image paragraph captioning used a CNN-LSTM model on the Visual Genome dataset, measured by CIDEr, with greedy inference. Abstractive text summarization employed a BertSum model (Transformer with pre-trained BERT encoder) on CNN/DM and NYT50 datasets. Evaluation used F1-based ROUGE scores (R-1, R-2, R-L) and MoverScore (WMD-1), with decoding strategies including length normalization, tri-gram blocking, and beam search. Hyper-parameters (alpha for UL, gamma for SG) were tuned on development sets.",
    "limitations": "The paper implies a trade-off between generation quality (e.g., perplexity) and diversity (unique tokens, lower repetition) when choosing the ScaleGrad hyper-parameter gamma. A smaller gamma leads to a stronger push for novel tokens, potentially increasing perplexity while improving diversity. While ScaleGrad shows consistent improvements, it does not explicitly state fundamental limitations of the approach itself, but rather positions it as a robust solution to a known problem. The hyper-parameter `gamma` needs to be chosen based on the baseline model's performance and the desired balance of quality and diversity, suggesting it's not universally optimal and requires tuning per task.",
    "future_research_directions": "The authors suggest two main avenues for future research. Firstly, they plan to repurpose the definition of the 'novel token set' that ScaleGrad operates on to enable the model to achieve other specific objectives beyond simply reducing repetition, citing an example of adapting ScaleGrad to prevent early endpointing in online automatic speech recognition. Secondly, they aim to investigate mechanisms for dynamically adjusting the hyper-parameter gamma during the decoding steps, allowing the model to learn with varying degrees of strength depending on the current context."
}
