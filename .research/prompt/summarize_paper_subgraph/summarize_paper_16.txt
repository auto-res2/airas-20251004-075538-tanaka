
Input:

You are an expert research assistant responsible for summarizing a research paper that will serve as the foundation (Research A) for further exploration and integration.

Your task is to generate a structured summary of the given research paper with a focus on:
- **Technical Contributions**: Identify the main research problem and key findings.
- **Methodology**: Describe the techniques, models, or algorithms used.
- **Experimental Setup**: Outline the datasets, benchmarks, and validation methods.
- **Limitations**: Highlight any weaknesses, constraints, or assumptions.
- **Future Research Directions**: Suggest possible extensions or new areas for research.

Below is the full text of the research paper:

```
Implicit Optimization Bias of Next-token Prediction in Linear Models Christos Thrampoulidis Department of Electrical and Computer Engineering University of British Columbia Vancouver, Canada cthrampo@ece.ubc.ca Abstract We initiate an investigation into the optimization properties of next-token prediction (NTP), the dominant training paradigm for modern language models. Specifically, we study the structural properties of the solutions selected by gradient-based optimizers among the many possible minimizers of the NTP objective. By framing NTP as cross-entropy minimization across distinct contexts, each tied with a sparse conditional probability distribution across a finite vocabulary of tokens, we introduce “NTP-separability conditions” that enable reaching the data-entropy lower bound. With this setup, and focusing on linear models with fixed context embeddings, we characterize the optimization bias of gradient descent (GD): Within the data subspace defined by the sparsity patterns of distinct contexts, GD selects parameters that equate the logits’ differences of in-support tokens to their log- odds. In the orthogonal subspace, the GD parameters diverge in norm and select the direction that maximizes a margin specific to NTP. These findings extend previous research on implicit bias in one-hot classification to the NTP setting, highlighting key differences and prompting further research into the optimization and generalization properties of NTP, irrespective of the specific architecture used to generate the context embeddings. 1 Introduction Next-token prediction (NTP) has emerged as the go-to paradigm in training modern language models, revolutionizing various applications such as machine translation, text-summarization, and language generation [70]. In NTP, models are trained to predict the most probable token given a sequence of preceding tokens, commonly referred to as the context. Concretely, the objective is to learn a mapping from the input context to the probability distribution over the (finite) vocabulary of possible tokens, enabling the model to generate a token that is contextually appropriate [9, 8]. Recently, the NTP paradigm has witnessed remarkable empirical success through its utilization on large-scale deep-learning architectures trained on vast corpora of data [ 70, 71, 90], leading to unprecedented advances in the field, and the swift integration of these advanced language models into society [65]. Concurrently, researchers have raised critical concerns about robustness, interpretability, and fairness-bias issues arising from our limited understanding of the fundamental operational principles of these models [10, 6]. Despite progress, a comprehensive theory that elucidates the fundamentals of modern language models—including key components like the NTP paradigm and transformer architecture, particularly in terms of optimization and generalization principles—is still lacking. We initiate an investigation when implicit optimization biases in training language models under the NTP paradigm, particularly in overparameterized regimes where the empirical-loss reaches its lower bound and there is many possible minimizers. To formalize the NTP paradigm, consider 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2402.18551v2  [cs.LG]  31 Oct 2024autoregressive model qθ parameterized by θ trained to predict the next-token on sequences of length T using the cross-entropy (CE) loss: min θ ˆEz∼Tn [∑t∈[T] −log (qθ(zt ∣z1, . . . , zt−1))]. (1) Here, sequences z = (z1, . . . , zT ) consist of tokens zt from a finite vocabulary V = {1, . . . , V} and ˆE is expectation over training set Tn of n such sequences sampled from some underlying true distribution over sequences. Typically, the modelqθ outputs probability of the next token computed via softmax applied on output logits, which are computed by projecting d-dimensional embeddings hθ′ to the V -dimensional space with a trainable linear decoder W ∈RV ×d. Formally, 1 qθ(zt ∣z1, . . . , zt−1) =Szt(Whθ′(z1, . . . , zt−1)) = 1 1 +∑z′∈V z′≠zt exp ((ez′ −ezt)⊺Whθ′(z1, . . . , zt−1)) . The CE loss is then minimized over θ =(W, θ′) using gradient-based methods, e.g. (S)GD, Adam. We pose the question: Given training set Tn, what are the structural properties of the weights θ found by minimizing the NTP objective with gradient-based optimizers? As in prior research in one-hot supervised classification 2 (e.g. [101, 7, 80, 36]), we specifically target this question in an overparameterized setting, where the NTP objective (1) may have an infinite number of solutions, representing an infinite number of models θ that minimize the training loss. The central challenge is to discern the particular solution the optimizer is inherently biased towards. Since this ‘bias’ is not explicitly introduced through regularization but is instead ingrained in the training objective and algorithmic structure, it is termed ‘implicit bias’ [ 64]. The exploration of implicit bias has a long history in the traditional supervised one-hot classification (see Related Work in Sec. 6). In this traditional scenario, the training set comprises feature-label pairs(x, y), where x ∈Rp is a continuous feature, and y represents its unique label. The optimization process minimizes the following training objective (over W, θ′): ˆE(x,y) [−log (Sy(Whθ′(x)))]. At first glance, excluding the sequential format of Eq.(1), the NTP training scenario might seem identi- cal to traditional one-hot prediction: both aim to minimize the same CE loss across models that param- eterize probabilities using the softmax of logits. Consider predicting the next token over fixed-length sequences, say sequences of length t −1, via optimizing: ˆEz [−log (Szt(Whθ(z1, . . . , zt−1)))]. The context here acts as the feature, and the next token as the label. Recent works [ 51, 55] draw on such apparent similarities to the traditional one-hot classification paradigm to extrapolate known results from the latter to the NTP setting. However, this comparison overlooks a fundamental, yet critical difference in the nature of the training data that distinguishes these two paradigms (even when the sequential format of Eq. (1) is disregarded): In the traditional setting, each feature (e.g., image) is assigned a single label (e.g., image category). In contrast, in the NTP setting, contexts z1, . . . , zt−1 of finite length sampled from finite vocabularies are naturally repeated in a (vast) training set, potentially multiple times, each time followed by different tokens zt [77]. Consequently, the NTP paradigm involves training over m ≤n distinct (non-repetitive) contexts, each followed by a multitude of possi- ble next tokens, appearing at varying frequencies. For instance, the context "She is excellent at her role as a" may be followed by next tokens such as "doctor," "lawyer," "reviewer," or "mother," each with different frequencies. Importantly, certain vocabulary tokens may not appear after a given context; e.g., in the above example, tokens like "run," "and," etc., will not follow. Model. We studyNTP training over a finite vocabulary employing the following model. Given a large training set of n total sequences, we identify m ≤n distinct contexts. Each distinct context j ∈[m] is linked to a V -dimensional empirical probability vector ˆpj, which encodes the frequency with which each vocabulary token follows the context throughout its occurrences in the training set. Crucially, the probability vectors ˆpj are sparse, i.e., the support set Sj of ˆpj satisfies ∣Sj∣ ≪ ∣V∣ = V . In an extreme where ∣Sj∣ =1, ∀j ∈[m], the probability vector ˆpj becomes one-hot, leading to a scenario reminiscent of the traditional classification setting described earlier. However, such an extreme is essentially improbable in practical language modeling [77]. With this framing, the NTP paradigm is 1Throughout, ev ∈RV is the v-th standard basis vector, and Sz(u) =e⊺ zS(u) the z-th entry of softmax output. 2In NTP, the ground-truth next token is inherently embedded within the underlying text, thus strictly speaking, it falls under the self-supervised learning paradigm [70]. Yet, the utilization of the CE training objective resembles to supervised training. We leverage this resemblance and regard NTP training as an instance of supervised learning, while also emphasizing how it differs from one-hot encoding supervision. 2also related to supervised vision classification with soft labels, which advocates for training models on datasets where each example is associated with a vector of soft labels (rather than a one-hot vector), such as by averaging multiple annotators’ hard labels [69], knowledge distillation [34] or label smoothing [83]. With this connection, our analysis can also be interpreted (more broadly) as investigating the implicit bias of sparse soft-label classification. 1.1 Contributions and Organization Formulation. Recognizing the differences between NTP and one-hot classification, we study the question of implicit optimization bias within the NTP setting. To facilitate this, we utilize the model outlined in the previous paragraph and detailed in Sec. 2. For concreteness, our analysis adopts a ’top-down’ approach, training only the decoding (also referred to as word-embedding) matrix W ∈RV ×d while keeping context-embeddings fixed. This approach mirrors foundational studies on implicit optimization bias in one-hot classification [ 80, 36], which first focused on linear models. It allows exploring the complexities of the NTP training objective, distinct from the embedding architecture3, and while it renders the logits linear and the objective convex, it still poses a technical challenge in terms of determining parameter convergence [80, 36, 39, 63, 40]. Conditions for reaching entropy. In Sec. 3, we identify the necessary and sufficient conditions for the logits of the trained model to enable the CE loss to approach its lower bound, the empirical conditional entropy. We introduce two conditions: NTPH-compatibility and NTP-separability, which impose constraints on mutually orthogonal subspaces that are determined by the sparsity patterns of distinct contexts within the dataset. These conditions determine the necessary and sufficient overparameterization a model needs to achieve the empirical entropy lower bound during training. Margin in NTP setting. Motivated by the NTP-separability condition, we introduce a margin concept for NTP in Sec. 4, which extends the classical definition of margin used in one-hot supervised classification [92]. We further establish the relevance of this new margin notion for optimization by demonstrating that a decoder maximizing the NTP-margin, denoted as Wmm, guides the directional convergence of the ridge-regularized CE minimizer, ̂Wλ, as the regularization parameter λ →0. Implicit bias of GD. We establish that Wmm also determines the implicit bias of gradient descent (GD) iterates in Sec. 5. Specifically, in the limit of iterations k → ∞, the GD iterates grow undoubtedly in norm and converge to a finite W⋆ within a data subspace F, while simultaneously aligning with Wmm in the complementary subspace F⊥. The finite component W⋆ ∈ F solves a system of linear equations associated with the NTPH-compatibility condition. Finally, we numerically verify these findings and discuss related and future work in Secs. 6 and 7. Additional experiments, further related work and detailed proofs are in the appendix. 2 Setup Let vocabulary V =[V ] ∶={1, . . . , V} represent a set of V tokens (e.g. words) and z1∶t =(z1, . . . , zt) denote sequence of t tokens zt ∈V. To simplify presentation, we focus on predicting the T-th token zT given contexts z<T ∶=z1∶T−1 of fixed length, and we further let x =z<t denote the context and z denote the last token. See App. C for straightforward extension to the sequential format of Eq. (1). We assume access to a training set consisting of n sequences Tn ∶={(xi, zi)}i∈[n], with xi ∈X ∶= VT−1 and zi ∈ V. Let h ∶X →Rd an embedding map that maps contexts (i.e., sequences of T −1 tokens) to d-dimensional embeddings. The map h can be parameterized (e.g. by a transformer [94] or an LSTM [5]), but this paper assumes that it is fixed. The next-token is predicted via a linear model fW ∶X →RV parameterized by decoding matrix W ∈RV ×d, such that fW (x) =Wh(x). When the model output passes through a softmax, it defines the model’s probability mass function for the next-token prediction, given as ˆqW (⋅∣x) =S(fW (x)), where S(⋅) ∶RV →∆V −1 is the softmax and ∆V −1 is the V -dimensional simplex. The decoder is trained by minimizing the empirical CE loss CE(W) ∶= 1 n ∑i∈[n] −log (ˆqW (zi∣xi)). Distinct sequences and next-token distributions. Given dataset Tn we denote ¯x1, . . . ,¯xm the m ≤n distinct contexts among the (large number of) total n contexts x1, . . . ,xn within Tn. Let ˆπj 3NTP is widely used across various modern language modeling architectures, including transformers [70, 71], state-space models [27, 29], and LSTMs [5]. 3be the empirical probability of distinct context ¯xj. That is, 1 ≤n ⋅ˆπj ≤n is the number of contexts xi that equal ¯xj. Furthermore, for each distinct context ¯xj, j∈[m] let ˆpj ∈∆V −1 denote the probability vector of conditional next-token distribution, i.e., ˆpj,z ∶= ˆp (z∣¯xj), z∈ V, j∈ [m]. In other words, n⋅ˆπj ⋅ˆpj,z is the number of occurences of token z as a follow-up to context ¯xj. Finally, we denote the support set and size of the support set of these conditional distributions as Sj ∶={z ∈V ∣ ˆpj,z >0} and Sj ∶=∣Sj∣. Tokens z ∈Sj and v ∉Sj are referred to as ’in-support’ and ’out-of-support’ respectively. Onwards, we implicitly assume that “not all tokens are likely after every context,” i.e. ∃j ∈[m] such that Sj < V . This mild assumption is naturally satisfied in language modeling under rich enough vocabulary. With this notation, 4 we can express the NTP training loss as CE(W) =− ∑ j∈[m] ˆπj ∑ z∈V ˆpj,z log (Sz(Wh(¯xj))) =− ∑ j∈[m] ˆπj ∑ z∈Sj ˆpj,z log (Sz(W ¯hj)), (2) where, in the last line we defined the shorthand ¯hj = h(¯xj). Similarly, we let hi = h(xi), i∈ [n]. With some abuse of notation, we then obtain the following equivalent descriptions of the training set {(xi, zi)}i∈[n] =∶Tn ≡Tm ∶={(¯hj, ˆπj, ˆpj,z∈V)}j∈[m] that emphasizes distinct contexts and their respsective sparse next-token probability distributions. Entropy. The empirical T-gram entropy (referred to hereafter as entropy for simplicity) of the dataset is [78, 77]: HT ∶= H ∶= ˆE(x,z)∼Tn [−log (ˆp(z∣x))] = −∑j∈[m] ∑z∈Sj ˆπj ˆpj,z log (ˆpj,z) . It lower bounds the CE loss since CE(W) =H +KL (ˆp∣∣ ˆqW ) and the KL divergence is nonnegative. 3 When can the NTP-loss reach the entropy lower-bound? The first question we ask is: Under what conditions on the training data can the CE loss reach its entropy lower-bound? By the entropy lower-bound, CE(W) = H ⇔KL (ˆp∣∣ ˆqW ) = 0 iff for all j ∈[m] and all z ∈V: ˆqW (z∣¯xj) = ˆpj,z. Equivalently, for all j ∈[m]: Sz(W ¯hj) = ˆpj,z, ∀z ∈Sj , (3a) Sv(W ¯hj) =0, ∀v ∉Sj . (3b) Beginning with (3a), this requires5 the training data to satisfy the NTPH-compatibility condition defined below. Definition 1 (NTPH-compatible). Let ev denote the v-th standard basis vector in RV . We say that training data Tm are NTP-entropy-compatible if there existsV ×d matrix Wp satisfying: ∀j ∈[m], z≠z′ ∈Sj ∶ (ez −ez′)⊺Wp¯hj =log (ˆpj,z/ˆpj,z′) . (4) We comment on the independence of the constraints: Fix any j ∈[m]. Then, the set of constraints (as expressed in Eq. (4)) for all z ≠z′ ∈Sj (yielding (Sj 2 ) constraints in total) is equivalent to the set of the same constraints for any anchor zj ∈Sj and z′ ≠zj ∈Sj, i.e., an effective total of Sj −1 linearly independent constraints for each j ∈[m]. Additionally, note that the system of equations in Eq. (4) constrains Wp with respect to a specific subspace of V ×d matrices: F =span ({(ez −ez′)¯h⊺ j ∶ z ≠z′ ∈Sj, j∈[m]}), (5) that is defined in terms of context embeddings and their respective support sets. Assuming Eqs. (4) have a solution, we denote the unique solution within the subspace F as W⋆ ∈F for later reference 6. Next, we examine Eq. (3b), which requires softmax outputs be zero for tokens that never occur following a fixed context throughout the dataset. Due to the strict positivity of softmax, the constraint is never satisfied for finite W. Thus, for all finite W, there exists a gap between the cross-entropy loss and its lower bound, i.e., CE(W) >H. Yet, it is possible to approach entropy as the norm of the weights W grows, provided that weights move in the appropriate direction formalized below. 4A complete list of notations is also given in Appendix D. 5It will be see below, and can be easily checked by the reader, this condition alone is insufficient; the NTP- separability condition in Defn. 2 is also needed. 6If Eqs. (4) have a solution, say W1, every other solution takes the form W p = W1 + Wnull, where Wnull is orthogonal to (ez − ez′)¯hT j ∶ z ≠z′ ∈Sj, j∈[m]. Thus, Wnull ∈F⊥ is in the orthogonal complement of F. 4Definition 2 (NTP-separable). We say that training data Tm are NTP-separable if there exists V ×d matrix Wd satisfying the following: ∀j ∈[m], z≠z′ ∈Sj ∶ (ez −ez′)⊺Wd¯hj =0 (6a) ∀j ∈[m], z∈Sj, v∉Sj ∶ (ez −ev)⊺Wd¯hj ≥1 . (6b) As before, it is easy to see that the constraints in (6) can be equivalently expressed by enforcing (6a) and (6b) for an anchor zj ∈Sj and all z′ ∈Sj /{zj} and v ∉Sj, respectively. Consequently, there exist effectively V −1 linearly independent constraints per context j ∈[m]. We now discuss the interpretation of these constraints. The subspace constraints in Eq. (6a) project Wd onto the subspace F⊥, which is the orthogonal complement of the subspace F defined in (5). This leaves the softmax probabilities of possible next tokens (in set Sj) intact, and fully determined by Wp as per the NTPH-compatibility condition. Formally, Wp +Wd continues satisfying (4). Moving on the halfspace constraints in (6b), we can interpret these using Kesler’s construction as enforcing linear separability in the space RV ×d [32]: Each d-dimensional context embedding ¯hj is mapped to Sj(V −Sj) higher-dimensional points (ez −ev)¯h⊺ j , z∈ Sj, v∉ Sj. These points collectively for allj ∈[m]must lie within the interior of the same halfspace induced by the hyperplane ⟨Wd, ⋅⟩ = 0. Refer to Fig. 1(Left) and its caption for an alternative interpretation of the rows of Wmm as word-embeddings in Rd (illustration in d =2). The impact of NTP-separability on the softmax probabilities can be understood algebraically by considering Wγ ∶=γWd and v ∉Sj. We have: Sv(Wγ ¯hj) =( ∑ z∈Sj eγ(ez−ev)⊺Wd¯hj + ∑ v′∉Sj eγ(ev′−ev)⊺Wd¯hj ) −1 ≤( ∑ z∈Sj eγ(ez−ev)⊺Wd¯hj ) −1 ≤e−γ, (7) where the first inequality removes non-negative exponential terms and the second one follows from (6b). The upper bound above approaches 0 as γ →∞, thus (3b) holds asymptotically in γ. Taking into account the observations made above, the satisfaction of both conditions guarantees convergence of the cross-entropy loss CE to H. This is formalized in the proposition below. Proposition 1. Assume training dataTm is NTPH-compatible and NTP-separable, with the respective matrices Wp and Wd satisfying conditions (4) and (6). While all finite W satisfy CE(W) >H, it holds for Wγ =Wp +γ ⋅Wd that CE(Wγ) γ→+∞ /leftr⫯g⊸tl⫯ne/leftr⫯g⊸tl⫯ne/leftr⫯g⊸tl⫯ne→H. Hence, CE approaches its lower-bound in the limit of a direction Wd ∶=Wd/∥Wd∥ and offset Wp satisfying the constraints of NTP-separability and NTP-compatibility, respectively. In other words, parameter weights W that minimize the CE loss consist of two components: a finite projection WF ∶=PF(W) =W⋆ onto the data subspace F and an infinite-norm component onto the orthogonal complement F⊥in the direction of Wd. Finally, we note that while Defns. 1 and 2 are stated for linear models, they naturally extend to a more general formulation for nonlinear models. Specifically, consider NTP-separability (similar for NTP-compatibility): the general conditions require that both the decoder weights W and model weights θ, which parameterize the embeddings ¯hj =hθ(¯xj), must satisfy Eq. (6) simultaneously. 3.1 The role of overparameterization We show that overparameterization provides a sufficient condition for the solvability of Eqs. (4) and (6). Start with the halfspace constraints in Eq. (4) for NTPH-compatibility. These can be compactly expressed as Ej,zj Wp¯hj = aj,z, where Ej,zj ∈ R(Sj−1)×V has rows ezj −ez′ and aj,zj ∈ R(Sj−1) has entries log (ˆpj,zj /ˆpj,z′) for some anchor zj ∈ Sj. Now, since the rows of Ej,zj are linearly independent, the question becomes equivalently that of determining when Wp[¯h1, . . . ,¯hm] = [E† 1,z1 a1,z1 , . . . ,E† m,zmam,zm] has a solution. This is always the case when d >m and the d ×m 5embedding matrix ¯H =[¯h1, . . . ,¯hm] is full rank (m). Then, there exists Wp such that condition (4) holds. In fact, ¯H⊺ has a nullspace, implying the existence of an infinite number of solutions to (4). These solutions take the form Wp =W⋆ +Wp ⊥, where W⋆ ∈F is the unique solution onto the subspace, and Wp ⊥ ∈F⊥. In contrast to (4), the constraints in (6) involve linear inequalities. However, a sufficient proxy for feasibility in this case is that the corresponding system of equations (instead of inequalities) has a solution. By following the exact same argument as before, we arrive at the same sufficient conditions for the existence of a solution Wd. We summarize these findings. Lemma 1 (Overparameterization implies NTP-separability). Assume overparameterization d >m and full-rank embedding matrix ¯H ∈Rd×m. Then, there exists an infinite number of solutions Wp and Wd that satisfy conditions (4) and (6), respectively. Thus, d > m, 7 which also generically favors full-rankness of the embedding matrix [96], implies both NTPH-compatibility and NTP-separability. Combined with Prop. 1, it also implies that there are infinitely many possible directions Wd along which the NTP loss approaches H, motivating the implicit-bias question: For a specific iterative algorithm aimed at minimizing the NTP loss, which direction does it prefer? We will address this question in the remainder of the paper. Remark 1. In the trivial case where Sj = 1, ∀j ∈ [m] (one-hot classification), the entropy lower bound is zero and is attained iff the data is linearly separable. Indeed, F reduces to the empty set, and NTP-separability simplifies to traditional multiclass separability. For binary classification, [20] showed that d/m > 1/2 is sufficient and necessary for data in general position to be linearly separable. More recently, several works have extended this analysis to structured (random) data, including [12, 75, 60, 57]. The exact threshold in corresponding mutliclass settings is more intricate, but [19, 85, 11] have made progress in this direction. An interesting question is determining exact thresholds for NTP-separability, which would improve upon the sufficient condition of Lemma 1. 4 Regularization path This section investigates the implicit bias of NTP by examining the minimization of CE loss through iterates defined as follows for an increasing sequence of positive regularization parameters B: ̂WB ∶=arg min∥W∥≤B CE(W). (8) This involves minimizing a strictly convex function in a bounded domain; thus, ̂WB is unique. This section’s main result characterizes the limit of ̂WB as B →∞under NTP-separability/compatibility. Before that, we first define the next-token prediction support-vector machines (SVM) problem. Definition 3 (NTP-SVM). Given NTP-separable training set Tm, NTP-SVM solves the following: Wmm ∶=arg minW ∥W∥ subj. to W ∈RV ×d satisfying (6a) and (6b). (NTP-SVM) This is a strongly convex quadratic program withmV −∑j∈[m] Sj linear inequality and ∑j∈[m] Sj −m linear equality constraints. Its solution can be also defined as the classifier that maximizes margin between in and out-of -support tokens while being constrained on the orthogonal compelemnt F⊥: Wmm =arg max∥W∥=1,W∈F⊥ minj∈[m],z∈Sj,v∉Sj (ez −ev)⊺W ¯hj. It turns out this direction determines the preferred limiting direction of the regularization path. Theorem 1 (Implicit bias of the regularization-path). Assume training data Tm is NTPH-compatible and NTP-separable. Let ̂WB be defined as in (8). Then, it holds that limB→∞ ⟨ ̂WB ∥̂WB∥, Wmm ∥Wmm∥⟩ =1 . The proof sketch below illustrates how the NTP-separability/compatibility assumptions influence the outcome and why the regularization path induces an optimization bias toward the NTP-SVM direction. Complementing Thm. 1, we also show (see Lemma 4 in the appendix) thatlimB→∞PF(WB) =W⋆. These together provide a complete characterization of the implicit optimization bias of (8). 7The necessity for such large d can be mitigated through the utilization of non-linear architectures (such as an MLP decoder), in which the total number of parameters can be increased by augmenting the width or depth, rather than directly modifying the embedding dimension d as in linear models. 6Proof sketch (App. E.2 for details). We first show ̂WB is on the boundary: ∥̂WB∥ =B. If not, then ⟨∇CE(̂WB), Wmm⟩ =0. But, few algebraic manipulations show ⟨−∇CE(̂WB), Wmm⟩ equals ∑ j∈[m] ˆπj ∑ z∈Sj ˆpj,z( ∑ z′∈Sj,z′≠z sj,z′ (ez −ez′)⊺Wmm¯hj + ∑ v∉Sj sj,v (ez −ev)⊺Wmm¯hj), where we denote sj,v ∶=Sv(̂WB ¯hj) >0, v∈V, j∈[m]. The first term in the parenthesis is zero by (6a), while the second term is strictly positive by (6b), leading to contradiction. Now, consider a ‘genie’ pointW⋆ B =W⋆+R(B)⋅Wmm, where W⋆ ∈F satisfies (4), and R =R(B) is chosen such that ∥W⋆ B∥ = B. We will show that W⋆ B attains a small CE loss as B (hence, R) grows. To do this, denote for convenience the logits ℓ⋆ j,v ∶=e⊺ vW⋆¯hj and ℓmm j,v ∶=e⊺ vWmm¯hj for all for v ∈V, j∈[m], and note that e⊺ vW⋆ B ¯hj =ℓ⋆ j,v +R ℓmm j,v . By using (4) and (6a): ∑ z′∈Sj e−(ℓ⋆ j,z+Rℓmm j,z −ℓ⋆ j,z′−Rℓmm j,z′) = ∑ z′∈Sj e−(ℓ⋆ j,z−ℓ⋆ j,z′) = ∑ z′∈Sj ˆpj,z′ ˆpj,z = 1 ˆpj,z . Moreover, using (6b) and defining C ∶=V e∥W⋆∥M for M ∶= √ 2 ⋅maxj∈[m] ∥¯hj∥, gives: ∑ v∉Sj e−(ℓ⋆ j,z+Rℓmm j,z −ℓ⋆ j,v−Rℓmm j,v ) ≤e−R ∑ v∉Sj e−(ℓ⋆ j,z−ℓ⋆ j,v) ≤C e−R. Combining the above within Eq. (2), using log(1 +x) ≤ x, x> 0 and the fact that ˆπj, ˆpj,z are probabilities, yields: CE(W⋆ B) ≤ ∑ j∈[m] ˆπj ∑ z∈Sj ˆpj,z log ( 1 ˆpj,z +C e−R) ≤H +C e−R . (9) Next, towards contradiction, we will show that if ̂WB is not in the direction of Wmm, then it incurs a loss that is larger than CE(W⋆ B). The trick here is to bound the KL divergence term: CE(̂WB)−H = ∑ j∈[m] ˆπj ∑ z∈Sj ˆpj,z log (ˆpj,z( ∑ z′∈Sj eℓj,z′−ℓj,z + ∑ v∉Sj eℓj,v−ℓj,z )), (10) where we denote logits ℓj,v ∶=e⊺ v ̂WB ¯hj. Assume there exists ϵ >0 and arbitrarily large B satisfying: ∥(∥Wmm∥/B) ̂WB −Wmm∥ >ϵ. (11) Define ̂W =(̂WB −W⋆)/R′(B), where R′ =R′(B) >0 can be chosen so that ∥̂W∥ =∥Wmm∥. Further choose B large enough so that Eq. (11) guarantees ∥̂W −Wmm∥ ≥ ϵ′, for some ϵ′ > 0. Since Wmm is the unique minimizer of (NTP-SVM) and ∥̂W∥ = ∥Wmm∥, there exists δ ∈ (0, 1) and j ∈ [m] such that at least one of the following is true: (i) ∃z and z′ ≠ z ∈ Sj such that ∣(ez −ez′)⊺̂W ¯hj∣ ≥δ (ii) ∃z ∈Sj, v∉Sj such that (ez −ev)⊺̂W ¯hj ≤1 −δ. Case (i): Without loss of generality (ez −ez′)⊺̂W ¯hj ≤−δ (otherwise, flip z, z′). Thus, ignoring all but the (j, z, z′)-term in (10) and using ℓj,z′ −ℓj,z ≥R′δ +log ( ˆpj,z′ ˆpj,z ) gives CE(̂WB)−H ≥ ˆπj ˆpj,z log (ˆpj,ze(ℓj,z′−ℓj,z)) ≥ 1 n log (eR′δ n ). Comparing this to (9) for large enough B gives that CE(̂WB) >CE(W⋆ B), a contradiction. Case (ii): We can assume ̂W ∈F⊥, since otherwise we are in Case (i). Now, again ignoring all but the (j, z) term in the CE loss for which the assumption holds for some v ∉Sj, we find CE(̂WB)−H ≥ ˆπj ˆpj,z log (ˆpj,z( ∑ z′∈Sj e(ℓj,z′−ℓj,z) +e(ℓj,v−ℓj,z))). 7Using PF(̂WB) =W⋆ and (4) yields ∑z′∈Sj e(ℓj,z′−ℓj,z) = 1 ˆpj,z . Moreover, by assumption of Case (ii): eℓj,v−ℓj,z ≥e−R′(1−δ) eℓ⋆ j,v−ℓ⋆ j,z ≥c′e−R′(1−δ), for c′ ∶=e−∥W⋆∥M . Putting together yields: CE(̂WB)−H ≥ ˆπj ˆpj,z log (1 +ˆpj,zc′e−R′(1−δ)) ≥c′e−R′(1−δ)/2n2 , where the second inequality uses log(1 +x) ≥ x 1+x , x>0. Compare this with (9): For large enough B, since R, R′ grow at the same rate, it holds c′ 2n2 e−R′(1−δ) >Ce−R. Thus, CE(̂WB) >CE(W⋆ B), a contradiction. In either case, we arrive at a contradiction, which completes the proof. 5 Gradient Descent This section studies the implicit bias of GD. Denote the GD iterates at time k by Wk = Wk−1 − η∇CE (Wk−1) for arbitrary initial point W0 and constant step-size η >0 small enough to guarantee descent. The first observation is that the norm of the GD iterates increases with iterations. Lemma 2 (Norm growth). If training data are NTPH-compatible and NTP-separable, then limk→∞CE(Wk) =H and limk→∞∥Wk∥ =∞. This is intuitive because the CE loss is convex in W (thus, GD approaches the objective’s infimum H), and, in view of Proposition 1, the CE loss at all finite W is bounded away from H. The relevant question then becomes that of determining the limit of the direction of the GD iterates. Theorem 2 (Implicit bias of GD). Assume NTPH-compatible and NTP-separable training data Tm. Then, it holds that limk→∞ ⟨ Wk ∥Wk∥, Wmm ∥Wmm∥⟩ =1 . Moreover,limk→∞PF(Wk) =W⋆. The theorem establishes 8 that in the limit of iterations: Wk ≈ W⋆ +∥P⊥(Wk)∥Wmm, which is analogous to the result we obtained previously for the regularization path. Although its proof is more involved compared to the proof of Thm. 1, the proof of its main ingredient (Lem. 5 in the appendix) is conceptually similar: It involves comparing the loss CE(Wk) for large iterations k to the loss evaluated at a “genie” point that is chosen so that: (i) On the subspace F, it agrees with Wk. This is because it is easy to show that PF(Wk) converges to W⋆ by standard gradient descent analysis for convex functions; (ii) On the orthogonal subspace F⊥, it follows the optimal (with respect to accelerating loss decrease) max-margin direction Wmm ∈F⊥. To establish the loss comparison, the ideas is to compare the values of the adjusted loss CE⊥(W) ∶=CE(W)−CE (PF(W)). We validate our analysis with experiments on synthetic data in App. A. For illustration, Fig. 1 shows a 2D setting with m =3 distinct contexts, each followed by Sj =3 tokens/words out of total V =5 words in the vocabulary. The left subfigure illustrates: (i) In black markers, the context-embedding geometry along with the associated support sets for each context A, B, and C. (ii) In colored markers, the geometry of word-embeddings, that is the max- NTP-margin vectors (Wmm)⊺ev, v∈ [5], to which GD directionally converges. See caption for interpretation and Fig. 2 in the App. for vis. of the finite component of word-embeddings on the subspace F. The right subfigure shows results of GD training with respect to training loss, norm growth, alignment with Wmm, and convergence to W⋆ on F. See App. A for further implementation details and additional experiments. 6 Related work We build on the literature on implicit optimization bias of CE loss in one-hot supervised classification. [80] show that for linear models and linearly-separable data, GD converges in direction to the max- margin classifier. This result strengthens [72] that showed the regularization path of CE minimization converges to the same limit. Closer to us, [36, 39] extend the analysis to encompass general binary data as follows: the data are linearly separable only on a certain subspace, and they show that GD converges, in direction, towards the max-margin classifier confined within that subspace. On the orthogonal subspace, it converges to a finite point. While operationally similar, Thms. 1, 2 cannot 8In line with observations in one-hot encoding [62], we anticipate the directional behavior remains unchanged under stochasticity, e.g. when using SGD to minimize (2). Yet, note a subtle but crucial difference in applying SGD to (1) vs (2), as the latter involves sampling distinct contexts in each iteration. In this latter case, we also point out that favorable interpolation conditions, such as strong-growth (e.g., [95]), can be shown to hold. 8-3 -2.5 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2-1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 w1 w2w3 w4 w5 125 145 234A B C Figure 1: Vis. of NTP implicit optimization bias in a setting with m =3 distinct contexts, embedding dimension d =2, vocabulary of ∣V∣ =5 words and support sets of length ∣Sj∣ =3, j∈[3]. Left: Vis. of context embeddings ¯hj in circle black markers (marked as A,B,C) and of their associated support sets Sj (colored text below each marker). Colored vectors (star markers) represent max-NTP-margin vectors w⊺ v ∶=e⊺ vWmm, v∈[5] found by GD. Interpreting decoder vectors as word embeddings leads to intuitive findings on their geometry learned by NTP training. E.g., word embedding w3 (almost) aligns with context-embedding A and the normal hyperplane it defines separates A from B and C, since word 3 only appears after context A. The rest of the words follow two contexts each and their word-representation naturally belongs to the cone defined by the embeddings of those respective contexts. The wider the cone, the larger the magnitude of the word embedding to compensate for the large angle between context-representations that share the same next-word. Note that geometry of depicted word embeddings only depends on support sets, but the conditional probabilities define another set of word representations on an orthogonal (matrix) subspace; see text for details and vis. Right: Upper/lower graphs confirm the predictions of Lemma 2 and of Theorem 2, respectively. be directly derived from theirs since our setting is neither binary nor one-hot. Nevertheless, our proofs extend the foundational work of [ 72, 36, 39], akin to numerous other studies that explore extensions to nonlinear architectures[52, 37, 30, 31, 87, 93], and to stochastic and adaptive algorithms [63, 68, 21, 49, 81, 3, 14, 2]. The implicit bias viewpoint has also created opportunities to study generalization in overparameterized settings. [ 33, 4, 60, 22] build a two-stage approach initially leveraging implicit bias to simplify the complexities of optimization before addressing generalization. This narrows the generalization question to the properties of the corresponding max-margin classifier [61, 13, 45, 82, 24, 105, 76, 98]. The same strategy has also been adopted to study model robustness to adversarial perturbations [35, 84, 16], out-of-distribution data [91], and imbalances [73, 15, 44]. Our results motivate such extensions in the richer NTP setting. Recent work [51] also studies forms of implicit bias for language models trained to reach the risk lower bound. However, they assume training with population loss and analyze implicit bias through Hessian-trace minimization without providing explicit parameter characterizations as in Thm. 2. Crucially, their results do not apply to CE loss9 or to sparse support-sets. Another interesting work [55] studies learning abilities of autoregressive training and inference. However, their findings do not apply to NTP as they inherently assume each context is followed by a unique next token. Finally, although stemming from different perspectives, the form of our convergence results echoes a recent conjecture by [86] regarding implicit optimization bias in transformers. Unlike their conjecture, which focuses on binary classification, our results are rigorously proven and apply to the NTP setting. Further detailed discussion on related follow-up work on implicit optimization bias in self-attention architectures, as initiated by [ 87], is deferred to Appendix B. In contrast to this line of work, we here focus on the optimization biases of the NTP training-paradigm itself, which is orthogonal to the intricacies of the specific architecture generating the context embeddings. 9[51, Thm. 4.3] uses [49, Cor. 5.2], which applies to regression on scalar labels; thus is not applicable in NTP. 97 Conclusion, limitations and future work Towards characterizing implicit regularization effects, we highlight two key aspects of NTP training: (i) Formulating it as CE optimization over distinct contexts; this is long recognized in language modeling (e.g., [46, 67]) since Shannon’s initial work, yet seemingly overlooked in recent studies, such as [51, 55]. (ii) Accounting for sparsity in the matrix of next-token conditional probabilities. While traditional language modeling techniques often mitigate sparsity using smoothing heuristics that assign non-zero probabilities to unobserved next tokens [46, 67, 41], we recognize sparsity as a critical factor in NTP optimization that influences parameter divergence10. As the first study of implicit biases in NTP training, our results are based on several assumptions essential for establishing an initial foundational understanding. The framework allows for various exciting promising research directions, some of which we outline below. Even within the assumed linear setting and GD, interesting directions involve: ●NTP-separability thresholds: Identifying exact thresholds for NTP-separability under distribu- tional assumptions, akin to previous work on one-hot separability (Remark 1). However, relaxing the overparameterization requirement that the embedding dimension d be proportional to the number of distinct contexts m would necessitate exploring non-convex architectures (see ’Memory capacity’ below). ●Generalization: Studying generalization in NTP settings by examining statistical properties of the NTP-SVM solution. Past research has successfully undertaken similar investigations for one-hot classification (see Sec. 6). While we acknowledge the importance of addressing specific challenges inherent to NTP —such as determining an appropriate measure of generalization, or establishing suitable statistical models for context-embeddings that respect the discrete nature of the underlying token subsequences—we believe this direction holds promise for further exploration. In addition to these, essential extensions include relaxing the linearity assumption. ● Architecture-specific embeddings: A bottom-up approach considering architecture-specific embeddings could begin by modeling the embeddings produced by, for instance, a shallow transformer and analyzing the effects of optimization biases on the training of both the transformer and the decoder weights. This complements the works of [ 87, 86], who investigate one-layer self-attention with a fixed decoder. A challenge in this approach is balancing the restriction to shallow transformers (for analytical tractability) with ensuring that the NTP loss reaches the entropy lower bound. This may require constraining the training data distribution, for example, to a Markov chain [54, 26]. ●Memory capacity in NTP settings: Without imposing further restrictions on the data beyond the discrete nature of tokens from a finite vocabulary, there is a strong case for investigating the memory capacity of sequence-to-sequence architectures, such as transformers, in the context of NTP. Recent studies on transformer memory capacity [42, 43] do not apply here. The interested reader is refered to follow-up work [53] for a formal definition of memory capacity in the NTP setting and initial results in this direction. ●Unconstrained features: Extending the top-down approach, one could consider freely optimizing context embeddings together with decoder vectors (also known as word embeddings). The resulting log-bilinear model, reminiscent of wor2vec models [67, 58], extends the unconstrained features model, which has recently been employed to investigate neural collapse geometry in one-hot classification settings [59]. This idea offers a promising avenue for uncovering structures in the geometries of context and word embeddings when learned jointly, potentially revealing new insights into the capabilities of sufficiently expressive language models (see Fig. 1 for cases involving only the latter). We refer the interested reader to [104] for follow up work in this direction. ●Other optimizers: Exploring the NTP implicit bias of adaptive algorithms, such as Adam, poten- tially building on recent works in this area focused on one-hot classification [100, 99]. We hope this work inspires further research in the discussed directions, contributing to a deeper understanding of the intricacies involved and potentially yielding improvements in NTP training. 10Parameter divergence in transformer-based language models has been empirically observed in [56]. 10Acknowledgements Thank you to Tina Behnia, Yize Zhao, Vala Vakilian, and Puneesh Deora for inspiring discussions that contributed to this work and for their valuable suggestions on the manuscript. I am also grateful to Gautam Goel for his careful reading and for pointing out several typos. Thanks to the anonymous reviewers for their feedback. This work is supported by the NSERC Discovery Grant No. 2021-03677, the Alliance Grant ALLRP 581098-22, NFRFE-2023-00936, and a CIFAR AI Catalyst Grant. References [1] Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview. net/forum?id=0g0X4H8yN4I. [2] Maksym Andriushchenko and Nicolas Flammarion. Towards understanding sharpness-aware minimization. In International Conference on Machine Learning, pages 639–668. PMLR, 2022. [3] Navid Azizan, Sahin Lale, and Babak Hassibi. Stochastic mirror descent on overparameterized nonlinear models. IEEE Transactions on Neural Networks and Learning Systems , 33(12): 7717–7727, 2021. [4] Peter L Bartlett, Philip M Long, Gábor Lugosi, and Alexander Tsigler. Benign overfitting in linear regression. arXiv preprint arXiv:1906.11300, 2019. [5] Maximilian Beck, Korbinian Pöppel, Markus Spanring, Andreas Auer, Oleksandra Prud- nikova, Michael Kopp, Günter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. xlstm: Extended long short-term memory. arXiv preprint arXiv:2405.04517, 2024. [6] Mikhail Belkin. The necessity of machine learning theory in mitigating ai risk. ACM/JMS Journal of Data Science, 2024. [7] Mikhail Belkin, Alexander Rakhlin, and Alexandre B Tsybakov. Does data interpolation contradict statistical optimality? arXiv preprint arXiv:1806.09471, 2018. [8] Samy Bengio and Yoshua Bengio. Taking on the curse of dimensionality in joint distributions using neural networks. IEEE Transactions on Neural Networks, 11(3):550–557, 2000. [9] Yoshua Bengio, Réjean Ducharme, and Pascal Vincent. A neural probabilistic language model. Advances in neural information processing systems, 13, 2000. [10] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021. [11] Burak Çakmak, Yue M Lu, and Manfred Opper. A convergence analysis of approximate message passing with non-separable functions and applications to multi-class classification. arXiv preprint arXiv:2402.08676, 2024. [12] Emmanuel J Candès and Pragya Sur. The phase transition for the existence of the maximum likelihood estimate in high-dimensional logistic regression. arXiv preprint arXiv:1804.09753, 2018. [13] Yuan Cao, Quanquan Gu, and Mikhail Belkin. Risk bounds for over-parameterized maximum margin classification on sub-gaussian mixtures. Advances in Neural Information Processing Systems, 34:8407–8418, 2021. [14] Matias D Cattaneo, Jason M Klusowski, and Boris Shigida. On the implicit bias of adam. arXiv preprint arXiv:2309.00079, 2023. 11[15] Niladri S Chatterji, Philip M Long, and Peter L Bartlett. When does gradient descent with logistic loss find interpolating two-layer networks? The Journal of Machine Learning Research, 22(1):7135–7182, 2021. [16] Jinghui Chen, Yuan Cao, and Quanquan Gu. Benign overfitting in adversarially robust linear classification. In Uncertainty in Artificial Intelligence, pages 313–323. PMLR, 2023. [17] Sitan Chen and Yuanzhi Li. Provably learning a multi-head attention layer. arXiv preprint arXiv:2402.04084, 2024. [18] Katherine M Collins, Umang Bhatt, and Adrian Weller. Eliciting and learning with soft labels from every annotator. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, volume 10, pages 40–52, 2022. [19] Elisabetta Cornacchia, Francesca Mignacco, Rodrigo Veiga, Cédric Gerbelot, Bruno Loureiro, and Lenka Zdeborová. Learning curves for the multi-class teacher–student perceptron.Machine Learning: Science and Technology, 4(1):015019, 2023. [20] Thomas M Cover. Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition. IEEE transactions on electronic computers, pages 326–334, 1965. [21] Alex Damian, Tengyu Ma, and Jason D Lee. Label noise sgd provably prefers flat global minimizers. Advances in Neural Information Processing Systems, 34:27449–27461, 2021. [22] Zeyu Deng, Abla Kammoun, and Christos Thrampoulidis. A model of double descent for high-dimensional binary linear classification. Information and Inference: A Journal of the IMA, 11(2):435–495, 2022. [23] Puneesh Deora, Rouzbeh Ghaderi, Hossein Taheri, and Christos Thrampoulidis. On the optimization and generalization of multi-head attention. arXiv preprint arXiv:2310.12680, 2023. [24] Konstantin Donhauser, Nicolo Ruggeri, Stefan Stojanovic, and Fanny Yang. Fast rates for noisy interpolation require rethinking the effect of inductive bias. In International Conference on Machine Learning, pages 5397–5428. PMLR, 2022. [25] Benjamin L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive biases and variable creation in self-attention mechanisms. arXiv preprint arXiv:2110.10090, 2021. [26] Benjamin L Edelman, Ezra Edelman, Surbhi Goel, Eran Malach, and Nikolaos Tsilivis. The evolution of statistical induction heads: In-context learning markov chains. arXiv e-prints, pages arXiv–2402, 2024. [27] Daniel Y Fu, Tri Dao, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher Ré. Hungry hungry hippos: Towards language modeling with state space models. arXiv preprint arXiv:2212.14052, 2022. [28] Borjan Geshkovski, Cyril Letrouit, Yury Polyanskiy, and Philippe Rigollet. A mathematical perspective on transformers. arXiv preprint arXiv:2312.10794, 2023. [29] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [30] Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in terms of optimization geometry. In International Conference on Machine Learning, pages 1832–1841. PMLR, 2018. [31] Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent on linear convolutional networks. Advances in Neural Information Processing Systems, 31:9461–9471, 2018. [32] Peter E Hart, David G Stork, and Richard O Duda. Pattern classification. Wiley Hoboken, 2000. 12[33] Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in high-dimensional ridgeless least squares interpolation. arXiv preprint arXiv:1903.08560 , 2019. [34] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015. [35] Adel Javanmard and Mahdi Soltanolkotabi. Precise statistical analysis of classification accura- cies for adversarial training. The Annals of Statistics, 50(4):2127–2156, 2022. [36] Ziwei Ji and Matus Telgarsky. Risk and parameter convergence of logistic regression. arXiv preprint arXiv:1803.07300, 2018. [37] Ziwei Ji and Matus Telgarsky. Directional convergence and alignment in deep learning. Advances in Neural Information Processing Systems, 33:17176–17186, 2020. [38] Ziwei Ji and Matus Telgarsky. Characterizing the implicit bias via a primal-dual analysis. In Algorithmic Learning Theory, pages 772–804. PMLR, 2021. [39] Ziwei Ji, Miroslav Dudík, Robert E Schapire, and Matus Telgarsky. Gradient descent follows the regularization path for general losses. InConference on Learning Theory, pages 2109–2136. PMLR, 2020. [40] Ziwei Ji, Nathan Srebro, and Matus Telgarsky. Fast margin maximization via dual acceleration. In International Conference on Machine Learning, pages 4860–4869. PMLR, 2021. [41] Daniel Jurafsky and James H. Martin. Speech and Language Processing. Draft, 3 edition, 2023. URL https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf. [42] Tokio Kajitsuka and Issei Sato. Are transformers with one layer self-attention using low-rank weight matrices universal approximators? 2024. [43] Junghwan Kim, Michelle Kim, and Barzan Mozafari. Provable memorization capacity of transformers. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=8JCg5xJCTPR. [44] Ganesh Ramachandra Kini, Orestis Paraskevas, Samet Oymak, and Christos Thrampoulidis. Label-imbalanced and group-sensitive classification under overparameterization. Advances in Neural Information Processing Systems, 34:18970–18983, 2021. [45] Frederic Koehler, Lijia Zhou, Danica J Sutherland, and Nathan Srebro. Uniform convergence of interpolators: Gaussian width, norm bounds and benign overfitting. Advances in Neural Information Processing Systems, 34:20657–20668, 2021. [46] Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. Advances in neural information processing systems, 27, 2014. [47] Yingcong Li, M. Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as algorithms: Generalization and stability in in-context learning, 2023. [48] Yingcong Li, Yixiao Huang, Muhammed E Ildiz, Ankit Singh Rawat, and Samet Oymak. Mechanics of next token prediction with self-attention. In International Conference on Artificial Intelligence and Statistics, pages 685–693. PMLR, 2024. [49] Zhiyuan Li, Tianhao Wang, and Sanjeev Arora. What happens after sgd reaches zero loss?–a mathematical framework. arXiv preprint arXiv:2110.06914, 2021. [50] Valerii Likhosherstov, Krzysztof Choromanski, and Adrian Weller. On the expressive power of self-attention matrices, 2021. [51] Hong Liu, Sang Michael Xie, Zhiyuan Li, and Tengyu Ma. Same pre-training loss, better downstream: Implicit bias matters for language models. In International Conference on Machine Learning, pages 22188–22214. PMLR, 2023. 13[52] Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks. In International Conference on Learning Representations, 2020. [53] Liam Madden, Curtis Fox, and Christos Thrampoulidis. Upper and lower memory capacity bounds of transformers for next-token prediction. arXiv preprint arXiv:2405.13718, 2024. [54] Ashok Vardhan Makkuva, Marco Bondaschi, Adway Girish, Alliot Nagle, Martin Jaggi, Hyeji Kim, and Michael Gastpar. Attention with markov: A framework for principled analysis of transformers via markov chains. arXiv preprint arXiv:2402.04161, 2024. [55] Eran Malach. Auto-regressive next-token predictors are universal learners. arXiv preprint arXiv:2309.06979, 2023. [56] William Merrill, Vivek Ramanujan, Yoav Goldberg, Roy Schwartz, and Noah A Smith. Effects of parameter norm growth during transformer training: Inductive bias from gradient descent. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1766–1781, 2021. [57] Francesca Mignacco, Florent Krzakala, Yue M Lu, and Lenka Zdeborová. The role of regularization in classification of high-dimensional noisy gaussian mixture. arXiv preprint arXiv:2002.11544, 2020. [58] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013. [59] Dustin G Mixon, Hans Parshall, and Jianzong Pi. Neural collapse with unconstrained features. arXiv preprint arXiv:2011.11619, 2020. [60] Andrea Montanari, Feng Ruan, Youngtak Sohn, and Jun Yan. The generalization error of max-margin linear classifiers: High-dimensional asymptotics in the overparametrized regime. arXiv preprint arXiv:1911.01544, 2019. [61] Vidya Muthukumar, Adhyyan Narang, Vignesh Subramanian, Mikhail Belkin, Daniel Hsu, and Anant Sahai. Classification vs regression in overparameterized regimes: Does the loss function matter? arXiv preprint arXiv:2005.08054, 2020. [62] Mor Shpigel Nacson, Nathan Srebro, and Daniel Soudry. Stochastic gradient descent on separable data: Exact convergence with a fixed learning rate. arXiv preprint arXiv:1806.01796, 2018. [63] Mor Shpigel Nacson, Jason Lee, Suriya Gunasekar, Pedro Henrique Pamplona Savarese, Nathan Srebro, and Daniel Soudry. Convergence of gradient descent on separable data. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 3420–3428. PMLR, 2019. [64] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014. [65] OpenAI. Openai: Introducing chatgpt, 2022. URL https://openai.com/blog/chatgpt, 2022. [66] Samet Oymak, Ankit Singh Rawat, Mahdi Soltanolkotabi, and Christos Thrampoulidis. On the role of attention in prompt-tuning. In International Conference of Machine Learning (ICML), 2023. [67] Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532–1543, 2014. [68] Scott Pesme, Loucas Pillaud-Vivien, and Nicolas Flammarion. Implicit bias of sgd for diagonal linear networks: a provable benefit of stochasticity.Advances in Neural Information Processing Systems, 34:29218–29230, 2021. 14[69] Joshua C Peterson, Ruairidh M Battleday, Thomas L Griffiths, and Olga Russakovsky. Human uncertainty makes classification more robust. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9617–9626, 2019. [70] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. OpenAI blog, 2018. [71] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [72] Saharon Rosset, Ji Zhu, and Trevor J. Hastie. Margin maximizing loss functions. In NIPS, 2003. [73] Shiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and Percy Liang. An investigation of why overparameterization exacerbates spurious correlations. In International Conference on Machine Learning, pages 8346–8356. PMLR, 2020. [74] Arda Sahiner, Tolga Ergen, Batu Ozturkler, John Pauly, Morteza Mardani, and Mert Pilanci. Unraveling attention via convex duality: Analysis and interpretations of vision transformers. International Conference on Machine Learning, 2022. [75] Fariborz Salehi, Ehsan Abbasi, and Babak Hassibi. A precise analysis of phasemax in phase retrieval. In 2018 IEEE International Symposium on Information Theory (ISIT), pages 976–980. IEEE, 2018. [76] Ohad Shamir. The implicit bias of benign overfitting. In Conference on Learning Theory, pages 448–478. PMLR, 2022. [77] Claude E Shannon. Prediction and entropy of printed english. Bell system technical journal, 30(1):50–64, 1951. [78] Claude Elwood Shannon. A mathematical theory of communication. The Bell system technical journal, 27(3):379–423, 1948. [79] Viktoriia Sharmanska, Daniel Hernández-Lobato, Jose Miguel Hernandez-Lobato, and Novi Quadrianto. Ambiguity helps: Classification with disagreements in crowdsourced annotations. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2194–2202, 2016. [80] Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):2822–2878, 2018. [81] Haoyuan Sun, Kwangjun Ahn, Christos Thrampoulidis, and Navid Azizan. Mirror descent maximizes generalized margin and can be implemented efficiently. Advances in Neural Information Processing Systems, 35:31089–31101, 2022. [82] Pragya Sur and Emmanuel J Candès. A modern maximum-likelihood theory for high- dimensional logistic regression. Proceedings of the National Academy of Sciences , page 201810420, 2019. [83] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Re- thinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2818–2826, 2016. [84] Hossein Taheri, Ramtin Pedarsani, and Christos Thrampoulidis. Asymptotic behavior of adversarial training in binary linear classification. IEEE Transactions on Neural Networks and Learning Systems, 2023. [85] Kai Tan and Pierre C Bellec. Multinomial logistic regression: Asymptotic normality on null covariates in high-dimensions. arXiv preprint arXiv:2305.17825, 2023. [86] Davoud Ataee Tarzanagh, Yingcong Li, Christos Thrampoulidis, and Samet Oymak. Trans- formers as support vector machines, 2023. 15[87] Davoud Ataee Tarzanagh, Yingcong Li, Xuechen Zhang, and Samet Oymak. Max-margin token selection in attention mechanism, 2023. [88] Yuandong Tian, Yiping Wang, Beidi Chen, and Simon Du. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer, 2023. [89] Yuandong Tian, Yiping Wang, Zhenyu Zhang, Beidi Chen, and Simon Du. Joma: Demys- tifying multilayer transformers via joint dynamics of mlp and attention. arXiv preprint arXiv:2310.00535, 2023. [90] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [91] Nilesh Tripuraneni, Ben Adlam, and Jeffrey Pennington. Overparameterization improves robustness to covariate shift in high dimensions. Advances in Neural Information Processing Systems, 34:13883–13897, 2021. [92] Vladimir N Vapnik and Alexey Ya Chervonenkis. A note on one class of perceptrons. Automa- tion and Remote Control, 25:774–780, 1964. [93] Bhavya Vasudeva, Puneesh Deora, and Christos Thrampoulidis. Implicit bias and fast conver- gence rates for self-attention. arXiv preprint arXiv:2402.05738, 2024. [94] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [95] Sharan Vaswani, Francis Bach, and Mark Schmidt. Fast and faster convergence of sgd for over- parameterized models and an accelerated perceptron. In The 22nd international conference on artificial intelligence and statistics, pages 1195–1204. PMLR, 2019. [96] R. Vershynin. Lectures in geometric functional analysis. Unpublished manuscript. Available at http://www-personal. umich. edu/romanv/papers/GFA-book/GFA-book. pdf, 2011. [97] Johannes von Oswald, Eyvind Niklasson, E. Randazzo, João Sacramento, Alexander Mordv- intsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. ArXiv, abs/2212.07677, 2022. [98] David Wu and Anant Sahai. Precise asymptotic generalization for multiclass classification with overparameterized linear models. Advances in Neural Information Processing Systems, 36, 2024. [99] Shuo Xie and Zhiyuan Li. Implicit bias of adamw: /ell_/infty norm constrained optimization. arXiv preprint arXiv:2404.04454, 2024. [100] Chenyang Zhang, Difan Zou, and Yuan Cao. The implicit bias of adam on separable data. arXiv preprint arXiv:2406.10650, 2024. [101] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understand- ing deep learning requires rethinking generalization, 2017. [102] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017. [103] Ruiqi Zhang, Spencer Frei, and Peter L. Bartlett. Trained transformers learn linear models in-context, 2023. [104] Yize Zhao, Tina Behnia, Vala Vakilian, and Christos Thrampoulidis. Implicit geometry of next-token prediction: From language sparsity patterns to model representations. In First Conference on Language Modeling, 2024. [105] Lijia Zhou, Danica J Sutherland, and Nati Srebro. On uniform convergence and low-norm interpolation learning. Advances in Neural Information Processing Systems, 33:6867–6877, 2020. 16-3 -2.5 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2-1 -0.5 0 0.5 1 1.5 w1 w2w3 w4 w5 125 145 234A B C -3 -2.5 -2 -1.5 -1 -0.5 0 0.5 1 1.5-1 -0.5 0 0.5 1 1.5 w1 w2 w3 w4 w5 125 145 234A B C 12345 Words A B C Contexts 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Figure 2: Same setup as Fig. 1. Left: Matrix P of conditional probabilities of words (cols.) per context (rows). Each row corresponds to the conditional probability vectors pj, j∈ [m]. Black entries correspond to off-support words. Middle: Shown as wz, z∈[5], the rows of the NTP-SVM solution Wmm to which GD directionally converges. Right: Shown as wz, z∈ [5], the rows of the finite parameter W⋆ to which GD iterates projected on F converge to. The geometry of Wmm depends only on the support-set of P. On the other hand, the geometry of W⋆ depends on the entries of P for in-support tokens/words. As seen from visualization of P, the words 1 and 5 have the same support pattern (i.e., both follow the same contexts A and B). Thus, w1 = w5 in the Middle plot. However, on the subspace F corresponding to the Right plot, w1 ≠w5, which allows matching the different conditional probabilities with which each follows contexts A and B. A Experiments All experiments were conducted on a MacBook Pro equipped with a 2.3 GHz Quad-Core Intel Core i7 processor and 32 GB of memory. The experiments are of relatively small scale and were implemented in Matlab. The code is straightforward to reproduce, following the detailed specifications provided in the subsequent sections. For completeness, the code will be made publicly available on Github in the final version of the paper. A.1 Additional details on 2D example of Fig. 1 Figure 1 illustrates a toy 2d example where the embeddings and the hyperplanes defined by each row of Wmm can be visualized. We used d = 2, m= 3, V= 5 and S1 = S2 = S3 = 3. The support sets of each embedding are shown in the figure color-coded to match the respective decoder hyperplane. Probabilities are assigned randomly. The empirical conditional entropy evaluates to H =0.8811 and the matrix of conditional probabilities is visualized in Figure 2. In the same figure, we also visualize the rows of the directional component Wmm (Middle) and of the finite component W⋆ (Right). Interpreting the V ×d decoder matrix as the matrix of learned word embeddings, this provides a visualization of their geometry. As per our results, the two word-embedding matrices W⋆ and Wmm lie on orthogonal subspaces. The geometry of the first depends on the probabilities of in-support tokens, while that of the second depends only on the support set of these probabilities. See also caption of Fig. 2. A.2 Overparameterized setting We examine the implicit bias of GD on NTP training with overparameterization on synthetic data generated as follows. We construct dataset with n = 5000 sequences involving m = 50 distinct contexts. Each distinct context gets mapped to a randomly generated embedding of dimension d =60 >m. We set vocabulary sizeV =10 and each context j ∈[m] is followed by Sj =6, ∀j ∈[m] possible next-tokens. The support sets Sj ⊂V and the probabilities ˆpj,z, z∈Sj are chosen randomly; see Fig. 3 for representative examples from the training dataset. For a fixed realization of the dataset (for which H ≈ 1.445nats), we run GD, normalized GD (NGD), and Adam from random LeCun initialization. For GD, we use learning rate η = 0.5 and for NGD and Adam η = 0.01. For Adam, we also set β1 =0.9, β2 =0.99. We run all algorithms for 1e4 iterations. For each case, we plot the following as a function of iterations: 1. Upper Left: CE loss versus entropy lower bound 2. Upper Right: parameter norm growth 173. Lower Left: correlation of Wmm with iterates Wk and of “corrected” iterates Wk −W⋆ after substracting the component on H 4. Lower Right: convergence of the subspace component Wk,F =PF(Wk). Fig. 4 shows an instance of these. As predicted by our analysis, in this overparameterized setting: CE loss converges to its lower-bound, parameter norm increases, iterates align in direction withWmm, and the subspace component converges to W⋆. Figure 3: Eight randomly picked contexts with their associated next-token empirical conditional probabilities ˆpj. The indices shown on the x-axis define the support set Sj of each context. Figure 5 illustrates the same plots, but this time for training over the same dataset with NGD and Adam. We observe same implicit bias, but faster convergence. For NGD, this is consistent with analogous findings (rigorous in that case) for one-hot classification [63, 38]. ℱ Figure 4: Experimental illustration of the implicit bias of GD in NTP over synthetic data with overparameterization. See App. A for detailed description of the experimental setting. The upper two graphs confirm the predictions of Lemma 2, while the lower two graphs adhere to the predictions of Theorem 2. 18ℱ NGD ℱ Adam Figure 5: Implicit bias of normalized GD (Left) and of Adam (Right) in NTP over synthetic data with overparameterization. Both exhibit the same implicit bias, but converge faster than GD, with Adam being slightly faster than NGD. B Additional related work Implicit bias in transformers. As already mentioned in Sec. 6, our work is closely related to [86], where the authors investigate the implicit bias of self-attention in transformers. The insight put forth in the prequel [87] is that softmax attention induces implicit-bias behaviors that bear similarities to vanilla implicit bias of one-hot prediction. Concretely, [86] studies GD optimization of one-layer self-attention with fixed decoder and one-hot binary classification. They show that, in the limit, GD finds attention weights that converge in direction to the solution of an SVM problem that separates optimal tokens from non-optimal ones. Their non-convex setting introduces locally optimal SVM directions to which GD may converge depending on initialization. Different to them, the NTP setting that we study involves predictions over multiple categories and isnot one-hot. Also, while they fix the decoder, here, we fix the embeddings. In these respects their results are rather different. More similarities arise when [ 86] replace the linear decoder with a MLP, which they note can induce multiple optimal tokens per sequence. This leads them to formulate a more general token-separating SVM program, which similar to ours confines the separation on a certain data subspace. However, the operational nature of the programs remains different as theirs optimizes attention weights and separates tokens within a sequence, while ours optimizes decoder weights and separates context embeddings based on their respective support sets. More importantly, while [86] only conjectures the convergence of GD to their general SVM program, we leverage convexity in our setting to prove an analogous statement rigorously. Eventually, as we move lower in our top-down approach and consider architecture-specific embeddings generated by attention, we anticipate to see integration of our ideas with theirs. Beyond [86], there is growing recent research investigating optimization and generalization principles of transformers, e.g., [74, 25, 50, 97, 103, 1, 47, 66, 87, 86, 23, 88, 17, 28] These efforts predominantly employ a ‘bottom-up’ approach that involves isolating shallow transformers, often with simplifications such as removing MLPs, utilizing single heads instead of multiple, and fixing certain parts while training only a subset of trainable parameters. Most of these studies have focused on classical one-hot supervised settings, and only a handful (e.g., [ 88, 89]) have seeked extending these ’bottom-up’ analyses to NTP settings. Yet, their primary emphasis remains on uncovering the role of attention and how attention weights evolve during training. Instead, our approach uniquely emphasizes the NTP training paradigm itself, shifting the focus from the intricacies of specific transformer architectures. Upon completing this paper, we became aware of independent contemporaneous research by Li et al. [48] that also examines the implicit bias of self-attention with a fixed linear decoder in next-token prediction scenarios. Unlike our study which utilizes the widely adopted CE loss, their approach is based on log-loss, which renders the training loss convex, a similarity shared with our model despite the inclusion of self-attention. Both our results and those of Li et al. substantiate the conjecture posited by Tarzanagh and colleagues [86], albeit in very distinct settings. Notably, contrary to both 19[87] and [48], we unveil the optimization intricacies of the NTP paradigm, even within the simplest linear settings. Classification with soft labels. Unlike one-hot classification, soft-label classification associates each example with a probability vector, where each entry represents the likelihood of a corresponding label characterizing the example. Although arguably less prevalent than one-hot (or hard-label) classification, soft-label classification arises in various contexts, including modeling human confusion during crowd-sourcing [69, 79, 18], knowledge distillation [34], label smoothing [83], and mixup [102]. Our model of last-token prediction also falls within this setting. Specifically, our approach is most closely related to soft-labels generated by averaging annotators’ hard labels [ 69], rather than following the winner-takes-all rule to assign labels. [ 69] and follow-up work have provided empirical evidence that using probabilistic soft labels generated from crowd annotations for training leads to improved performance in terms of model generalization, calibration, and robustness to out-of-distribution data. To the best of our knowledge, no prior work has investigated the implicit bias of gradient descent in this or other soft-label classification settings; thus, our results are of direct relevance to these contexts as well. C Autoregressive setting For concreteness and simplified notation, in the paper’s main body we focus on NTP over sequences of fixed length. We show here that this encompasses the autoregressive (i.e., sequential) setting with minimal changes. This also emphasizes the role played in our results by the sequence length. As pointed in (1), the full autoregressive NTP objective averages T individual losses (without loss of generality assume sequences of equal maximum length T). In order to make our analysis applicable, we first need to express (1) in terms of unique contexts. Mirroring the notations in Sec. 2, define the following for t ∈[T −1]: • mt, t∈[T −1] is the number of distinct contexts of size t. Note that m1 ≥m2 ≥⋯≥mT−1. • m =∑T−1 t=1 mt is the total number of distinct contexts in the dataset • ¯ht,j ∶=hθ(¯xj,t), t∈[T −1], j∈[mt] is the embedding of the j-th (among all t-long contexts) distinct context ¯xj,t. • ˆπj,t is the empirical probability of ¯xj,t. • ˆpj,t,z is the empirical probability that context ¯xj,t is followed by token z ∈V. • Sj,t is the support set of the next-token distribution of context ¯xj,t. With this notation, the NTP objective becomes CE =− ∑ t∈[T−1] ∑ j∈[mt] ˆπt,j ∑ z∈Sj,t ˆpt,j,z log (Sz(W ¯ht,j)) . To continue enumerate the multi-set I ∶={i =(j, t)∣ t ∈[T −1], j∈[mt]}. We may then rewrite the above as CE =−∑ i∈I ˆπi ∑ z∈Si ˆpi,z log (Sz(W ¯hi)) . At this point note that this is of identical form to (2). Consequently, the definitions (e.g., NTP- separability, NTP-margin) and results derived in the main body for sequences of fixed length are applicable to the AR setting, extending mutatis mutandis. Remark 2 (The role of sequence length.) . Despite the above reduction of the AR setting to the fixed-length setting, it is crucial to recognize that sequence length remains a significant factor in the AR model. Specifically, it influences the formulation through support sets and their associated probabilities. As sequences extend in length, their corresponding support sets generally become sparser, indicative of less ambiguity in predicting the next token. This dynamic is captured by Shannon’s inequality, Ht ≥Ht+1, where Ht =− ∑ j∈[mt] ∑ z∈Sℓ t,j πt,j ˆpt,j,z log(ˆpt,j,z), reflecting the incremental reduction in entropy as sequence length increases. 20D Notations Throughout, lowercase and uppercase bold letters (e.g., a and A) represent vectors and matrices, respectively. ⟨⋅, ⋅⟩ and ∥⋅∥ denote Euclidean inner product and norm, respectively. For matrix A, we denote its pseudoinverse as A†. All logarithms are natural logarithms (base e). We denote ev the v-th standard basis vector in RV . ∆V −1 denotes the V -dimensional unit simplex and S() ∶RV →∆V −1 the softmax map: S(a) =[S1(a), . . . ,SV (a)]⊺, with Sv(a) = ee⊺ va ∑v′∈[V ] ee⊺ v′a . As explained in Section 2 we represent a training set as Tm ∶={(¯hj, ˆπj, ˆpj,z∈V)}j∈[m] . We assume that embeddings are bounded and denote M ∶= √ 2 max j∈[m] ∥¯hj∥. Given Tm, let F =span ({(ez −ez′)¯h⊺ j ∶ z ≠z′ ∈Sj, j∈[m]}) a subspace of V ×d matrices and F⊥ its orthogonal complement. Denote PF, P⊥ the orthogonal projections onto F and F⊥, respectively. For convenience, for W ∈RV ×d, we denote WF ∶=PF(W) and W⊥=P⊥(W). Define CEF(W) = ∑ j∈[m] ˆπj ∑ z∈Sj ˆpj,z log (1 +∑ z≠z e−(ez−ez′)⊺W ¯hj ) . (12) Clearly, for all W ∈RV ×d, it holds CE(W) ≥CEF(W). Note also that for all W ∈F and for all Wd ∈ F⊥that satisfy Eq. (6a), it holds CEF(W) = limR→∞CE(W +RWd). Thus, under NTP compatibility and NTP separability, inf W∈F CEF(W) =inf W CE(W) =H. (13) E Proofs E.1 Gradient Descent Throughout we assume GD is ran with step-size η ≤1/(2L) where L is the smoothness of CE loss. This condition is not explicitly mentioned thereafter. E.1.1 Auxiliary Lemmata The following result follows from standard optimization analysis for smooth convex functions specialized to functions that do not attain their infimum. The version presented here is adopted from Lemma 2 in [39]. Lemma 3. It holds lim k→∞ CE(Wk) =inf W CE(W) and also limk→∞∥Wk∥ =∞. In the lemma below, we collect some useful and simple-to-show properties of the GD and regular- ization paths. These are adaptations of corresponding results for one-hot binary classification over general non-separable data established in [36]. Lemma 4. Suppose conditions (6) hold for some Wd. Also, that there exists Wp = W⋆ ∈ F satisfying condition (4). The following hold: 211. CEF(W⋆) =infW∈F CEF(W) =H, 2. W⋆ is the unique minimizer of CEF on the subspace F, 3. limk→∞PF(Wk) =W⋆, where Wk are GD iterates, 4. limk→∞∥P⊥(Wk)∥ =∞, 5. limB→∞PF(̂WB) =W⋆, where ̂WB is the reguarlized solution (8), 6. limB→∞∥P⊥(̂WB)∥ =∞. Proof. It is easy to check by direct substitution of W⋆ in (12) and use of (4) that CEF(W⋆) =H. This and (13) show the first claim. The first claim shows W⋆ is a minimizer. Suppose for the sake of contradiction there is a different minimizer W⋆ ≠ W1 ∈ F. Then, since CEF(W1) = H, it also holds for WR ∶= W1 +RWd that limR→∞CE(WR) =H. In turn, this implies for all j ∈[m]: lim R→∞ Sz(WR¯hj) = ˆpj,z, ∀z ∈Sj , and lim R→∞ Sv(WR¯hj) =0, ∀v ∉Sj . The first condition gives then that W1 must satisfy (4). Since W⋆ also satisfies these equations, denoting W∆ =W⋆ −W1 ≠0, it holds: ⟨W∆, (ez −ez′)⊺¯hj)⟩ =0, ∀j ∈[m], z≠z′ ∈Sj. But W∆ ∈ F, so this forms a contradiction. Hence, W⋆ is unique solution in F of (4) and unique minimizer of CEF on the subspace F. The proof of the third claim follows the same way as the proof of part (1) of Thm. 15 of [ 39]. For completeness: It follows by the lemma’s assumptions and Lemma 3 that limk→∞CE(Wk) = H. Combining with the first claim of the lemma yields limk→∞CE(Wk) = CEF(W⋆). Since CEF(Wk) ≤CE(Wk), this finally gives lim k→∞ CEF(Wk) = lim k→∞ CEF (PF(Wk)) =CEF(W⋆). Since W⋆ is unique by the second claim, the desired then follows. For the fourth claim, recall from Lemma 3 thatlimk→∞∥Wk∥ =∞. From the previous claim, we also have limk→∞∥PF(Wk)∥ <C for some constant C >∥W⋆∥. Thus, the desired follows by applying the fact that ∥Wk∥ =∥PF(Wk)∥+∥P⊥(Wk)∥. The proof of the last two claim is exactly same as that of the third and fourth claim. Only now use the facts that limB→∞CE(WB) =H and limB→∞∥WB∥ =∞(see proof of Theorem 1). E.1.2 Key Lemma Lemma 5. Let Wk denote the GD iterate at iteration k. Recall the decomposition Wk =PF(Wk)+ P⊥(Wk) =Wk,F +Wk,⊥. Fix any α ∈(0, 1). There exists large enough R =R(α) and k0 =k0(R) such that for any k ≥k0, it holds that ∥Wk,⊥∥ ≥R and CE (Wk,F +(1 +α)∥Wk,⊥∥Wmm ) ≤CE(Wk). (14) Proof. We drop the subscript k to lighten notation. First, note by Lemma 4.D that, for arbitrary R, we can pick k1 = k1(R) such that for all k ≥ k1: ∥W⊥∥ ≥R. 22Thus next, we will prove the main claim, i.e. for large enough ∥W⊥∥ inequality (14) holds. Denote R′ = ∥W⊥∥ ∥Wmm∥. Substituting in CE expression (2), and using the fact that Wmm ∈F⊥by (6a) yield: CE (WF +(1 +α)R′Wmm ) = ∑ j∈[m] ˆπj ∑ z∈Sj ˆpj,z log ⎛ ⎝ ∑ z′∈Sj e−(ez−ez′)⊺WF¯hj + ∑ v∉Sj e−(ez−ev)⊺WF¯hj + ∑ v∉Sj e−(1+α)R′(ez−ev)⊺Wmm¯hj ⎞ ⎠ . = ∑ j∈[m] ˆπj ∑ z∈Sj ˆpj,z log ⎛ ⎝∑ v∈V e−(ez−ev)⊺WF¯hj + ∑ v∉Sj e−(1+α)R′(ez−ev)⊺Wmm¯hj ⎞ ⎠ . (15) Moreover, decomposing W =WF +W⊥, and defining ̃W⊥∶= ∥Wmm∥ ∥W⊥∥ W⊥ = 1 RW⊥, we have CE (W) = ∑ j∈[m] ˆπj ∑ z∈Sj ˆpj,z log ⎛ ⎝ ∑ z′∈Sj e−(ez−ez′)⊺WF¯hj + ∑ v∉Sj e−(ez−ev)⊺WF¯hj + ∑ v∉Sj e−R′(ez−ev)⊺ ̃W⊥ ¯hj ⎞ ⎠ = ∑ j∈[m] ˆπj ∑ z∈Sj ˆpj,z log ⎛ ⎝∑ v∈V e−(ez−ev)⊺WF¯hj + ∑ v∉Sj e−R′(ez−ev)⊺ ̃W⊥ ¯hj ⎞ ⎠ , (16) where we used that, by definition, W⊥∈F⊥. Thus, our goal becomes showing (15) ≤(16), for large enough R. To do this, we consider two cases as follows below. For the remaining of the proof recall M ∶=maxj∈[m] √ 2∥¯hj∥ and use the logits shorthand: ̃ℓj,v =e⊺ v ̃W⊥¯hj and ℓmm j,v =e⊺ vWmm¯hj . Case 1: W⊥is well aligned with Wmm. Suppose ∥Wmm −̃W⊥∥ ≤ϵ ∶= α M . (17) Using this, linearity of logits, and Cauchy-Schwartz, yields ̃ℓj,z −̃ℓj,v ≤ℓmm j,z −ℓmm j,v +ϵM, ∀j ∈[m], z∈Sj, v∉Sj . Thus, ∑ v∉Sj e−R′(ez−ev)⊺ ̃W⊥ ¯hj ≥e−ϵMR′ ∑ v∉Sj e−R′(ez−ev)⊺Wmm¯hj =e−αR′ ∑ v∉Sj e−R′(ez−ev)⊺Wmm¯hj Also recall by feasibility of Wmm that ℓmm j,z −ℓmm j,v ≥1, ∀j ∈[m], z∈Sj, v∉Sj . (18) Thus, ∑ v∉Sj e−(1+α)R′(ez−ev)⊺ ̃W⊥ ¯hj ≤e−αR′ ∑ v∉Sj e−R′(ez−ev)⊺Wmm¯hj Comparing the above two displays yields ∑ v∉Sj e−(1+α)R′(ez−ev)⊺ ̃W⊥ ¯hj ≤ ∑ v∉Sj e−R′(ez−ev)⊺ ̃W⊥ ¯hj , which implies the desired (15)≤(16) for any value of R′ (eqv. ∥W⊥∥). Case 2: No alignment. Suppose now that (17) does not hold. Note that ∥̃W⊥∥ =∥Wmm∥ and since (NTP-SVM) has a unique solution it must be that ̃W⊥is not feasible. But ̃W⊥∈F⊥, thus it satisfies the equality constraints. This then means that there exist δ ∶=δ(ϵ) and j⋆ ∈[m], v⋆ ∉Sj⋆ such that ̃ℓj⋆,z −̃ℓj⋆,v⋆ ≤1 −δ , ∀z ∈Sj⋆. (19) 23(Note the above holds for all z ∈Sj⋆ because ̃ℓj⋆,z =̃ℓj⋆,z′ since ̃W⊥∈F⊥.) To continue, we introduce the shorthand notation Aj,z ∶=Aj,z(W) = ∑ v∈V e−(ez−ev)⊺WF¯hj as well as Amin ∶= min j∈[m],z∈Sj Aj,z, and Amax ∶= max j∈[m],z∈Sj Aj,z . Using (19) we may lower bound (16) as follows: CE(W)− ∑ j∈[m] ˆπj ∑ z∈Sj ˆpj,z log (∑ v∈V e−(ez−ev)⊺WF¯hj ) ≥ ˆπj⋆ ∑ z∈Sj ˆpj,z log ⎛ ⎝1 + e−R′(ez−ev⋆)⊺ ̃W⊥ ¯hj⋆ Aj⋆,z ⎞ ⎠ ≥ ˆπj⋆ ∑ z∈Sj ˆpj,z log (1 + e−R′(1−δ) Amax ) ≥ e−R′(1−δ) n(Amax +1) , (20) where in the last line we used ˆπj ≥1/n, ∀j ∈[m] as well as log(1 +x) ≥ x 1+x , x>0. On the other hand, using property (18) for max-margin logits, we can upper bound (15) as follows: CE (WF +(1 +α)R′Wmm )− ∑ j∈[m] ˆπj ∑ z∈Sj ˆpj,z log (∑ v∈V e−(ez−ev)⊺WF¯hj ) ≤log (1 + V e−R′(1+α) Amin ) ≤ V e−R′(1+α) Amin , (21) where in the last line we used log(1 +x) ≤x, x>0. In view of the two last displays, it suffices that V e−R′(1+α) Amin ≤ e−R′(1−δ) n(Amax +1) ⇐⇒ R′ ≥ 1 δ +α log (nV (Amax +1) Amin ) . All it remains is obtaining bounds for Amin, Amax specifically showing that they do not depend on R. By Cauchy-Schwartz: V e−M∥WF∥ ≤Amin ≤Amax ≤V eM∥WF∥ Further recall by Lemma 4.C that if k is large enough then ∥WF −W⋆∥ ≤∥W⋆∥ /Leftr⫯g⊸tl⫯ne⇒∥WF∥ ≤2∥W⋆∥. (22) Thus, there exists k⋆ =k⋆(∥W⋆∥) such that for all k ≥k⋆: V e−2M∥W⋆∥ ≤Amin ≤Amax ≤V e2M∥W⋆∥. Hence, the desired (21)≤(20) holds provided ∥W⊥∥ ≥ ∥Wmm∥ α log (2nV e4∥W⋆∥) . (23) Set R =R(α) ={RHS of (23)} and k0(R) ∶=max{k1(R), k⋆}. We have shown this guarantees for all k ≥k0: ∥W⊥∥ ≥R and by choice of R also (21)≤(20). This in turn implies (15)≤(16), as desired to complete the proof. E.1.3 Proof of Theorem 2 For the subspace component, see Lemma 4.C. For the directional convergence, the key ingredient of the proof is Lemma 5. After that, the proof follows identically to Thm. 15(2) in [ 39]. We include the details for completeness, but there are no novel aspects in the rest of this section. 24Let any ϵ ∈(0, 1) and choose α =ϵ/(1 −ϵ). By Lemma 5, there exists k0 such that for any k ≥k0, we have ∥Wk,⊥∥ ≥max{R(α), 1/2} and ⟨∇CE(Wk), Wk,⊥−(1 +α)∥Wk,⊥∥Wmm⟩ =⟨∇CE(Wk), Wk −(Wk,F +(1 +α)∥Wk,⊥∥Wmm)⟩ ≥CE(Wk)−CE(Wk,F +(1 +α)∥Wk,⊥∥Wmm) ≥0 , where we also used convexity of the loss. Consequently, ⟨Wk+1 −Wk, Wmm⟩ =⟨−η∇CE(Wk), Wmm⟩ ≥(1 −ϵ)⟨−η∇CE(Wk), Wk,⊥⟩ ≥(1 −ϵ)⟨Wk+1,⊥−Wk,⊥, Wk,⊥⟩ ≥(1 −ϵ)⟨Wk+1,⊥−Wk,⊥, Wk,⊥⟩ = (1 −ϵ) 2∥Wk,⊥∥ (∥Wk+1,⊥∥2 −∥Wk,⊥∥2 −∥Wk+1,⊥−Wk,⊥∥2) ≥(1 −ϵ)(∥Wk+1,⊥∥−∥Wk,⊥∥−2η(CE(Wk,⊥)−CE(Wk+1,⊥)), where the last step used ∥Wk,⊥∥ ≥1/2, the fact that x2 −y2 ≥2y(x −y), ∀x, yand smoothness of the CE loss. Telescoping the above expression and rearranging yields ⟨Wk, Wmm⟩ ≥(1 −ϵ)∥Wk,⊥∥ ∥Wk∥ − ⟨Wk0 , Wmm⟩−(1 −ϵ)∥wk0,⊥∥−η CE(Wk0 ) ∥Wk∥ ≥(1 −ϵ)− ∥Wk,F∥2 +⟨Wk0 , Wmm⟩−(1 −ϵ)∥wk0,⊥∥−η CE(Wk0 ) ∥Wk∥ Now recall from Lemma 4 that limk→∞∥Wk∥ =∞and limk→∞∥Wk,F∥ =∥W⋆∥. Thus, lim infk→∞⟨Wk, Wmm⟩ ≥1 −ϵ. Since ϵ is arbitrary, the desired follows. E.2 Regularization Path We provide a detailed proof of Theorem 1 filling in missing details from the proof sketch in the main paper. E.2.1 Proof of Theorem 1 First, we show that ̂WB is on the boundary, i.e. ∥̂WB∥ =B. Suppose not, then ⟨∇CE(̂WB), U⟩ =0 for all U ∈RV ×d. Using the CE expression in (2) and a few algebraic manipulations, yields ⟨−∇CE(̂WB), U⟩ = ∑ j∈[m] ˆπj ∑ z∈Sj ˆpj,z( ∑ z′∈Sj z′≠z sj,z′ (ez −ez′)⊺U¯hj + ∑ v∉Sj sj,v (ez −ev)⊺U¯hj), (24) where we denote the output probabilities at ̂WB as sj,v ∶= Sv(̂WB ¯hj), v∈ V, j∈ [m]. Choose U =Wmm in (24). Then, the first term in the parenthesis in (24) is zero by (6a), while the second term is strictly positive by (6b) and strict positivity of softmax entries, leading to contradiction. Now, consider point W⋆ B = W⋆ +R(B) ⋅Wmm, where, W⋆ ∈ F satisfies (4), and R = R(B) is chosen such that ∥W⋆ B∥ =B. Concretely, for B >∥W⋆∥, set R = 1 ∥Wmm∥ √ B2 −∥W⋆∥2. Note also that R/B →1/∥Wmm∥ as B →∞. We will show that W⋆ B attains a small CE loss as B (hence, R) grows. To do this, denote for convenience the logits for all v ∈V, j∈[m] ∶ ℓ⋆ j,v ∶=e⊺ vW⋆¯hj and ℓmm j,v ∶=e⊺ vWmm¯hj , 25and note that e⊺ vW⋆ B ¯hj =ℓ⋆ j,v +R ℓmm j,v . By using (4) and (6a): ∑ z′∈Sj e−(ℓ⋆ j,z+Rℓmm j,z −ℓ⋆ j,z′−Rℓmm j,z′) = 1 ˆpj . Moreover, using (6b) ∑ v∉Sj e−(ℓ⋆ j,z+Rℓmm j,z −ℓ⋆ j,v−Rℓmm j,v ) ≤e−R ∑ v∉Sj e−(ℓ⋆ j,z−ℓ⋆ j,v) ≤C e−R, where we define constant (independent of R) C ∶=V e∥W⋆∥M , for M ∶= √ 2 ⋅maxj/∈[m] ∥¯hj∥. Combining the above displays and using in Eq. (2), yields CE(W⋆ B) ≤ ∑ j∈[m] ˆπj ∑ z∈Sj ˆpj,z log ( 1 ˆpj,z +C e−R) ≤ ∑ j∈[m] ˆπj ∑ z∈Sj ˆpj,z(log ( 1 ˆpj,z )+ˆpj,zC e−R) ≤H +C e−R , (25) where, the second line uses log(1 +x) ≤x, x>0, and the third line uses ˆπj, ˆpj,z are probabilities. Next, towards arriving at a contradiction, we will show that if ̂WB is not in the direction of Wmm, then it incurs a loss that is larger than CE(W⋆ B). Concretely, assuming the statement of the theorem is not true, we we will upper bound CE(̂WB)−H = ∑ j∈[m] ˆπj ∑ z∈Sj ˆpj,z log ( ˆpj,z Sz(̂WB ¯hj) ). (26) By our assumption, there exists ϵ >0, such that there exists arbitrarily large B satisfying: ∥∥Wmm∥ B ̂WB −Wmm∥ >ϵ. (27) Define ̂W = 1 R′(B)(̂WB −W⋆), where, R′ =R′(B) >0 is chosen so that ∥̂W∥ =∥Wmm∥. Concretely, for large enough B ≥2∥W⋆∥, set R′ = 1 ∥Wmm∥ √ B2 −2B⟨WB, W⋆⟩+∥W⋆∥2 . Note that it holds limB→∞R′/B =1/∥Wmm∥. Thus, we can always choose B large enough so that Eq. (27) guarantees ∥̂W −Wmm∥ ≥ ϵ′, for some ϵ′ > 0. Since Wmm is the unique minimizer of (NTP-SVM) and ∥̂W∥ =∥Wmm∥, it follows that there exists δ ∈(0, 1) and j ∈[m] such that at least one of the following is true (i) ∃z and z′ ≠z ∈Sj such that ∣(ez −ez′)⊺̂W ¯hj∣ ≥δ , (28) (ii) ∃z ∈Sj, v∉Sj such that (ez −ev)⊺̂W ¯hj ≤1 −δ. (29) Case (i): Without loss of generality (ez −ez′)⊺̂W ¯hj ≤−δ (otherwise, flip z, z′). Thus, ignoring all but one term in (26) gives CE(̂WB)−H ≥ ˆπj ˆpj,z log ( ˆpj,z Sz(̂WB ¯hj) ) ≥ ˆπj ˆpj,z log (ˆpj,ze(ℓj,z′−ℓj,z)), (30) where we use ℓj,v =e⊺ v ̂WB ¯hj, v∈V to denote logits of ̂WB. Using (4) and (28), yields ℓj,z′ −ℓj,z =(ez′ −ez)⊺(R′ ̂W +W⋆)¯hj ≥R′δ +log (ˆpj,z′ ˆpj,z ) . 26Put in (26) and using ˆpj,z ≥ ˆπj ˆpj,z ≥1/n shows CE(̂WB) ≥H + 1 n log (eR′δ n ) Compare this with (25). For large enough B, it is clear that ˆπj ˆpj,z log (ˆpj,z c eR′δ) >Ce−R. Thus, CE(̂WB) >CE(W⋆ B), a contradiction. Case (ii): We can assume ̂W ∈F⊥, since otherwise we are in Case (i). Now, again ignoring all but the (j, z) term in the CE loss for which (29) holds for some v ∉Sj, we find CE(̂WB)−H ≥ ˆπj ˆpj,z log (ˆpj,z( ∑ z′∈Sj e(ℓj,z′−ℓj,z) +e(ℓj,v−ℓj,z))). Using PT (̂WB) =W⋆ yields ∑ z′∈Sj e(ℓj,z′−ℓj,z) = ∑ z′∈Sj ˆpj,z′ ˆpj,z = 1 ˆpj,z . Moreover, by (29): eℓj,v−ℓj,z ≥e−R′(1−δ) eℓ⋆ j,v−ℓ⋆ j,z ≥c′e−R′(1−δ), for constant (independent of B) c′ ∶=e−∥W⋆∥M . Putting the above together yield: CE(̂WB)−H ≥ ˆπj ˆpj,z log (1 +ˆpj,zc′e−R′(1−δ)) ≥ c′e−R′(1−δ) 2n2 . where the second inequality uses log(1 +x) ≥ x 1+x , x>0. Compare this with (25). For large enough B, (recall R, R′ grow at the same rate) it holds c′ 2n2 e−R′(1−δ) >Ce−R. Thus, CE(̂WB) >CE(W⋆ B), a contradiction. In either case, we arrive at a contradiction, which completes the proof. 27
```

## **Instructions:**
1. Analyze the paper based on the categories listed below.
2. Your response **must be a valid JSON object** that can be directly parsed using `json.loads()`.
3. Do not include any extra text, explanations, or formatting outside of the JSON object.
4. **If a field has no available information, set its value to `"Not mentioned"` instead of leaving it empty.**
5. Ensure that the JSON format is correct, including the use of **double quotes (`"`) for all keys and values.**
## **Output Format (JSON)**:
```json
{
    "main_contributions": "<Concise description of the main research problem and contributions>",
    "methodology": "<Brief explanation of the key techniques, models, or algorithms>",
    "experimental_setup": "<Description of datasets, benchmarks, and validation methods>",
    "limitations": "<Summary of weaknesses, constraints, or assumptions>",
    "future_research_directions": "<Potential areas for extending this research>"
}
```
Output:
{
    "main_contributions": "This research initiates an investigation into the implicit optimization bias of next-token prediction (NTP), the dominant training paradigm for modern language models. It characterizes the structural properties of solutions selected by gradient-based optimizers, particularly in overparameterized regimes with linear models and fixed context embeddings. Key contributions include introducing \"NTP-separability conditions\" for reaching the data-entropy lower bound, and demonstrating that gradient descent (GD) selects parameters that equate logit differences of in-support tokens to their log-odds within the data subspace. In the orthogonal subspace, GD parameters diverge in norm and align with a direction that maximizes an NTP-specific margin. These findings extend previous work on implicit bias in one-hot classification to the more complex NTP setting.",
    "methodology": "The study frames next-token prediction as cross-entropy minimization across distinct contexts, each associated with sparse conditional probability distributions over a finite vocabulary. It adopts a \"top-down\" approach, focusing on linear models where only the decoding (word-embedding) matrix W is trained, while context embeddings are kept fixed. The methodology introduces and formalizes \"NTPH-compatibility\" and \"NTP-separability\" conditions that constrain parameter spaces. A novel margin concept for NTP, extending classical SVM definitions, is introduced. The implicit bias of gradient descent (GD) and the regularization path (minimizing L2-regularized CE loss) are mathematically characterized through proofs demonstrating directional convergence and parameter behavior in different subspaces.",
    "experimental_setup": "The research numerically verifies its findings using synthetic data experiments. A 2D toy example with 3 distinct contexts, 2-dimensional embeddings, 5 words in the vocabulary, and support sets of length 3 is used to visualize context and word embedding geometries, and to illustrate GD's behavior (loss convergence, norm growth, alignment with Wmm, and subspace convergence to W⋆). For a more general overparameterized setting, a dataset with 5000 sequences, 50 distinct contexts, 60-dimensional embeddings, 10 vocabulary words, and 6 possible next-tokens per context is generated. Gradient Descent (GD), Normalized GD (NGD), and Adam are tested for 10,000 iterations with specified learning rates (GD: 0.5; NGD/Adam: 0.01) and Adam hyperparameters (β1=0.9, β2=0.99). The experiments were conducted in Matlab on a MacBook Pro.",
    "limitations": "The current research is based on several assumptions for establishing a foundational understanding. It primarily focuses on linear models and assumes fixed context embeddings, meaning only the linear decoder matrix is trained. The theoretical findings are limited to settings where overparameterization (embedding dimension d being greater than the number of distinct contexts m) is sufficient for NTP-separability and NTPH-compatibility. While NGD and Adam are empirically tested, a rigorous theoretical analysis of implicit bias for stochastic or adaptive algorithms is not provided. The study does not explore non-linear embedding architectures or the joint optimization of context embeddings and decoder weights (unconstrained features).",
    "future_research_directions": "Future research directions include identifying exact NTP-separability thresholds under various distributional assumptions, moving beyond the sufficient condition of d > m, potentially requiring the exploration of non-convex architectures. Another promising avenue is to study generalization in NTP settings by examining the statistical properties of the NTP-SVM solution, requiring the development of appropriate generalization measures and statistical models for context embeddings. A \"bottom-up\" approach focusing on architecture-specific embeddings (e.g., shallow transformers) could analyze the effects of optimization biases on both transformer and decoder weights. Investigating the memory capacity of sequence-to-sequence architectures in NTP settings is also suggested. Extending the analysis to \"unconstrained features\" by jointly optimizing context and word embeddings (log-bilinear models) could reveal new insights into embedding geometries. Finally, exploring the implicit bias of adaptive algorithms, such as Adam, in NTP is another critical direction."
}
