
Input:
You are an accomplished researcher in machine learning. You are considering a new method described in "New Method" for the research theme provided in "Research Topic". "Related Works" is a list of research papers that are highly relevant to this new method.
Based on the following instructions, output the reasons for the novelty and significance of the newly proposed method, and quantitatively evaluate them.

# Research Topic
LLMの新しい損失関数

# New Method
{
    "Open Problems": "Token-level strictly–proper scoring rules such as the Brier or Spherical score improve generation quality when used for fine-tuning, but they converge slowly and under-perform when models are trained from scratch.  The root cause is that bounded scores give much smaller early gradients than the unbounded logarithmic score, so learning stalls in the high-entropy regime.  A minimal change that keeps the advantages of bounded scores while restoring strong early gradients is still missing.",
    "Methods": "We propose Hybrid Proper Scoring (HyProS).\n1.  Loss definition\n    L(θ) = (1−λ) · Slog(θ)  +  λ · SBrier(θ)\n    where Slog is the usual token-level negative log-likelihood and SBrier is the token-level Brier score.\n2.  Dynamic weight λ\n    λ starts near 0 (pure log-loss) and is annealed to 1 (pure Brier) over training steps t:\n        λ(t) = sigmoid((t−T0)/τ)\n    with T0≈0.3·Ttotal and τ≈0.1·Ttotal.\n3.  Motivation\n    • Large early gradients from Slog drive rapid entropy reduction.\n    • As the model becomes competent, λ→1 lets the bounded Brier score shape a better calibration / generation trajectory.\n    • The schedule needs no extra hyper-parameters beyond (T0,τ) and introduces <10 lines of code.",
    "Experimental Setup": "Model: Transformer-base trained from scratch on WMT14 En-De.\nBaselines: (a) pure Log loss, (b) pure Brier loss, (c) HyProS.\nTraining: identical optimiser / lr schedule for all runs, 100 k steps.\nEvaluation: BLEU on newstest14, validation perplexity every 5 k steps to measure convergence speed.\nAblation: fixed λ=0.5 to show the benefit of the annealing schedule.",
    "Experimental Code": "class HybridBrierLoss(nn.Module):\n    def __init__(self, vocab_size, t0, tau, total_steps):\n        super().__init__()\n        self.v = vocab_size\n        self.t0, self.tau = t0, tau\n        self.total = total_steps\n        self.register_buffer('step', torch.tensor(0))\n\n    def forward(self, logits, targets):\n        # logits: (B,T,V), targets: (B,T)\n        p = torch.softmax(logits, -1)\n        one_hot = torch.nn.functional.one_hot(targets, self.v).float()\n        sbrier = ((p - one_hot)**2).sum(-1)          # Brier per token\n        slog   = torch.nn.functional.cross_entropy(\n                    logits.view(-1, self.v), targets.view(-1), reduction='none')\n        slog   = slog.view_as(sbrier)\n        # weight\n        t = self.step.item()\n        lam = torch.sigmoid((t - self.t0) / self.tau)\n        self.step += 1\n        loss = (1 - lam) * slog + lam * sbrier\n        return loss.mean()\n\n# Usage inside training loop\ncriterion = HybridBrierLoss(vocab_size, t0=0.3*TOTAL_STEPS, tau=0.1*TOTAL_STEPS, total_steps=TOTAL_STEPS)\nfor step, (inp, tgt) in enumerate(loader):\n    logits = model(inp)\n    loss = criterion(logits, tgt)\n    loss.backward(); opt.step(); opt.zero_grad()",
    "Expected Result": "•   Convergence: HyProS matches the log-loss perplexity curve for the first ~30 k steps and overtakes the pure Brier run by >2× at 20 k steps.\n•   Final BLEU: Log = 27.3, Brier = 26.8, HyProS ≈ 27.8 (+0.5 over log, +1.0 over Brier).\n•   Calibration (Brier score on dev): HyProS equals Brier and beats Log.\nThus the hybrid schedule removes Brier’s slow-start disadvantage while retaining its generation benefits.",
    "Expected Conclusion": "A simple, two-line weighted combination of log-loss and Brier loss with a sigmoid schedule eliminates the slow-convergence problem of bounded strictly proper scores.  Because no additional model parameters or complex algorithms are introduced, HyProS can be dropped into any existing training script.  The result is faster training, equal-or-better BLEU, and improved probability calibration, demonstrating that minimal objective tweaks can yield meaningful gains."
}

# Related Works
{
    "Title": "Language Generation with Strictly Proper Scoring Rules",
    "Main Contributions": "The paper addresses the predominant use of the logarithmic score (log-likelihood loss) in language generation, which is a local strictly proper scoring rule but suffers from unboundedness and sensitivity. The main contribution is a straightforward strategy to adapt non-local strictly proper scoring rules, such as the Brier score and Spherical score, for language modeling by distributing them at the token level. It also introduces 'score smoothing' to enable label smoothing for arbitrary scoring rules, including a masked logarithmic score for enhanced effect. Experimental results demonstrate that simply substituting the loss function with these alternative scores, without adjusting other hyperparameters, yields substantial improvements in model generation capabilities, even scaling up to large language models (LLMs) like LLaMA-7B and LLaMA-13B, particularly during fine-tuning. The work suggests that different strictly proper scoring rules offer complementary optimization trajectories, leading to enhanced generative performance, and proposes considering multiple scores for a more accurate evaluation of language models.",
    "Methodology": "The core methodology involves adapting non-local strictly proper scoring rules to language generation. This is achieved by first distributing the scoring rule at the token level, leveraging the autoregressive decomposition of sequence probabilities. This transforms the sequence prediction problem into a series of conditional token prediction tasks, making non-local scores computationally feasible within the vocabulary space. The paper then introduces 'score smoothing,' a generalized label smoothing technique for arbitrary scoring rules, defined as Sϵ(p, i) = (1− ϵ) · S(p, i) + ϵ/m · Σ_j=1^m S(p, j). To address scenarios where the smoothing term might be ignored due to flat loss landscapes, a 'masked logarithmic score' is proposed, which augments the smoothing term for 'under-smooth' labels. The paper uses classic strictly proper scoring rules like the Brier score and Spherical score, along with their generalized forms (α-power and pseudo-spherical scores), as loss functions for training and fine-tuning Transformer-based models and LLMs (LLaMA-7B, LLaMA-13B). Training involves both training from scratch and fine-tuning pre-trained models, as well as instruction tuning for LLMs, comparing against the traditional logarithmic score.",
    "Experimental Setup": "The method was evaluated on various language generation tasks: machine translation and abstractive summarization. For machine translation, experiments were conducted on WMT14 English-French (En-Fr, 35.8M pairs), WMT14 English-German (En-De, 4.5M pairs), and the TED bilingual dataset (~200K pairs per direction). For LLMs, WMT22 translation tasks (Chinese-to-English, English-to-Chinese, German-to-English, English-to-German) were used. Abstractive summarization experiments were performed on the CNN/DailyMail dataset (311K pairs). Instruction tuning for LLMs used the Alpaca dataset (52K instruction-following demonstrations). Models included Transformer-base (for most datasets) and Transformer-big (for WMT14 En-Fr), as well as LLaMA-7B and LLaMA-13B for large language model experiments. Evaluation metrics were BLEU for machine translation, ROUGE-1, ROUGE-2, and ROUGE-L for summarization, and MT-bench for multi-turn question answering capabilities of LLMs. Standard preprocessing with BPE (32K merge operations) was applied for WMT datasets. Hyperparameters for Transformer models and LLM instruction tuning were generally kept fixed to those optimized for the logarithmic score, except for exploring the alpha parameter in generalized scores. Beam search with varying beam sizes and length penalties was used for decoding.",
    "Limitations": "When training language generation models from scratch, the Brier and Spherical scores exhibited slower convergence and lower overall performance compared to the logarithmic score. This is hypothesized to be due to their distinct learning dynamics (e.g., logarithmic score's larger initial gradient) and the use of hyperparameters optimized for the logarithmic score, which may not be ideal for other rules. Furthermore, the effectiveness of score smoothing can be limited for bounded scores like Brier and Spherical scores, as they may impose only a mild penalty when smoothing is ignored, potentially leading the model to disregard the smoothing term if the loss landscape is flat around the optimal smoothed distribution. The paper also notes that when used as a beam search objective, the logarithmic score generally still yielded the best performance among the examined scores, suggesting limitations in their direct application for decoding. Finally, the research highlights that while different strictly proper scores share the same global optimum, their optimization trajectories can conflict, and the factors contributing to these performance differences are not fully understood.",
    "Future Research Directions": "The paper suggests several avenues for future research. One direction is to investigate other strictly proper scoring rules that might exhibit superior performance during either pre-training or fine-tuning. Another is to delve deeper into understanding the factors contributing to the observed performance differences among various strictly proper scores. This includes developing additional metrics or properties to better determine a score's suitability for training language generation models. The paper also proposes exploring whether these alternative scores can function as effective evaluation metrics for language generation models, similar to Perplexity, specifically for assessing model calibration. Lastly, it encourages further investigation into the application of different scoring rules as objectives for beam search during decoding, with the potential to discover rules that surpass the logarithmic score in this context.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Discovering Preference Optimization Algorithms with and for Large Language Models",
    "Main Contributions": "This research introduces an LLM-driven objective discovery pipeline to automatically find novel state-of-the-art offline preference optimization algorithms for Large Language Models (LLMs). The core problem addressed is the limitation of human creativity in exploring the vast search space of possible loss functions for preference optimization. The key finding is the discovery of new high-performing algorithms, including Discovered Preference Optimization (DiscoPOP), a novel algorithm that adaptively blends logistic and exponential losses. DiscoPOP achieves strong performance across various held-out evaluation tasks, and its analysis reveals surprising features such as its non-convex nature.",
    "Methodology": "The study employs an LLM-driven objective discovery pipeline that iteratively prompts GPT-4 to propose and implement new preference optimization loss functions in PyTorch code. This process is informed by previously evaluated performance metrics (e.g., MT-Bench scores) provided as in-context examples. The proposed objective functions are validated via unit tests and then used to fine-tune an LLM. The performance of the resulting model is fed back to the LLM for iterative refinement. DiscoPOP, identified as the Log Ratio Modulated Loss (LRML), is described as a dynamically weighted sum of logistic and exponential losses, where the weights are determined by a sigmoid calculation of the difference of log-ratios (ρ).",
    "Experimental Setup": "The LLM used for objective discovery was GPT-4. For fine-tuning, the 'zephyr-7b-gemma-sft' model (a 7 billion Gemma model supervised fine-tuned on 'deita-10k-v0-sft') was utilized. The primary discovery task involved multi-turn dialogue on MT-Bench, using the 'Argilla DPO Mix 7K' preference dataset. Models were trained with fixed hyperparameters (β=0.05, learning rate 5e-7, bfloat16, 2 epochs, batch size 2, gradient accumulation 8, cosine scheduler, AdamW optimizer) on 8 Nvidia A100 GPUs. Held-out evaluations included Alpaca Eval 2.0 for single-turn dialogue (comparing win rates against GPT-4 or SFT base model), summarization on a 10% subsample of the Reddit TL;DR dataset (evaluated using a custom Alpaca Eval 2.0 setup with a GPT-4 annotator), and positive sentiment generation on the IMDb dataset (using a GPT-2 model and evaluated with a pre-trained sentiment classifier and KL-Divergence across a β sweep from 0.01 to 5).",
    "Limitations": "The discovered DiscoPOP algorithm struggles to converge when the beta (β) parameter is set too low (β ≤ 0.01) or too high (β ≥ 2.5), which is attributed to these values not being encountered during the discovery process (which used a fixed β=0.05). Specifically, at low β, DiscoPOP can get stuck in local minima, generating predominantly negative reviews, and at high β, models can collapse due to sharp spikes in loss, potentially from large gradients in its non-convex part. Other limitations include only a superficial exploration of effective LLM objective proposal generation, the highest-performing loss re-purposing β in a way that affects both functional behavior and KL penalty, and the reliance on closed-source models (GPT-4) for code generation, which hinders reproducibility and is costly.",
    "Future Research Directions": "Future work could focus on improving the effectiveness of LLM objective proposals, such as by leveraging more training run information (e.g., learning curve plots via Visual Language Models) or meta-meta-optimizing the LLM prompt templates. It is also suggested to study different forms of loss functions with multiple floating-point parameters that can be tuned separately, as the current top-performing loss re-purposes a single β parameter. Additionally, using the produced models themselves to generate code could lead to code-level self-improvement, addressing the current reliance on costly closed-source LLMs and improving reproducibility. Gradient clipping is also suggested as a potential amendment for instability issues with high beta values.",
    "Experiment Code": "def log_ratio_modulated_loss(\n        self,\n        policy_chosen_logps: torch.FloatTensor,\n        policy_rejected_logps: torch.FloatTensor,\n        reference_chosen_logps: torch.FloatTensor,\n        reference_rejected_logps: torch.FloatTensor,\n    ) -> torch.FloatTensor:\n        tau = 0.05\n        pi_logratios = policy_chosen_logps - policy_rejected_logps\n        ref_logratios = reference_chosen_logps - reference_rejected_logps\n        logits = pi_logratios - ref_logratios\n        logits = logits * self.beta\n        # Modulate the mixing coefficient based on the log ratio magnitudes\n        log_ratio_modulation = torch.sigmoid(logits / tau)\n        logistic_component = -F.logsigmoid(logits)\n        exp_component = torch.exp(-logits)\n        # Blend between logistic and exponential component based on log ratio modulation\n        losses = logistic_component * (1 - log_ratio_modulation) + exp_component * log_ratio_modulation\n        return losses\n\n\n# Code for LLM-driven objective discovery pipeline (from scripts/launch_evo.py)\nsystem_prompt = \"\"\"\nYou are a machine learning researcher who is testing out different RLHF loss functions. When you respond, output a JSON where the first key (\"thought\") corresponds to your thought process when designing the next function. The second key (\"name\") corresponds to the name of your next function. Finally, the last key (\"code\") corresponds to the exact python code that you would like to try. Here is an example:\n\n{\"thought\": \"Based on the previous outputs, I should try the direct preference optimization algorithm.\",\n\"name\": \"dpo\",\n\"code\": \"def sigmoid_loss(\n    self,\n    policy_chosen_logps: torch.FloatTensor,\n    policy_rejected_logps: torch.FloatTensor,\n    reference_chosen_logps: torch.FloatTensor,\n    reference_rejected_logps: torch.FloatTensor,\n) -> torch.FloatTensor:\n    pi_logratios = policy_chosen_logps - policy_rejected_logps\n    ref_logratios = reference_chosen_logps - reference_rejected_logps\n    logits = pi_logratios - ref_logratios\n    losses = -F.logsigmoid(self.beta * logits)\n    return losses\"\n}\n\nYou are deeply familiar with binary classification losses from the literature. Be creative and reference prior literature when possible.\n\nYou must use the exact function interface used above. Feel free to define extra hyperparameters within your function as constants. Do not make them attributes of self.\n\nNote that `self.beta = 0.05`.\n\nRLHF loss functions train on a dataset of pairs of preferred and rejected completions.\n`policy_chosen_logps` refers to the policy's log probabilities of the preferred completion, and `policy_rejected_logps` refers to the policy's log probabilities of the rejected completion.\n`reference_chosen_logps` and `reference_rejected_logps` refer to the same for the reference (base) model.\n\nThe user will then return to you a fitness that corresponds to the performance of the resulting model on a downstream task. Your goal is to maximize performance.\n\"\"\"\n\n# Main loop for LLM-driven generation and evaluation (from scripts/launch_evo.py)\n# (Simplified for brevity, actual code includes API retry logic and full subprocess management)\nfor i in range(args.num_generations):\n    # Generate code using GPT-4\n    # completion = openai.ChatCompletion.create(\n    #     engine=gpt_model,\n    #     messages=messages,\n    #     max_tokens=2048,\n    #     n=1,\n    #     response_format={\"type\": \"json_object\"},\n    # ).choices[0]\n    # messages.append(completion.message.to_dict())\n    # out = json.loads(completion.message.content)\n\n    # Example 'out' structure from GPT-4 for a generated loss function\n    out = {\"name\": \"new_loss_function\", \"code\": \"def new_loss_function(...): ...\"}\n\n    # Validate code\n    valid, error = validate_code(out[\"code\"])\n    if not valid:\n        next_prompt = f\"Code not valid. Error:\\n{error}\\nPlease generate the next one.\"\n        messages.append({\"role\": \"user\", \"content\": next_prompt})\n        fitness = -1\n        continue\n\n    # Train GPO with the new loss function\n    trained, error = train_gpo(out, config)\n    if not trained:\n        next_prompt = f\"Training failed. Error:\\n{error}\\nPlease generate the next one.\"\n        messages.append({\"role\": \"user\", \"content\": next_prompt})\n        fitness = -1\n        continue\n\n    # Evaluate GPO and get fitness\n    evaluated, val = evaluate_gpo(out, config)\n    if not evaluated:\n        next_prompt = f\"Evaluation failed. Error:\\n{val}\\nPlease generate the next one.\"\n        messages.append({\"role\": \"user\", \"content\": next_prompt})\n        fitness = -1\n        continue\n\n    # Feed back fitness to GPT-4\n    next_prompt = f\"Fitness: {val}.\\nPlease generate the next one.\"\n    messages.append({\"role\": \"user\", \"content\": next_prompt})\n    fitness = val",
    "Experiment Result": "The LLM-driven objective discovery pipeline iteratively prompts GPT-4 (`gpt_model = \"gpt-4\"`) to propose and implement new preference optimization loss functions in PyTorch code. The process begins with an `archive` of baseline loss functions (DPO, HINGE, IPO, KTO) and their initial MT-Bench scores. These scores are provided as in-context examples to GPT-4 to inform subsequent proposals.\n\n**Pipeline Steps:**\n1.  **Code Generation:** GPT-4 receives a `system_prompt` instructing it to act as an ML researcher, propose a loss function, and output its thought process, name, and PyTorch code in JSON format.\n2.  **Code Validation:** Proposed loss functions undergo unit tests (`validate_code`) to ensure syntactic correctness, valid execution, and gradient computation. Invalid code leads to negative feedback to GPT-4.\n3.  **LLM Fine-tuning:** Validated loss functions are used to fine-tune an LLM. This is managed by the `train_gpo` function, which calls `scripts/run_gpo.py` using `accelerate launch` with `deepspeed_zero3.yaml` configuration. The training typically uses 4 or 8 GPUs and targets 2B or 7B parameter models.\n4.  **Performance Evaluation:** The fine-tuned LLM's performance is evaluated using MT-Bench scores (`evaluate_gpo`), which leverages FastChat's LLM judge (`LLM_JUDGE_DIR` is `../FastChat/fastchat/llm_judge`).\n5.  **Iterative Refinement:** The resulting MT-Bench score (fitness) is fed back to GPT-4 as an in-context example for the next iteration of loss function proposal.\n\n**DiscoPOP (Log Ratio Modulated Loss - LRML) Specifics:**\n-   **Function Definition:** LRML is defined as a dynamically weighted sum of logistic and exponential losses.\n-   **Weighting Mechanism:** The weights for the logistic and exponential components are determined by a sigmoid calculation of `logits / tau`, where `logits` is the difference of log-ratios (ρ) scaled by `self.beta`.\n-   **Key Parameters:**\n    -   `self.beta`: A scaling factor applied to the `logits` (difference of log-ratios) before being passed to the logistic and exponential components, and also used in the sigmoid modulation. Sweeps of `beta` values include `[0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2.5, 5]` for analysis.\n    -   `tau`: An internal constant parameter set to `0.05` within the `log_ratio_modulated_loss` function, used in the sigmoid modulation calculation (`torch.sigmoid(logits / tau)`).\n-   **Implementation:** The LRML is implemented as `log_ratio_modulated_loss` within the `GPOTrainer` in `src/alignment/gpo.py`."
}{
    "Title": "Beyond MLE: Convex Learning for Text Generation",
    "Main Contributions": "The paper proposes a novel class of training objectives based on convex functions to address the suboptimality of Maximum Likelihood Estimation (MLE) for closed-ended text generation tasks. It theoretically demonstrates that convex functions can sharpen the optimal distribution, enabling models to focus on highly probable outputs without estimating the entire data distribution. The approach is empirically validated to significantly improve performance across autoregressive (AR) models, non-autoregressive (NAR) models, and large language models (LLMs) in machine translation and text summarization. Specifically, it enables AR models to bridge the gap between greedy and beam search, facilitates NAR models' learning with over 9 BLEU points improvement by alleviating multi-modality issues, and substantially enhances LLMs' generative capability.",
    "Methodology": "The methodology addresses MLE's limitations in closed-ended text generation, where the goal is to produce the most probable output rather than estimate the full data distribution. A general learning framework, Lf(θ) = -Ex~pdata(x)[f(pθ(x))], is introduced, where f is a differentiable, increasing function. While a pure convex f leads to a desirable one-hot optimal distribution, it causes gradient vanishing during training. To overcome this, a convex-composition approach is proposed: Lfg(θ) = -Σ_i pdata(xi) * fg(pθ(xi)), combining an increasing convex function (f) with an increasing concave function (g, typically log-probability). This composition theoretically yields a sharper optimal distribution (pfg) than MLE's (pg), reducing Shannon entropy and concentrating probability mass on highly probable outputs. In experiments, an exponential function is primarily used for the convex component (Lf(θ) = -Ex~pdata(x)[pθ(x) k / T]). A two-step training strategy is employed: initial MLE pre-training followed by fine-tuning with the convex-composition loss to ensure stable training.",
    "Experimental Setup": "Experiments are conducted on neural machine translation (NMT) and text summarization tasks, using autoregressive (AR), non-autoregressive (NAR), and large language models (LLMs). For NMT, WMT14 English-German (4.5M pairs) and WMT22 (EN-DE, DE-EN, EN-ZH, ZH-EN) for LLMs are used. Summarization involves CNN/DailyMail (311,971 article-summary pairs) and XSum (226,677 article-summary pairs). Models include Transformer-base (AR), Vanilla-NAT, CMLM, CTC (NAR), and LLaMA-7B/13B (LLMs fine-tuned on Alpaca). Data is tokenized using BPE (NMT), GPT-2 (CNN/DailyMail), and Berttokenizer (XSum). Training uses a two-step process (MLE pre-training then convex-composition fine-tuning) with Adam optimizer. The exponent 'k' is 1 for AR models and tuned for NAR models. Decoding for AR models involves greedy and beam search (beam sizes 1, 2, 3, 5, 8) with length penalties. NAR models use argmax decoding or length candidates, with speedup measured on GeForce RTX 3090 GPUs. LLMs use beam search (beam size 4 for NMT, 2 for summarization) with specific prompts. Evaluation metrics include BLEU, COMET (NMT), ROUGE-1, ROUGE-2, ROUGE-L (summarization), and for NAR models, prediction confidence (Output NLL) and generation fluency (External PPL). Diversity for VAE-based models is assessed with Reference-BLEU and Pairwise-BLEU.",
    "Limitations": "Directly applying loss with convex functions leads to inefficient training due to vanishing gradients when prediction probabilities are near zero, necessitating a convex-composition approach and two-step training. An alternative convex-composition (power function) performed worse and exhibited training instability, particularly when the exponent 'k' approached zero. While convex learning improves generation quality, it can reduce output diversity, as demonstrated in VAE-based models that show higher reference-BLEU but significantly lower pairwise-BLEU (mode collapse). The theoretical analysis relies on the simplifying assumption of distinctness of sample probabilities, though this is considered reasonable for textual data.",
    "Future Research Directions": "Future research could involve exploring synergistic combinations of convex learning with knowledge distillation to further enhance performance. Identifying and investigating other stable and effective convex functions for the convex-composition framework, beyond the exponential and power functions tested, is another promising area. Addressing the observed trade-off between generation quality and diversity in models like VAEs, perhaps by integrating diversity-promoting mechanisms with the sharpness objective, is also suggested. Additionally, extending the application of convex learning beyond closed-ended tasks to open-ended text generation, potentially by adaptively encouraging sharpness for specific quality aspects while maintaining overall creativity and diversity, could be explored.",
    "Experiment Code": "class CustomTrainer(Trainer):\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        \n        logits = outputs[\"logits\"] if isinstance(outputs, dict) else outputs[0]\n        logits = logits[..., :-1, :].contiguous()\n        labels = labels[..., 1:].contiguous()\n\n        lprobs = torch.nn.functional.log_softmax(logits, dim = -1)\n        batch_size, length, vocab_size = lprobs.size()\n        padding_mask = labels.eq(IGNORE_INDEX).view(batch_size, length)\n        labels = torch.clamp(labels, min=0)\n        tgt_lprob = lprobs.gather(dim=-1, index=labels.unsqueeze(-1)).view(batch_size, length)\n        tgt_length = torch.sum(~padding_mask, dim = -1)\n        tgt_lprob.masked_fill_(padding_mask, 0.0)\n        tgt_lprob = torch.sum(tgt_lprob, dim = -1) / tgt_length\n        tgt_prob = torch.exp(tgt_lprob * 1)\n        custom_loss = - torch.sum(tgt_prob * tgt_length)\n\n        num_active_elements = padding_mask.numel() - padding_mask.long().sum()\n\n        custom_loss = custom_loss / num_active_elements\n        return (custom_loss, outputs) if return_outputs else custom_loss",
    "Experiment Result": "The training methodology employs a two-step strategy: initial MLE pre-training (implied by `run_mle.py`) followed by fine-tuning with a convex-composition loss (implemented in `run_convex.py`).\n\nFor the fine-tuning step, a `CustomTrainer` is used, which overrides the standard loss computation. This custom loss calculates the average log-probability per token for non-padded parts of the target sequence (`tgt_lprob`). It then applies an exponential function: `tgt_prob = torch.exp(tgt_lprob * 1)`. This effectively interprets the exponentiated average log-probability as `pθ(x)` in the `Lf(θ) = -Ex~pdata(x)[pθ(x) k / T]` framework, with `k/T` implicitly set to 1. The custom loss is then computed as the negative sum of `tgt_prob` multiplied by `tgt_length`, averaged over active elements.\n\nKey settings and components for training:\n- **Model Loading**: Models are loaded using `AutoModelForCausalLM.from_pretrained`, supporting `torch_dtype` (e.g., `float16`) and options to freeze embedding layers or specific transformer layers.\n- **Tokenizer**: `AutoTokenizer.from_pretrained` is used. Special tokens like `[PAD]`, `<s>`, `</s>`, and `<unk>` are added or set for compatibility, particularly for Llama models. The `padding_side` for the tokenizer can be explicitly set.\n- **Data Preprocessing**: The `preprocess_function` handles tokenization, truncates sequences to a `block_size` (defaulting to `tokenizer.model_max_length` or 1024), and creates labels by masking prompt inputs with `IGNORE_INDEX` (-100). Padding to batch is handled by `DataCollatorForSeq2Seq`.\n- **Optimization**: Standard HuggingFace `TrainingArguments` are used to configure optimization parameters (e.g., learning rate, batch size, epochs, gradient accumulation, etc.)."
}{
    "Title": "Model Alignment as Prospect Theoretic Optimization",
    "Main Contributions": "The paper introduces Kahneman-Tversky Optimization (KTO), a novel model alignment objective for LLMs inspired by Kahneman & Tversky's prospect theory. It demonstrates that existing successful alignment methods (e.g., DPO) are 'human-aware losses' (HALOs) that implicitly model human decision biases. KTO directly maximizes the utility of generations, unlike preference-based methods that maximize log-likelihood. KTO significantly reduces data requirements by only needing a binary signal (desirable/undesirable) and matches or exceeds DPO's performance across various LLM scales (1B to 30B), even with fewer desirable examples or unpaired data. It also suggests that the best alignment loss depends on appropriate inductive biases for a given setting, challenging the idea of a universally superior HALO. Theoretically, KTO is shown to avoid fitting to noise and handle contradictory feedback better than DPO in certain worst-case scenarios, by implicitly ignoring noisy/intransitive data and maximizing utility directly.",
    "Methodology": "The core methodology involves defining 'Human-Aware Losses' (HALOs) as a class of loss functions where a human value function (non-decreasing, concave in gains) is applied to an implied reward (log ratio of policy and reference model probabilities), relative to a reference point. KTO is derived as a specific HALO, modifying the Kahneman-Tversky value function: it replaces the exponent with a logistic function for numerical stability, introduces hyperparameters `β` for risk aversion and `{λD, λU}` for loss aversion (for desirable/undesirable outputs), and sets the reference point `z0` as the KL divergence between the current and reference policy distributions. In practice, `z0` is estimated as a biased average of shifted outputs in a microbatch and is not backpropagated through. KTO learns from binary feedback; preference data is converted by treating the preferred output as desirable and the dispreferred as undesirable. Hyperparameters like learning rate, batch size, `β`, and `λD, λU` are tuned, with specific recommendations provided for different model sizes and whether SFT is performed prior to KTO.",
    "Experimental Setup": "Experiments were conducted on Pythia-{1.4B, 2.8B, 6.9B, 12B} and Llama-{7B, 13B, 30B} model families, later extended to Mistral-7B derivatives and Llama-3 8B/Qwen2.5 3B Instruct. Training data included Anthropic-HH, OpenAssistant, and SHP datasets, as well as UltraFeedback for specific experiments (Zephyr-β-SFT). Baselines included Conditional SFT (CSFT), Sequence Likelihood Calibration (SLiC), Direct Preference Optimization (DPO), and an offline PPO variant using dummy +1/-1 rewards. Evaluation metrics comprised GPT-4-0613-judged winrates against SFT targets (assessing helpfulness, harmlessness, conciseness), validated against human judgments. Performance was also assessed on generative benchmarks like MMLU, GSM8K (mathematical reasoning), HumanEval, BigBench-Hard, TydiQA, and AlpacaEval 2. Training configurations aimed for an 'apples-to-apples' comparison, keeping settings identical where possible, with method-specific hyperparameter adjustments. Experiments included evaluating KTO with data imbalances (up to 90% desirable data discarded) and with unpaired data (one-y-per-x setup), as well as ablation studies on KTO's design choices.",
    "Limitations": "The KTO model's use of a logistic function instead of the original Kahneman-Tversky power-law exponent `α` may simplify human utility perception. The reference point `z0` estimation is biased, though practical. The naive conversion of preference data into binary signals is a simplification. KTO's mechanism of ignoring 'hard-to-learn' data, while potentially beneficial for noisy labels, could lead to underfitting complex distributions if the data is genuinely difficult but necessary. The paper acknowledges that there is no single universally superior HALO, implying a need for careful selection based on context-specific inductive biases. KTO's default loss-neutral value function resolves contradictory feedback by favoring the majority, which may not align with all theories of fairness. Furthermore, the human feedback datasets used (SHP, HH, OASST) might be unrepresentative, potentially propagating biases, and large-scale deployment of KTO-aligned models could risk homogenization of preferences and utility functions.",
    "Future Research Directions": "Future work includes investigating alternative value functions and reference point distributions that more accurately describe human perception of language across various domains and individuals, and developing methods to identify the optimal HALO for specific individuals and settings. Technical extensions involve developing HALOs that can incorporate granular feedback (e.g., scores) for multi-objective optimization, adapting KTO for other modalities (e.g., images) and model classes that lack explicit output distributions, designing HALOs that can resolve feedback contradictions according to diverse fairness definitions, and creating HALOs for online data streams. The authors also emphasize the need for ecologically valid evaluations, deploying aligned models in real-world settings to fully assess their merits. More complex methods for deconstructing preferences into binary feedback and dynamic hyperparameter selection schemes are also suggested.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Risk-Averse Fine-tuning of Large Language Models",
    "Main Contributions": "The paper introduces a novel approach for fine-tuning Large Language Models (LLMs) by integrating risk-averse principles, specifically optimizing the Conditional Value at Risk (CVaR) measure, to mitigate the generation of negative or toxic content. This Risk-Averse Reinforcement Learning with Human Feedback (RA-RLHF) algorithm is designed to minimize rare but significant harmful outputs while maintaining generative effectiveness. It is the first to apply a nuanced understanding of risk in LLM content generation, implementing CVaR with a regularized reinforcement learning objective in an Actor-Critic setting to learn complex parameterized policies. Empirical evaluations demonstrate RA-RLHF's superior performance in steering language away from toxicity, particularly for challenging prompts, and it lays groundwork for exploring other risk measures for LLM safety.",
    "Methodology": "The proposed RA-RLHF algorithm adapts the standard Reinforcement Learning from Human Feedback (RLHF) pipeline by optimizing the Conditional Value at Risk (CVaR) of the return, rather than the expected value. It incorporates a soft-risk scheduling mechanism with two key elements: (1) an initial training phase (i0 iterations) that utilizes the full data batch and a supervised fine-tuned (SFT) baseline model to ensure recognition of positive episodes and general task performance, and (2) a gradual reduction in batch size (B0) based on the risk target (α) to focus on challenging, worst-case episodes with the lowest returns. The policy update leverages Proximal Policy Optimization (PPO) in an Actor-Critic setup, using a modified dense reward function that combines the environment reward with a KL-Divergence regularization term against a reference SFT policy. The β parameter for KL regularization is dynamically adjusted using a log-space proportional controller.",
    "Experimental Setup": "The RA-RLHF algorithm was evaluated on generative versions of three classification tasks: IMDB-Gen (sentiment modification), Jigsaw-Gen, and RealToxicityPrompts-Gen (toxicity mitigation). Base LLMs used were GPT-2 (117M parameters) and GPT-J (6B parameters). Datasets included the IMDB movie review dataset, Jigsaw toxic comment classification dataset, and RealToxicityPrompts dataset (from OPEN-WEBTEXT CORPUS). Reward models used were `lvwerra/distilbert-imdb` for sentiment scoring and `unitary/toxic-bert` for toxicity scoring. Performance was benchmarked against baselines: Base LLM, Prompted base LLM, DExperts, SFT, standard RLHF, and Quark. Evaluation metrics included standard task performance scores (sentiment/toxicity), Perplexity (for linguistic coherence, calculated on positive class samples), and Distinct-n (for textual diversity). Experiments included hyperparameter analysis and were run on NVIDIA Tesla V100 SXM2 32 GB and NVIDIA A100 80 GB GPUs, with results for RLHF and RA-RLHF reported with standard deviations over three seeds.",
    "Limitations": "The effectiveness of the risk-averse fine-tuning strategy may vary across different domains and languages, necessitating further investigation. The work primarily focused on generative tasks and did not directly address the Question-Answer (Q&A) format. While RA-RLHF promotes safer content, a marginal increase in model perplexity was observed, possibly due to more aggressive adjustments. Ethical concerns regarding potential biases and unintended consequences of LLMs, and the possibility of reversing alignment strategies to induce harmful content, remain critical. Additionally, challenges with larger models like GPT-J included Out-Of-Memory (OOM) errors even with bfloat16 precision and LoRA, requiring a smaller batch size and limiting training to 70% of the dataset due to time constraints.",
    "Future Research Directions": "Future research should explore the applicability and adaptation of the risk-averse fine-tuning strategy across a wider array of domains and languages. Building upon the current generative tasks, the approach can be extended to more complex applications, such as conversational AI, and potentially the Question-Answer format. Continued attention to ethical considerations, particularly regarding potential biases and unintended consequences of LLMs, is crucial. The work also establishes a foundation for exploring additional risk measures and criteria, such as Entropic Value at Risk (EVaR), in the context of LLM safety and uncertainty quantification. A critical area for future inquiry is also addressing the regulation of LLM outputs in response to malicious prompts.",
    "Experiment Code": "import inspect\nimport math\nimport os\nimport time\nimport typing\nimport warnings\nfrom typing import Callable, List, Optional, Union\n\nimport datasets\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom accelerate import Accelerator\nfrom accelerate.utils import ProjectConfiguration, is_deepspeed_available\nfrom datasets import Dataset\nfrom huggingface_hub import whoami\nfrom packaging import version\nfrom torch.optim import Adam\nfrom transformers import (\n    DataCollatorForLanguageModeling,\n    PreTrainedTokenizer,\n    PreTrainedTokenizerBase,\n    PreTrainedTokenizerFast,\n)\n\nfrom ..core import (\n    WANDB_PADDING,\n    PPODecorators,\n    clip_by_value,\n    convert_to_scalar,\n    entropy_from_logits,\n    flatten_dict,\n    logprobs_from_logits,\n    masked_mean,\n    masked_var,\n    masked_whiten,\n    set_seed,\n    stack_dicts,\n    stats_to_np,\n)\nfrom ..import_utils import is_torch_greater_2_0\nfrom ..models import SUPPORTED_ARCHITECTURES, PreTrainedModelWrapper, create_reference_model\nfrom . import AdaptiveKLController, BaseTrainer, FixedKLController, PPOConfig, RunningMoments\n\nimport pdb\n\nif is_deepspeed_available():\n    import deepspeed\n\nMODEL_CARD_TEMPLATE = \"\"\"---\nlicense: apache-2.0\ntags:\n- trl\n- transformers\n- reinforcement-learning\n---\n\n# {model_name}\n\nThis is a [TRL language model](https://github.com/huggingface/trl) that has been fine-tuned with reinforcement learning to\n guide the model outputs according to a value, function, or human feedback. The model can be used for text generation.\n\n## Usage\n\nTo use this model for inference, first install the TRL library:\n\n```bash\npython -m pip install trl\n```\n\nYou can then generate text as follows:\n\n```python\nfrom transformers import pipeline\n\ngenerator = pipeline(\"text-generation\", model=\"{model_id}\")\noutputs = generator(\"Hello, my llama is cute\")\n```\n\nIf you want to use the model for training or to obtain the outputs from the value head, load the model as follows:\n\n```python\nfrom transformers import AutoTokenizer\nfrom trl import AutoModelForCausalLMWithValueHead\n\ntokenizer = AutoTokenizer.from_pretrained(\"{model_id}\")\nmodel = AutoModelForCausalLMWithValueHead.from_pretrained(\"{model_id}\")\n\ninputs = tokenizer(\"Hello, my llama is cute\", return_tensors=\"pt\")\noutputs = model(**inputs, labels=inputs[\"input_ids\"])\n```\n\"\"\"\n\n\nclass PPOTrainer(BaseTrainer):\n    \"\"\"\n    The PPOTrainer uses Proximal Policy Optimization to optimise language models.\n    Note, this trainer is heavily inspired by the original OpenAI learning to summarize work here:\n    https://github.com/openai/summarize-from-feedback\n\n    Attributes:\n        **config** (`PPOConfig`) -- Configuration object for PPOTrainer. Check the documentation of `PPOConfig` for more\n            details.\n        **model** (`PreTrainedModelWrapper`) -- Model to be optimized, Hugging Face transformer model with a value head.\n            Check the documentation of `PreTrainedModelWrapper` for more details.\n        **ref_model** (`PreTrainedModelWrapper`, *optional*) -- Reference model to be used for KL penalty, Hugging Face\n            transformer model with a casual language modelling head. Check the documentation of `PreTrainedModelWrapper`\n            for more details. If no reference model is provided, the trainer will create a reference model with the same\n             architecture as the model to be optimized with shared layers.\n        **tokenizer** (`PreTrainedTokenizerBase`) -- Tokenizer to be used for encoding the\n            data. Check the documentation of `transformers.PreTrainedTokenizer` and\n            `transformers.PreTrainedTokenizerFast` for more details.\n        **dataset** (Union[`torch.utils.data.Dataset`, `datasets.Dataset`], *optional*) -- PyTorch dataset or Hugging\n            Face dataset. This is used to create a PyTorch dataloader. If no dataset is provided, the dataloader must be\n             created outside the trainer users needs to design their own dataloader and make sure the batch\n            size that is used is the same as the one specified in the configuration object.\n        **optimizer** (`torch.optim.Optimizer`, *optional*) -- Optimizer to be used for training. If no optimizer is\n            provided, the trainer will create an Adam optimizer with the learning rate specified in the configuration\n            object.\n        **data_collator** (DataCollatorForLanguageModeling, *optional*) -- Data collator to be used for training and\n            passed along the dataloader\n        **num_shared_layers** (int, *optional*) -- Number of layers to be shared between the model and the reference\n            model, if no reference model is passed. If no number is provided, all the layers will be shared.\n        **lr_scheduler** (`torch.optim.lr_scheduler`, *optional*) -- Learning rate scheduler to be used for training.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: PPOConfig = None,\n        model: PreTrainedModelWrapper = None,\n        ref_model: Optional[PreTrainedModelWrapper] = None,\n        tokenizer: PreTrainedTokenizerBase = None,\n        dataset: Optional[Union[torch.utils.data.Dataset, Dataset]] = None,\n        optimizer: Optional[torch.optim.Optimizer] = None,\n        data_collator: Optional[typing.Callable] = None,\n        num_shared_layers: Optional[int] = None,\n        lr_scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,\n    ):\n        \"\"\"\n        Initialize PPOTrainer.\n\n        Args:\n            config (`PPOConfig`):\n                Configuration object for PPOTrainer. Check the documentation of `PPOConfig` for more details.\n            model (`PreTrainedModelWrapper`):\n                Hugging Face transformer model with a value head.\n            ref_model (`PreTrainedModelWrapper`):\n                Hugging Face transformer model with a casual language modelling head. Used for KL penalty\n            tokenizer (`transformers.PreTrainedTokenizerBase`):\n                Hugging Face tokenizer\n            dataset (Optional[Union[`torch.utils.data.Dataset`, `datasets.Dataset`]]):\n                PyTorch dataset or Hugging Face dataset. If a Hugging Face dataset is passed, the dataset\n                will be preprocessed by removing the columns that are not used by the model. If none is passed,\n                a warning will be raised in a multi-GPU setting.\n            optimizer (Optional[`torch.optim.Optimizer`]):\n                Optimizer used for training. If `None`, the `Adam` is used as default.\n            data_collator (Optional[function]):\n                Data collator function.\n            num_shared_layers (Optional[int]):\n                Number of shared layers between the model and the reference model. If `None`, all layers are shared.\n                used only if `ref_model` is `None`.\n            lr_scheduler (Optional[`torch.optim.lr_scheduler`]):\n                Learning rate scheduler used for training.\n        \"\"\"\n        super().__init__(config)\n\n        # initial seed for reproducible experiments\n        set_seed(config.seed)\n\n        # Step 0: check positional arguments validity\n        if not isinstance(config, PPOConfig):\n            raise ValueError(f\"config must be a PPOConfig, got {type(config)}\")\n        if not isinstance(tokenizer, (PreTrainedTokenizerBase)):\n            raise ValueError(\n                f\"tokenizer must be a PreTrainedTokenizerBase like a PreTrainedTokenizer or a PreTrainedTokenizerFast, got {type(tokenizer)}\"\n            )\n        if not isinstance(model, (SUPPORTED_ARCHITECTURES)):\n            raise ValueError(\n                f\"model must be a PreTrainedModelWrapper, got {type(model)} - supported architectures are: {SUPPORTED_ARCHITECTURES}\"\n            )\n        # Step 1: Initialize Accelerator\n        self.accelerator = Accelerator(\n            log_with=config.log_with,\n            gradient_accumulation_steps=config.gradient_accumulation_steps,\n            project_config=ProjectConfiguration(**config.project_kwargs),\n            **config.accelerator_kwargs,\n        )\n\n        # Step 1.1 Runtime variables filled by the accelerator\n        config.world_size = self.accelerator.num_processes\n        config.global_backward_batch_size = config.backward_batch_size * config.world_size\n        config.global_batch_size = config.batch_size * config.world_size\n\n        self.model = model\n        self.model_params = filter(lambda p: p.requires_grad, self.model.parameters())\n        self.is_encoder_decoder = hasattr(self.model, \"is_encoder_decoder\")\n        self.is_peft_model = getattr(self.model, \"is_peft_model\", False)\n        config.is_encoder_decoder = self.is_encoder_decoder\n        config.is_peft_model = self.is_peft_model\n\n        is_using_tensorboard = config.log_with is not None and config.log_with == \"tensorboard\"\n        self.accelerator.init_trackers(\n            config.tracker_project_name,\n            config=dict(trl_ppo_trainer_config=config.to_dict()) if not is_using_tensorboard else config.to_dict(),\n            init_kwargs=config.tracker_kwargs,\n        )\n        self.is_using_text_environment = getattr(config, \"use_text_environment\", False)\n\n        if isinstance(ref_model, SUPPORTED_ARCHITECTURES):\n            self.ref_model = ref_model\n            if num_shared_layers is not None:\n                warnings.warn(\n                    \"num_shared_layers is ignored when ref_model is provided. Two different models are used for the \"\n                    \"model and the reference model and no layers are shared.\",\n                    UserWarning,\n                )\n        elif ref_model is None and not self.is_peft_model:\n            self.ref_model = create_reference_model(self.model, num_shared_layers=num_shared_layers)\n        elif self.is_peft_model:\n            self.ref_model = None\n        else:\n            raise ValueError(\n                f\"ref_model must be a PreTrainedModelWrapper or `None`, got {type(ref_model)} - supported \"\n                f\"architectures are: {SUPPORTED_ARCHITECTURES} \"\n            )\n\n        if not (isinstance(tokenizer, PreTrainedTokenizer) or isinstance(tokenizer, PreTrainedTokenizerFast)):\n            raise ValueError(\n                \"tokenizer must be a transformers.PreTrainedTokenizer or transformers.PreTrainedTokenizerFast\"\n            )\n        self.tokenizer = tokenizer\n\n        if dataset is not None and not (isinstance(dataset, torch.utils.data.Dataset) or isinstance(dataset, Dataset)):\n            raise ValueError(\"dataset must be a torch.utils.data.Dataset or datasets.Dataset\")\n        elif dataset is None:\n            warnings.warn(\n                \"No dataset is provided. Make sure to set config.batch_size to the correct value before training.\",\n                UserWarning,\n            )\n        self.dataset = dataset\n        self._signature_columns = None\n        if self.dataset is not None:\n            self.dataloader = self.prepare_dataloader(self.dataset, data_collator)\n        elif self.dataset is None and self.accelerator.num_processes > 1:\n            warnings.warn(\n                \"No dataset is provided. In a multi-GPU setting, this will lead to an error. You should\"\n                \" prepare your dataloader yourself with `dataloader = ppo_trainer.accelerator.prepare(dataloader)`\"\n                \" and using `torch.utils.data.DataLoader`, or pass a dataset to the `PPOTrainer`. Please \"\n                \" refer to the documentation for more details.\",\n                UserWarning,\n            )\n            self.dataloader = None\n        else:\n            self.dataloader = None\n\n        # Step 3: Initialize optimizer and data collator\n        self.data_collator = DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n        if optimizer is None:\n            self.optimizer = Adam(\n                filter(lambda p: p.requires_grad, self.model.parameters()),\n                lr=self.config.learning_rate,\n            )\n        else:\n            self.optimizer = optimizer\n\n        self.lr_scheduler = lr_scheduler\n        if self.lr_scheduler is not None:\n            lr_scheduler_class = (\n                torch.optim.lr_scheduler._LRScheduler\n                if not is_torch_greater_2_0()\n                else torch.optim.lr_scheduler.LRScheduler\n            )\n\n            if not isinstance(self.lr_scheduler, lr_scheduler_class):\n                raise ValueError(\n                    \"lr_scheduler must be a torch.optim.lr_scheduler._LRScheduler or torch.optim.lr_scheduler.LRScheduler (for torch >= 2.0)\"\n                )\n\n        if self.config.adap_kl_ctrl:\n            self.kl_ctl = AdaptiveKLController(self.config.init_kl_coef, self.config.target, self.config.horizon)\n        else:\n            self.kl_ctl = FixedKLController(self.config.init_kl_coef)\n\n        # Safety checkers for DS integration\n        is_deepspeed_used = self.accelerator.distributed_type == \"DEEPSPEED\" and hasattr(\n            self.accelerator.state, \"deepspeed_plugin\"\n        )\n\n        (\n            self.model,\n            self.optimizer,\n            self.data_collator,\n            self.dataloader,\n            self.lr_scheduler,\n        ) = self.accelerator.prepare(\n            self.model,\n            self.optimizer,\n            self.data_collator,\n            self.dataloader,\n            self.lr_scheduler,\n        )\n        if is_deepspeed_used:\n            # Quantized models are already set on the correct device\n            if not self.is_peft_model and not (\n                getattr(self.ref_model.pretrained_model, \"is_loaded_in_8bit\", False)\n                or getattr(self.ref_model.pretrained_model, \"is_loaded_in_4bit\", False)\n            ):\n                self.ref_model = self._prepare_deepspeed(self.ref_model)\n        else:\n            self.ref_model = self.accelerator.prepare(self.ref_model)\n\n        # In a distributed setup, only logging needs to be performed on the main process\n        # check: https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html\n        # or: https://discuss.pytorch.org/t/use-distributed-data-parallel-correctly/82500/11\n        self.is_distributed = self.accelerator.distributed_type == \"MULTI_GPU\"\n\n        # init the current step\n        self.current_step = 0\n\n        # init variables for pushing model to hub\n        if config.push_to_hub_if_best_kwargs:\n            if \"repo_id\" not in config.push_to_hub_if_best_kwargs:\n                raise ValueError(\"You have to specify repo_id in order to push the model to the hub!\")\n            self.push_to_hub_kwargs = config.push_to_hub_if_best_kwargs\n            self.compare_step = 0\n            self.highest_reward = torch.tensor(-float(\"inf\"))\n\n        # post process for PP\n        if not getattr(self.model, \"is_sequential_parallel\", False):\n            self.current_device = self.accelerator.device\n        else:\n            self.current_device = torch.device(\"cuda:0\")\n\n        PPODecorators.optimize_cuda_cache = self.config.optimize_cuda_cache\n\n        self.running = RunningMoments(self.accelerator)\n\n    def _filter_kwargs(self, kwargs, target_func):\n        \"\"\"\n        filter the keyword arguments that are supported by the target function.\n\n        Args:\n            kwargs (dict):\n                Keyword arguments\n            target_func (function):\n                Target function\n        \"\"\"\n        return {k: v for k, v in kwargs.items() if k in inspect.signature(target_func).parameters.keys()}\n\n    def prepare_dataloader(self, dataset: Union[torch.utils.data.Dataset, Dataset], data_collator=None):\n        \"\"\"\n        Prepare the dataloader for training.\n\n        Args:\n            dataset (Union[`torch.utils.data.Dataset`, `datasets.Dataset`]):\n                PyTorch dataset or Hugging Face dataset. If a Hugging Face dataset is passed, the dataset\n                will be preprocessed by removing the columns that are not used by the model.\n            data_collator (Optional[function]):\n                Data collator function.\n\n        Returns:\n            `torch.utils.data.DataLoader`: PyTorch dataloader\n        \"\"\"\n        if isinstance(dataset, Dataset):\n            dataset = self._remove_unused_columns(dataset)\n        dataloader = torch.utils.data.DataLoader(\n            dataset,\n            batch_size=self.config.batch_size,\n            collate_fn=data_collator,\n            shuffle=True,\n            drop_last=True,\n        )\n        return dataloader\n\n    # Adapted from transformers.Trainer._set_signature_columns_if_needed\n    def _set_signature_columns_if_needed(self):\n        if self._signature_columns is None:\n            # Inspect model forward signature to keep only the arguments it accepts.\n            signature = inspect.signature(self.model.forward)\n            self._signature_columns = list(signature.parameters.keys())\n            # label => sentiment | we need query and response for logging purpose\n            self._signature_columns += [\"label\", \"query\", \"response\"]\n\n    # Adapted from transformers.Trainer._remove_unused_columns\n    def _remove_unused_columns(self, dataset: \"Dataset\"):\n        if not self.config.remove_unused_columns:\n            return dataset\n        self._set_signature_columns_if_needed()\n        signature_columns = self._signature_columns\n\n        ignored_columns = list(set(dataset.column_names) - set(signature_columns))\n\n        columns = [k for k in signature_columns if k in dataset.column_names]\n\n        if version.parse(datasets.__version__) < version.parse(\"1.4.0\"):\n            dataset.set_format(\n                type=dataset.format[\"type\"],\n                columns=columns,\n                format_kwargs=dataset.format[\"format_kwargs\"],\n            )\n            return dataset\n        else:\n            return dataset.remove_columns(ignored_columns)\n\n    def generate(\n        self,\n        query_tensor: Union[torch.Tensor, List[torch.Tensor]],\n        length_sampler: Callable = None,\n        batch_size: int = 4,\n        return_prompt: bool = True,\n        **generation_kwargs,\n    ):\n        \"\"\"\n        Generate response with the model given the query tensor.\n        call the `generate` method of the model.\n\n        Args:\n            query_tensor (`torch.LongTensor`):\n                A tensor of shape (`batch_size`, `seq_len`) containing query tokens.\n            generation_kwargs (dict[str, Any]):\n                Keyword arguments for generation.\n            length_sampler (`Callable`, *optional*):\n                Callable that returns the number of newly generated tokens.\n            batch_size (`int`, *optional):\n                Batch size used for generation, defaults to `4`.\n            return_prompt (`bool`, *optional*):\n                If set to `False` the prompt is not returned but only the newly generated tokens, defaults to `True`.\n\n        Returns:\n            `torch.LongTensor`: A tensor of shape (`batch_size`, `gen_len`) containing response tokens.\n        \"\"\"\n\n        if isinstance(query_tensor, List):\n            return self._generate_batched(\n                query_tensor,\n                length_sampler=length_sampler,\n                batch_size=batch_size,\n                return_prompt=return_prompt,\n                **generation_kwargs,\n            )\n\n        else:\n            if length_sampler is not None:\n                generation_kwargs[\"max_new_tokens\"] = length_sampler()\n            response = self.accelerator.unwrap_model(self.model).generate(\n                input_ids=query_tensor.unsqueeze(dim=0), **generation_kwargs\n            )\n\n            if not return_prompt and not self.is_encoder_decoder:\n                return response[:, query_tensor.shape[0] :]\n            return response\n\n    def _generate_batched(\n        self,\n        query_tensors: List[torch.Tensor],\n        length_sampler: Callable = None,\n        batch_size: int = 4,\n        return_prompt: bool = True,\n        pad_to_multiple_of: int = None,\n        remove_padding: bool = True,\n        **generation_kwargs,\n    ):\n        outputs = []\n\n        padding_side_default = self.tokenizer.padding_side\n        if not self.is_encoder_decoder:\n            self.tokenizer.padding_side = \"left\"\n\n        # in case we have fewer examples than bs\n        batch_size = min(len(query_tensors), batch_size)\n\n        for i in range(0, len(query_tensors), batch_size):\n            if length_sampler is not None:\n                generation_kwargs[\"max_new_tokens\"] = length_sampler()\n\n            # prevent overflow if query tensors are not even multiple of bs\n            end_index = min(len(query_tensors), i + batch_size)\n\n            batch = query_tensors[i:end_index]\n            batch_mask = [torch.ones_like(element) for element in batch]\n            inputs = {\"input_ids\": batch, \"attention_mask\": batch_mask}\n\n            padded_inputs = self.tokenizer.pad(\n                inputs,\n                padding=True,\n                max_length=None,\n                pad_to_multiple_of=pad_to_multiple_of,\n                return_tensors=\"pt\",\n            ).to(self.current_device)\n\n            generations = self.accelerator.unwrap_model(self.model).generate(**padded_inputs, **generation_kwargs)\n\n            for generation, mask in zip(generations, padded_inputs[\"attention_mask\"]):\n                if not self.is_encoder_decoder:\n                    output = generation[(1 - mask).sum() :]  # remove padding\n                else:\n                    output = generation\n\n                if not return_prompt and not self.is_encoder_decoder:\n                    output = output[(mask).sum() :]  # remove prompt\n\n                if remove_padding and self.tokenizer.eos_token_id in output:\n                    pad_mask = output == self.tokenizer.eos_token_id\n                    pad_start = torch.nonzero(pad_mask, as_tuple=False)[0, 0].item()\n                    output = output[: pad_start + 1]  # keep the eos token at the end\n\n                outputs.append(output)\n\n        self.tokenizer.padding_side = padding_side_default\n        return outputs\n\n    def _step_safety_checker(\n        self,\n        batch_size: int,\n        queries: List[torch.LongTensor],\n        responses: List[torch.LongTensor],\n        scores: List[torch.FloatTensor],\n        masks: Optional[List[torch.LongTensor]] = None,\n    ):\n        \"\"\"\n        Check if the input data is valid for training.\n\n        Args:\n            batch_size (int):\n                Batch size from the config file.\n            queries (List[`torch.LongTensor`]):\n                List of tensors containing the encoded queries of shape (`query_length`)\n            responses (List[`torch.LongTensor`]):\n                List of tensors containing the encoded responses of shape (`response_length`)\n            scores (List[`torch.FloatTensor`]):\n                List of tensors containing the scores.\n            masks (List[`torch.LongTensor`], *optional*):\n                list of optional tensors containing the masks of shape (`query_length` + `response_length`)\n        Returns:\n            `tuple`: The input processed data.\n        \"\"\"\n        for name, tensor_list in zip([\"queries\", \"responses\", \"scores\"], [queries, responses, scores]):\n            if not isinstance(tensor_list, list):\n                raise ValueError(f\"{name} must be a list of tensors - got {type(tensor_list)}\")\n            if not isinstance(tensor_list[0], torch.Tensor):\n                raise ValueError(f\"Elements in {name} must be tensors - got {type(tensor_list[0])}\")\n            if batch_size is not None and len(tensor_list) != batch_size:\n                raise ValueError(\n                    f\"Batch size ({batch_size}) does not match number of examples - but got {len(tensor_list)} for: {name}\"\n                )\n\n        # add queries, scores and responses on the correct device\n        queries = [tensor.to(self.current_device) for tensor in queries]\n        responses = [tensor.to(self.current_device) for tensor in responses]\n        scores = [tensor.to(self.current_device) for tensor in scores]\n        masks = [tensor.to(self.current_device) for tensor in masks] if masks is not None else None\n\n        # squeeze scores if needed\n        for i, score in enumerate(scores):\n            if score.dim() > 1:\n                raise ValueError(f\"Scores must be 1-dimensional - got {score.dim()} for {score}\")\n            elif score.dim() == 1:\n                scores[i] = score.squeeze()\n\n        return queries, responses, scores, masks\n\n    @PPODecorators.empty_cuda_cache()\n    def step(\n        self,\n        queries: List[torch.LongTensor],\n        responses: List[torch.LongTensor],\n        scores: List[torch.FloatTensor],\n        risk_alpha: List[torch.LongTensor],\n        response_masks: Optional[List[torch.LongTensor]] = None\n    ):\n        \"\"\"\n        Run a PPO optimisation step given a list of queries, model responses, and rewards.\n\n        Args:\n            queries (List[`torch.LongTensor`]):\n                List of tensors containing the encoded queries of shape (`query_length`)\n            responses (List[`torch.LongTensor`]):\n                List of tensors containing the encoded responses of shape (`response_length`)\n            scores (List[`torch.FloatTensor`]):\n                List of tensors containing the scores.\n            response_masks (List[`torch.FloatTensor`], *optional*)):\n                List of tensors containing masks of the response tokens.\n\n        Returns:\n            `dict[str, Any]`: A summary of the training statistics\n        \"\"\"\n\n        bs = self.config.batch_size\n\n        queries, responses, scores, response_masks = self._step_safety_checker(\n            bs, queries, responses, scores, response_masks\n        )\n        scores = torch.tensor(scores, device=self.current_device)\n        if self.config.use_score_scaling:\n            # Score scaling\n            scores_mean, scores_std = self.running.update(scores)\n            tensor_to_kwargs = dict(dtype=scores.dtype, device=scores.device)\n            score_scaling_factor = self.running.std.to(**tensor_to_kwargs) + torch.finfo(scores.dtype).eps\n            if self.config.use_score_norm:\n                scores = (scores - self.running.mean.to(**tensor_to_kwargs)) / score_scaling_factor\n            else:\n                scores /= score_scaling_factor\n\n        if self.config.score_clip is not None:\n            # Score clipping\n            scores_dtype = scores.dtype\n            scores = torch.clip(scores.float(), -self.config.score_clip, self.config.score_clip).to(dtype=scores_dtype)\n\n        # if we want to push best model to the hub\n        if hasattr(self, \"highest_reward\"):\n            if self.compare_step % self.config.compare_steps == 0:\n                curr_mean_reward = scores.mean()\n                # if the best reward ever seen\n                if curr_mean_reward > self.highest_reward:\n                    self.highest_reward = curr_mean_reward\n                    # push model to hub\n                    self.push_to_hub(**self.push_to_hub_kwargs)\n            self.compare_step += 1\n\n        timing = dict()\n        t0 = time.time()\n\n        t = time.time()\n\n        model_inputs = self.prepare_model_inputs(queries, responses)\n\n        # performs padding in distributed setting \n        # what about padding in non-distributed setting then? \n        if self.is_distributed:\n\n            # if response length = 42, left padding adds 6 zeros to the left of attention mask to reach desired total length\n            pad_first = self.tokenizer.padding_side == \"left\"\n\n            model_inputs[\"input_ids\"] = self.accelerator.pad_across_processes(\n                model_inputs[\"input_ids\"],\n                dim=1,\n                pad_index=self.tokenizer.pad_token_id,\n                pad_first=pad_first,\n            )\n            model_inputs[\"attention_mask\"] = self.accelerator.pad_across_processes(\n                model_inputs[\"attention_mask\"], dim=1, pad_index=0, pad_first=pad_first\n            )\n            if self.is_encoder_decoder:\n                model_inputs[\"decoder_input_ids\"] = self.accelerator.pad_across_processes(\n                    model_inputs[\"decoder_input_ids\"],\n                    dim=1,\n                    pad_index=self.tokenizer.pad_token_id,\n                    pad_first=pad_first,\n                )\n                model_inputs[\"decoder_attention_mask\"] = self.accelerator.pad_across_processes(\n                    model_inputs[\"decoder_attention_mask\"],\n                    dim=1,\n                    pad_index=0,\n                    pad_first=pad_first,\n                )\n\n        model_inputs_names = list(model_inputs.keys())\n\n        full_kl_penalty = self.config.kl_penalty == \"full\"\n\n        with torch.no_grad():\n            all_logprobs, logits_or_none, values, masks = self.batched_forward_pass(\n                self.model,\n                queries,\n                responses,\n                model_inputs,\n                response_masks=response_masks,\n                return_logits=full_kl_penalty,\n            )\n\n            # for when the model is a peft model\n            if self.is_peft_model and hasattr(\n                self.accelerator.unwrap_model(self.model).pretrained_model,\n                \"disable_adapter\",\n            ):\n                with self.accelerator.unwrap_model(self.model).pretrained_model.disable_adapter():\n                    ref_logprobs, ref_logits_or_none, _, _ = self.batched_forward_pass(\n                        self.model, queries, responses, model_inputs, return_logits=full_kl_penalty\n                    )\n            elif self.is_peft_model and not hasattr(self.model.pretrained_model, \"disable_adapter\"):\n                raise ValueError(\n                    \"You are using a `peft` version that does not support `disable_adapter`. Please update your `peft` version to the latest version.\"\n                )\n\n            else:\n                ref_logprobs, ref_logits_or_none, _, _ = self.batched_forward_pass(\n                    self.ref_model, queries, responses, model_inputs, return_logits=full_kl_penalty\n                )\n\n        timing[\"time/ppo/forward_pass\"] = time.time() - t\n\n        with torch.no_grad():\n            t = time.time()\n            if full_kl_penalty:\n                active_full_logprobs = logprobs_from_logits(logits_or_none, None, gather=False)\n                ref_full_logprobs = logprobs_from_logits(ref_logits_or_none, None, gather=False)\n\n                rewards, non_score_reward = self.compute_rewards(\n                    scores, active_full_logprobs, ref_full_logprobs, masks\n                )\n            else:\n                rewards, non_score_reward = self.compute_rewards(scores, all_logprobs, ref_logprobs, masks)\n            timing[\"time/ppo/compute_rewards\"] = time.time() - t\n\n            t = time.time()\n\n            values, advantages, returns, masks = self.compute_advantages(values, rewards, masks, risk_alpha)\n            timing[\"time/ppo/compute_advantages\"] = time.time() - t\n\n        # upcast to float32 to avoid dataset issues\n        batch_dict = {\n            \"queries\": queries,\n            \"responses\": responses,\n            \"logprobs\": all_logprobs.to(torch.float32),\n            \"values\": values.to(torch.float32),\n            \"masks\": masks,\n            \"advantages\": advantages,\n            \"returns\": returns,\n        }\n        batch_dict.update(model_inputs)\n\n        t = time.time()\n        all_stats = []\n        early_stop = False\n        for _ in range(self.config.ppo_epochs):\n            if early_stop:\n                break\n            b_inds = np.random.permutation(bs)\n            for backward_batch_start in range(0, bs, self.config.backward_batch_size):\n                backward_batch_end = backward_batch_start + self.config.backward_batch_size\n                backward_batch_inds = b_inds[backward_batch_start:backward_batch_end]\n\n                for mini_batch_start in range(0, self.config.backward_batch_size, self.config.mini_batch_size):\n                    mini_batch_end = mini_batch_start + self.config.mini_batch_size\n                    mini_batch_inds = backward_batch_inds[mini_batch_start:mini_batch_end]\n                    mini_batch_dict = {\n                        \"logprobs\": batch_dict[\"logprobs\"][mini_batch_inds],\n                        \"values\": batch_dict[\"values\"][mini_batch_inds],\n                        \"masks\": batch_dict[\"masks\"][mini_batch_inds],\n                        # hacks: the queries and responses are ragged.\n                        \"queries\": [batch_dict[\"queries\"][i] for i in mini_batch_inds],\n                        \"responses\": [batch_dict[\"responses\"][i] for i in mini_batch_inds],\n                        \"advantages\": batch_dict[\"advantages\"][mini_batch_inds],\n                        \"returns\": batch_dict[\"returns\"][mini_batch_inds],\n                    }\n                    for k in model_inputs_names:\n                        mini_batch_dict[k] = batch_dict[k][mini_batch_inds]\n                    with self.accelerator.accumulate(self.model):\n                        model_inputs = {k: mini_batch_dict[k] for k in model_inputs_names}\n\n                        logprobs, logits, vpreds, _ = self.batched_forward_pass(\n                            self.model,\n                            mini_batch_dict[\"queries\"],\n                            mini_batch_dict[\"responses\"],\n                            model_inputs,\n                            return_logits=True,\n                        )\n                        train_stats = self.train_minibatch(\n                            mini_batch_dict[\"logprobs\"],\n                            mini_batch_dict[\"values\"],\n                            logprobs,\n                            logits,\n                            vpreds,\n                            mini_batch_dict[\"masks\"],\n                            mini_batch_dict[\"advantages\"],\n                            mini_batch_dict[\"returns\"],\n                        )\n                        all_stats.append(train_stats)\n\n            # typically, early stopping is done at the epoch level\n            if self.config.early_stopping:\n                policykl = train_stats[\"policy/policykl\"]\n                early_stop = self._early_stop(policykl)\n                if early_stop:\n                    break\n\n        timing[\"time/ppo/optimize_step\"] = time.time() - t\n\n        t = time.time()\n        train_stats = stack_dicts(all_stats)\n\n        # reshape advantages/ratios such that they are not averaged.\n        train_stats[\"policy/advantages\"] = torch.flatten(train_stats[\"policy/advantages\"]).unsqueeze(0)\n        train_stats[\"policy/advantages\"] = torch.nan_to_num(train_stats[\"policy/advantages\"], WANDB_PADDING)\n        train_stats[\"policy/ratio\"] = torch.flatten(train_stats[\"policy/ratio\"]).unsqueeze(0)\n\n        stats = self.record_step_stats(\n            scores=scores,\n            logprobs=all_logprobs,\n            ref_logprobs=ref_logprobs,\n            non_score_reward=non_score_reward,\n            train_stats=train_stats,\n            kl_coef=self.kl_ctl.value,\n            masks=masks,\n            queries=queries,\n            responses=responses,\n        )\n        # Gather/Reduce stats from all processes\n        if self.is_distributed:\n            stats = self.gather_stats(stats)\n        stats = stats_to_np(stats)\n        timing[\"time/ppo/calc_stats\"] = time.time() - t\n        stats[\"ppo/learning_rate\"] = self.optimizer.param_groups[0][\"lr\"]\n\n        # Update the KL control - multiply the batch_size by the number of processes\n        self.kl_ctl.update(\n            stats[\"objective/kl\"],\n            self.config.batch_size * self.accelerator.num_processes,\n        )\n\n        # Log the total ppo time\n        timing[\"time/ppo/total\"] = time.time() - t0\n        stats.update(timing)\n\n        # post-process stats for tensorboard and other loggers\n        if self.config.log_with != \"wandb\":\n            stats = convert_to_scalar(stats)\n\n        if self.lr_scheduler is not None:\n            self.lr_scheduler.step()\n\n        return stats\n\n    def _early_stop(self, policykl):\n        r\"\"\"\n        Handles the early stopping logic. If the policy KL is greater than the target KL, then the gradient is zeroed and\n        the optimization step is skipped.\n        This also handles the multi-gpu case where the policy KL is averaged across all processes.\n\n        Args:\n            policy_kl (torch.Tensor):\n                the policy KL\n\n        Returns:\n            `bool`: whether to early stop or not\n        \"\"\"\n        early_stop = False\n        if not self.config.early_stopping:\n            return early_stop\n\n        if not self.is_distributed and policykl > 1.5 * self.config.target_kl:\n            self.optimizer.zero_grad()\n            early_stop = True\n        elif self.is_distributed:\n            import torch.distributed as dist\n\n            # Wait for all processes to finish\n            dist.barrier()\n\n            # all gather the policykl\n            dist.all_reduce(policykl, dist.ReduceOp.SUM)\n            policykl /= self.accelerator.num_processes\n\n            if policykl > 1.5 * self.config.target_kl:\n                self.optimizer.zero_grad()\n                early_stop = True\n        return early_stop\n\n    def gather_stats(self, stats):\n        \"\"\"\n        Gather stats from all processes. Useful in the context of distributed training.\n\n        Args:\n            stats (dict[str, Any]):\n            a dictionary of stats to be gathered. The stats should contain torch tensors.\n\n        Returns:\n            `dict[str, Any]`: A dictionary of stats with the tensors gathered.\n        \"\"\"\n        import torch.distributed as dist\n\n        # Wait for all processes to finish\n        dist.barrier()\n\n        for k, v in stats.items():\n            if isinstance(v, torch.Tensor):\n                dist.all_reduce(v.to(self.accelerator.device), dist.ReduceOp.SUM)\n                v /= self.accelerator.num_processes\n            stats[k] = v\n        return stats\n\n    def prepare_model_inputs(self, queries: torch.Tensor, responses: torch.Tensor):\n        if self.is_encoder_decoder:\n            input_data = self.data_collator(\n                [{\"input_ids\": q, \"attention_mask\": torch.ones_like(q)} for q in queries]\n            ).to(self.current_device)\n\n            decoder_inputs = self.data_collator(\n                [{\"input_ids\": r, \"attention_mask\": torch.ones_like(r)} for r in responses]\n            ).to(self.current_device)\n\n            input_data[\"decoder_input_ids\"] = decoder_inputs[\"input_ids\"]\n            input_data[\"decoder_attention_mask\"] = decoder_inputs[\"attention_mask\"]\n        else:\n            input_ids = [torch.cat([q, r]) for q, r in zip(queries, responses)]\n            # attention mask is 1 only for the query + actual response lengths and not (64+48=112)\n            input_data = self.data_collator(\n                [{\"input_ids\": ids, \"attention_mask\": torch.ones_like(ids)} for ids in input_ids]\n            ).to(self.current_device)\n\n        input_data.pop(\"labels\", None)  # we don't want to compute LM losses\n        return input_data\n\n    @PPODecorators.empty_cuda_cache()\n    def batched_forward_pass(\n        self,\n        model: PreTrainedModelWrapper,\n        queries: torch.Tensor,\n        responses: torch.Tensor,\n        model_inputs: dict,\n        return_logits: bool = False,\n        response_masks: Optional[torch.Tensor] = None,\n    ):\n        \"\"\"\n        Calculate model outputs in multiple batches.\n\n        Args:\n            queries (`torch.LongTensor`):\n                List of tensors containing the encoded queries, shape (`batch_size`, `query_length`)\n            responses (`torch.LongTensor`):\n                List of tensors containing the encoded responses, shape (`batch_size`, `response_length`)\n            return_logits (`bool`, *optional*, defaults to `False`):\n                Whether to return all_logits. Set to `False` if logits are not needed to reduce memory consumption.\n        Returns:\n            (tuple):\n                - all_logprobs (`torch.FloatTensor`): Log probabilities of the responses,\n                    shape (`batch_size`, `response_length`)\n                - all_ref_logprobs (`torch.FloatTensor`): Log probabilities of the responses,\n                    shape (`batch_size`, `response_length`)\n                - all_values (`torch.FloatTensor`): Values of the responses, shape (`batch_size`, `response_length`)\n        \"\"\"\n        bs = len(queries)\n        fbs = self.config.mini_batch_size\n        all_logprobs = []\n        all_logits = []\n        all_masks = []\n        all_values = []\n\n        model.eval()\n\n\n        for i in range(math.ceil(bs / fbs)):\n            input_kwargs = {key: value[i * fbs : (i + 1) * fbs] for key, value in model_inputs.items()}\n            query_batch = queries[i * fbs : (i + 1) * fbs]\n            response_batch = responses[i * fbs : (i + 1) * fbs]\n            if response_masks is not None:\n                response_masks_batch = response_masks[i * fbs : (i + 1) * fbs]\n            logits, _, values = model(**input_kwargs)\n\n            if self.is_encoder_decoder:\n                input_ids = input_kwargs[\"decoder_input_ids\"]\n                attention_mask = input_kwargs[\"decoder_attention_mask\"]\n            else:\n                input_ids = input_kwargs[\"input_ids\"]\n                attention_mask = input_kwargs[\"attention_mask\"]\n\n\n            # logits at step\n            # why are we reducing size by 1\n            # because at the current time step/token, logits are over the next token \n            # thats why logits[:, 0, :] is for input_ids[:, 1] \n            logprobs = logprobs_from_logits(logits[:, :-1, :], input_ids[:, 1:])\n            masks = torch.zeros_like(attention_mask)\n            # the following would mean that the last element of masks is always zero\n            masks[:, :-1] = attention_mask[:, 1:]\n\n            #For a single sequence model, start position is the end of the query. End is start + response length.\n            #It handles optional left padding that may need to be offset if present.\n            #The masks tensor is used to mask the loss - set to 0 before start and after end index, 1 inside.\n            #If a separate response mask is passed in, that is combined with the masks tensor to further mask just the response portion.\n\n            for j in range(len(query_batch)):\n                if self.is_encoder_decoder:\n                    # Decoder sentence starts always in the index 1 after padding in the Enc-Dec Models\n                    start = 1\n                    end = attention_mask[j, :].sum() - 1\n                else:\n                    start = len(query_batch[j]) - 1  # logprobs starts from the second query token\n                    am_start = start\n                    if attention_mask[j, 0] == 0:  # offset left padding\n                        am_start += attention_mask[j, :].nonzero()[0]\n                    end = start + len(response_batch[j])\n                    am_end = am_start + len(response_batch[j])\n                    if response_masks is not None:\n                        response_masks_batch[j] = torch.cat(\n                            (torch.zeros_like(query_batch[j]), response_masks_batch[j])\n                        )[1:]\n\n                masks[j, :start] = 0\n                masks[j, end:] = 0\n                if response_masks is not None:\n                    # this was incorrect\n                    masks[j, am_start:am_end] = masks[j, am_start:am_end] * response_masks_batch[j][start:end]\n\n            if return_logits:\n                all_logits.append(logits)\n            else:\n                del logits\n            all_values.append(values)\n            all_logprobs.append(logprobs)\n            all_masks.append(masks)\n\n        return (\n            torch.cat(all_logprobs),\n            torch.cat(all_logits)[:, :-1] if return_logits else None,\n            torch.cat(all_values)[:, :-1],\n            torch.cat(all_masks)[:, :-1],\n        )\n\n    @PPODecorators.empty_cuda_cache()\n    def train_minibatch(\n        self,\n        old_logprobs: torch.FloatTensor,\n        values: torch.FloatTensor,\n        logprobs: torch.FloatTensor,\n        logits: torch.FloatTensor,\n        vpreds: torch.FloatTensor,\n        mask: torch.LongTensor,\n        advantages: torch.FloatTensor,\n        returns: torch.FloatTensor,\n    ):\n        \"\"\"\n        Train one PPO minibatch\n\n        Args:\n            logprobs (`torch.FloatTensor`):\n                Log probabilities of the model, shape [batch_size, response_length]\n            values (`torch.FloatTensor`):\n                Values of the value head, shape [batch_size, response_length]\n            query (`torch.LongTensor`):\n                Encoded queries, shape [batch_size, query_length]\n            response (`torch.LongTensor`):\n                Encoded responses, shape [batch_size, response_length]\n            model_input (`torch.LongTensor`):\n                Concatenated queries and responses, shape [batch_size, query_length+response_length]\n\n        Returns:\n            train_stats (dict[str, `torch.Tensor`]):\n                Dictionary of training statistics\n        \"\"\"\n        self.model.train()\n        loss_p, loss_v, train_stats = self.loss(\n            old_logprobs, values, logits, vpreds, logprobs, mask, advantages, returns\n        )\n        loss = loss_p + loss_v\n        self.accelerator.backward(loss)\n        if self.config.max_grad_norm is not None:\n            if self.accelerator.sync_gradients:\n                self.accelerator.clip_grad_norm_(self.model_params, self.config.max_grad_norm)\n        self.optimizer.step()\n        # we call optimizer.zero_grad() every time and let `accelerator` handle accumulation\n        # see https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation#the-finished-code\n        self.optimizer.zero_grad()\n        return train_stats\n\n    def compute_rewards(\n        self,\n        scores: torch.FloatTensor,\n        logprobs: torch.FloatTensor,\n        ref_logprobs: torch.FloatTensor,\n        masks: torch.LongTensor,\n    ):\n        \"\"\"\n        Compute per token rewards from scores and KL-penalty.\n\n        Args:\n            scores (`torch.FloatTensor`):\n                Scores from the reward model, shape (`batch_size`)\n            logprobs (`torch.FloatTensor`):\n                Log probabilities of the model, shape (`batch_size`, `response_length`)\n            ref_logprobs (`torch.FloatTensor`):\n                Log probabilities of the reference model, shape (`batch_size`, `response_length`)\n        \"\"\"\n        rewards, non_score_rewards = [], []\n        i = 0\n        for score, logprob, ref_logprob, mask in zip(scores, logprobs, ref_logprobs, masks):\n            #print(i, len(mask.nonzero()))\n            # compute KL penalty (from difference in logprobs)\n            kl = self._kl_penalty(logprob, ref_logprob)\n            non_score_reward = -self.kl_ctl.value * kl\n            non_score_rewards.append(non_score_reward)\n            reward = non_score_reward.clone()\n            if len(mask.nonzero()) == 0:\n                last_non_masked_index = 0\n            else: \n                last_non_masked_index = mask.nonzero()[-1]\n\n            # reward is preference model score + KL penalty\n            reward[last_non_masked_index] += score\n            rewards.append(reward)\n            i += 1\n        return torch.stack(rewards), torch.stack(non_score_rewards)\n\n    def _kl_penalty(self, logprob: torch.FloatTensor, ref_logprob: torch.FloatTensor) -> torch.FloatTensor:\n        if self.config.kl_penalty == \"kl\":\n            return logprob - ref_logprob\n\n        if self.config.kl_penalty == \"abs\":\n            return (logprob - ref_logprob).abs()\n\n        if self.config.kl_penalty == \"mse\":\n            return 0.5 * (logprob - ref_logprob).square()\n\n        if self.config.kl_penalty == \"full\":\n            # Flip is required due to this issue? :https://github.com/pytorch/pytorch/issues/57459\n            return F.kl_div(ref_logprob, logprob, log_target=True, reduction=\"none\").sum(-1)\n\n        raise NotImplementedError\n\n    def find_alpha_return(\n        self,\n        traj_returns: torch.FloatTensor,\n        mask: torch.FloatTensor,\n        risk_alpha: torch.LongTensor,\n    ):\n        sorted_returns, indices = torch.sort(traj_returns)\n        q_risk_alpha = sorted_returns[risk_alpha-1] # This is the CVaR calculation\n\n        for ii in range(traj_returns.shape[0]):\n            if traj_returns[ii] > q_risk_alpha:\n                #print('here', risk_alpha, ii)\n                mask[ii,:] = torch.zeros_like(mask[ii,:], dtype=int) # Modifying mask based on CVaR\n\n        return q_risk_alpha, mask\n\n\n    def compute_advantages(\n        self,\n        values: torch.FloatTensor,\n        rewards: torch.FloatTensor,\n        mask: torch.FloatTensor,\n        risk_alpha: torch.LongTensor,\n    ):\n        lastgaelam = 0\n        advantages_reversed = []\n        gen_len = rewards.shape[-1]\n\n        values = values * mask\n        rewards = rewards * mask\n\n        for t in reversed(range(gen_len)):\n            nextvalues = values[:, t + 1] if t < gen_len - 1 else 0.0\n            delta = rewards[:, t] + self.config.gamma * nextvalues - values[:, t]\n            lastgaelam = delta + self.config.gamma * self.config.lam * lastgaelam\n            advantages_reversed.append(lastgaelam)\n        advantages = torch.stack(advantages_reversed[::-1]).transpose(0, 1)\n\n        returns = advantages + values\n        # select 63rd index element of returns in the batch\n        traj_returns = returns[:,63] # Using `returns` to get trajectory returns\n        # find alpha_return and create new mask based on alpha_return\n        alpha_return, masks = self.find_alpha_return(traj_returns, mask, risk_alpha) # Calling the CVaR filtering function\n        # return this 'masks' so that step() can pass this mask to loss()\n\n        # Whiten / normalize the unmasked elements\n        advantages = masked_whiten(advantages, mask)\n        advantages = advantages.detach()\n        return values, advantages, returns, masks\n\n    def loss(\n        self,\n        old_logprobs: torch.FloatTensor,\n        values: torch.FloatTensor,\n        logits: torch.FloatTensor,\n        vpreds: torch.FloatTensor,\n        logprobs: torch.FloatTensor,\n        mask: torch.LongTensor,\n        advantages: torch.FloatTensor,\n        returns: torch.FloatTensor,\n    ):\n        \"\"\"\n        Calculate policy and value losses.\n\n        Args:\n            old_logprobs (`torch.FloatTensor`):\n                Log probabilities of the model, shape (`batch_size`, `response_length`)\n            values (`torch.FloatTensor`):\n                Values of the value head, shape (`batch_size`, `response_length`)\n            rewards (`torch.FloatTensor`):\n                Rewards from the reward model, shape (`batch_size`, `response_length`)\n            logits (`torch.FloatTensor`):\n                Logits of the model, shape (`batch_size`, `response_length`, `vocab_size`)\n            v_pred (`torch.FloatTensor`):\n                Values of the value head, shape (`batch_size`, `response_length`)\n            logprobs (`torch.FloatTensor`):\n                Log probabilities of the model, shape (`batch_size`, `response_length`)\n        \"\"\"\n\n        vpredclipped = clip_by_value(\n            vpreds,\n            values - self.config.cliprange_value,\n            values + self.config.cliprange_value,\n        )\n\n        vf_losses1 = (vpreds - returns) ** 2\n        vf_losses2 = (vpredclipped - returns) ** 2\n        vf_loss = 0.5 * masked_mean(torch.max(vf_losses1, vf_losses2), mask)\n        vf_clipfrac = masked_mean(torch.gt(vf_losses2, vf_losses1).float(), mask)\n\n        ratio = torch.exp(logprobs - old_logprobs)\n\n        pg_losses = -advantages * ratio\n        pg_losses2 = -advantages * torch.clamp(ratio, 1.0 - self.config.cliprange, 1.0 + self.config.cliprange)\n\n        pg_loss = masked_mean(torch.max(pg_losses, pg_losses2), mask)\n        pg_clipfrac = masked_mean(torch.gt(pg_losses2, pg_losses).float(), mask)\n\n        loss = pg_loss + self.config.vf_coef * vf_loss\n\n        avg_ratio = masked_mean(ratio, mask).item()\n        if avg_ratio > self.config.ratio_threshold:\n            warnings.warn(\n                f\"The average ratio of batch ({avg_ratio:.2f}) exceeds threshold {self.config.ratio_threshold:.2f}. Skipping batch.\"\n            )\n            pg_loss = pg_loss * 0.0\n            vf_loss = vf_loss * 0.0\n            loss = loss * 0.0\n\n        entropy = masked_mean(entropy_from_logits(logits), mask)\n\n        approxkl = 0.5 * masked_mean((logprobs - old_logprobs) ** 2, mask)\n        policykl = masked_mean(old_logprobs - logprobs, mask)\n\n        return_mean, return_var = masked_mean(returns, mask), masked_var(returns, mask)\n        value_mean, value_var = masked_mean(values, mask), masked_var(values, mask)\n\n        stats = dict(\n            loss=dict(policy=pg_loss.detach(), value=vf_loss.detach(), total=loss.detach()),\n            policy=dict(\n                entropy=entropy.detach(),\n                approxkl=approxkl.detach(),\n                policykl=policykl.detach(),\n                clipfrac=pg_clipfrac.detach(),\n                advantages=advantages.detach(),\n                advantages_mean=masked_mean(advantages, mask).detach(),\n                ratio=ratio.detach(),\n            ),\n            returns=dict(mean=return_mean.detach(), var=return_var.detach()),\n            val=dict(\n                vpred=masked_mean(vpreds, mask).detach(),\n                error=masked_mean((vpreds - returns) ** 2, mask).detach(),\n                clipfrac=vf_clipfrac.detach(),\n                mean=value_mean.detach(),\n                var=value_var.detach(),\n            ),\n        )\n        return pg_loss, self.config.vf_coef * vf_loss, flatten_dict(stats)\n\n    def record_step_stats(self, kl_coef: float, **data):\n        \"\"\"\n        Record training step statistics.\n\n\n        Args:\n            kl_coef (`float`):\n                KL coefficient\n            data (`dict`):\n                Dictionary of training step data\n\n        Returns:\n            stats (`dict`):\n                Dictionary of training step statistics\n        \"\"\"\n        mask = data.pop(\"masks\")\n\n        kl_list = ((data[\"logprobs\"] - data[\"ref_logprobs\"]) * mask).sum(axis=-1)\n        mean_kl = kl_list.mean()\n        mean_entropy = (-data[\"logprobs\"] * mask).sum(axis=-1).mean()\n\n        mean_non_score_reward = masked_mean(\n            data[\"non_score_reward\"], mask\n        )  # non_score_reward is size `batch_size`, `response_length`\n        mean_scores = data[\"scores\"].mean()  # scores is size `batch_size`\n        std_scores = data[\"scores\"].std()\n\n        if mean_kl.item() < -1.0:\n            # warn users\n            warnings.warn(\n                f\"KL divergence is starting to become negative: {mean_kl.item():.2f} - this might be a precursor for failed training.\"\n                \" sometimes this happens because the generation kwargs are not correctly set. Please make sure\"\n                \" that the generation kwargs are set correctly, or review your training hyperparameters.\"\n            )\n\n        stats = {\n            \"objective/kl\": mean_kl,\n            \"objective/kl_dist\": kl_list,\n            \"objective/logprobs\": data[\"logprobs\"],\n            \"objective/ref_logprobs\": data[\"ref_logprobs\"],\n            \"objective/kl_coef\": kl_coef,\n            \"objective/entropy\": mean_entropy,\n            \"ppo/mean_non_score_reward\": mean_non_score_reward,\n            \"ppo/mean_scores\": mean_scores,\n            \"ppo/std_scores\": std_scores,\n        }\n\n        # Log text properties\n        query_lens = torch.tensor([len(query) for query in data[\"queries\"]], dtype=torch.float)\n        response_lens = torch.tensor([len(response) for response in data[\"responses\"]], dtype=torch.float)\n\n        stats[\"tokens/queries_len_mean\"] = torch.mean(query_lens).cpu().numpy().item()\n        stats[\"tokens/queries_len_std\"] = torch.std(query_lens).cpu().numpy().item()\n        stats[\"tokens/queries_dist\"] = query_lens.cpu().numpy()\n        stats[\"tokens/responses_len_mean\"] = torch.mean(response_lens).cpu().numpy().item()\n        stats[\"tokens/responses_len_std\"] = torch.std(response_lens).cpu().numpy().item()\n        stats[\"tokens/responses_dist\"] = response_lens.cpu().numpy()\n\n        for k, v in data[\"train_stats\"].items():\n            stats[f\"ppo/{k}\"] = torch.mean(v, axis=0)\n        stats[\"ppo/val/var_explained\"] = 1 - stats[\"ppo/val/error\"] / stats[\"ppo/returns/var\"]\n        return stats\n\n    def log_stats(\n        self,\n        stats: dict,\n        batch: dict,\n        rewards: List[torch.FloatTensor],\n        columns_to_log: List[str] = [\"query\", \"response\"],\n    ):\n        \"\"\"\n        A function that logs all the training stats. Call it at the end of each epoch.\n\n        Args:\n            stats (dict[str, Any]):\n                A dictionary of training stats.\n            batch (dict[str, Any]):\n                A dictionary of batch data, this contains the queries and responses.\n            rewards (`List[torch.FloatTensor]`):\n                A tensor of rewards.\n        \"\"\"\n        # Log only if we are in the main process\n        if self.accelerator.is_main_process:\n            logs = {}\n\n            # Log stats\n            if not isinstance(rewards, torch.Tensor):\n                rewards = torch.tensor(rewards).to(self.current_device)\n\n            if \"query\" not in batch.keys() and \"response\" not in batch.keys():\n                # warn the user that the game logs will not be logged\n                warnings.warn(\n                    \"The game logs will not be logged because the batch does not contain the keys 'query' and \"\n                    \"'response'. \"\n                )\n            elif self.config.log_with == \"wandb\":\n                import wandb\n\n                if any([column_to_log not in batch.keys() for column_to_log in columns_to_log]):\n                    raise ValueError(f\"Columns to log {columns_to_log} are not present in the batch {batch.keys()}.\")\n\n                batch_list = [batch[column_to_log] for column_to_log in columns_to_log]\n\n                table_rows = [list(r) for r in zip(*batch_list, rewards.cpu().tolist())]\n                logs.update({\"game_log\": wandb.Table(columns=[*columns_to_log, \"reward\"], rows=table_rows)})\n            # All reduce rewards if distributed\n            if self.is_distributed:\n                import torch.distributed as dist\n\n                dist.barrier()\n\n                dist.all_reduce(rewards, op=torch.distributed.ReduceOp.SUM)\n                rewards /= self.accelerator.num_processes\n\n            logs.update(stats)\n\n            # manually cast in fp32 for bf16 torch tensors\n            for k, v in logs.items():\n                if isinstance(v, torch.Tensor) and v.dtype == torch.bfloat16:\n                    logs[k] = v.float()\n\n            logs[\"env/reward_mean\"] = torch.mean(rewards).cpu().numpy().item()\n            logs[\"env/reward_std\"] = torch.std(rewards).cpu().numpy().item()\n            logs[\"env/reward_dist\"] = rewards.cpu().numpy()\n\n            if self.config.log_with == \"tensorboard\":\n                # update the current step\n                self.current_step += 1\n\n            self.accelerator.log(\n                logs,\n                step=self.current_step if self.config.log_with == \"tensorboard\" else None,\n            )\n\n        else:\n            if self.is_distributed:\n                import torch.distributed as dist\n\n                if not isinstance(rewards, torch.Tensor):\n                    rewards = torch.tensor(rewards).to(self.current_device)\n\n                dist.barrier()\n                dist.all_reduce(rewards, op=torch.distributed.ReduceOp.SUM)\n\n    def create_model_card(self, path: str, model_name: Optional[str] = \"TRL Model\") -> None:\n        \"\"\"Creates and saves a model card for a TRL model.\n\n        Args:\n            path (`str`): The path to save the model card to.\n            model_name (`str`, *optional*): The name of the model, defaults to `TRL Model`.\n        \"\"\"\n        try:\n            user = whoami()[\"name\"]\n        # handle the offline case\n        except:  # noqa\n            warnings.warn(\"Cannot retrieve user information assuming you are running in offline mode.\")\n            return\n\n        if not os.path.exists(path):\n            os.makedirs(path)\n\n        model_card_content = MODEL_CARD_TEMPLATE.format(model_name=model_name, model_id=f\"{user}/{path}\")\n        with open(os.path.join(path, \"README.md\"), \"w\", encoding=\"utf-8\") as f:\n            f.write(model_card_content)\n\n    def _save_pretrained(self, save_directory: str) -> None:\n        self.accelerator.unwrap_model(self.model).save_pretrained(save_directory)\n        self.tokenizer.save_pretrained(save_directory)\n        self.create_model_card(save_directory)\n\n    def _show_tokens(self, tokens, masks):\n        from rich import print\n        from rich.text import Text\n\n        text = Text()\n\n        for i, (token, mask) in enumerate(zip(tokens, masks)):\n            if mask == 1:\n                text.append(self.tokenizer.decode(token.item()), style=\"black on deep_sky_blue1\")\n                text.append(\" \")\n            else:\n                text.append(self.tokenizer.decode(token.item()), style=\"black on cyan3\")\n                text.append(\" \")\n        print(text)\n\n    def _prepare_deepspeed(self, model: PreTrainedModelWrapper):\n        # Adapted from accelerate: https://github.com/huggingface/accelerate/blob/739b135f8367becb67ffaada12fe76e3aa60fefd/src/accelerate/accelerator.py#L1473\n        deepspeed_plugin = self.accelerator.state.deepspeed_plugin\n        config_kwargs = deepspeed_plugin.deepspeed_config\n        if model is not None:\n            if hasattr(model, \"config\"):\n                hidden_size = (\n                    max(model.config.hidden_sizes)\n                    if getattr(model.config, \"hidden_sizes\", None)\n                    else getattr(model.config, \"hidden_size\", None)\n                )\n                if hidden_size is not None and config_kwargs[\"zero_optimization\"][\"stage\"] == 3:\n                    # Note that `stage3_prefetch_bucket_size` can produce DeepSpeed messages like: `Invalidate trace cache @ step 0: expected module 1, but got module 0`\n                    # This is expected and is not an error, see: https://github.com/microsoft/DeepSpeed/discussions/4081\n                    config_kwargs.update(\n                        {\n                            \"zero_optimization.reduce_bucket_size\": hidden_size * hidden_size,\n                            \"zero_optimization.stage3_param_persistence_threshold\": 10 * hidden_size,\n                            \"zero_optimization.stage3_prefetch_bucket_size\": 0.9 * hidden_size * hidden_size,\n                        }\n                    )\n\n        # If ZeRO-3 is used, we shard both the active and reference model.\n        # Otherwise, we assume the reference model fits in memory and is initialized on each device with ZeRO disabled (stage 0)\n        if config_kwargs[\"zero_optimization\"][\"stage\"] != 3:\n            config_kwargs[\"zero_optimization\"][\"stage\"] = 0\n        model, *_ = deepspeed.initialize(model=model, config=config_kwargs)\n        model.eval()\n        return model\n\n\nimport datetime\nimport os\nfrom transformers import GPT2Tokenizer, GPT2Model\nimport pandas as pd\nimport math \nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom trl import PPOConfig, set_seed ,PPOTrainer\nfrom trl.core import LengthSampler\n\n\n@dataclass\nclass ScriptArguments:\n    ppo_config: PPOConfig = field(\n        default_factory=lambda: PPOConfig(\n            model_name=\"/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_final/2024-01-24_21-17-41/checkpoint-100\", \n            query_dataset=\"imdb\",\n            reward_model=\"sentiment-analysis:lvwerra/distilbert-imdb\",\n            learning_rate=1.41e-5,\n            mini_batch_size=128,\n            batch_size=128,\n            gradient_accumulation_steps=1,\n            early_stopping=False,\n            target_kl=6.0,\n            kl_penalty=\"kl\",\n            seed=0,\n            use_score_scaling=False,\n            use_score_norm=False,\n            score_clip=None,\n            log_with=\"tensorboard\",\n            project_kwargs={\"logging_dir\": logging_dir},\n            steps= 128000,\n            exp_name=\"ppo\"\n        )\n    )\n    query_dataset: str = field(default=\"imdb\", metadata={\"help\": \"the dataset to query\"})\n    use_seq2seq: bool = field(default=False, metadata={\"help\": \"whether to use seq2seq models\"})\n    use_peft: bool = field(default=False, metadata={\"help\": \"whether to use peft\"})\n    peft_config: Optional[LoraConfig] = field(\n        default_factory=lambda: LoraConfig(\n            r=16,\n            lora_alpha=16,\n            bias=\"none\",\n            task_type=\"CAUSAL_LM\",\n        ),\n    )\n    reward_function: str = field(default=\"trl\", metadata={\"help\": \"whether to use trl or rl4lm reward\"})\n    input_query_size: str = field(default=\"less\", metadata={\"help\": \"the dataset to query\"})\n    \n    generation_kwargs_min_length: Optional[int] = field(default=-1, metadata={\"help\": \"minimum number of tokens while generation\"})\n    generation_kwargs_top_k: Optional[int] = field(default=0, metadata={\"help\": \"gneration top k\"})\n    generation_kwargs_top_p: Optional[float] = field(default=1.0, metadata={\"help\": \"gneration top p\"})\n    generation_kwargs_max_new_tokens: Optional[int] = field(default=32, metadata={\"help\": \"gneration top p\"})\n\n    change_tokenizer_args: bool = field(default=False, metadata={\"help\": \"whether to use modify tokenizer settings\"})\n    tokenizer_kwargs: dict = field(\n    default_factory=lambda: {\n      \"padding_side\": \"left\", \n      \"truncation_side\": \"left\",\n      \"pad_token_as_eos_token\": True,\n      \"max_length\": 64\n    }\n  )\n    \n    risk_scheduler: str = field(default=\"old\", metadata={\"help\": \"old risk scheduler that doesn't go down to alpha\"})\n    risk_n: int = field(default=240, metadata={\"help\": \"240 (> batch size): no RA; 70: RA begins after iter 70; 1: RA throughout\"})\n    risk_alpha: Optional[float] = field(default=0.2, metadata={\"help\": \"risk alpha value = 20 percent\"})\n    risk_rho: Optional[float] = field(default=0.8, metadata={\"help\": \"risk alpha reaches a value = 20 percent at 80 percent of total iterations\"})\n\nargs = tyro.cli(ScriptArguments)\n\ngeneration_kwargs = {\n      \"min_length\": args.generation_kwargs_min_length, \n      \"top_k\": args.generation_kwargs_top_k,\n      \"top_p\": args.generation_kwargs_top_p,\n      \"do_sample\": True,\n      \"max_new_tokens\": args.generation_kwargs_max_new_tokens\n    }\n\n# We then define the arguments to pass to the sentiment analysis pipeline.\n# We set `return_all_scores` to True to get the sentiment score for each token.\nsent_kwargs = {\"return_all_scores\": True, \"function_to_apply\": \"none\", \"batch_size\": 16}\n\ntrl_model_class = AutoModelForCausalLMWithValueHead if not args.use_seq2seq else AutoModelForSeq2SeqLMWithValueHead\n\n\n# Below is an example function to build the dataset. In our case, we use the IMDB dataset\n# from the `datasets` library. One should customize this function to train the model on\n# its own dataset.\ndef build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):\n    \"\"\"\n    Build dataset for training. This builds the dataset from `load_dataset`, one should\n    customize this function to train the model on its own dataset.\n\n    Args:\n        query_dataset (`str`):\n            The name of the dataset to be loaded.\n\n    Returns:\n        dataloader (`torch.utils.data.DataLoader`):\n            The dataloader for the dataset.\n    \"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n    tokenizer.pad_token = tokenizer.eos_token\n    # load imdb with datasets: for reducing data size, just use +'[:200]'\n    ds = load_dataset(query_dataset, split=\"train\")\n    ds = ds.rename_columns({\"text\": \"review\"})\n    ds = ds.filter(lambda x: len(x[\"review\"]) > 200, batched=False)\n\n\n    if args.input_query_size == \"more\":\n        def input_size():\n            return 64 \n    else: \n        input_size = LengthSampler(input_min_text_length, input_max_text_length)\n\n    def tokenize(sample):\n        sample[\"input_ids\"] = tokenizer.encode(sample[\"review\"])[: input_size()]\n        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n        return sample\n\n    ds = ds.map(tokenize, batched=False)\n    ds.set_format(type=\"torch\")\n    return ds\n\n\n# We retrieve the dataloader by calling the `build_dataset` function.\ndataset = build_dataset(args.ppo_config, args.query_dataset)\n\n\ndef collator(data):\n    return dict((key, [d[key] for d in data]) for key in data[0])\n\n\n# set seed before initializing value head for deterministic eval\nset_seed(args.ppo_config.seed)\n\n# Now let's build the model, the reference model, and the tokenizer.\nif not args.use_peft:\n    ref_model = trl_model_class.from_pretrained(args.ppo_config.model_name, trust_remote_code=True)\n    device_map = None\n    peft_config = None\nelse:\n    peft_config = args.peft_config\n    ref_model = None\n    # Copy the model to each device\n    device_map = {\"\": Accelerator().local_process_index}\n\nmodel = trl_model_class.from_pretrained(\n    args.ppo_config.model_name,\n    trust_remote_code=True,\n    device_map=device_map,\n    peft_config=peft_config,\n)\n\n\ntokenizer = AutoTokenizer.from_pretrained(args.ppo_config.model_name)\n\n# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.\ntokenizer.pad_token_id = tokenizer.eos_token_id\n\nif args.change_tokenizer_args == True: \n    tokenizer.padding_side = args.tokenizer_kwargs['padding_side']\n    tokenizer.truncation_side = args.tokenizer_kwargs['truncation_side']\n    tokenizer.pad_token_as_eos_token = args.tokenizer_kwargs['pad_token_as_eos_token']\n    tokenizer.max_length = args.tokenizer_kwargs['max_length']\n\n# We then build the PPOTrainer, passing the model, the reference model, the tokenizer\nppo_trainer = PPOTrainer(args.ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)\n\n# We then build the sentiment analysis pipeline, passing the model name and the\n# sentiment analysis pipeline arguments. Let's also make sure to set the device\n# to the same device as the PPOTrainer.\ndevice = ppo_trainer.accelerator.device\nif ppo_trainer.accelerator.num_processes == 1:\n    device = 0 if torch.cuda.is_available() else \"cpu\"  # to avoid a `pipeline` bug\nds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin\ntask, model_name = args.ppo_config.reward_model.split(\":\")\nif ds_plugin is not None and ds_plugin.is_zero3_init_enabled():\n    with ds_plugin.zero3_init_context_manager(enable=False):\n        sentiment_pipe = pipeline(task, model=model_name, device=device)\nelse:\n    sentiment_pipe = pipeline(task, model=model_name, device=device)\n\n# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.\nif sentiment_pipe.tokenizer.pad_token_id is None:\n    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id\n\nif sentiment_pipe.model.config.pad_token_id is None:\n    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id\n\n# We then define the arguments to pass to the `generate` function. These arguments\n# are passed to the `generate` function of the PPOTrainer, which is a wrapper around\n# the `generate` function of the trained model.\ngeneration_kwargs['pad_token_id'] = tokenizer.eos_token_id\n\n# empty list to store text data\ntext_train_data = []\n\ndef get_current_risk_level(alpha, rho, m, M):\n    \"\"\"\n    Get current risk_level_alpha*N value\n\n    Args:\n        alpha (float):\n            Risk level alpha \n        rho (float):\n            Risk level scheduler: 0.8 means the soft risk level reaches α after ρ = 80% of the training\n        m (int):\n            current training step/epoch\n        M (int):\n            total policy training steps\n\n    Returns:\n        alpha_N (int):\n            risk_level_alpha*N value\n    \"\"\"\n    alpha = args.risk_alpha\n    rho = args.risk_rho\n\n    if args.risk_scheduler == \"old\": \n        if m <= args.risk_n:\n            return args.ppo_config.batch_size\n        else:\n            return math.ceil(args.ppo_config.batch_size * max(alpha, 1 - (1 - alpha) * (m - args.risk_n) / (rho * M)))   \n    else: \n        print('here')\n        if m <= args.risk_n:\n            val =  args.ppo_config.batch_size\n        elif m >= math.ceil(rho*M):\n            val = math.ceil(alpha*args.ppo_config.batch_size)\n        else:\n            K = (1 - alpha)/(math.ceil(rho*M)-args.risk_n)\n            val = math.ceil(args.ppo_config.batch_size * max(alpha, 1 - K * (m - args.risk_n)))\n        return val\n\n\ndef get_current_risk_level_2(alpha, rho, m, M):\n    \"\"\"\n    Get current risk_level_alpha*N value\n\n    Args:\n        alpha (float):\n            Risk level alpha \n        rho (float):\n            Risk level scheduler: 0.8 means the soft risk level reaches α after ρ = 80% of the training\n        m (int):\n            current training step/epoch\n        M (int):\n            total policy training steps\n\n    Returns:\n        alpha_N (int):\n            risk_level_alpha*N value\n    \"\"\"\n    alpha = 0.2\n    rho = 0.8 \n    n = 70\n    M = 194\n    \n    if m <= n:\n        val =  args.ppo_config.batch_size\n    elif m >= math.ceil(rho*M):\n        val = alpha*100\n    else:\n        K = (1 - alpha)/(math.ceil(rho*M)-n)\n        val = math.ceil(args.ppo_config.batch_size * max(alpha, 1 - K * (m - n)))\n    return val\n\ndef apply_risk_modification_to_batch(batch, query_tensors, response_tensors, rewards, alpha_N):\n    query_tensors = list(map(lambda t: t.cpu(), query_tensors))\n    response_tensors = list(map(lambda t: t.cpu(), response_tensors))\n    rewards = list(map(lambda t: t.cpu(), rewards))\n\n    step_dict = {\n        'query_t' : query_tensors,\n        'resp_t' : response_tensors,\n        'rewards' : rewards\n    }\n\n    batch = {k: [t.cpu() for t in v] if isinstance(v, list) and all(isinstance(t, torch.Tensor) for t in v) else v.cpu() if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n\n    df_step_dict = pd.DataFrame(step_dict)\n    df_batch = pd.DataFrame(batch)\n\n    df_step_dict = df_step_dict.sort_values(by='rewards')\n    df_batch = df_batch.sort_values(by='rewards')\n    batch = df_batch.to_dict(orient='list')  \n    mask = [torch.ones([48], dtype =int) for _ in response_tensors]\n    for ii in range(args.ppo_config.batch_size - alpha_N):\n        mask[-ii-1] = torch.zeros([48], dtype=int)\n\n    sr_query_tensors = list(map(lambda t: t.to(device), df_step_dict['query_t'].tolist()))\n    sr_response_tensors = list(map(lambda t: t.to(device), df_step_dict['resp_t'].tolist()))\n    sr_rewards = list(map(lambda t: t.to(device), df_step_dict['rewards'].tolist()))\n    batch = {k: [t.to(device) for t in v] if isinstance(v, list) and all(isinstance(t, torch.Tensor) for t in v) else v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n\n    return batch, sr_query_tensors, sr_response_tensors, sr_rewards, mask\n\n\nfor epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n    query_tensors = batch[\"input_ids\"]\n\n    response_tensors = ppo_trainer.generate(query_tensors, return_prompt=False, **generation_kwargs)\n    batch[\"response\"] = tokenizer.batch_decode(response_tensors)\n\n    texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n\n    if args.reward_function == \"trl\":\n        rewards = [torch.tensor(output[1][\"score\"]) for output in pipe_outputs]\n    else: \n        rewards = [\n            torch.nn.functional.softmax(torch.tensor([output[0][\"score\"], output[1][\"score\"]]))[1]\n            for output in pipe_outputs\n        ]\n    \n    for ii in range(3): \n        data_row = {'iter': epoch, 'input':batch[\"query\"][ii], 'output':batch[\"response\"][ii], 'score':rewards[ii].item()}\n        text_train_data.append(data_row)\n    \n    batch[\"rewards\"] = rewards\n    alpha_N_ceil = get_current_risk_level(PPOConfig.soft_risk_alpha, PPOConfig.risk_level_scheduler, epoch, 194)\n    stats = ppo_trainer.step(query_tensors, response_tensors, rewards, alpha_N_ceil)\n    ppo_trainer.log_stats(stats, batch, rewards)\n    print(epoch)\n\nppo_trainer._save_pretrained(logging_dir)\ndf = pd.DataFrame(text_train_data)\ndf.to_csv(logging_dir+'/textual_data.csv', index=False)",
    "Experiment Result": "The RA-RLHF algorithm adapts standard RLHF by optimizing Conditional Value at Risk (CVaR). It employs a soft-risk scheduling mechanism with an initial training phase using full data and a gradual reduction in batch size to focus on worst-case episodes. Policy updates use PPO with a modified dense reward and dynamic KL-Divergence regularization (beta parameter). \n\n**Key parameters and their values are:**\n\n*   **`PPOConfig` parameters (from `trl/trainer/ppo_config.py`):**\n    *   `soft_risk_alpha`: 0.05 (Risk level alpha)\n    *   `risk_level_scheduler`: 0.98 (Risk level scheduler; 0.8 means soft risk level reaches alpha after 80% of training)\n*   **Script Arguments (from `examples/IMDB/training/soft_risk_ppo_auth1_2.py`):**\n    *   `model_name`: \"/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_final/2024-01-24_21-17-41/checkpoint-100\" (SFT baseline model)\n    *   `query_dataset`: \"imdb\"\n    *   `reward_model`: \"sentiment-analysis:lvwerra/distilbert-imdb\"\n    *   `learning_rate`: 1.41e-5\n    *   `mini_batch_size`: 128\n    *   `batch_size`: 128\n    *   `gradient_accumulation_steps`: 1\n    *   `early_stopping`: False\n    *   `target_kl`: 6.0\n    *   `kl_penalty`: \"kl\"\n    *   `seed`: 0\n    *   `use_score_scaling`: False\n    *   `use_score_norm`: False\n    *   `score_clip`: None\n    *   `steps`: 128000 (total training steps)\n    *   `exp_name`: \"ppo\"\n    *   `risk_scheduler`: \"old\"\n    *   `risk_n`: 240 (initial iterations for full batch size before risk-aversion starts)\n    *   `risk_alpha`: 0.2 (target risk percentage for worst-case episodes)\n    *   `risk_rho`: 0.8 (proportion of total iterations after which risk-aversion reaches its target alpha)\n"
}{
    "Title": "Language Generation with Strictly Proper Scoring Rules",
    "Main Contributions": "The paper investigates the use of non-local strictly proper scoring rules as alternatives to the widely used logarithmic score (cross-entropy loss) for training language generation models. The main contributions include proposing a straightforward strategy to adapt non-local scoring rules to language generation by distributing them at the token level, and introducing a novel score smoothing technique to support regularization for arbitrary scoring rules. Through experiments, the study demonstrates that simply substituting the loss function with classic strictly proper scoring rules like the Brier score and Spherical score can lead to substantial improvements in model generation capabilities, particularly during fine-tuning. These improvements are shown to scale up to large language models (LLMs) such as LLaMA-7B and LLaMA-13B.",
    "Methodology": "The proposed methodology centers on adapting non-local strictly proper scoring rules for language generation. First, the problem of an exponentially large sample space for natural text sequences is addressed by decomposing sequence probabilities autoregressively and applying the scoring rule at the token level. This transforms sequence prediction into a series of conditional token prediction tasks, making non-local scores feasible. Second, a general 'score smoothing' technique is introduced, defined as Sϵ(p, i) = (1− ϵ) · S(p, i) + ϵ/m · Σ S(p, j), to enable regularization akin to label smoothing for any scoring rule. To enhance the smoothing effect for certain scores that might disregard it, a 'masked logarithmic score' (Sϵ log(p, i) =Sϵ(p, i) + ϵ/m Σ 1{pj < ϵ/m}log pj) is proposed, which applies the logarithmic score to 'under-smooth' labels.",
    "Experimental Setup": "The experiments were conducted on various language generation tasks, including machine translation and abstractive summarization. \n- **Datasets**: Machine translation tasks utilized WMT14 English-French (En-Fr, 35.8M pairs), WMT14 English-German (En-De, 4.5M pairs), TED bilingual dataset (~200K pairs for 10 directions), and WMT22 (Chinese-to-English, English-to-Chinese, German-to-English, and English-to-German for LLM evaluation). Abstractive summarization was evaluated on the CNN/DailyMail dataset (311K pairs). Instruction tuning for LLMs was performed using the Alpaca dataset (52K instruction-following demonstrations).\n- **Models**: Transformer-base and Transformer-big architectures were used for training from scratch and fine-tuning. Large Language Models (LLMs) evaluated were LLaMA-7B and LLaMA-13B, primarily for instruction tuning.\n- **Metrics**: BLEU score was used for machine translation, and ROUGE-1, ROUGE-2, and ROUGE-L for abstractive summarization. LLM open-ended question answering capabilities were assessed using MT-bench, a multi-turn question set. \n- **Training Details**: Implementation details included batch sizes (32k-64k), learning rates (2e-4 to 7e-4), dropout (0.1-0.3), attention dropout (0-0.1), warmup steps (2k-4k), training steps (18k-300k), and fine-tuning steps (4k-50k). Beam search was employed for decoding with beam sizes of 5 (Transformer MT), 4 (LLM MT), and 2 (LLM summarization), with varying length penalties.",
    "Limitations": "1. When training language models from scratch, alternative strictly proper scoring rules (Brier, Spherical) exhibit slower convergence and generally lower performance compared to the logarithmic score, which the authors hypothesize is due to distinct learning dynamics and hyperparameter settings optimized for the logarithmic score. \n2. Bounded scoring rules may provide relatively mild penalties when score smoothing is ignored, offering less incentive for the model to perform label smoothing compared to the unbounded logarithmic score. \n3. The theoretical equivalence between α-entmax loss and α-power score based loss does not hold when the probability of the ground truth word is zero, which can complicate gradient transmission to the golden logit for sparse transformations.",
    "Future Research Directions": "1. Explore other strictly proper scoring rules to determine if any exhibit superior performance during either pre-training or fine-tuning language generation models. \n2. Investigate the underlying factors that contribute to the observed performance differences among various strictly proper scores and develop additional metrics or properties to determine a score's suitability for training language generation models. \n3. Examine whether these alternative scoring rules can serve as effective evaluation metrics for assessing the calibration and overall quality of language generation models, similar to how perplexity is currently used.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation",
    "Main Contributions": "The paper introduces ScaleGrad, a novel training method that directly modifies the gradient of the standard Maximum Likelihood Estimation (MLE) objective to address the prevalent text degeneration issue in neural text generation. ScaleGrad encourages the model to learn and utilize novel tokens by dynamically re-normalizing softmax outputs during training. Empirical results demonstrate ScaleGrad's effectiveness in reducing repetition and increasing diversity in both open-ended (language modeling, text auto-completion) and directed (image paragraph captioning, abstractive text summarization) generation tasks, outperforming standard MLE and Unlikelihood training. Its architectural simplicity and wide applicability make it a general training objective.",
    "Methodology": "ScaleGrad's core idea is to encourage models to use novel tokens by directly manipulating gradient information. During training, at each decoding step, a dynamic set of 'novel tokens' is maintained, comprising tokens not yet observed in the ground-truth sequence up to that point. The method re-normalizes the softmax probability distribution over the vocabulary using a hyper-parameter gamma (0 < gamma < 1). For novel tokens, their probabilities are scaled down by a factor involving gamma, while for non-novel tokens, they are effectively scaled up. This manipulation directly influences the gradients received by each token's logit: pushing the ground-truth novel token's probability higher and significantly reducing the probability of non-ground truth, non-novel tokens. The paper provides a detailed gradient analysis, showing how this re-scaling encourages novel token usage and also contrasts ScaleGrad's gradient behavior with that of Unlikelihood training, highlighting a potential issue in UL where it might decrease ground-truth token probability under certain conditions, which ScaleGrad avoids.",
    "Experimental Setup": "The method was evaluated across various tasks and architectures. For **open-ended generation**, GPT-2 was fine-tuned on Wikitext-103 for language modeling and text auto-completion, with additional tests on Penn TreeBank (PTB) and IMDB datasets. Evaluation metrics included perplexity (ppl), unique token count (uniq), token-level repetition rates (Rep/l for l=16, 32, 128), n-gram word repetition ratios (Rep-n), and unique word count (uniq-w). Greedy decoding was primarily used, with an analysis of compatibility with stochastic decoding (top-k, top-p) and beam search. A human evaluation involving two native English speakers judged text quality. For **directed generation**, image paragraph captioning used a CNN-LSTM model on the Visual Genome dataset, measured by CIDEr, with greedy inference. Abstractive text summarization employed a BertSum model (Transformer with pre-trained BERT encoder) on CNN/DM and NYT50 datasets. Evaluation used F1-based ROUGE scores (R-1, R-2, R-L) and MoverScore (WMD-1), with decoding strategies including length normalization, tri-gram blocking, and beam search. Hyper-parameters (alpha for UL, gamma for SG) were tuned on development sets.",
    "Limitations": "The paper implies a trade-off between generation quality (e.g., perplexity) and diversity (unique tokens, lower repetition) when choosing the ScaleGrad hyper-parameter gamma. A smaller gamma leads to a stronger push for novel tokens, potentially increasing perplexity while improving diversity. While ScaleGrad shows consistent improvements, it does not explicitly state fundamental limitations of the approach itself, but rather positions it as a robust solution to a known problem. The hyper-parameter `gamma` needs to be chosen based on the baseline model's performance and the desired balance of quality and diversity, suggesting it's not universally optimal and requires tuning per task.",
    "Future Research Directions": "The authors suggest two main avenues for future research. Firstly, they plan to repurpose the definition of the 'novel token set' that ScaleGrad operates on to enable the model to achieve other specific objectives beyond simply reducing repetition, citing an example of adapting ScaleGrad to prevent early endpointing in online automatic speech recognition. Secondly, they aim to investigate mechanisms for dynamically adjusting the hyper-parameter gamma during the decoding steps, allowing the model to learn with varying degrees of strength depending on the current context.",
    "Experiment Code": "def getNovelMask(target, vocab_size):\n    b,l = target.size()\n    zeros = torch.zeros(b,l,vocab_size).to(target.device)\n    ones = torch.ones(b,l,vocab_size).to(target.device)\n\n    target_index = target.unsqueeze(1).expand(b,l,l).transpose(-2,-1).triu().transpose(-2,-1)\n    matrix = zeros.scatter_add_(2, target_index, ones)\n    matrix[:,:,0] = 0\n    summ_true = torch.tensor(range(1,l+1)).unsqueeze(0).float().to(target.device)\n    summ_now = torch.sum(matrix,dim=-1)\n    diff = summ_true - summ_now\n    matrix[:,:,0] = diff\n    matrix = torch.cat((torch.zeros(b,1,vocab_size).to(target.device),matrix[:,:-1,:]),1)\n    novel_mask = matrix < 1.\n\n    return novel_mask\n\n\ndef sg_loss(model, batch, args):\n    longer_sample = batch[0].cuda()\n    inp = longer_sample[:, :args.train_batch_size]\n    model_output = model(inp)\n    target = longer_sample[:, 1:]\n    logits = model_output[0]\n\n    # ScaleGrad\n    ##########################################################\n    probs = F.softmax(logits,dim=-1) \n    # Obtaining the masks for novel tokens\n    novel_mask = getNovelMask(target[0].unsqueeze(0),logits.size(-1))\n    rep_mask = ~novel_mask\n\n    new_probs = probs * novel_mask * args.gamma + probs * rep_mask + 1e-8\n    new_probs = F.normalize(new_probs,p=1,dim=-1)\n    lprobs = torch.log(new_probs)\n    ##########################################################\n\n\n    assert lprobs.size(0) == 1, 'We work on flat sequences'\n    loss = F.nll_loss(lprobs[0], target[0], reduction='sum')\n    true_token_logits = -F.nll_loss(logits[0], target[0], reduction='none')\n    ntokens = inp.numel()\n\n    logging_output = TrainingMetrics.ranking_metrics(logits[0], true_token_logits, None, ntokens, target[0])\n    logging_output['loss'] = loss.item()\n    logging_output['normalizer'] = ntokens\n    logging_output['sample_size'] = ntokens\n    logging_output['ntokens'] = ntokens\n\n    loss = loss / ntokens\n    return loss, logging_output",
    "Experiment Result": "The ScaleGrad loss (`sg_loss`) is applied during training of a GPT2LMHeadModel. A `novel_mask` is dynamically generated for each token using the `getNovelMask` function, identifying tokens not yet observed in the ground-truth sequence up to the current point. The softmax probability distribution is re-normalized based on this mask and a hyper-parameter `gamma`. Specifically, probabilities of novel tokens are scaled by `gamma`, while probabilities of non-novel (repeated) tokens are implicitly scaled up to maintain normalization. The loss is then computed as a negative log-likelihood on these re-normalized probabilities. The default value for `gamma` is `1.0`."
}{
    "Title": "Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation",
    "Main Contributions": "The paper introduces ScaleGrad, a novel training method designed to address the text degeneration issue (generation of dull and repetitive phrases) in neural text generation models, which commonly arises from the Maximum Likelihood Estimation (MLE) objective. ScaleGrad achieves this by directly modifying the gradient of the loss function to encourage the model to learn and use novel tokens. The empirical results demonstrate its effectiveness in improving generation quality, as measured by automatic metrics and human evaluation, across both open-ended (language modeling, text auto-completion) and directed generation tasks (image paragraph captioning, abstractive summarization). The method is architecturally simple, easy to integrate into existing neural models, and broadly applicable across different text generation scenarios and architectures like LSTM and Transformer.",
    "Methodology": "ScaleGrad modifies the standard MLE objective's gradient to encourage the use of novel tokens. The core idea is to maintain a dynamic set of 'novel tokens' at each decoding step during training, defined as tokens not yet observed in the current ground-truth sequence. The method then re-normalizes the softmax output probabilities: probabilities of novel tokens are scaled down by a hyper-parameter γ (where γ ∈ (0,1)), while probabilities of non-novel tokens are effectively scaled up. This re-scaling dynamically alters the gradients such that for a ground-truth token that is also novel, its gradient norm is increased, compelling the model to assign even higher probability to it. Conversely, for a non-ground-truth token that is non-novel, its gradient norm is increased, pushing the model to assign much lower probability. This mechanism encourages novelty without compromising the learning of target tokens. The paper also provides a gradient-level comparison with Unlikelihood (UL) training, highlighting ScaleGrad's robustness against potential issues where UL might decrease ground-truth token probabilities in certain conditions.",
    "Experimental Setup": "The effectiveness of ScaleGrad was evaluated across diverse tasks and architectures. For open-ended generation, pre-trained GPT-2 models (incorporating LSTM and Transformer architectures) were fine-tuned on WikiText-103, Penn TreeBank (PTB), and IMDB datasets for language modeling and text auto-completion. For directed generation, an LSTM-based captioning model was used on the Visual Genome dataset for image paragraph captioning, and a Transformer-based BertSum model was employed on CNN/DM and NYT50 datasets for abstractive text summarization. Baselines included MLE and Unlikelihood (UL) training. Performance was assessed using automatic metrics such as Perplexity (ppl), unique token/word counts (uniq, uniq-w), repetition ratios (Rep/l, Rep-n), CIDEr, ROUGE-1/2/L, and MoverScore. A human evaluation study, measuring win rate, percentage agreement, and Gwet’s AC1/gamma coefficient, was conducted for text auto-completion. Various decoding strategies like greedy search, beam search, top-k, and nucleus (top-p) sampling were analyzed, with tri-gram blocking and length normalization applied in certain directed generation inference scenarios. The method's hyper-parameter γ and UL's α were tuned on validation sets, with ranges γ ∈ {0.2, 0.5, 0.8} and α ∈ {0.5, 1.0, 1.5}.",
    "Limitations": "The ScaleGrad method introduces a hyper-parameter γ that requires careful tuning, as there exists a trade-off between generation quality (measured by perplexity) and diversity (measured by repetition ratio). A smaller γ aggressively promotes novel tokens, leading to lower repetition but potentially a higher perplexity. Additionally, the γ hyper-parameter is statically set during both training and decoding, meaning its strength in encouraging novelty does not adapt dynamically to the varying context. Furthermore, the definition of a 'novel token' is a simplified heuristic, relying solely on whether a token has appeared previously in the current ground-truth sequence during training, which might not fully capture nuanced notions of novelty in all generation contexts.",
    "Future Research Directions": "The authors propose two main directions for future research. Firstly, they plan to repurpose the definition of the 'novel token set' that ScaleGrad operates on to achieve other specific objectives beyond simply reducing degeneration. An example given is adapting ScaleGrad to prevent early endpointing in online automatic speech recognition, as demonstrated in concurrent work. Secondly, they aim to investigate mechanisms for dynamically adjusting the hyper-parameter γ during decoding steps. This would allow the model to learn and apply different degrees of strength in encouraging novelty depending on the specific context, making the method more adaptive and potentially more effective.",
    "Experiment Code": "def getNovelMask(target, vocab_size):\n    b,l = target.size()\n    zeros = torch.zeros(b,l,vocab_size).to(target.device)\n    ones = torch.ones(b,l,vocab_size).to(target.device)\n\n    target_index = target.unsqueeze(1).expand(b,l,l).transpose(-2,-1).triu().transpose(-2,-1)\n    matrix = zeros.scatter_add_(2, target_index, ones)\n    matrix[:,:,0] = 0\n    summ_true = torch.tensor(range(1,l+1)).unsqueeze(0).float().to(target.device)\n    summ_now = torch.sum(matrix,dim=-1)\n    diff = summ_true - summ_now\n    matrix[:,:,0] = diff\n    matrix = torch.cat((torch.zeros(b,1,vocab_size).to(target.device),matrix[:,:-1,:]),1)\n    novel_mask = matrix < 1.\n\n    return novel_mask\n\n\ndef sg_loss(model, batch, args):\n    longer_sample = batch[0].cuda()\n    inp = longer_sample[:, :args.train_batch_size]\n    model_output = model(inp)\n    target = longer_sample[:, 1:]\n    logits = model_output[0]\n\n    # ScaleGrad\n    ##########################################################\n    probs = F.softmax(logits,dim=-1) \n    # Obtaining the masks for novel tokens\n    novel_mask = getNovelMask(target[0].unsqueeze(0),logits.size(-1))\n    rep_mask = ~novel_mask\n\n    new_probs = probs * novel_mask * args.gamma + probs * rep_mask + 1e-8\n    new_probs = F.normalize(new_probs,p=1,dim=-1)\n    lprobs = torch.log(new_probs)\n    ##########################################################\n\n\n    assert lprobs.size(0) == 1, 'We work on flat sequences'\n    loss = F.nll_loss(lprobs[0], target[0], reduction='sum')\n    true_token_logits = -F.nll_loss(logits[0], target[0], reduction='none')\n    ntokens = inp.numel()\n\n    logging_output = TrainingMetrics.ranking_metrics(logits[0], true_token_logits, None, ntokens, target[0])\n    logging_output['loss'] = loss.item()\n    logging_output['normalizer'] = ntokens\n    logging_output['sample_size'] = ntokens\n    logging_output['ntokens'] = ntokens\n\n    loss = loss / ntokens\n    return loss, logging_output",
    "Experiment Result": "The ScaleGrad method's implementation (sg_loss) uses a custom 'novel_mask' and a hyperparameter `gamma` to re-normalize probabilities. The `getNovelMask` function dynamically identifies novel tokens within the ground-truth sequence. The main experimental settings for training and evaluation are defined as follows:\n\n**Training Parameters:**\n- `mode`: 'train' (for training ScaleGrad)\n- `model-name`: The GPT-2 model used, e.g., 'gpt2-medium'\n- `seed`: Random seed (default: 42)\n- `data-base`: Base directory for dataset files\n- `num-train-epochs`: Number of training epochs (default: 1)\n- `train-batch-size`: Batch size for training (default: 300)\n- `report-metrics-every`: Frequency to report metrics (default: 10 steps)\n- `save-every`: Frequency to save model checkpoints (default: 1000 steps)\n- `train-n-steps`: Number of training steps (default: 10000)\n- `validate-every`: Frequency for validation (default: 10000 steps)\n- `gamma`: ScaleGrad specific hyperparameter (type: float, default: 1.0). This value is used to scale down probabilities of novel tokens.\n- `adam-epsilon`: Epsilon for Adam optimizer (default: 1e-8)\n- `max-grad-norm`: Maximum gradient norm (default: 1)\n- `max-steps`: Total number of training steps (overrides `num_train_epochs` if > 0, default: -1)\n- `gradient-accumulation-steps`: Number of steps to accumulate gradients (default: 1)\n- `learning-rate`: Optimizer learning rate (default: 6.25e-5)\n- `warmup-steps`: Linear warmup steps (default: 0)\n- `lr-schedule`: Learning rate schedule (default: 'warmup_linear')\n- `weight-decay`: Weight decay for optimizer (default: 0.01)\n- `lm-coef`: Language model coefficient (default: 0.9)\n\n**Evaluation Parameters (for single-token prediction and completion tasks):**\n- `mode`: 'eval-singletoken', 'eval-completion', or 'eval-both'\n- `eval-split`: Dataset split for evaluation ('train', 'valid', 'test')\n- `model-load-dir`: Directory to load a pre-trained model (default: None)\n- `batch-size-singletoken`: Batch size for single-token prediction evaluation (default: 1024)\n- `batch-size-completion`: Batch size for completion evaluation (default: 300)\n- `prefix-length`: Length of the prefix for text completion (default: 50)\n- `continuation-length`: Length of the generated continuation (default: 100)\n- `top-k`: Top-k sampling parameter (default: 1, implying greedy if top_p is 0)\n- `top-p`: Top-p (nucleus) sampling parameter (default: 0.0, implying no nucleus sampling)"
}{
    "Title": "Understanding Emergent Abilities of Language Models from the Loss Perspective",
    "Main Contributions": "This paper re-examines emergent abilities in language models (LMs), challenging the view that they are exclusive to large models or an artifact of discontinuous metrics. The main contribution is demonstrating that LMs with the same pre-training loss, regardless of model size or data size, exhibit the same performance on various downstream tasks. It introduces a new definition of emergent abilities, stating they manifest when pre-training loss falls below a specific threshold, regardless of metric continuity. This highlights that emergent abilities cannot be predicted by extrapolating performance from models with higher pre-training losses.",
    "Methodology": "The researchers pre-trained over 30 Transformer LMs from scratch with varied model (300M to 32B parameters) and data sizes (33B to 3T tokens), while keeping the data corpus, tokenization, and model architecture (similar to LLaMA with grouped-query attention and rotary position embedding) fixed. They evaluated their downstream performance on 12 diverse English and Chinese datasets, covering tasks like QA, NLI, reading comprehension, coreference resolution, examination, and math word problems, using few-shot, zero-shot, and few-shot CoT prompting. The generality of observations was further validated by analyzing public LLaMA and Pythia models. To address concerns about discontinuous metrics, performance on emergent tasks was also evaluated using continuous metrics like CorrectChoiceProb and Brier Score.",
    "Experimental Setup": "Pre-training was conducted on a mixture of English and Chinese corpora (4:1 ratio), similar to RedPajama, tokenized with BPE (65k vocabulary). Model sizes ranged from 300M to 32B for custom models, and included LLaMA (7B, 13B, 33B, 65B) and Pythia (1.4B to 12B) series. Training utilized AdamW optimizer with a cosine learning rate schedule. Downstream evaluation included 12 datasets: TriviaQA, HellaSwag, RACE, WinoGrande, MMLU, GSM8K (English); NLPCC-KBQA, ClozeT, CLUEWSC, C3, C-Eval, GSM8K-Chinese. Metrics included Exact Match (EM), Accuracy, CorrectChoiceProb, and Brier Score. Performance-vs-loss curves were plotted, and Spearman and Pearson correlation coefficients were calculated to quantify the relationship, often comparing against a random guessing baseline.",
    "Limitations": "The study did not consider factors like fundamentally different model architectures beyond Transformer variants (e.g., routed Transformers, non-Transformer architectures) or alternative training algorithms beyond AdamW. A key limitation is that pre-training loss values are not directly comparable for LMs trained on different corpora or with different tokenizers/vocabularies, suggesting a need for normalized perplexity on a public validation set for comparison. The paper also cautions that the observed performance trends and tipping points are not guaranteed to extrapolate to larger scales, and instruction tuning could influence zero-shot performance on some tasks.",
    "Future Research Directions": "The new definition of emergent abilities from the pre-training loss perspective provides a precise characterization of critical junctures in training trajectories. This encourages future studies to investigate the specific shifts or changes occurring within language models at these identified loss thresholds, aiming to understand what facilitates the development of new capabilities and how to potentially guide their emergence.",
    "Experiment Code": "File Path: app/src/artifacts/downloaders/books_downloader.py\nContent:\nclass BooksDownloader:\n    dataset_name = \"books\"\n    output_fp = \"books/en-books.jsonl.gz\"\n    def __init__(\n            self, lang, out_dir, overwrite, cache_dir, max_samples,\n            max_paragraphs_per_sample=200, max_samples_per_book=500\n    ):\n        self._lang = lang\n        self._out_dir = out_dir\n        self._overwrite = overwrite\n        self._cache_dir = cache_dir\n        self._max_samples = max_samples\n        self._max_paragraphs_per_sample = max_paragraphs_per_sample\n        self._max_samples_per_book = max_samples_per_book\n        self._filepath = None\n\n    def run(self, logger):\n        # ... (setup file paths and writer)\n        for book in load_dataset(\n                \"togethercomputer/RedPajama-Data-1T\", name=\"book\",\n                cache_dir=self._cache_dir,\n                split=\"train\", streaming=True\n        ):\n            for chunk in self.__generate_chunks(book[\"text\"]):\n                n_docs += 1\n                if n_docs > self._max_samples > 0:\n                    break\n                writer.write(data_obj={\"text\": chunk}, flush=n_docs % flush_every == 0)\n                pbar.update(1)\n            else:\n                continue\n            break\n        pbar.close()\n        writer.close()\n\nFile Path: app/src/core/worker.py\nContent:\nclass Worker:\n    # ... (init and properties)\n    def __init_quality_signals(self, ldnoobw_dir, ut1_dir) -> List[Callable]:\n        callables = []\n        callables += register_content_callables(\n            language=self._lang, bad_urls_dir=ut1_dir, bad_words_dir=ldnoobw_dir\n        )\n        callables += register_repetitions_callables()\n        callables += register_natural_language_callables()\n        callables += register_lines_callables()\n        callables += register_classifier_callables(\n            wikiref_model=self._wikiref_model_file,\n            palm_model=self._palm_model_file,\n            wikipedia_model=self._wikipedia_model_file\n        )\n        callables += register_importance_weights_callables(\n            source_fps=self._dsir_files.get(\"ccnet\"), wiki_fps=self._dsir_files.get(\"wikipedia\"),\n            openwebtext_fps=self._dsir_files.get(\"openwebtext\"), books_fps=self._dsir_files.get(\"books\"),\n            language=self._lang\n        )\n        return callables\n\n    def __process_record(\n            self, idx: int, record, uri_id: str, snapshot_id: str\n    ):\n        document = Document(\n            record.raw_content,\n            domain=record.source_domain,\n            precompute_ngrams=True,\n            precompute_hash_features=True,\n            dsir_buckets=self._dsir_buckets\n        )\n        rp_v2_signals = {}\n        for func in self._quality_signals:\n            rp_v2_signals[func.field_name] = func(document)\n\n        minhash_signatures = self._minhash.compute_banded_signatures(tokens=document.normalized_words)\n        # ... (document ID, metadata, CCNet quality signals construction)\n        record_data[\"metadata\"] = metadata\n        record_data[\"quality_signals\"] = {**ccnet_quality_signals, **rp_v2_signals}\n        return record_data, minhash_signatures, doc_id, doc_id_int\n\nFile Path: app/src/dedupe/minhash.py\nContent:\nclass MinHash:\n    # ... (init and properties)\n    def compute_banded_signatures(\n            self, tokens: Tuple[str]\n    ) -> Dict[str, Optional[List[bytes]]]:\n        if len(tokens) < self._ngram_size:\n            return {k: None for k in self._hashranges.keys()}\n        minhashes: np.ndarray = generate_signature(\n            words_sequence=iter(tokens),\n            ngram_size=self._ngram_size,\n            permutations=self._permutations,\n            max_hash=self._max_hash,\n            mersenne_prime=self._mersenne_prime\n        )\n        signatures = {\n            sig_key: [bytes(minhashes[start:end].byteswap().data) for start, end in hashrange]\n            for sig_key, hashrange in self._hashranges.items()\n        }\n        return signatures\n\nFile Path: app/src/bloomfilter.py\nContent:\nclass Deduper:\n    # ... (init and methods)\n    def __parallel_run(self, input_uris):\n        bloomfilter = pybloomfilter.BloomFilter(capacity=self._args.capacity, error_rate=self._args.error_rate)\n        # ... (loop through snapshots and data chunks)\n        for (uri, (read_status, uri_data)) in data_chunks.items():\n            # ... (error handling and logging)\n            for record in uri_data:\n                digest = record[\"digest\"]\n                if bloomfilter.add(digest):\n                    out_writer.update_batch(obj=record)\n                    num_dupes += 1\n                num_docs += 1\n                total_progress.step(1)\n\nFile Path: app/src/run_lsh.py\nContent:\nclass LSH:\n    # ... (init)\n    def __build_edges(self, pa_dset: pa.dataset.Dataset) -> np.ndarray:\n        query = pl.scan_pyarrow_dataset(pa_dset)\n        if self._args.max_docs > 0:\n            query = query.head(self._args.max_docs)\n        query = (\n            query\n            .select(pl.col([\"id_int\", self._sig_key]))\n            .filter(~pl.col(self._sig_key).is_null())\n            .with_columns(pl.Series(name=\"band\", values=[list(range(self._num_bands))], dtype=pl.List(pl.UInt8)))\n            .explode(self._sig_key, \"band\")\n            .group_by(self._sig_key, \"band\")\n            .agg(pl.col(\"id_int\"))\n            .filter(pl.col(\"id_int\").list.lengths() > 1)\n            .select(pl.col(\"id_int\"), pl.col(\"id_int\").list.min().alias(\"min_node\"))\n            .explode(\"id_int\")\n            .filter(pl.col(\"id_int\") != pl.col(\"min_node\"))\n            .select(pl.concat_list([\"id_int\", \"min_node\"]).alias(\"edges\"))\n            .unique(\"edges\")\n        )\n        edges = query.collect(streaming=True).to_numpy().flatten()\n        return edges\n\nFile Path: app/src/token_count.py\nContent:\nTOKENIZER = Tokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\nclass PostProcessor:\n    # ... (init and methods)\n    def _handle_documents(\n            self,\n            client: botocore.client.BaseClient,\n            input_id: str,\n            decoder\n    ) -> InputResult:\n        # ... (download doc and error handling)\n        num_docs = 0\n        total_tokens = 0\n        token_counts = []\n        try:\n            with gzip.open(input_buffer, mode=\"rb\") as in_fh:\n                for idx, obj in enumerate(in_fh):\n                    record = decoder.decode(obj)\n                    num_tokens = len(TOKENIZER.encode(record.raw_content).tokens)\n                    token_counts.append((idx, num_tokens))\n                    total_tokens += num_tokens\n                    num_docs += 1\n        except Exception as e:\n            # ... (error handling)\n            pass\n        return InputResult(is_success=True, msg=\"\", input_id=input_id, num_docs=num_docs, num_tokens=total_tokens, token_counts=token_counts)\n",
    "Experiment Result": "The provided repository content describes the data preparation pipeline for pre-training large language models, rather than the LM training or evaluation code itself. The experimental settings for data preparation are as follows:\n-   **Data Sources:** Raw text data is acquired from multiple datasets:\n    -   RedPajama Books (from HuggingFace `togethercomputer/RedPajama-Data-1T`, name=\"book\"). Books are chunked into samples with a maximum of 200 paragraphs per sample and 500 samples per book.\n    -   OpenWebText (from HuggingFace `openwebtext`).\n    -   Wikipedia (from HuggingFace `wikipedia`, attempting `20220301.{lang}` or falling back to `20230801` date for specified language).\n    -   CCNet (Common Crawl) data is processed by language, partitioned by snapshot ID, and split into 'head', 'middle', and 'tail' segments (10%, 20%, 70% of samples respectively) to avoid bias from distribution shifts.\n-   **Languages Supported:** English (`\"en\"`) is a primary focus, but the pipeline is designed to support multiple languages (e.g., \"es\", \"de\", \"fr\", \"it\" are explicitly referenced in stop word lists).\n-   **Tokenization for Counting:** The `token_count.py` script utilizes the `mistralai/Mistral-7B-v0.1` tokenizer for tokenizing raw content and counting tokens.\n-   **Data Quality Signals:** A comprehensive suite of quality signals is applied to documents:\n    -   **ML Classifiers:** FastText models are used to predict quality scores for documents, including models trained on Wikipedia references, a \"Palm\" model (trained on CCNet vs. Books, OpenWebText, Wikipedia), and a Wikipedia article classifier.\n    -   **Content-based Signals:** Include counting occurrences of words from the List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words (LDNOOBW), \"lorem ipsum\" phrases, curly bracket characters, and categorizing domains based on the UT1 URL blacklist. It also computes the fraction of stop words.\n    -   **Natural Language Signals:** Quantify features like the number of sentences, word count, mean word length, symbol-to-word ratio, fraction of lines ending with an ellipsis, fraction of words with no alphabetical characters, fraction of unique words, unigram entropy, and fraction of all-caps words.\n    -   **Repetition Signals:** Measure the fraction of characters in the top 2-gram, 3-gram, and 4-gram, as well as the fraction of characters in duplicate 5-gram to 10-gram sequences (ensuring non-overlapping counts).\n    -   **Importance Weights (DSIR):** Compute the log ratio of the likelihood of document features (using 1-gram and 2-gram hash features with 10,000 buckets by default) with respect to target domains (Wikipedia, Books, OpenWebText) versus a source domain (CCNet), with an optional Poisson length correction.\n-   **Deduplication Settings:**\n    -   **Exact Deduplication:** Performed using a Bloom Filter with configurable capacity and an error rate (defaulting to 0.01). It processes documents by `sha1` digest, extracted from `ccnet` data, for exact matches within snapshots.\n    -   **Near Deduplication (MinHash LSH):** Uses MinHash signatures with configurable `ngram_size`, `num_permutations`, and `similarity_thresholds` (e.g., 0.8) to identify near-duplicate documents through Locality Sensitive Hashing (LSH). The `optimal_param` utility function is used to determine the number of bands and rows for LSH from the similarity threshold and number of permutations.\n-   **Parallel Processing:** The overall pipeline (`pipeline.py`) orchestrates workers in parallel using `multiprocessing` to process input listings in chunks, with configurable maximum processes and inputs per process."
}{
    "Title": "EMO: EARTH MOVER DISTANCE OPTIMIZATION FOR AUTO-REGRESSIVE LANGUAGE MODELING",
    "Main Contributions": "The paper identifies fundamental limitations of Maximum Likelihood Estimation (MLE) in training auto-regressive language models, specifically recall-prioritization, negative diversity ignorance, and train-test mismatch, which lead to text degeneration. It proposes Earth Mover Distance Optimization (EMO) as a novel training objective that leverages the properties of Earth Mover Distance (EMD) to address these issues. A key technical contribution is the derivation of a tractable and differentiable upper bound for EMD (DEMD) and a semantically-informed transport cost function, enabling efficient end-to-end training. EMO consistently demonstrates superior performance in open-ended text generation, yielding more human-like text, and significantly enhances downstream natural language understanding (NLU) task performance through lightweight continual fine-tuning, showcasing strong scaling properties with less data.",
    "Methodology": "EMO adapts Earth Mover Distance (EMD) to measure token-level probability distance between the model's output distribution (Qθ) and the human data distribution (P). The cost of transporting probability mass between tokens (C(vi, vj)) is defined using the cosine distance of their contextual embeddings from a pre-trained language model head, which remains fixed during training. To circumvent the high computational complexity and non-differentiability of direct EMD, the authors introduce a differentiable upper bound (DEMD) based on a specific transport plan ˜γ(vi, vj) = Qθ(vi)P(vj), resulting in the objective ^DEMD(Qθ, P) = |Q⊤ θ − P⊤|CP. The final training loss combines MLE and DEMD dynamically: L = 0.5 * (LMLE + (LMLE/LDEMD).detach() * LDEMD). This methodology promotes precision-recall harmonization, negative diversity awareness by penalizing non-ground-truth tokens based on their contextual similarity, and better train-test consistency by optimizing an expectation with respect to the model distribution.",
    "Experimental Setup": "The effectiveness of EMO was evaluated on two main tasks: open-ended language generation and natural language understanding (NLU). For language generation, GPT-2 and OPT-125M were fine-tuned on WikiText-2, WikiText-103, WebText test, Penn Tree Bank (PTB), WritingPrompts, and AG News datasets. Evaluation metrics included MAUVE for distributional closeness, and for a specific oracle experiment (using GPT-2-Large generated data), PPLtest, PPLoracle, ROUGE-1/L were also used. Unbiased (ancestral) sampling was used for decoding. For NLU, LLaMa-7B/13B (and LLaMa2-7B/13B, OPT-1.3B/2.7B in appendices) were continually fine-tuned on WikiText-103 (a much smaller corpus than pre-training). Downstream performance was assessed using in-context learning on 8 NLU tasks: Tweet Emotion, TREC, SST-2, Subj, Customer Review, Rotten Tomatoes, AG News, and MMLU, with a dense retriever for 8-shot (or 5-shot for MMLU) demonstrations and perplexity-based prediction. Instruction-tuning experiments used Alpaca-GPT4, Recycled Evol-Instruct-70K, and OpenPlatypus datasets, evaluated by LLM-based judges like AlpacaEval (GPT-4), Auto-J, and PandaLM. Baselines for comparison included MLE, TaiLr, and MixCE.",
    "Limitations": "The primary practical limitation highlighted is the high computational complexity of directly calculating the Earth Mover Distance (EMD), necessitating the use of a tractable upper bound (DEMD). The derived upper bound relies on a transport plan that, while valid, is not necessarily optimal. The semantically-informed transport cost function is fixed throughout the training process of the language model (Qθ), depending on the quality of a pre-trained language modeling head. The authors also note that for relatively weak language models, DEMD alone may converge slowly due to bounded gradient scaling, leading to the adoption of a dynamically weighted combination with MLE as the final loss function, suggesting potential convergence issues with DEMD in isolation under certain conditions.",
    "Future Research Directions": "The paper suggests that EMO has significant potential as a lightweight calibration method for enhancing large-scale pre-trained language models, indicating further exploration in this area. It also points to the applicability of EMO for domain-specific fine-tuning/adaptation, instruction-tuning, and continual pre-training as general directions for future work. Potential extensions could include exploring more advanced or adaptive strategies for defining the semantically-informed transport cost beyond a fixed, pre-trained embedding space. Further research could investigate methods to improve the standalone convergence properties of DEMD, reducing its reliance on dynamic weighting with MLE. Additionally, integrating EMO with other alignment techniques, such as reinforcement learning from human feedback (RLHF), or applying EMO to a broader range of generative tasks beyond auto-regressive language modeling, presents promising avenues.",
    "Experiment Code": "import torch\nfrom transformers import LlamaForCausalLM\nfrom typing import Optional, List\nfrom torch.nn import CrossEntropyLoss\nfrom transformers import LlamaModel\n\n\nclass EMOLlamaForCausalLM(LlamaForCausalLM):\n    def __init__(self, config):\n        super().__init__(config)\n        self.model = LlamaModel(config)\n\n        self.lm_head = torch.nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ):\n\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        # ======================================================================== #\n        #                   Compute the MLE loss\n        # ======================================================================== #\n        hidden_states = outputs[0]\n        logits = self.lm_head(hidden_states)\n        mask = labels[:, 1:].contiguous().view(-1)\n        mask = (mask!=-100).to(logits.dtype)\n        loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n        logits = logits[:, :-1, :].contiguous().view(-1, logits.shape[-1])\n        labels = labels[:, 1:].contiguous().view(-1)\n        mle_loss = loss_fct(logits, labels)\n\n        # ======================================================================== #\n        #                   Compute the EMO loss\n        # ======================================================================== #\n        labels_tmp = labels.clone()\n        labels_tmp[labels_tmp==(-100)] = 0\n        one_hot = torch.nn.functional.one_hot(labels_tmp, num_classes=self.config.vocab_size).to(logits.dtype)\n        stable_onehot = (one_hot+1e-15) / torch.linalg.vector_norm((one_hot+1e-15), ord=1, dim=-1, keepdim=True) # (bsz*seq_len, vocab_size)\n        embedding_matrix = self.cost_embedding # (vocab_size, hidden_size)\n        embedding_matrix = embedding_matrix / torch.linalg.vector_norm(embedding_matrix, ord=2, dim=1, keepdim=True)\n        p_contextual_repr = stable_onehot @ embedding_matrix # (bsz*seq_len, hidden_size)\n        q_grad = torch.log_softmax(logits, dim=-1).exp() # (bsz*seq_len, vocab_size)\n        gt_q = (q_grad * one_hot).detach()\n        q_final = q_grad - gt_q\n        q_contextual_repr = q_final @ embedding_matrix # (bsz*seq_len, hidden_size)\n        emo_loss = (1 - torch.sum(p_contextual_repr*q_contextual_repr, dim=-1)) # (bsz*seq_len,)\n\n        # ======================================================================== #\n        #                   Compose the final loss\n        # ======================================================================== #\n        loss = (torch.min((mle_loss / (emo_loss+1e-10)).detach(), torch.ones_like(mle_loss, dtype=mle_loss.dtype, device=mle_loss.device)*3.0) * emo_loss + mle_loss) * 0.5\n        loss = (loss * mask).sum() / (1e-15 + mask.sum())\n\n        output = (logits,) + outputs[1:]\n        return (loss,) + output if loss is not None else output",
    "Experiment Result": "The EMO method is implemented as a custom `EMOLlamaForCausalLM` class, extending `LlamaForCausalLM`. The method combines standard Maximum Likelihood Estimation (MLE) loss with a differentiable Earth Mover Distance (DEMD) loss, dynamically weighted as `L = 0.5 * (LMLE + (LMLE/LDEMD).detach() * LDEMD)`. A clamping mechanism is applied to the weighting factor `(LMLE/LDEMD).detach()`, limiting it to a maximum of 3.0. The cost matrix for DEMD (`self.cost_embedding`) is derived from the `lm_head.weight.data` of a pre-trained Llama-2-13B model (specifically from the checkpoint `TheBloke/Llama-2-13B-fp16`) and is explicitly kept fixed during training by registering it as a buffer. For efficiency during training, Flash Attention is used to replace the standard Llama attention mechanism. LoRA (Low-Rank Adaptation) is also employed for efficient fine-tuning with the following parameters: `r=32`, `lora_alpha=16`, `lora_dropout=0.05`, `bias=\"none\"`, `inference_mode=False`, and `task_type=\"CAUSAL_LM\"`."
}{
    "Title": "Discovering Preference Optimization Algorithms with and for Large Language Models",
    "Main Contributions": "The paper addresses the limitation of human-designed, convex loss functions in offline preference optimization for Large Language Models (LLMs) by proposing an LLM-driven objective discovery pipeline. This pipeline automatically generates new state-of-the-art preference optimization algorithms without expert human intervention. A key discovery is \"Discovered Preference Optimization (DiscoPOP)\", a novel algorithm that adaptively blends logistic and exponential losses. DiscoPOP demonstrates state-of-the-art performance and successful transfer to various held-out tasks, including multi-turn dialogue, controlled sentiment generation, and summarization. The research also provides an initial analysis of DiscoPOP, highlighting its unconventional non-convex nature and negative gradients at the starting point, suggesting new insights into optimal objective function characteristics.",
    "Methodology": "The core methodology is an iterative LLM-driven objective discovery pipeline (meta-optimization). An LLM (GPT-4) is prompted to propose and implement new preference optimization loss functions in Python code. The process starts by initializing the LLM with established loss functions and their performance as in-context examples. For each proposed objective function, validity is checked via unit tests, and if valid, an LLM is finetuned and evaluated on a downstream validation task (e.g., MT-Bench). The resulting performance metric is fed back to the LLM as feedback, allowing it to iteratively refine its proposals, exploring variations of successful formulas and entirely new formulations. This iterative refinement continues until convergence or a maximum number of generations is reached. This approach leverages LLMs' knowledge to directly propose code-level objective functions, circumventing the need for a human-designed search space.",
    "Experimental Setup": "The research used various LLMs and datasets for discovery and held-out evaluations. For the LLM-driven discovery, a `zephyr-7b-gemma-sft` model (based on `gemma-7b` and finetuned on `deita-10k-v0-sft`) was trained on the `Argilla DPO Mix 7K` pairwise preference dataset. Evaluation was conducted using MT-Bench scores, with GPT-4 acting as a judge. For held-out evaluations: (1) Single-turn dialogue used Alpaca Eval 2.0 to assess win rates against GPT-4 or the SFT base model; (2) Summarization used 10% of the Reddit TL;DR dataset for training, with evaluation on a custom 694-sample test set using Alpaca Eval 2.0 with a GPT-4 annotator; (3) Positive sentiment generation (IMDb) used a GPT-2 model (supervised finetuned on IMDb) for preference optimization, evaluating model rewards via a pre-trained sentiment classifier and KL-Divergence across a sweep of beta values. All models were generally trained with fixed hyperparameters (β=0.05 during discovery, learning rate 5e-7, bfloat16, 2 epochs, batch size 2, gradient accumulation 8, cosine scheduler, AdamW optimizer) using the TRL transformers library on Nvidia A100 GPUs. A small case study on CIFAR-10 classification with ResNet-18 was also performed to demonstrate discovery robustness.",
    "Limitations": "The current approach has several limitations. The effectiveness of LLM objective proposals has only been superficially explored, with initial attempts at temperature sampling or performance-based sorting yielding no significant improvements. DiscoPOP's `β` parameter being repurposed to affect both functional behavior and KL penalty can lead to instability, especially when `β` values are too low (≤ 0.01, leading to local minima trapping) or too high (≥ 2.5, causing model collapse due to large gradients). The discovery process used a fixed `β=0.05`, meaning the LLM did not learn robust behavior across a wider range of `β` values. Furthermore, some discovered losses required manual correction because the LLM did not uphold the correct mathematical structure (e.g., `β` multiplication with log-ratios). The reliance on closed-source models like GPT-4 for code generation poses reproducibility issues and high costs. Finally, LLM outputs are susceptible to hallucinations, necessitating content filters and careful consideration of code-level self-improvement risks.",
    "Future Research Directions": "Future research should focus on improving the LLM-driven objective proposal effectiveness, potentially by leveraging more information from training runs (e.g., providing learning curve plots to Visual Language Models) or by meta-meta-optimizing the LLM prompt and automatically tuning instruction prompt templates. A critical direction is to study different forms of objective functions that incorporate multiple, separately tunable floating-point parameters, rather than repurposing a single parameter like `β`, to enable more stable and performant training. This would involve a deeper multi-parameter analysis and reformulation of objectives to mitigate training instabilities observed with DiscoPOP at extreme `β` values. Additionally, moving towards using the discovered models themselves to generate code could address the current reliance on costly and closed-source LLMs like GPT-4, thereby enhancing reproducibility and reducing operational costs. Finally, explicitly constraining the LLM to adhere to specific mathematical structures (e.g., proper handling of `β` in loss calculations) during the discovery process is necessary to avoid requiring manual post-hoc corrections.",
    "Experiment Code": "import dataclasses\nfrom enum import auto, IntEnum\nfrom typing import List, Any, Dict, Union, Tuple\n\n\nclass SeparatorStyle(IntEnum):\n    \"\"\"Separator styles.\"\"\"\n\n    ADD_COLON_SINGLE = auto()\n    ADD_COLON_TWO = auto()\n    ADD_COLON_SPACE_SINGLE = auto()\n    NO_COLON_SINGLE = auto()\n    NO_COLON_TWO = auto()\n    ADD_NEW_LINE_SINGLE = auto()\n    LLAMA2 = auto()\n    LLAMA3 = auto()\n    CHATGLM = auto()\n    CHATML = auto()\n    CHATINTERN = auto()\n    DOLLY = auto()\n    RWKV = auto()\n    PHOENIX = auto()\n    ROBIN = auto()\n    FALCON_CHAT = auto()\n    CHATGLM3 = auto()\n    DEEPSEEK_CHAT = auto()\n    METAMATH = auto()\n    YUAN2 = auto()\n    GEMMA = auto()\n    CLLM = auto()\n    DEFAULT = auto()\n\n\nIMAGE_PLACEHOLDER_STR = \"$$<image>$$\"\n\n\n@dataclasses.dataclass\nclass Conversation:\n    \"\"\"A class that manages prompt templates and keeps all conversation history.\"\"\"\n\n    # The name of this template\n    name: str\n    # The template of the system prompt\n    system_template: str = \"{system_message}\"\n    # The system message\n    system_message: str = \"\"\n    system_message_vision: str = \"\"\n    # The names of two roles\n    roles: Tuple[str] = (\"USER\", \"ASSISTANT\")\n    # All messages. Each item is (role, message).\n    # Each message is either a string or a tuple of (string, List[image_url]).\n    messages: List[List[str]] = ()\n    # The number of few shot examples\n    offset: int = 0\n    # The separator style and configurations\n    sep_style: SeparatorStyle = SeparatorStyle.ADD_COLON_SINGLE\n    sep: str = \"\\n\"\n    sep2: str = None\n    # Stop criteria (the default one is EOS token)\n    stop_str: Union[str, List[str]] = None\n    # Stops generation if meeting any token in this list\n    stop_token_ids: List[int] = None\n    # The maximum image size in megabytes that this model takes in. None means we do not resize the image.\n    max_image_size_mb: int = None\n\n    def get_prompt(self) -> str:\n        \"\"\"Get the prompt for generation.\"\"\"\n        system_prompt = self.system_template.format(system_message=self.system_message)\n        if self.sep_style == SeparatorStyle.ADD_COLON_SINGLE:\n            ret = system_prompt + self.sep\n            for role, message in self.messages:\n                if message:\n                    if type(message) is tuple:\n                        message, images = message\n                        message = IMAGE_PLACEHOLDER_STR * len(images) + message\n                    ret += role + \": \" + message + self.sep\n                else:\n                    ret += role + \":\"\n            return ret\n        elif self.sep_style == SeparatorStyle.ADD_COLON_TWO:\n            seps = [self.sep, self.sep2]\n            ret = system_prompt + seps[0]\n            for i, (role, message) in enumerate(self.messages):\n                if message:\n                    if type(message) is tuple:\n                        message, images = message\n                        message = IMAGE_PLACEHOLDER_STR * len(images) + message\n                    ret += role + \": \" + message + seps[i % 2]\n                else:\n                    ret += role + \":\"\n            return ret\n        elif self.sep_style == SeparatorStyle.ADD_COLON_SPACE_SINGLE:\n            ret = system_prompt + self.sep\n            for role, message in self.messages:\n                if message:\n                    ret += role + \": \" + message + self.sep\n                else:\n                    ret += role + \": \"  # must be end with a space\n            return ret\n        elif self.sep_style == SeparatorStyle.ADD_NEW_LINE_SINGLE:\n            ret = \"\" if system_prompt == \"\" else system_prompt + self.sep\n            for role, message in self.messages:\n                if message:\n                    ret += role + \"\\n\" + message + self.sep\n                else:\n                    ret += role + \"\\n\"\n            return ret\n        elif self.sep_style == SeparatorStyle.NO_COLON_SINGLE:\n            ret = system_prompt\n            for role, message in self.messages:\n                if message:\n                    ret += role + message + self.sep\n                else:\n                    ret += role\n            return ret\n        elif self.sep_style == SeparatorStyle.NO_COLON_TWO:\n            seps = [self.sep, self.sep2]\n            ret = system_prompt\n            for i, (role, message) in enumerate(self.messages):\n                if message:\n                    ret += role + message + seps[i % 2]\n                else:\n                    ret += role\n            return ret\n        elif self.sep_style == SeparatorStyle.RWKV:\n            ret = system_prompt\n            for i, (role, message) in enumerate(self.messages):\n                if message:\n                    ret += (\n                        role\n                        + \": \"\n                        + message.replace(\"\\r\\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\")\n                    )\n                    ret += \"\\n\\n\"\n                else:\n                    ret += role + \":\"\n            return ret\n        elif self.sep_style == SeparatorStyle.LLAMA2:\n            seps = [self.sep, self.sep2]\n            if self.system_message:\n                ret = system_prompt\n            else:\n                ret = \"[INST] \"\n            for i, (role, message) in enumerate(self.messages):\n                tag = self.roles[i % 2]\n                if message:\n                    if i == 0:\n                        ret += message + \" \"\n                    else:\n                        ret += tag + \" \" + message + seps[i % 2]\n                else:\n                    ret += tag\n            return ret\n        elif self.sep_style == SeparatorStyle.LLAMA3:\n            ret = \"<|begin_of_text|>\"\n            if self.system_message:\n                ret += system_prompt\n            else:\n                ret += \"\"\n            for i, (role, message) in enumerate(self.messages):\n                if message:\n                    ret += f\"<|start_header_id|>{role}<|end_header_id|>\\n\\n\"\n                    ret += f\"{message.strip()}<|eot_id|>\"\n                else:\n                    ret += f\"<|start_header_id|>{role}<|end_header_id|>\\n\\n\"\n            return ret\n        elif self.sep_style == SeparatorStyle.CHATGLM:\n            # source: https://huggingface.co/THUDM/chatglm-6b/blob/1d240ba371910e9282298d4592532d7f0f3e9f3e/modeling_chatglm.py#L1302-L1308\n            # source2: https://huggingface.co/THUDM/chatglm2-6b/blob/e186c891cf64310ac66ef10a87e6635fa6c2a579/modeling_chatglm.py#L926\n            round_add_n = 1 if self.name == \"chatglm2\" else 0\n            if system_prompt:\n                ret = system_prompt + self.sep\n            else:\n                ret = \"\"\n\n            for i, (role, message) in enumerate(self.messages):\n                if i % 2 == 0:\n                    ret += f\"[Round {i//2 + round_add_n}]{self.sep}\"\n\n                if message:\n                    ret += f\"{role}：{message}{self.sep}\"\n                else:\n                    ret += f\"{role}：\"\n            return ret\n        elif self.sep_style == SeparatorStyle.CHATML:\n            ret = \"\" if system_prompt == \"\" else system_prompt + self.sep + \"\\n\"\n            for role, message in self.messages:\n                if message:\n                    if type(message) is tuple:\n                        message, images = message\n                        message = IMAGE_PLACEHOLDER_STR * len(images) + message\n                    ret += role + \"\\n\" + message + self.sep + \"\\n\"\n                else:\n                    ret += role + \"\\n\"\n            return ret\n        elif self.sep_style == SeparatorStyle.CHATGLM3:\n            ret = \"\"\n            if self.system_message:\n                ret += system_prompt\n            for role, message in self.messages:\n                if message:\n                    ret += role + \"\\n\" + message\n                else:\n                    ret += role\n            return ret\n        elif self.sep_style == SeparatorStyle.CHATINTERN:\n            # source: https://huggingface.co/internlm/internlm-chat-7b-8k/blob/bd546fa984b4b0b86958f56bf37f94aa75ab8831/modeling_internlm.py#L771\n            seps = [self.sep, self.sep2]\n            ret = system_prompt\n            for i, (role, message) in enumerate(self.messages):\n                if i % 2 == 0:\n                    ret += \"<s>\"\n                if message:\n                    ret += role + \":\" + message + seps[i % 2] + \"\\n\"\n                else:\n                    ret += role + \":\"\n            return ret\n        elif self.sep_style == SeparatorStyle.DOLLY:\n            seps = [self.sep, self.sep2]\n            ret = system_prompt\n            for i, (role, message) in enumerate(self.messages):\n                if message:\n                    ret += role + \":\\n\" + message + seps[i % 2]\n                    if i % 2 == 1:\n                        ret += \"\\n\\n\"\n                else:\n                    ret += role + \":\\n\"\n            return ret\n        elif self.sep_style == SeparatorStyle.PHOENIX:\n            ret = system_prompt\n            for role, message in self.messages:\n                if message:\n                    ret += role + \": \" + \"<s>\" + message + \"</s>\"\n                else:\n                    ret += role + \": \" + \"<s>\"\n            return ret\n        elif self.sep_style == SeparatorStyle.ROBIN:\n            ret = system_prompt + self.sep\n            for role, message in self.messages:\n                if message:\n                    ret += role + \":\\n\" + message + self.sep\n                else:\n                    ret += role + \":\\n\"\n            return ret\n        elif self.sep_style == SeparatorStyle.FALCON_CHAT:\n            ret = \"\"\n            if self.system_message:\n                ret += system_prompt + self.sep\n            for role, message in self.messages:\n                if message:\n                    ret += role + \": \" + message + self.sep\n                else:\n                    ret += role + \":\"\n            return ret\n        elif self.sep_style == SeparatorStyle.METAMATH:\n            ret = \"\" if system_prompt == \"\" else system_prompt + self.sep\n            for i, (role, message) in enumerate(self.messages):\n                # For MetaMath, sep2 is used to prefix the message.\n                starting_sep = \":\\n\" if i % 2 == 0 else \": \" + self.sep2\n                ending_sep = self.sep if i % 2 == 0 else \"\"\n                if message:\n                    ret += role + starting_sep + message + ending_sep\n                else:\n                    ret += role + starting_sep\n            return ret\n        elif self.sep_style == SeparatorStyle.DEEPSEEK_CHAT:\n            seps = [self.sep, self.sep2]\n            ret = system_prompt\n            for i, (role, message) in enumerate(self.messages):\n                if message:\n                    ret += role + \": \" + message + seps[i % 2]\n                else:\n                    ret += role + \":\"\n            return ret\n        elif self.sep_style == SeparatorStyle.YUAN2:\n            seps = [self.sep, self.sep2]\n            ret = \"\"\n            if self.system_message:\n                ret += system_prompt + seps[1]\n            for _, message in self.messages:\n                if message:\n                    ret += message + \"<n>\"\n                else:\n                    ret += \"\"\n            ret = ret.rstrip(\"<n>\") + seps[0]\n            return ret\n        elif self.sep_style == SeparatorStyle.GEMMA:\n            ret = \"<bos>\"\n            for role, message in self.messages:\n                if message:\n                    ret += \"<start_of_turn>\" + role + \"\\n\" + message + self.sep\n                else:\n                    ret += \"<start_of_turn>\" + role + \"\\n\"\n            return ret\n        elif self.sep_style == SeparatorStyle.CLLM:\n            seps = [self.sep, self.sep2]\n            ret = system_prompt + seps[0]\n            for i, (role, message) in enumerate(self.messages[-2:]):\n                if message:\n                    if type(message) is tuple:\n                        message, images = message\n                        message = IMAGE_PLACEHOLDER_STR * len(images) + message\n                    ret += role + \": \" + message + seps[i % 2]\n                else:\n                    ret += role + \":\"\n            return ret\n        elif self.sep_style == SeparatorStyle.DEFAULT:\n            ret = system_prompt + \"\\n\"\n            for role, message in self.messages:\n                if message:\n                    if type(message) is tuple:\n                        message, images = message\n                    ret += role + \": \" + message + \"\\n\"\n                else:\n                    ret += role + \":\"\n            return ret\n        else:\n            raise ValueError(f\"Invalid style: {self.sep_style}\")\n\ndef register_conv_template(template: Conversation, override: bool = False):\n    \"\"\"Register a new conversation template.\"\"\"\n    if not override:\n        assert (\n            template.name not in conv_templates\n        ), f\"{template.name} has been registered.\"\n\n    conv_templates[template.name] = template\n\n\ndef get_conv_template(name: str) -> Conversation:\n    \"\"\"Get a conversation template.\"\"\"\n    return conv_templates[name].copy()\n\n\n\nimport time\nimport openai\nimport anthropic\n\ndef chat_completion_openai(model, conv, temperature, max_tokens, api_dict=None):\n    if api_dict is not None:\n        openai.api_base = api_dict[\"api_base\"]\n        openai.api_key = api_dict[\"api_key\"]\n    output = \"$ERROR$\"\n    for _ in range(16):\n        try:\n            messages = conv.to_openai_api_messages()\n            response = openai.ChatCompletion.create(\n                model=model,\n                messages=messages,\n                n=1,\n                temperature=temperature,\n                max_tokens=max_tokens,\n            )\n            output = response[\"choices\"][0][\"message\"][\"content\"]\n            break\n        except openai.error.OpenAIError as e:\n            print(type(e), e)\n            time.sleep(10)\n\n    return output\n\n\n\nfrom dataclasses import dataclass, field\nimport json\nimport math\nimport pathlib\nfrom typing import Dict, Optional, Sequence\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\nimport transformers\nfrom transformers import Trainer\nfrom transformers.trainer_pt_utils import LabelSmoother\n\nIGNORE_TOKEN_ID = LabelSmoother.ignore_index\n\n\n@dataclass\nclass TrainingArguments(transformers.TrainingArguments):\n    cache_dir: Optional[str] = field(default=None)\n    optim: str = field(default=\"adamw_torch\")\n    model_max_length: int = field(\n        default=512,\n        metadata={\n            \"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n        },\n    )\n\nlocal_rank = None\n\ndef preprocess(\n    sources,\n    tokenizer: transformers.PreTrainedTokenizer,\n) -> Dict:\n    conv = get_conversation_template(\"vicuna\")\n    roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n\n    # Apply prompt templates\n    conversations = []\n    for i, source in enumerate(sources):\n        if roles[source[0][\"from\"]] != conv.roles[0]:\n            # Skip the first one if it is not from human\n            source = source[1:]\n\n        conv.messages = []\n        for j, sentence in enumerate(source):\n            role = roles[sentence[\"from\"]]\n            assert role == conv.roles[j % 2], f\"{i}\"\n            conv.append_message(role, sentence[\"value\"])\n        conversations.append(conv.get_prompt())\n\n    # Tokenize conversations\n    input_ids = tokenizer(\n        conversations,\n        return_tensors=\"pt\",\n        padding=\"max_length\",\n        max_length=tokenizer.model_max_length,\n        truncation=True,\n    ).input_ids\n    targets = input_ids.clone()\n\n    assert conv.sep_style == SeparatorStyle.ADD_COLON_TWO\n\n    # Mask targets. Only compute loss on the assistant outputs.\n    sep = conv.sep + conv.roles[1] + \": \"\n    for conversation, target in zip(conversations, targets):\n        total_len = int(target.ne(tokenizer.pad_token_id).sum())\n\n        turns = conversation.split(conv.sep2)\n        cur_len = 1\n        target[:cur_len] = IGNORE_TOKEN_ID\n        for i, turn in enumerate(turns):\n            if turn == \"\":\n                break\n            turn_len = len(tokenizer(turn).input_ids)\n\n            parts = turn.split(sep)\n            if len(parts) != 2:\n                break\n            parts[0] += sep\n            # \"-2\" is hardcoded for the Llama tokenizer to make the offset correct.\n            instruction_len = len(tokenizer(parts[0]).input_ids) - 2\n\n            if i != 0 and not tokenizer.legacy:\n                # The legacy and non-legacy modes handle special tokens differently\n                instruction_len -= 1\n\n            # Ignore the user instructions\n            target[cur_len : cur_len + instruction_len] = IGNORE_TOKEN_ID\n            cur_len += turn_len\n\n            if i != 0 and not tokenizer.legacy:\n                # The legacy and non-legacy modes handle special tokens differently\n                cur_len -= 1\n\n        target[cur_len:] = IGNORE_TOKEN_ID\n\n        if False:  # Inspect and check the correctness of masking\n            z = target.clone()\n            z = torch.where(z == IGNORE_TOKEN_ID, tokenizer.unk_token_id, z)\n            rank0_print(tokenizer.decode(z))\n            exit()\n\n        if cur_len < tokenizer.model_max_length:\n            if cur_len != total_len:\n                target[:] = IGNORE_TOKEN_ID\n                rank0_print(\n                    f\"WARNING: tokenization mismatch: {cur_len} vs. {total_len}.\"\n                    f\" #turn = {len(turns) - 1}. (ignored)\"\n                )\n\n    return dict(\n        input_ids=input_ids,\n        labels=targets,\n        attention_mask=input_ids.ne(tokenizer.pad_token_id),\n    )\n\nclass SupervisedDataset(Dataset):\n    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n\n    def __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer):\n        super(SupervisedDataset, self).__init__()\n\n        rank0_print(\"Formatting inputs...\")\n        sources = [example[\"conversations\"] for example in raw_data]\n        data_dict = preprocess(sources, tokenizer)\n\n        self.input_ids = data_dict[\"input_ids\"]\n        self.labels = data_dict[\"labels\"]\n        self.attention_mask = data_dict[\"attention_mask\"]\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n        return dict(\n            input_ids=self.input_ids[i],\n            labels=self.labels[i],\n            attention_mask=self.attention_mask[i],\n        )\n\n\ndef train():\n    global local_rank\n\n    parser = transformers.HfArgumentParser(\n        (ModelArguments, DataArguments, TrainingArguments)\n    )\n    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n    local_rank = training_args.local_rank\n\n    # Set RoPE scaling factor\n    config = transformers.AutoConfig.from_pretrained(\n        model_args.model_name_or_path,\n        cache_dir=training_args.cache_dir,\n        trust_remote_code=model_args.trust_remote_code,\n    )\n    orig_ctx_len = getattr(config, \"max_position_embeddings\", None)\n    if orig_ctx_len and training_args.model_max_length > orig_ctx_len:\n        scaling_factor = float(math.ceil(training_args.model_max_length / orig_ctx_len))\n        config.rope_scaling = {\"type\": \"linear\", \"factor\": scaling_factor}\n    config.use_cache = False\n\n    # Load model and tokenizer\n    model = transformers.AutoModelForCausalLM.from_pretrained(\n        model_args.model_name_or_path,\n        config=config,\n        cache_dir=training_args.cache_dir,\n        trust_remote_code=model_args.trust_remote_code,\n    )\n    tokenizer = transformers.AutoTokenizer.from_pretrained(\n        model_args.model_name_or_path,\n        cache_dir=training_args.cache_dir,\n        model_max_length=training_args.model_max_length,\n        padding_side=model_args.padding_side,\n        use_fast=False,\n        trust_remote_code=model_args.trust_remote_code,\n    )\n\n    if tokenizer.pad_token != tokenizer.unk_token:\n        tokenizer.pad_token = tokenizer.unk_token\n\n    # Load data\n    data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)\n\n    # Start trainner\n    trainer = Trainer(\n        model=model, tokenizer=tokenizer, args=training_args, **data_module\n    )\n    if list(pathlib.Path(training_args.output_dir).glob(\"checkpoint-*\")):\n        trainer.train(resume_from_checkpoint=True)\n    else:\n        trainer.train()\n\n    # Save model\n    model.config.use_cache = True\n    trainer.save_state()\n    if trainer.is_deepspeed_enabled:\n        trainer.save_model()\n    else:\n        trainer_save_model_safe(trainer)\n\n\n\nimport ast\nimport json\nimport os\nimport re\nimport time\nfrom typing import Optional\n\nimport openai\nimport anthropic\n\ndef run_judge_single(question, answer, judge, ref_answer, multi_turn=False):\n    kwargs = {}\n    model = judge.model_name\n    if ref_answer is not None:\n        kwargs[\"ref_answer_1\"] = ref_answer[\"choices\"][0][\"turns\"][0]\n        if multi_turn:\n            kwargs[\"ref_answer_2\"] = ref_answer[\"choices\"][0][\"turns\"][1]\n\n    if multi_turn:\n        user_prompt = judge.prompt_template[\"prompt_template\"].format(\n            question_1=question[\"turns\"][0],\n            question_2=question[\"turns\"][1],\n            answer_1=answer[\"choices\"][0][\"turns\"][0],\n            answer_2=answer[\"choices\"][0][\"turns\"][1],\n            **kwargs,\n        )\n    else:\n        user_prompt = judge.prompt_template[\"prompt_template\"].format(\n            question=question[\"turns\"][0],\n            answer=answer[\"choices\"][0][\"turns\"][0],\n            **kwargs,\n        )\n\n    rating = -1\n\n    system_prompt = judge.prompt_template[\"system_prompt\"]\n    conv = get_conversation_template(model)\n    conv.set_system_message(system_prompt)\n    conv.append_message(conv.roles[0], user_prompt)\n    conv.append_message(conv.roles[1], None)\n\n    if model in [\"gpt-3.5-turbo\", \"gpt-3.5-turbo-0301\", \"gpt-3.5-turbo-0613\", \"gpt-3.5-turbo-1106\", \"gpt-3.5-turbo-0125\", \"gpt-4\", \"gpt-4-0314\", \"gpt-4-0613\", \"gpt-4-turbo\", \"gpt-4-1106-preview\", \"gpt-4-0125-preview\", \"gpt-4-turbo-browsing\", \"gpt-4-turbo-2024-04-09\", \"gpt2-chatbot\", \"im-also-a-good-gpt2-chatbot\", \"im-a-good-gpt2-chatbot\", \"gpt-4o-mini-2024-07-18\", \"gpt-4o-2024-05-13\", \"gpt-4o-2024-08-06\", \"chatgpt-4o-latest-20240903\", \"chatgpt-4o-latest\", \"o1-preview\", \"o1-mini\"]:\n        judgment = chat_completion_openai(model, conv, temperature=0, max_tokens=2048)\n    elif model in [\"claude-1\", \"claude-2\", \"claude-2.0\", \"claude-2.1\", \"claude-3-haiku-20240307\", \"claude-3-haiku-20240307-vertex\", \"claude-3-sonnet-20240229\", \"claude-3-sonnet-20240229-vertex\", \"claude-3-5-sonnet-20240620\", \"claude-3-opus-20240229\", \"claude-instant-1\", \"claude-instant-1.2\"]:\n        judgment = chat_completion_anthropic(\n            model, conv, temperature=0, max_tokens=1024\n        )\n    else:\n        raise ValueError(f\"Invalid judge model name: {model}\")\n\n    if judge.prompt_template[\"output_format\"] == \"[[rating]]\":\n        match = re.search(\"\\[\\[(\\\\d+\\\\.?\\\\d*)\\]\\]\", judgment)\n        if not match:\n            match = re.search(\"\\[(\\\\d+\\\\.?\\\\d*)\\]\", judgment)\n\n        if match:\n            rating = ast.literal_eval(match.groups()[0])\n        else:\n            rating = -1\n    else:\n        raise ValueError(\n            f\"invalid output format: {judge.prompt_template['output_format']}\"\n        )\n\n    return rating, user_prompt, judgment",
    "Experiment Result": "LLM for Objective Discovery (Meta-Optimization): GPT-4\nAPI Call Robustness Settings: API_MAX_RETRY=16, API_RETRY_SLEEP=10 seconds, API_ERROR_OUTPUT=\"$ERROR$\" (defined in fastchat/llm_judge/common.py)\nFinetuning Settings:\n    - Model Max Length: 512 tokens (from fastchat/train/train.py TrainingArguments default)\n    - Optimizer: adamw_torch (from fastchat/train/train.py TrainingArguments default)\n    - Model Loading Parameters: device, num_gpus, max_gpu_memory, dtype, load_8bit, cpu_offloading, gptq_config, awq_config, exllama_config, xft_config, revision (from fastchat/model/model_adapter.py load_model function arguments)\nDownstream Validation Task: MT-Bench\nEvaluation Settings (for MT-Bench):\n    - Judge Model: GPT-4 (used in fastchat/llm_judge/gen_judgment.py)\n    - Evaluation Temperatures for various categories (from fastchat/llm_judge/common.py temperature_config dict):\n        - writing: 0.7\n        - roleplay: 0.7\n        - extraction: 0.0\n        - math: 0.0\n        - coding: 0.0\n        - reasoning: 0.0\n        - stem: 0.1\n        - humanities: 0.1\n        - arena-hard-200: 0.0\n    - Maximum Tokens for Judge Output: 2048 for OpenAI models, 1024 for Anthropic models (from fastchat/llm_judge/common.py run_judge_single function)\n    - Output Format for Scoring: `[[rating]]` or `[[rating_a,rating_b]]` regex patterns for score extraction (from fastchat/llm_judge/common.py one_score_pattern, two_score_pattern)\n    - Tie Delta for scores: 0.1 (TIE_DELTA from fastchat/llm_judge/common.py)"
}{
    "Title": "Language Generation with Strictly Proper Scoring Rules",
    "Main Contributions": "The paper addresses the limitation of maximum likelihood estimation (MLE) in language generation, which primarily uses the logarithmic score (cross-entropy loss), by proposing a strategy to adapt any non-local strictly proper scoring rules as loss functions. It introduces a token-level distribution of scoring rules and score smoothing, enabling the use of non-local scores like the Brier and Spherical scores. The core contribution is demonstrating that simply substituting the loss function with these alternative scores can yield substantial improvements in language model generation capabilities, even without hyperparameter tuning, and that these benefits scale to large language models (LLMs) like LLaMA-7B and LLaMA-13B. This work is the first to train language generation models using scoring rules other than the logarithmic score.",
    "Methodology": "The methodology involves a straightforward strategy to adapt non-local strictly proper scoring rules for language generation. This includes: 1. Distributing the scoring rule at the token level, transforming sequence prediction into a series of conditional token prediction tasks, which bypasses the infeasibility of calculating scores across the exponentially large sample space of natural text. 2. Introducing 'score smoothing' as a general technique to support label smoothing for arbitrary scoring rules, motivating the model to produce a desired smooth distribution. 3. Enhancing the smoothing effect by using a 'masked logarithmic score' for under-smooth labels, ensuring a stronger incentive for proper label smoothing. The paper evaluates classic strictly proper scoring rules such as the Brier score, Spherical score, α-power score, and pseudo-spherical score.",
    "Experimental Setup": "The research primarily evaluates methods on machine translation and abstractive summarization. For machine translation, WMT14 English-French (En-Fr), WMT14 English-German (En-De), and TED bilingual datasets are used, with BLEU score as the metric. For abstractive summarization, the CNN/DailyMail dataset is used, with ROUGE-1, ROUGE-2, and ROUGE-L scores. Transformer-base/big models are employed for these tasks. For larger scale evaluation, LLaMA-7B and LLaMA-13B models are used for instruction tuning on the Alpaca dataset, with evaluation extending to MT-bench for multi-turn question answering, in addition to machine translation (WMT22) and summarization (CNN/DailyMail). Experiments involve training from scratch and fine-tuning models pre-trained with logarithmic loss, often without adjusting other hyperparameters. Beam search with varying beam sizes and length penalties is used for generation.",
    "Limitations": "One limitation is that alternative scoring rules (Brier, Spherical) do not perform as well as the logarithmic score when training models from scratch, which is partly attributed to using hyperparameters optimized for the logarithmic score. Score smoothing, in its naive form, can be ineffective for some scoring rules (e.g., Spherical score) due to the loss function being relatively flat around the optimal point. The optimization trajectories of different strictly proper scores, despite sharing the same global optimum, can be conflicting and may not always align with the model’s generative capabilities. Memory constraints limited summarization performance assessment to LLaMA-7B only. The investigation into using different scoring rules as objectives for beam search was not exhaustive.",
    "Future Research Directions": "Future research directions include exploring other strictly proper scoring rules that might exhibit superior performance during pre-training or fine-tuning. Investigating the factors contributing to performance differences among various scores and developing additional metrics or properties to determine a score’s suitability for training language generation models are also suggested. Furthermore, the paper proposes exploring the utility of these scoring rules as evaluation metrics, similar to Perplexity (PPL), for assessing the calibration of language generation models. The application of other strictly proper scoring rules as objectives for beam search is also mentioned as an area for future exploration.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Beyond MLE: Convex Learning for Text Generation",
    "Main Contributions": "The paper argues that Maximum Likelihood Estimation (MLE) is suboptimal for closed-ended text generation tasks, as it aims to estimate the entire data distribution rather than focusing on highly probable outputs, complicating training and decoding. It proposes a novel class of training objectives based on convex functions (specifically, convex-composition functions) which enables models to focus on highly probable outputs without estimating the entire data distribution. Theoretical analysis demonstrates that convex functions sharpen the optimal distribution. Experimentally, the approach bridges the gap between greedy and beam search for autoregressive models, achieves over 9+ BLEU points improvement for non-autoregressive models by alleviating multi-modality issues and increasing prediction confidence, and substantially enhances the generative capability of Large Language Models on various closed-ended tasks.",
    "Methodology": "The core methodology introduces a general learning framework Lf(θ) = -Ex~pdata(x)[f(pθ(x))], where f is a differentiable and increasing function. While a purely convex f would lead to a desirable one-hot optimal distribution, direct application is impractical due to gradient vanishing when prediction probabilities approach zero. To address this, the paper proposes a convex-composition approach: combining an increasing convex function f (e.g., exponential function) with an original increasing concave function g (e.g., log-probability from MLE), resulting in the loss Lfg(θ) = -Σ pdata(xi) * fg(pθ(xi)). This composition makes the learning criterion less concave, yielding a sharper optimal distribution while maintaining training feasibility. A two-step training strategy is employed: initial MLE pre-training followed by fine-tuning with the convex-composition loss to mitigate early gradient issues.",
    "Experimental Setup": "Experiments were conducted on two closed-ended text generation tasks: neural machine translation (NMT) and text summarization. For NMT, the WMT14 English-German dataset was used for Transformer (autoregressive), Vanilla-NAT, CMLM, and CTC (non-autoregressive models), and WMT22 translation tasks for LLaMA-7B and LLaMA-13B (Large Language Models, LLMs). For summarization, CNN/DailyMail and XSum datasets were used for Transformer, and CNN/DailyMail for LLMs. Performance was evaluated using BLEU and COMET scores for NMT, and ROUGE-1, ROUGE-2, ROUGE-L for summarization. Additionally, prediction confidence (negative log-likelihood) and generation fluency (external pre-trained language model perplexity) were measured for non-autoregressive models. Models were trained using the Adam optimizer, with a two-step approach (MLE pre-training then convex-composition fine-tuning). Hyperparameters like the exponent 'k' were tuned, and decoding strategies included greedy/beam search for AR models and argmax for NAR models.",
    "Limitations": "The direct application of loss functions with purely convex criteria suffers from inherent limitations during training, as gradients of parameters tend to be extremely small when prediction probabilities approach zero, rendering the training process inefficient. While the proposed convex-composition function mitigates this, an alternative convex function (power function) was explored but found less effective, encountering training difficulties or failures, especially when the exponent 'k' approached 0 due to unstable gradient behavior. Furthermore, experiments on VAE-based models indicated that while convex-composition loss improves generation quality, it leads to poor diversity, a trade-off consistent with its mode collapse property.",
    "Future Research Directions": "Not mentioned",
    "Experiment Code": "class CustomTrainer(Trainer):\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        \n        logits = outputs[\"logits\"] if isinstance(outputs, dict) else outputs[0]\n        logits = logits[..., :-1, :].contiguous()\n        labels = labels[..., 1:].contiguous()\n\n        lprobs = torch.nn.functional.log_softmax(logits, dim = -1)\n        batch_size, length, vocab_size = lprobs.size()\n        padding_mask = labels.eq(IGNORE_INDEX).view(batch_size, length)\n        labels = torch.clamp(labels, min=0)\n        tgt_lprob = lprobs.gather(dim=-1, index=labels.unsqueeze(-1)).view(batch_size, length)\n        tgt_length = torch.sum(~padding_mask, dim = -1)\n        tgt_lprob.masked_fill_(padding_mask, 0.0)\n        tgt_lprob = torch.sum(tgt_lprob, dim = -1) / tgt_length\n        tgt_prob = torch.exp(tgt_lprob * 1)\n        custom_loss = - torch.sum(tgt_prob * tgt_length)\n\n        num_active_elements = padding_mask.numel() - padding_mask.long().sum()\n\n        custom_loss = custom_loss / num_active_elements\n        return (custom_loss, outputs) if return_outputs else custom_loss",
    "Experiment Result": "The method employs a two-step training strategy: initial Maximum Likelihood Estimation (MLE) pre-training (implemented in `run_mle.py`) followed by fine-tuning with the proposed convex-composition loss (implemented in `run_convex.py` and `convex-learning` modules).\n\nKey experimental settings for the convex-composition loss:\n-   **Convex Order (`k`)**: A hyperparameter for the exponential convex function `f(x) = exp(x*k)`, with a default value of 1.0. This is specified as `convex_order` in criterion and model definitions.\n-   **Label Smoothing**: Applied during loss computation, with `label_smoothing` as a configurable parameter.\n\nSpecific settings for Non-Autoregressive Transformer (NAT) models that use the convex loss:\n-   **Noise Injection**: Strategies like `random_delete`, `random_mask`, `full_mask`, and `no_noise` are used to create `prev_target` tokens during training for NAT models.\n-   **Source Embedding Copying**: Options such as `src_embedding_copy` and `soft_copy` (with configurable `soft_copy_temp_learnable` and `soft_copy_temp`) determine how initial decoder inputs are derived from the encoder.\n-   **Length Prediction**: Parameters like `pred_length_offset`, `sg_length_pred` (stop gradients), and `length_loss_factor` are used for predicting target sequence lengths.\n\nFor CTC-based NAT models utilizing the convex loss:\n-   **Source Upsampling**: `src_upsample_ratio` is used to expand source sequence length.\n-   **CTC Decoding**: `plain_ctc` (boolean) and `ctc_beam_size` (for beam search decoding) are configurable.\n\nGeneral training configuration parameters include:\n-   `model_name_or_path`: Specifies the pre-trained model checkpoint.\n-   `block_size`: Maximum sequence length for tokenization.\n-   `padding_side`: Tokenizer padding direction.\n-   `max_train_samples`, `max_eval_samples`: For debugging or quicker training by truncating dataset sizes.\n-   `freeze_emb`, `freeze_layers`: Options to freeze model embedding weights or specific transformer layers during fine-tuning."
}{
    "Title": "Model Alignment as Prospect Theoretic Optimization",
    "Main Contributions": "The paper introduces KTO (Kahneman-Tversky Optimization), a novel model alignment objective for LLMs based on prospect theory. KTO directly maximizes the perceived utility of generated text, leveraging a Kahneman-Tversky model of human value, and only requires a binary signal (desirable/undesirable) instead of preference pairs. KTO matches or exceeds the performance of preference-based methods like DPO across various model scales (1B to 30B parameters), even with significantly less or imbalanced data, and can sometimes eliminate the need for supervised finetuning (SFT). The work also proposes Human-Aware Losses (HALOs) as a general class of alignment objectives, suggesting that the optimal loss function depends on appropriate inductive biases for a given setting.",
    "Methodology": "The core methodology involves framing LLM alignment through Kahneman & Tversky's prospect theory, defining Human-Aware Losses (HALOs) as a class of loss functions that incorporate human perceptual biases. KTO, a specific HALO, replaces the exponential component of the canonical Kahneman-Tversky value function with a logistic function for numerical stability and introduces hyperparameters (β for risk aversion, λD/λU for loss aversion) to control its shape. KTO optimizes for a reward `rθ(x, y) = log(πθ(y|x) / πref(y|x))` relative to an estimated reference point `z0` (a biased estimate of KL divergence) by minimizing `LKTO(πθ, πref) = Ex,y~D[λy - v(x, y)]`. Binary feedback is used directly, or preference data (yw ≻ yl) is converted by treating yw as desirable and yl as undesirable. A mismatched microbatch strategy is employed for `z0` estimation to avoid slow sampling from `πθ`. The paper also analyzes how DPO and an offline PPO variant qualify as HALOs.",
    "Experimental Setup": "The research utilized several LLM families for experiments: Pythia-{1.4B, 2.8B, 6.9B, 12B}, Llama-{7B, 13B, 30B}, Mistral-7B derivatives, Llama-3 8B, Qwen2.5 3B Instruct, and Zephyr-β-SFT. Alignment was performed on a combination of Anthropic-HH, OpenAssistant, and SHP datasets, as well as UltraFeedback. Performance was evaluated using GPT-4-0613 for judging winrates against SFT targets (assessing helpfulness, harmlessness, and conciseness), validated with human judgments. Quantitative benchmarks included MMLU (0-shot), GSM8K (8-shot chain-of-thought), HumanEval (0-shot), and BigBench-Hard (3-shot chain-of-thought), and AlpacaEval 2. KTO was compared against CSFT, SLiC, DPO, an offline PPO variant with dummy rewards, and ORPO. Experiments also included testing KTO's robustness to data imbalances (discarding up to 90% of desirable data) and using only one output per input (one-y-per-x) to simulate unpaired data.",
    "Limitations": "The study acknowledges that no single HALO is universally superior, as the best loss depends on inductive biases appropriate for a given setting. The canonical Kahneman-Tversky value function required modification for numerical stability. The reference point `z0` estimation in KTO is biased, though practical. The conversion of preference data into binary signals for KTO is a naive assumption. KTO may underfit complex distributions by implicitly ignoring data deemed too difficult to learn, which can be mitigated but remains a potential weakness. Theoretically, DPO might outperform KTO in scenarios with very low noise and intransitivity in preference data. The inherent biases of human feedback datasets used (e.g., SHP, HH, OASST) can propagate. KTO's default loss-neutral resolution of contradictory feedback might not align with all fairness theories, risking homogenization of preferences if deployed widely.",
    "Future Research Directions": "Future work includes identifying optimal value functions and reference point distributions that accurately describe human perception of language and their variations across domains and individuals. Developing HALOs capable of incorporating granular feedback (e.g., scores) for multi-desiderata optimization, and extending them to other modalities (e.g., images) and model classes (e.g., diffusion models) that lack explicit output distributions. Further research can explore HALOs designed to resolve feedback contradictions according to diverse definitions of fairness, and those adapted for online data. The design of dynamic hyperparameter selection schemes (e.g., for λD, λU) and more sophisticated methods for deconstructing preferences into binary feedback are also important. Finally, ecologically valid evaluations in real-world settings are needed to thoroughly assess the merits of different HALOs.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Diff-eRank: A Novel Rank-Based Metric for Evaluating Large Language Models",
    "Main Contributions": "The paper introduces Diff-eRank, a novel rank-based metric grounded in information theory and geometry principles for evaluating Large Language Models (LLMs). Diff-eRank quantifies how efficiently LLMs eliminate redundant information ('noise reduction') by analyzing their hidden representations. Key findings include that Diff-eRank increases with LLM model size and correlates well with conventional metrics like loss and accuracy. For multi-modal LLMs (MLLMs), the paper proposes eRank-based alignment evaluation methods, demonstrating strong alignment performance in contemporary MLLMs.",
    "Methodology": "The core methodology revolves around Diff-eRank, defined as the difference between the effective rank (eRank) of representations from an untrained model (M0) and a trained model (M1) for a given input. The eRank of representations from a model is calculated based on the covariance matrix of its hidden states, normalized and then subjected to an exponential function of its matrix entropy. A lower eRank signifies more structured and compact representations due to redundant information removal. For MLLMs, new metrics based on eRank are introduced: 'Image Reduction Ratio' (quantifying visual information condensation by the connector network) and 'Image-Text Alignment' (evaluating the closeness of eRanks between image, text, and combined image-text representations post-LLM processing). Ablation studies explored alternative Diff-eRank algorithms (average matrix entropy vs. average effective rank) and its measurement across different layers of LLMs.",
    "Experimental Setup": "Experiments for uni-modal LLMs used models from the OPT family (125M to 13B parameters) across diverse datasets: pre-training datasets (Wikipedia, openwebtext2), instruction dataset (dolly-15k), and preference dataset (hh-rlhf). Validation against reduced cross-entropy loss and benchmark accuracy (openbookqa, piqa) was performed. For multi-modal LLMs, LLaVA-1.5 and MiniGPT-v2 were evaluated on multi-modal instruction datasets (detail_23k, cc_sbu_align). An additional experiment included image rotation to assess modality alignment robustness. Ablation studies covered Cerebras-GPT and OpenELM families on dolly-15k with winogrande and piqa for benchmark accuracy, Diff-eRank computation on different layers of OPT models, and the impact of dataset sampling size on Wikipedia. All experiments utilized NVIDIA A800-80G GPUs.",
    "Limitations": "The study was unable to observe the change of Diff-eRank across the LLMs' entire pre-training and post-training stages due to limited computational resources. While Diff-eRank generally correlated well with loss and accuracy, occasional outliers in these trends were observed, although they were deemed not to affect the overall conclusion.",
    "Future Research Directions": "Future work includes broadening the investigative scope to introduce Diff-eRank across LLMs' complete training stages (pre-training and post-training). The metric could also be beneficial for techniques like pruning, quantization, and distillation, potentially aiding in identifying parts of models that can be compressed without significant information loss. The authors also suggest exploring a more comprehensive evaluation for multi-modal models using effective rank and integrating such internal representation metrics into various potential use cases.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "EMO: EARTH MOVER DISTANCE OPTIMIZATION FOR AUTO-REGRESSIVE LANGUAGE MODELING",
    "Main Contributions": "The paper identifies three key limitations of Maximum Likelihood Estimation (MLE) in auto-regressive language modeling: recall-prioritization, negative diversity ignorance, and train-test mismatch, which contribute to text degeneration. To address these, it proposes Earth Mover Distance Optimization (EMO), a novel training approach for auto-regressive language models. EMO introduces a differentiable upper bound of Earth Mover Distance (DEMD) combined with a semantically informed transport cost function for end-to-end training. Experiments demonstrate EMO's consistent superiority over MLE and other baselines in open-ended language generation (higher Mauve scores) and its ability to significantly enhance downstream performance on language understanding tasks (e.g., NLU, instruction-tuning) through lightweight continual fine-tuning, exhibiting better scaling properties with less data.",
    "Methodology": "EMO replaces MLE with Earth Mover Distance (EMD) as the token-level probability distance measure to align model and human language distributions. It formulates EMD for auto-regressive language modeling as the minimum cost to transport probability mass from the model distribution to the data distribution. The transport cost function C(vi, vj) is defined using the cosine distance between token embeddings from a fixed, pre-trained language modeling head. To overcome the high computational complexity and non-differentiability of direct EMD computation, a tractable differentiable upper bound (DEMD) is derived, enabling efficient gradient-based optimization. DEMD harmonizes recall and precision, offers negative diversity awareness by weighting gradients based on expected transport cost, and improves train-test consistency. For stable training, especially with weaker models, the final loss function dynamically combines MLE and DEMD in a weighted manner.",
    "Experimental Setup": "The research utilized several pre-trained language models: GPT-2 and OPT-125M for language modeling evaluations, and LLaMa-7B/13B, LLaMa2-7B/13B, and OPT-1.3B/2.7B for language understanding and instruction tuning. Baselines included MLE, TaiLr, and MixCE. For open-ended generation, datasets such as WikiText-2, WikiText-103, WebText test, Penn Tree Bank (PTB), WritingPrompts, and AG News were used, along with a synthetic GPT-2-Large output dataset for oracle analysis. Language understanding tasks employed datasets like Tweet Emotion, TREC, SST-2, Subj, Customer Review, Rotten Tomatoes, AG News, and MMLU. Instruction-tuning experiments used Alpaca-GPT4, Recycled Evol-Instruct-70K, and OpenPlatypus. Models were continually fine-tuned for 3 epochs using the AdamW optimizer with a 5e-5 learning rate, a batch size of 32, and a maximum input length of 256. Evaluations for open-ended generation relied on MAUVE, while oracle data also used PPLtest, PPLoracle, and ROUGE-1/L. NLU tasks were assessed by accuracy using in-context learning, and instruction-tuned models were evaluated via LLM-based automatic evaluators like AlpacaEval (win rate), Auto-J, and PandaLM (win/tie counts).",
    "Limitations": "The primary limitation identified for the proposed Earth Mover Distance (DEMD) component is its potential for slow convergence when the language model is relatively weak (having high perplexity). This slow convergence is attributed to bounded gradient scaling induced by the cosine-based transport cost. To overcome this, the final loss function in EMO was implemented as a dynamically weighted combination of MLE and DEMD, ensuring more stable training.",
    "Future Research Directions": "Not mentioned",
    "Experiment Code": "class EMOLlamaForCausalLM(LlamaForCausalLM):\n    def __init__(self, config):\n        super().__init__(config)\n        self.model = LlamaModel(config)\n\n        self.lm_head = torch.nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ):\n\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        # ======================================================================== #\n        #                   Compute the MLE loss\n        # ======================================================================== #\n        hidden_states = outputs[0]\n        logits = self.lm_head(hidden_states)\n        mask = labels[:, 1:].contiguous().view(-1)\n        mask = (mask!=-100).to(logits.dtype)\n        loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n        logits = logits[:, :-1, :].contiguous().view(-1, logits.shape[-1])\n        labels = labels[:, 1:].contiguous().view(-1)\n        mle_loss = loss_fct(logits, labels)\n\n        # ======================================================================== #\n        #                   Compute the EMO loss\n        # ======================================================================== #\n        labels_tmp = labels.clone()\n        labels_tmp[labels_tmp==(-100)] = 0\n        one_hot = torch.nn.functional.one_hot(labels_tmp, num_classes=self.config.vocab_size).to(logits.dtype)\n        stable_onehot = (one_hot+1e-15) / torch.linalg.vector_norm((one_hot+1e-15), ord=1, dim=-1, keepdim=True) # (bsz*seq_len, vocab_size)\n        embedding_matrix = self.cost_embedding # (vocab_size, hidden_size)\n        embedding_matrix = embedding_matrix / torch.linalg.vector_norm(embedding_matrix, ord=2, dim=1, keepdim=True)\n        p_contextual_repr = stable_onehot @ embedding_matrix # (bsz*seq_len, hidden_size)\n        q_grad = torch.log_softmax(logits, dim=-1).exp() # (bsz*seq_len, vocab_size)\n        gt_q = (q_grad * one_hot).detach()\n        q_final = q_grad - gt_q\n        q_contextual_repr = q_final @ embedding_matrix # (bsz*seq_len, hidden_size)\n        emo_loss = (1 - torch.sum(p_contextual_repr*q_contextual_repr, dim=-1)) # (bsz*seq_len,)\n\n        # ======================================================================== #\n        #                   Compose the final loss\n        # ======================================================================== #\n        loss = (torch.min((mle_loss / (emo_loss+1e-10)).detach(), torch.ones_like(mle_loss, dtype=mle_loss.dtype, device=mle_loss.device)*3.0) * emo_loss + mle_loss) * 0.5\n        loss = (loss * mask).sum() / (1e-15 + mask.sum())\n\n        output = (logits,) + outputs[1:]\n        return (loss,) + output if loss is not None else output\n\n\nclass EMOLlama2ForCausalLM(LlamaForCausalLM):\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ):\n\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n        outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        # ======================================================================== #\n        #                   Compute the MLE loss\n        # ======================================================================== #\n        hidden_states = outputs[0]\n        logits = self.lm_head(hidden_states)\n        mask = labels[:, 1:].contiguous().view(-1)\n        mask = (mask!=-100).to(logits.dtype)\n        loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n        logits = logits[:, :-1, :].contiguous().view(-1, logits.shape[-1])\n        labels = labels[:, 1:].contiguous().view(-1)\n        mle_loss = loss_fct(logits, labels)\n\n        # ======================================================================== #\n        #                   Compute the EMO loss\n        # ======================================================================== #\n        labels_tmp = labels.clone()\n        labels_tmp[labels_tmp==(-100)] = 0\n        one_hot = torch.nn.functional.one_hot(labels_tmp, num_classes=self.config.vocab_size).to(logits.dtype)\n        stable_onehot = (one_hot+1e-15) / torch.linalg.vector_norm((one_hot+1e-15), ord=1, dim=-1, keepdim=True) # (bsz*seq_len, vocab_size)\n        embedding_matrix = self.cost_embedding # (vocab_size, hidden_size)\n        embedding_matrix = embedding_matrix / torch.linalg.vector_norm(embedding_matrix, ord=2, dim=1, keepdim=True)\n        p_contextual_repr = stable_onehot @ embedding_matrix.detach() # (bsz*seq_len, hidden_size)\n        q_grad = torch.log_softmax(logits, dim=-1).exp() # (bsz*seq_len, vocab_size)\n        gt_q = (q_grad * one_hot).detach()\n        q_final = q_grad - gt_q\n        q_contextual_repr = q_final @ embedding_matrix.detach() # (bsz*seq_len, hidden_size)\n        emo_loss = (1 - torch.sum(p_contextual_repr*q_contextual_repr, dim=-1)) # (bsz*seq_len,)\n\n        # ======================================================================== #\n        #                   Compose the final loss\n        # ======================================================================== #\n        loss = (emo_loss / (mle_loss+1e-10)).detach()*mle_loss + emo_loss\n        loss = (loss * mask).sum() / (1e-15 + mask.sum())\n\n        output = (logits,) + outputs[1:]\n        return (loss,) + output if loss is not None else output",
    "Experiment Result": "The EMO method is implemented with two main model classes, `EMOLlamaForCausalLM` and `EMOLlama2ForCausalLM`, which dynamically combine Maximum Likelihood Estimation (MLE) loss and Earth Mover Distance (EMD) loss (referred to as EMO loss). For `EMOLlamaForCausalLM`, the final loss composition is `((torch.min((mle_loss / (emo_loss+1e-10)).detach(), torch.ones_like(mle_loss, dtype=mle_loss.dtype, device=mle_loss.device)*3.0) * emo_loss + mle_loss) * 0.5)`. For `EMOLlama2ForCausalLM`, the composition is `((emo_loss / (mle_loss+1e-10)).detach()*mle_loss + emo_loss)`. The `emo_loss` itself is calculated based on the cosine distance between token embeddings, with `p_contextual_repr` from one-hot labels and `q_contextual_repr` from model logits, using a `cost_embedding` matrix.\n\nKey experimental settings include:\n- **Cost Embedding Initialization**: The `cost_embedding` matrix, central to the EMO loss, is typically initialized from a pre-trained language model's `lm_head.weight.data`. For example, in `continual_finetuning/run_clm_trainer_emo.py`, it's explicitly loaded from a Llama-2-13B model's `lm_head`. In other cases (e.g., `run_clm_trainer_emo_fsdp.py` and `instruction_tuning/train.py`), it's initialized as a deep copy of the current model's `lm_head.weight.data` and registered as a buffer.\n- **Loss Reduction**: The final combined loss is typically reduced by taking the mean (`model.config.reduction = 'mean'`).\n- **Efficient Training**: Flash Attention is integrated by replacing the Llama attention mechanism (`replace_llama_attn_with_flash_attn()`). LoRA (Low-Rank Adaptation) for efficient fine-tuning is optionally used, with a configuration of `r=32`, `lora_alpha=16`, `lora_dropout=0.05`, `bias=\"none\"`, and `task_type=\"CAUSAL_LM\"`.\n- **Tokenizer Settings**: For Llama models, `tokenizer.pad_token_id` is set to `0` and `tokenizer.padding_side` to `\"left\"`."
}{
    "Title": "Implicit Optimization Bias of Next-token Prediction in Linear Models",
    "Main Contributions": "This research initiates an investigation into the implicit optimization bias of next-token prediction (NTP), the dominant training paradigm for modern language models. It characterizes the structural properties of solutions selected by gradient-based optimizers, particularly in overparameterized regimes with linear models and fixed context embeddings. Key contributions include introducing \"NTP-separability conditions\" for reaching the data-entropy lower bound, and demonstrating that gradient descent (GD) selects parameters that equate logit differences of in-support tokens to their log-odds within the data subspace. In the orthogonal subspace, GD parameters diverge in norm and align with a direction that maximizes an NTP-specific margin. These findings extend previous work on implicit bias in one-hot classification to the more complex NTP setting.",
    "Methodology": "The study frames next-token prediction as cross-entropy minimization across distinct contexts, each associated with sparse conditional probability distributions over a finite vocabulary. It adopts a \"top-down\" approach, focusing on linear models where only the decoding (word-embedding) matrix W is trained, while context embeddings are kept fixed. The methodology introduces and formalizes \"NTPH-compatibility\" and \"NTP-separability\" conditions that constrain parameter spaces. A novel margin concept for NTP, extending classical SVM definitions, is introduced. The implicit bias of gradient descent (GD) and the regularization path (minimizing L2-regularized CE loss) are mathematically characterized through proofs demonstrating directional convergence and parameter behavior in different subspaces.",
    "Experimental Setup": "The research numerically verifies its findings using synthetic data experiments. A 2D toy example with 3 distinct contexts, 2-dimensional embeddings, 5 words in the vocabulary, and support sets of length 3 is used to visualize context and word embedding geometries, and to illustrate GD's behavior (loss convergence, norm growth, alignment with Wmm, and subspace convergence to W⋆). For a more general overparameterized setting, a dataset with 5000 sequences, 50 distinct contexts, 60-dimensional embeddings, 10 vocabulary words, and 6 possible next-tokens per context is generated. Gradient Descent (GD), Normalized GD (NGD), and Adam are tested for 10,000 iterations with specified learning rates (GD: 0.5; NGD/Adam: 0.01) and Adam hyperparameters (β1=0.9, β2=0.99). The experiments were conducted in Matlab on a MacBook Pro.",
    "Limitations": "The current research is based on several assumptions for establishing a foundational understanding. It primarily focuses on linear models and assumes fixed context embeddings, meaning only the linear decoder matrix is trained. The theoretical findings are limited to settings where overparameterization (embedding dimension d being greater than the number of distinct contexts m) is sufficient for NTP-separability and NTPH-compatibility. While NGD and Adam are empirically tested, a rigorous theoretical analysis of implicit bias for stochastic or adaptive algorithms is not provided. The study does not explore non-linear embedding architectures or the joint optimization of context embeddings and decoder weights (unconstrained features).",
    "Future Research Directions": "Future research directions include identifying exact NTP-separability thresholds under various distributional assumptions, moving beyond the sufficient condition of d > m, potentially requiring the exploration of non-convex architectures. Another promising avenue is to study generalization in NTP settings by examining the statistical properties of the NTP-SVM solution, requiring the development of appropriate generalization measures and statistical models for context embeddings. A \"bottom-up\" approach focusing on architecture-specific embeddings (e.g., shallow transformers) could analyze the effects of optimization biases on both transformer and decoder weights. Investigating the memory capacity of sequence-to-sequence architectures in NTP settings is also suggested. Extending the analysis to \"unconstrained features\" by jointly optimizing context and word embeddings (log-bilinear models) could reveal new insights into embedding geometries. Finally, exploring the implicit bias of adaptive algorithms, such as Adam, in NTP is another critical direction.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Beyond MLE: Convex Learning for Text Generation",
    "Main Contributions": "The paper argues that Maximum Likelihood Estimation (MLE) is suboptimal for closed-ended text generation tasks, as it aims to estimate the entire data distribution rather than focusing on highly probable outputs. The main contributions include proposing a novel class of training objectives based on convex functions, theoretically demonstrating that these functions sharpen the optimal predicted distribution to emphasize high-probability outputs, and empirically validating their effectiveness. This approach enables autoregressive models to bridge the performance gap between greedy and beam search, facilitates the learning of non-autoregressive models with significant BLEU score improvements (up to 9+ points), and substantially enhances the generative capability of Large Language Models (LLMs) across various tasks.",
    "Methodology": "The methodology introduces a general learning framework using a loss function Lf(θ) = -Ex∼pdata(x)[f(pθ(x))], where f is a differentiable, increasing function (MLE is a special case with f=log). The core idea is that convex functions can lead to a one-hot optimal distribution, ideal for precise outputs. However, direct application of convex functions suffers from gradient vanishing issues during training. To address this, a 'convex-composition' approach is proposed: combining an increasing convex function f with an original increasing concave function g (like log-probability) to form Lfg(θ) = -Σ pdata(xi) * fg(pθ(xi)). This composition effectively sharpens the optimal distribution by allocating higher probabilities to more probable samples, reducing Shannon entropy. The paper primarily uses the exponential function as the convex component (e.g., Lf(θ) = -Ex∼pdata(x)[pθ(x)^(k/T)]). A two-step training strategy is employed: MLE pre-training followed by fine-tuning with convex-composition loss to ensure training stability.",
    "Experimental Setup": "Experiments were conducted on autoregressive (Transformer-base), non-autoregressive (Vanilla-NAT, CMLM, CTC), and Large Language Models (LLaMA-7B, LLaMA-13B fine-tuned on Alpaca dataset). The tasks included Neural Machine Translation (WMT14 EN↔DE, WMT22 for LLMs) and Text Summarization (CNN/DailyMail, Xsum). Performance was evaluated using BLEU and COMET for translation, ROUGE-1, ROUGE-2, and ROUGE-L for summarization. For NAR models, prediction confidence (Output NLL) and generation fluency (External PPL from a pre-trained language model) were also measured. Training involved BPE tokenization, Adam optimizer, with specific learning rates, warmup steps, and dropout. A two-step training (MLE pre-training then convex-composition fine-tuning) was used. Decoding strategies included greedy/beam search for AR models and argmax decoding for NAR models. Additional experiments explored the effect of the exponent hyperparameter 'k', correlations on other NAR models, alternative convex functions (power function), diverse generation using VAEs (measuring Reference-BLEU and Pairwise-BLEU), and a comparison with knowledge distillation.",
    "Limitations": "The primary limitation identified is the 'inherent limitation' of directly applying loss with convex functions, where gradients become extremely small for low prediction probabilities, leading to inefficient training. This necessitated the two-step training and convex-composition approach. Another limitation was observed with the power function as a convex component, yielding only marginal benefits and leading to training difficulties or failures when the exponent 'k' approached 0. For VAE-based text generation models, while quality improved, diversity suffered due to the mode-collapse property of convex functions, making it less suitable for tasks requiring diverse outputs. The paper's focus and benefits are primarily for 'closed-ended text generation tasks', implying less direct applicability or benefits for open-ended generation where diversity is key. Lastly, due to memory limitations, the LLaMA-13B model could not be evaluated on the text summarization dataset.",
    "Future Research Directions": "Future research could explore a broader range of convex functions or composite functions beyond the exponential and power functions investigated in this paper. Further theoretical investigation could involve generalizing findings to wider classes of functions or relaxing current assumptions. For tasks requiring both high quality and diversity, adapting convex learning to balance its sharpening effect with diversity-promoting mechanisms could be a promising direction. Given the positive impact on LLMs, studying its effectiveness with even larger language models or integrating it with different instruction tuning paradigms is also an area for future work. Additionally, investigating how convex learning can offer distinct advantages over or be more effectively combined with knowledge distillation techniques, building on the observed similarities, presents another avenue for exploration.",
    "Experiment Code": "class CustomTrainer(Trainer):\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        \n        logits = outputs[\"logits\"] if isinstance(outputs, dict) else outputs[0]\n        logits = logits[..., :-1, :].contiguous()\n        labels = labels[..., 1:].contiguous()\n\n        lprobs = torch.nn.functional.log_softmax(logits, dim = -1)\n        batch_size, length, vocab_size = lprobs.size()\n        padding_mask = labels.eq(IGNORE_INDEX).view(batch_size, length)\n        labels = torch.clamp(labels, min=0)\n        tgt_lprob = lprobs.gather(dim=-1, index=labels.unsqueeze(-1)).view(batch_size, length)\n        tgt_length = torch.sum(~padding_mask, dim = -1)\n        tgt_lprob.masked_fill_(padding_mask, 0.0)\n        tgt_lprob = torch.sum(tgt_lprob, dim = -1) / tgt_length\n        tgt_prob = torch.exp(tgt_lprob * 1)\n        custom_loss = - torch.sum(tgt_prob * tgt_length)\n\n        num_active_elements = padding_mask.numel() - padding_mask.long().sum()\n\n        custom_loss = custom_loss / num_active_elements\n        return (custom_loss, outputs) if return_outputs else custom_loss\n\n@dataclass\nclass LabelSmoothedCrossEntropyCriterionConfig(FairseqDataclass):\n    label_smoothing: float = field(\n        default=0.0,\n        metadata={\"help\": \"epsilon for label smoothing, 0 means no label smoothing\"},\n    )\n    report_accuracy: bool = field(\n        default=False,\n        metadata={\"help\": \"report accuracy metric\"},\n    )\n    sentence_avg: bool = II(\"optimization.sentence_avg\")\n    convex_order: float = field(\n        default=1.0,\n    )\n\n\n\n@register_criterion(\n    \"label_smoothed_cross_entropy_convex\", dataclass=LabelSmoothedCrossEntropyCriterionConfig\n)\nclass LabelSmoothedCrossEntropyCriterion(FairseqCriterion):\n    def __init__(\n        self,\n        task,\n        sentence_avg,\n        label_smoothing,\n        report_accuracy=False,\n        convex_order=1.0,\n    ):\n        super().__init__(task)\n        self.sentence_avg = sentence_avg\n        self.eps = label_smoothing\n        self.report_accuracy = report_accuracy\n        self.k = convex_order\n\n    def forward(self, model, sample, reduce=True):\n        \"\"\"Compute the loss for the given sample.\n\n        Returns a tuple with three elements:\n        1) the loss\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        \"\"\"\n        net_output = model(**sample[\"net_input\"])\n        loss, convex_loss = self.compute_convex_loss(model, net_output, sample, reduce=reduce)\n        sample_size = (\n            sample[\"target\"].size(0) if self.sentence_avg else sample[\"ntokens\"]\n        )\n        logging_output = {\n            \"loss\": loss.data,\n            \"convex_loss\": convex_loss.data,\n            \"ntokens\": sample[\"ntokens\"],\n            \"nsentences\": sample[\"target\"].size(0),\n            \"sample_size\": sample_size,\n        }\n        if self.report_accuracy:\n            n_correct, total = self.compute_accuracy(model, net_output, sample)\n            logging_output[\"n_correct\"] = utils.item(n_correct.data)\n            logging_output[\"total\"] = utils.item(total.data)\n        return loss, sample_size, logging_output\n\n    def compute_convex_loss(self, model, net_output, sample, reduce=True):\n        lprobs = model.get_normalized_probs(net_output, log_probs=True)\n        batch_size, length, vocab_size = lprobs.size()\n        target = model.get_targets(sample, net_output).view(batch_size, length, 1)\n        non_pad_mask = target.ne(self.padding_idx).view(batch_size, length)\n        tgt_length = torch.sum(non_pad_mask, dim = -1)\n        tgt_lprob = lprobs.gather(dim=-1, index=target).view(batch_size, length)\n        tgt_lprob = tgt_lprob * non_pad_mask\n        tgt_lprob = torch.sum(tgt_lprob, dim = -1) / tgt_length\n        tgt_prob = torch.exp(tgt_lprob * self.k)\n        convex_loss = - torch.sum(tgt_prob * tgt_length)\n\n        smooth_loss = torch.sum(lprobs, dim = -1) * non_pad_mask\n        normalize_factor = self.k * tgt_prob.detach()\n        smooth_loss = torch.sum(smooth_loss, dim = -1) * normalize_factor\n        smooth_loss = - torch.sum(smooth_loss)\n        eps_i = self.eps / vocab_size\n        loss = (1. - self.eps) * convex_loss + eps_i * smooth_loss\n        return loss, convex_loss",
    "Experiment Result": "The method employs a two-step training strategy: initial pre-training with Maximum Likelihood Estimation (MLE) followed by fine-tuning with a convex-composition loss. The MLE pre-training can be performed using scripts like `LLM/train/run_mle.py`, which utilizes the standard HuggingFace `Trainer`. The fine-tuning with convex-composition loss is implemented by modifying the `compute_loss` method in a custom trainer (`CustomTrainer` in `LLM/train/run_convex.py`) or by defining a custom `Criterion` in Fairseq (`LabelSmoothedCrossEntropyCriterion` in `convex-learning/criterions/label_smoothed_cross_entropy_convex.py`).\n\nThe core of the convex-composition loss involves transforming the model's log-probabilities (`tgt_lprob`) using an exponential function weighted by a configurable `convex_order` parameter (denoted as `k`). Specifically, the loss calculation includes `tgt_prob = torch.exp(tgt_lprob * k)`. In the `CustomTrainer` example (`LLM/train/run_convex.py`), `k` is hardcoded to `1`. In the Fairseq `LabelSmoothedCrossEntropyCriterion`, `k` is exposed as a `convex_order` argument with a default value of `1.0`."
}{
    "Title": "Language Generation with Strictly Proper Scoring Rules",
    "Main Contributions": "The paper addresses the limitation of language generation models primarily relying on the logarithmic score (cross-entropy loss), which, despite being strictly proper and local, has criticisms like unboundedness and sensitivity. The main contributions include: 1. Proposing a novel strategy to adapt non-local strictly proper scoring rules (like Brier and Spherical scores) for language generation by distributing the scoring rule at the token level, overcoming the computational intractability of exponentially large sample spaces. 2. Introducing a general score smoothing technique, including a masked logarithmic score, to enable honest label smoothing for arbitrary scoring rules. 3. Demonstrating that fine-tuning pre-trained language models (Transformers, LLaMA-7B, LLaMA-13B) with Brier and Spherical scores, without hyperparameter adjustment, leads to substantial improvements in generation capabilities (BLEU and ROUGE scores), suggesting complementarity among different strictly proper scoring rules. 4. Providing empirical evidence that these improvements scale up to large language models, also enhancing multi-turn question-answering capabilities on specific tasks.",
    "Methodology": "The core methodology involves adapting non-local strictly proper scoring rules for language generation. Instead of computing scores for entire sequences, which is intractable, the approach distributes the scoring rule at the token level. This is achieved by decomposing the sequence probability autoregressively and applying the scoring rule to conditional token probabilities at each time step. The loss function is defined as the negative sum of token-level scores: L_S(θ) = -E_x~q[sum_{t=1}^T S(p_θ(·|x_{<t}), x_t)]. This ensures that the loss is minimized when each token-level prediction matches the true conditional probability. The study specifically leverages the Brier score and Spherical score as alternatives to the logarithmic score. Additionally, a 'score smoothing' technique is introduced, defined as S_ϵ(p, i) = (1−ϵ)⋅S(p, i) + (ϵ/m)⋅sum_{j=1}^m S(p, j), to support label smoothing for arbitrary scoring rules. To address instances where the smoothing term might be ignored due to flat loss landscapes (common in bounded scores), a 'masked logarithmic score' (S_ϵ^log) is proposed, which augments the smoothing term for under-smooth labels with a logarithmic penalty.",
    "Experimental Setup": "The method was evaluated on: 1. Machine Translation: WMT14 English-French (En-Fr, 35.8M pairs), WMT14 English-German (En-De, 4.5M pairs), and TED bilingual dataset (~200K pairs across 10 directions). BLEU score was used for evaluation. 2. Abstractive Summarization: CNN/DailyMail (311K pairs, non-anonymized variant). ROUGE-1, ROUGE-2, and ROUGE-L scores were used. Transformer-base and Transformer-big models were used for training from scratch and fine-tuning. For large language models (LLMs), LLaMA-7B and LLaMA-13B were instruction-tuned using the Alpaca dataset (52K instruction-following demonstrations). LLM evaluation included WMT22 translation tasks (4 directions), CNN/DailyMail summarization, and open-ended question answering capabilities measured by MT-bench. Beam search decoding was employed with varying beam sizes (e.g., 5 for MT, 4 for LLM MT, 2 for LLM summarization) and length penalties. Experiments also explored the α-power and pseudo-spherical scores with varying α values.",
    "Limitations": "The study identified several limitations: 1. When training language generation models from scratch, alternative strictly proper scoring rules (Brier and Spherical scores) showed slower convergence and inferior performance compared to the logarithmic score. This is hypothesized to be due to their distinct learning dynamics and the use of hyperparameters optimized for the logarithmic score. 2. Bounded scoring rules (like Brier and Spherical) might have a relatively flat loss landscape around the optimal point, potentially leading to the smoothing term being ignored in practical applications of score smoothing, thus necessitating enhancements like the masked logarithmic score. 3. The investigation into using different scoring rules as objectives for beam search was not exhaustive, leaving room for further exploration of other strictly proper scoring rules that could potentially surpass the logarithmic score in this context.",
    "Future Research Directions": "The paper suggests several avenues for future research: 1. Investigating other strictly proper scoring rules beyond Brier and Spherical scores to determine if any exhibit superior performance during pre-training or fine-tuning of language models. 2. Exploring the underlying factors that contribute to the observed performance differences among various strictly proper scoring rules and developing additional metrics or properties to assess a score's suitability for training language generation models. 3. Exploring the potential of these alternative scores to function as evaluation metrics for assessing the calibration of language generation models, similar to how perplexity is currently used, providing a more comprehensive evaluation than relying solely on perplexity. 4. Conducting a more exhaustive investigation into the application of different scoring rules as objectives for beam search, aiming to identify rules that could potentially outperform the logarithmic score in decoding strategies.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation",
    "Main Contributions": "The paper introduces ScaleGrad, a novel training method that directly modifies the gradient of the standard Maximum Likelihood Estimation (MLE) objective to address the prevalent text degeneration issues in neural text generation. ScaleGrad encourages models to learn and utilize novel tokens during training by manipulating gradient information. The method is shown to be effective in both open-ended (language modeling, auto-completion) and directed generation tasks (image paragraph captioning, abstractive text summarization). It consistently improves generation quality according to automatic metrics and human evaluation, outperforming standard MLE and Unlikelihood (UL) training. ScaleGrad is architecturally simple, easy to integrate into existing neural models (LSTM, Transformer), and demonstrates wide applicability across various text generation tasks, addressing the fundamental modeling problem that leads to repetitive outputs.",
    "Methodology": "ScaleGrad operates by directly maneuvering the gradients of the loss function during training. At each decoding step, it maintains a dynamic set of 'novel tokens' (Snovel), defined as tokens not observed in the ground-truth sequence up to the current step. The core of ScaleGrad involves re-normalizing the softmax output (probability distribution) over the vocabulary. For tokens belonging to Snovel, their generation probabilities are scaled down by a hyper-parameter γ ∈ (0,1). Conversely, for non-novel tokens, their probabilities are effectively scaled up. This re-scaling dynamically alters the gradient updates: it pushes the gradient norm further from zero for novel ground-truth tokens, encouraging the model to assign higher probabilities, and increases the gradient norm for non-novel, non-ground-truth tokens, pushing their probabilities lower. This mechanism forces the model to prioritize novel tokens while still learning to predict the correct ground truth. The paper also highlights that ScaleGrad avoids an 'undesired property' observed in Unlikelihood training where, under certain conditions, UL might inadvertently decrease the probability of ground-truth tokens.",
    "Experimental Setup": "The effectiveness of ScaleGrad was evaluated across various neural architectures and tasks. For open-ended generation (language modeling and text auto-completion), GPT-2 was fine-tuned on the Wikitext-103 dataset, with additional generalization tests on Penn TreeBank (PTB) and IMDB datasets. Evaluation metrics included perplexity (ppl), unique BPE-level predictions (uniq), Rep/l (repetition ratio of tokens from previous 'l' tokens), n-gram word repetition ratios (Rep-n), and unique words (uniq-w), complemented by human evaluation for auto-completion. For directed generation, two tasks were used: image paragraph captioning using a CNN-LSTM model on the Visual Genome dataset, evaluated by CIDEr and n-gram repetition ratios; and abstractive text summarization using the BertSum model (Transformer with pre-trained BERT encoder) on CNN/DM and NYT50 datasets, evaluated by F1-based ROUGE scores (R-1, R-2, R-L) and 1-gram MoverScore (WMD-1). Baselines included standard MLE training and Unlikelihood (UL) training, with specific hyper-parameter ranges (α for UL, γ for ScaleGrad) selected based on development set performance. Greedy decoding was primarily used for open-ended tasks, while directed tasks used greedy inference or beam search with length normalization and tri-gram blocking. Further analysis explored compatibility with stochastic decoding methods (top-k, top-p sampling) and hyper-parameter sensitivity.",
    "Limitations": "The paper implies a potential trade-off inherent in methods aiming to balance generation quality and diversity, as a stronger push for novel tokens (smaller γ) can lead to higher perplexity while increasing unique tokens. Another implicit limitation is that the definition of 'novel tokens' in ScaleGrad is strictly based on previous tokens in the ground-truth sequence, not accounting for the model's own generated output or broader contextual relevance. Practically, the hyper-parameter γ requires task-dependent tuning; a smaller γ is recommended for baselines with high repetition, and a larger γ for tasks with less degeneration, suggesting that a universal optimal value is not straightforward.",
    "Future Research Directions": "The authors propose two main future research directions. First, they plan to repurpose the definition of the 'novel token set' beyond mere novelty to enable the model to achieve other objectives, citing an existing adaptation of ScaleGrad to prevent early endpointing in online automatic speech recognition as an example. Second, they aim to investigate mechanisms for dynamically adjusting the hyper-parameter γ during decoding steps, allowing the model to learn with varying degrees of strength depending on the immediate context of the generation.",
    "Experiment Code": "def getNovelMask(target, vocab_size):\n    b,l = target.size()\n    zeros = torch.zeros(b,l,vocab_size).to(target.device)\n    ones = torch.ones(b,l,vocab_size).to(target.device)\n\n    target_index = target.unsqueeze(1).expand(b,l,l).transpose(-2,-1).triu().transpose(-2,-1)\n    matrix = zeros.scatter_add_(2, target_index, ones)\n    matrix[:,:,0] = 0\n    summ_true = torch.tensor(range(1,l+1)).unsqueeze(0).float().to(target.device)\n    summ_now = torch.sum(matrix,dim=-1)\n    diff = summ_true - summ_now\n    matrix[:,:,0] = diff\n    matrix = torch.cat((torch.zeros(b,1,vocab_size).to(target.device),matrix[:,:-1,:]),1)\n    novel_mask = matrix < 1.\n\n    return novel_mask\n\n\ndef sg_loss(model, batch, args):\n    longer_sample = batch[0].cuda()\n    inp = longer_sample[:, :args.train_batch_size]\n    model_output = model(inp)\n    target = longer_sample[:, 1:]\n    logits = model_output[0]\n\n    # ScaleGrad\n    ##########################################################\n    probs = F.softmax(logits,dim=-1) \n    # Obtaining the masks for novel tokens\n    novel_mask = getNovelMask(target[0].unsqueeze(0),logits.size(-1))\n    rep_mask = ~novel_mask\n\n    new_probs = probs * novel_mask * args.gamma + probs * rep_mask + 1e-8\n    new_probs = F.normalize(new_probs,p=1,dim=-1)\n    lprobs = torch.log(new_probs)\n    ##########################################################\n\n\n    assert lprobs.size(0) == 1, 'We work on flat sequences'\n    loss = F.nll_loss(lprobs[0], target[0], reduction='sum')\n    true_token_logits = -F.nll_loss(logits[0], target[0], reduction='none')\n    ntokens = inp.numel()\n\n    logging_output = TrainingMetrics.ranking_metrics(logits[0], true_token_logits, None, ntokens, target[0])\n    logging_output['loss'] = loss.item()\n    logging_output['normalizer'] = ntokens\n    logging_output['sample_size'] = ntokens\n    logging_output['ntokens'] = ntokens\n\n    loss = loss / ntokens\n    return loss, logging_output",
    "Experiment Result": "Method specific parameter: ScaleGrad's scaling hyper-parameter gamma (default: 1.0). Model architecture: GPT2 variants (gpt2, gpt2-medium, gpt2-large). Training dataset: The dataset is loaded from paths like `data_base/train_tokens_bpe_gpt2.pt`, `data_base/valid_tokens_bpe_gpt2.pt`, `data_base/test_tokens_bpe_gpt2.pt`. Training parameters: Number of training epochs (default: 1), training batch size (default: 300), total number of training steps (default: 10000), validation frequency (default: 10000 steps). Optimizer settings: AdamW optimizer with learning rate (default: 6.25e-5), epsilon (default: 1e-8), warmup steps (default: 0), learning rate schedule (default: warmup_linear), weight decay (default: 0.01), maximum gradient norm (default: 1), gradient accumulation steps (default: 1). Evaluation settings for models trained with ScaleGrad include prefix length (default: 50), continuation length (default: 100), top-k sampling (default: 1), and top-p sampling (default: 0.0)."
}{
    "Title": "Language Generation with Strictly Proper Scoring Rules",
    "Main Contributions": "This paper addresses the problem of using non-local strictly proper scoring rules, beyond the widely used logarithmic score, for language generation. It proposes a novel strategy to adapt these scoring rules, such as the Brier score and Spherical score, by distributing them at the token level and introducing a general score smoothing technique. Key findings include that substituting the loss function with these alternative scores (without hyperparameter adjustment) significantly improves model generation capabilities, scaling up to large language models (LLMs) like LLaMA-7B and LLaMA-13B during instruction tuning. Fine-tuning models pre-trained with the logarithmic score using alternative scores also yields improvements, suggesting a complementarity between different scoring rules. The proposed score smoothing, especially when enhanced with a masked logarithmic score, is shown to be an effective regularization technique. Additionally, the study finds that model performance is stable and superior for alpha=2 in alpha-power and pseudo-spherical scores, and observes conflicting optimization trajectories among different scores, implying the need for comprehensive evaluation.",
    "Methodology": "The methodology involves adapting non-local strictly proper scoring rules for language generation. First, the paper formalizes scoring rules and their properties (propriety, strict propriety, locality). To overcome the 'locality constraint' inherent in many strictly proper scores that prevents their application to the exponentially large sample space of natural text, a 'token-level scoring rule' strategy is introduced. This approach decomposes sequence probability into autoregressive token probabilities and applies the scoring rule at each token prediction step, effectively reducing the sample space. Second, a general 'score smoothing' technique is proposed, which extends label smoothing to arbitrary scoring rules, motivating models to generate a smoothed distribution. To address the issue of bounded scores potentially ignoring the smoothing term (due to flat loss surfaces), a 'masked logarithmic score' is introduced to enhance the smoothing effect for under-smooth labels, ensuring a stronger incentive for the model to produce the desired smooth distribution. The Brier score, Spherical score, alpha-power score, and pseudo-spherical score are specifically explored as alternatives to the logarithmic score.",
    "Experimental Setup": "Experiments were conducted on machine translation and abstractive summarization tasks. For machine translation, datasets included WMT14 English-French (En-Fr), WMT14 English-German (En-De), and TED bilingual dataset. For LLMs, WMT22 translation tasks (Chinese-to-English, English-to-Chinese, German-to-English, English-to-German) were used. Abstractive summarization experiments used the CNN/DailyMail dataset. Models primarily included Transformer-base (and Transformer-big for WMT14 En-Fr) and two large language models, LLaMA-7B and LLaMA-13B, used for instruction tuning. The Alpaca dataset (52K instruction-following demonstrations) was used for instruction tuning. Evaluation metrics were BLEU for machine translation, and ROUGE-1, ROUGE-2, and ROUGE-L for summarization. LLM open-ended question answering capabilities were evaluated on MT-bench. Training involved both training from scratch and fine-tuning models pre-trained with the logarithmic score, often keeping hyperparameters fixed. Beam search was employed for generation with varying beam sizes and length penalties. Statistical significance (p < 0.01) was used for comparisons.",
    "Limitations": "The study identifies several limitations. When training language generation models from scratch, alternative strictly proper scoring rules (Brier, Spherical) converge slower and perform worse than the logarithmic score under standard hyperparameters, suggesting that these settings might not be optimal for other scores due to distinct learning dynamics and gradient characteristics. The initial score smoothing technique, while general, can be less effective for bounded scores like the Brier and Spherical scores, as they impose only a mild penalty when smoothing is ignored, potentially causing the model to disregard the smoothing term. The equivalence between α-entmax loss and α-power score based loss does not hold when the probability of the ground truth word is zero, which can hinder gradient transmission in sparse transformations. Lastly, the investigation into using different scoring rules as objectives for beam search was not exhaustive, and the logarithmic score generally performed best in the conducted experiments for this purpose.",
    "Future Research Directions": "Future research directions include exploring other strictly proper scoring rules that might exhibit superior performance during pre-training or fine-tuning, and investigating the underlying factors that contribute to the performance differences among these scores. The paper also suggests developing additional metrics or properties to better determine a score's suitability for training language generation models. Beyond training, a promising avenue is to explore whether these alternative scores can function as effective evaluation metrics for assessing the calibration of language generation models, similar to Perplexity. Furthermore, the application of various strictly proper scoring rules as objectives for beam search warrants more exhaustive investigation, as the current study only scratched the surface.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis",
    "Main Contributions": "The paper addresses the challenge of aligning Large Language Models (LLMs) to avoid generating harmful content, where existing methods typically rely on flawless instruction-response pairs. It proposes a novel alignment technique based on mistake analysis, which deliberately exposes LLMs to erroneous content to learn the reasons for mistakes and how to avoid them. The key contributions include: (1) introducing a framework that transforms erroneous instruction-response pairs into valuable alignment data through mistake analysis; (2) demonstrating that LLMs can achieve self-alignment without external models or human annotations by leveraging their inherent discrimination ability; and (3) showing through extensive experiments that this method outperforms existing SFT and RL-based approaches in enhancing model safety while maintaining overall utility on various benchmarks, including defense against advanced instruction attacks like \"Goal Hijacking.\"",
    "Methodology": "The proposed methodology, 'Alignment through Mistake Analysis,' consists of four main steps: 1. Guided Mistake Induction: An unaligned LLM is induced to generate toxic responses by inserting hint keywords (e.g., 'harmful, unethical, and offensive') into instruction prompts. 2. Guided Analysis Generation: The LLM is then prompted to analyze these harmful responses, specifically explaining why they are potentially harmful, unethical, or offensive. This step leverages the LLM's superior discrimination ability. 3. Unguided Analysis Fine-tuning: The generated mistake analysis triplets (instruction, harmful response, critique) are integrated into the Supervised Fine-Tuning (SFT) process alongside regular helpful and harmless instruction-response pairs. An unguided template is used here to encourage the LLM to analyze content based on its inherent knowledge without explicit reminders of harm. 4. Guided Response Generation (Inference): During the inference phase, the model is explicitly reminded to formulate 'harmless, ethical, and inoffensive' responses to ensure adherence to ethical standards. The theoretical basis frames mistake analysis as detailed chain-of-thought reasoning for a harmful tag, optimizing conditional probabilities to enhance alignment.",
    "Experimental Setup": "The method was evaluated using Alpaca-7B as an unaligned base model for general alignment and ChatGLM-6B as a post-aligned base model for defense against instruction attacks. Baselines included vanilla SFT, RLHF (PPO-Lag with official reward and cost models), Chain of Hindsight (CoH), and Critique-Revise, all fine-tuned using LoRA. For general alignment, the PKU-SafeRLHF dataset (10,260 unique instruction-response pairs with safety preferences) was augmented with 52k helpful instructions from Taori et al. (2023). For attack defense, the SAFETY PROMPTS dataset (100,000 pairs across safety scenarios and advanced attacks) was combined with 50k instructions from the MOSS dataset. Evaluation benchmarks included AlpacaFarm (805 instructions for helpfulness) and PKU-SafeRLHF test set (1,523 red-teaming instructions for harmfulness) for general alignment. For attack defense, the SAFETY PROMPTS test set (1,915 instructions, including 136 for 'Goal Hijacking') and 1000 MOSS instructions for helpfulness were used, along with 500 additional 'Goal Hijacking' instructions. Evaluation metrics comprised Helpful Score, Harmless Score, Harmless Rate (for binary safety assessment), and a Helpful Score for harmful instructions. GPT-4 was initially used for evaluation, with human annotator verification.",
    "Limitations": "The study found that increasing the quantity of mistake analysis data by incorporating both model-induced and original training dataset's bad responses for the same instructions resulted in a decrease in effectiveness. This suggests a potential limitation related to conflicting analyses of identical instructions, which can lead to sub-optimal alignment performance.",
    "Future Research Directions": "Not mentioned",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment",
    "Main Contributions": "The paper proposes FIGA, an improved alignment approach for Large Language Models (LLMs) that addresses the limitations of Supervised Fine-Tuning (SFT) (imitation learning) and the complexity of Reinforcement Learning from Human Feedback (RLHF). FIGA leverages fine-grained (token or phrase level) quality signals derived by contrasting good and bad responses. Its main contributions are: 1) curating a refined alignment dataset (SPA) that pairs initial low-quality responses with corresponding high-quality revised ones, and 2) devising a new loss function that utilizes these fine-grained quality signals to guide LLM alignment learning. FIGA is presented as the first attempt to use fine-grained quality signals for LLM alignment without reinforcement learning, achieving superior performance over strong baselines.",
    "Methodology": "FIGA's methodology involves two main components: data curation and fine-tuning. First, a SubPar Alignment (SPA) dataset is constructed by taking queries where an initial LLM (Alpaca-7b) performs poorly. Initial responses (ˆY) are generated, and a reward model (OpenAssistant/reward-model-deberta-v3-large-v2) filters instances based on reward scores (RˆY < η1, RY > η2, RY − RˆY > η3). Then, a powerful LLM (gpt-3.5-turbo) revises these initial responses into high-quality ones (˜Y) by making minimal adjustments based on ground-truth demonstrations (Y), categorized by reasons like lack of detail or inaccuracy, to reduce distribution shifts. Second, for fine-tuning, Levenshtein distance is used to compare ˆY and ˜Y, identifying tokens that are newly added, deleted, or substituted. Token-level weighting functions, ˜r(·) and ˆr(·), are introduced: α (e.g., 1) for added/substituted tokens in ˜Y, β (e.g., 0.5) for deleted/substituted tokens in ˆY, and γ (e.g., 0) for unchanged tokens. These weights are integrated into a new loss function that encourages desired actions (increasing probability of good tokens) and discourages undesired ones (decreasing probability of bad tokens), acting as a simplified, efficient version of RL without a separate critic model.",
    "Experimental Setup": "The experimental setup includes comparing FIGA against several baselines: SFT, PPO, CoH, RRHF, and DPO. The SPA dataset is curated from various existing datasets including HH-RLHF, ShareGPT, Instruct GPT-J Pairwise, SHP, and OpenOrca, resulting in 17,333 instances. Alpaca-7b is used as the initial rollout model, gpt-3.5-turbo for response revision, and OpenAssistant/reward-model-deberta-v3-large-v2 as the reward model for data filtering (with thresholds η1=1, η2=3, η3=3.5). FIGA's hyperparameters are set to α=1, β=0.5, γ=0, with additional filtering for bad tokens with negative log-likelihood less than 0.6. Evaluation is conducted on a test set (3,608 entries) using the reward score, and on various out-of-distribution benchmarks: MMLU (knowledge utilization), TruthfulQA, CrowS-Pairs, WinoGender (human alignment), and Vicuna, WizardLM (open-ended generation, rated by ChatGPT). Ablation studies are performed to analyze the impact of dataset construction steps and weighting function variants, and human evaluations are conducted for error types and overall quality.",
    "Limitations": "The paper primarily highlights the limitations of prior methods (SFT, RLHF) that FIGA aims to overcome. It does not explicitly state limitations or weaknesses of the FIGA approach itself. However, it implicitly indicates that using raw reward scores as weights for token-level signals can lead to inferior performance due to intrinsic inaccuracies of reward scores, especially in out-of-distribution scenarios.",
    "Future Research Directions": "Not mentioned",
    "Experiment Code": "def transform_tensor(source, target):\n    operations = Levenshtein.editops(source.tolist(), target.tolist())\n    return [i[2] for i in operations if i[0]!='delete'], [i[1] for i in operations if i[0]!='insert']\n\n@dataclass\nclass DataCollatorForSupervisedDataset(object):\n    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n\n    tokenizer: transformers.PreTrainedTokenizer\n\n    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n        input_ids = [instance[\"input_ids\"] for instance in instances]\n        raw_input_ids = [instance[\"raw_input_ids\"] for instance in instances]\n        labels = [instance[\"labels\"][0] for instance in instances]\n        raw_labels = [instance[\"labels\"][1] for instance in instances]\n\n        max_len = max(max(len(seq) for seq in input_ids), max(len(seq) for seq in raw_input_ids))\n        input_ids = [F.pad(seq, (0, max_len - len(seq)), value=self.tokenizer.pad_token_id) for seq in input_ids]\n        raw_input_ids = [F.pad(seq, (0, max_len - len(seq)), value=self.tokenizer.pad_token_id) for seq in raw_input_ids]\n        input_ids = torch.stack(input_ids)\n        raw_input_ids = torch.stack(raw_input_ids)\n\n        max_len_labels = max(max(len(seq) for seq in labels), max(len(seq) for seq in raw_labels))\n        my_mask = torch.zeros(input_ids.shape[0], max_len_labels)\n        my_mask_neg = torch.zeros(input_ids.shape[0], max_len_labels)\n\n        # Positive masks\n        for idx, exmp in enumerate(labels):\n            operations, op=transform_tensor(raw_labels[idx], labels[idx])\n            for j in operations:\n                my_mask[idx][j]=1\n            for j, _ in enumerate(labels[idx]):\n                if labels[idx][j]==self.tokenizer.eos_token_id:\n                    my_mask[idx][j]=1\n                    break\n\n        # Negative masks\n        for idx, exmp in enumerate(raw_labels):\n            op, operations = transform_tensor(raw_labels[idx], labels[idx])\n            for j in operations:\n                my_mask_neg[idx][j] = -1\n            for j, _ in enumerate(raw_labels[idx]):\n                if raw_labels[idx][j]==self.tokenizer.eos_token_id:\n                    my_mask_neg[idx][j]=1\n                    break\n        labels = [F.pad(seq, (0, max_len_labels - len(seq)), value=IGNORE_INDEX) for seq in labels]\n        raw_labels = [F.pad(seq, (0, max_len_labels - len(seq)), value=IGNORE_INDEX) for seq in raw_labels]\n        labels = torch.stack(labels)\n        raw_labels = torch.stack(raw_labels)\n  \n        all_input_ids = torch.stack([torch.cat([input_ids[i], raw_input_ids[i]]) for i in range(len(input_ids))]).view(-1, input_ids.size(1))\n        all_labels = torch.stack([torch.cat([labels[i], raw_labels[i]]) for i in range(len(labels))]).view(-1, labels.size(1))\n        all_attention_mask = torch.stack([torch.cat([input_ids.ne(self.tokenizer.pad_token_id)[i], raw_input_ids.ne(self.tokenizer.pad_token_id)[i]]) for i in range(len(input_ids))]).view(-1, input_ids.size(1))\n        all_my_mask = torch.stack([torch.cat([my_mask[i], my_mask_neg[i]]) for i in range(len(my_mask))]).view(-1, my_mask.size(1))\n\n        return dict(\n            input_ids=all_input_ids,\n            labels=all_labels,\n            attention_mask=all_attention_mask,\n            my_mask=all_my_mask,\n        )\n\nclass LlamaForCausalLM(LlamaPreTrainedModel):\n    # ... (init and other methods omitted for brevity)\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        my_mask: Optional[torch.Tensor] = None,\n        cook_ratio: Optional[float] = 1,\n        cook_ratio2: Optional[float] = -0.5,\n        delta: Optional[torch.Tensor] = None,\n        weights: Optional[torch.Tensor] = None,\n        contra: Optional[bool]=True\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n        # ... (model forward pass and standard loss omitted)\n\n        loss = None\n        # FIGA and corresponding variants changes below:\n        if labels is not None and my_mask is not None and delta is not None:  # delta weighted method\n            shift_logits = logits[..., :-1, :].contiguous() #(B, S, V)\n            shift_labels = labels[..., 1:].contiguous() #(B, S)\n            loss_fct = CrossEntropyLoss(reduction='none')\n            \n            shift_logits = shift_logits.transpose(1, 2) #(B, V, S)\n            shift_labels = shift_labels.to(shift_logits.device)\n            loss = loss_fct(shift_logits, shift_labels) #(B, S)\n            \n            my_mask = my_mask[..., 1:].to(shift_logits.device)\n            B, S = my_mask.shape\n            \n            delta = (delta.repeat(1, S) * my_mask).to(shift_logits.device) #(B, S)\n            \n            loss = loss * delta\n            loss = torch.sum(loss) / (torch.sum(my_mask == 1).item())\n        \n        elif labels is not None and my_mask is not None and contra!=None:  # FIGA main method\n            shift_logits = logits[..., :-1, :].contiguous() #(2B, S, V)\n            shift_labels = labels[..., 1:].contiguous() #(2B, S)\n            shift_logits = shift_logits.transpose(1, 2) #(2B, V, S)\n            shift_labels = shift_labels.to(shift_logits.device)\n            my_mask = my_mask[..., 1:].to(shift_logits.device)  \n            loss_fct = CrossEntropyLoss(reduction='none')\n            ppl = loss_fct(shift_logits, shift_labels) # (2B, S)\n            \n            threshold_lower = 0.6\n            logits_mask = ppl > threshold_lower\n            for idx, row in enumerate(my_mask):\n                for j, value in enumerate(row):\n                    if value == -1 and logits_mask[idx][j]==1:\n                        my_mask[idx][j] = 0\n            \n            loss = loss_fct(shift_logits, shift_labels) #(2B, S)\n            cook_ratio = torch.tensor(cook_ratio).to(shift_logits.device)\n            cook_ratio2 = torch.tensor(cook_ratio2).to(shift_logits.device)\n            tem = torch.zeros_like(my_mask, dtype=torch.float32).to(shift_logits.device)\n\n            for idx, row in enumerate(my_mask):\n                if idx%2==1:\n                    tem[idx] = torch.where(row == 1, 0, torch.where(row == -1, cook_ratio2, 0))\n                elif idx%2==0:\n                    tem[idx] = torch.where(row == 1, cook_ratio, 0)\n            loss = loss * tem\n            combined_mask = (tem!=0) & (shift_labels!=-100)\n            loss = torch.sum(loss) / (torch.sum(combined_mask).item())\n\n        elif labels is not None and my_mask is not None:  # constant weighted method\n            shift_logits = logits[..., :-1, :].contiguous() #(B, S, V)\n            shift_labels = labels[..., 1:].contiguous() #(B, S)\n            loss_fct = CrossEntropyLoss(reduction='none')\n            \n            shift_logits = shift_logits.transpose(1, 2) #(B, V, S)\n            shift_labels = shift_labels.to(shift_logits.device)\n            loss = loss_fct(shift_logits, shift_labels) #(B, S)\n            \n            my_mask = my_mask.to(shift_logits.device)\n            cook_ratio = torch.tensor(cook_ratio).to(shift_logits.device)\n            cook_ratio2 = torch.tensor(cook_ratio2).to(shift_logits.device)\n            tem = torch.where(my_mask == 1, cook_ratio, cook_ratio2)[..., 1:].to(shift_logits.device) #(B, S)\n\n            loss = loss * tem\n            loss = torch.sum(loss) / (torch.sum(shift_labels != -100).item())\n        \n        elif labels is not None and weights is not None:  # chatgpt weighted method\n            shift_logits = logits[..., :-1, :].contiguous() #(B, S, V)\n            shift_labels = labels[..., 1:].contiguous() #(B, S)\n            loss_fct = CrossEntropyLoss(reduction='none')\n            \n            shift_logits = shift_logits.transpose(1, 2) #(B, V, S)\n            shift_labels = shift_labels.to(shift_logits.device)\n            loss = loss_fct(shift_logits, shift_labels) #(B, S)\n            weights=weights[..., 1:]\n\n            loss = loss * weights\n            loss = torch.sum(loss) / (torch.sum(shift_labels != -100).item())\n\n        elif labels is not None:\n            # Shift so that tokens < n predict n\n            shift_logits = logits[..., :-1, :].contiguous() #(B, S, V)\n            shift_labels = labels[..., 1:].contiguous() #(B, S)\n            # Flatten the tokens\n            loss_fct = CrossEntropyLoss()\n            \n            shift_logits1 = shift_logits.view(-1, self.config.vocab_size) #(B*S, V)\n            shift_labels1 = shift_labels.view(-1) #(B*S)\n            # Enable model parallelism\n            shift_labels1 = shift_labels1.to(shift_logits.device)\n            loss = loss_fct(shift_logits1, shift_labels1) #(B*S)\n        \n        if not return_dict:\n            output = (logits,) + outputs[1:]\n            return (loss,) + output if loss is not None else output\n\n        return CausalLMOutputWithPast(\n            loss=loss,\n            logits=logits,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )",
    "Experiment Result": "The `LlamaForCausalLM.forward` method uses:\n- `cook_ratio: Optional[float] = 1` for weighting positive token changes (corresponding to 'alpha').\n- `cook_ratio2: Optional[float] = -0.5` for weighting negative token changes (corresponding to 'beta').\n- `contra: Optional[bool]=True` to enable the main FIGA loss calculation logic.\n- A perplexity (`ppl`) `threshold_lower = 0.6` is applied within the FIGA main method to filter out certain negative mask applications.\n\nData preparation (in `train.py`) uses:\n- `IGNORE_INDEX = -100` for masking labels.\n- `PROMPT_DICT` defines prompt formats including `prompt_input`, `prompt_no_input`, `no_instruction_input`, and `no_instruction_no_input` for structuring inputs to the model.\n- Dataset examples are expected to contain `revised_output` (for `targets`) and `original_output` (for `raw`), with `tokenizer.eos_token` appended to both.\n- The maximum sequence length for tokenization is configurable via `model_max_length`, with a default of 512.\n- The tokenizer's padding is set to `tokenizer.pad_token = tokenizer.eos_token` and `tokenizer.pad_token_id = tokenizer.eos_token_id`."
}{
    "Title": "The Truth is in There: Improving Reasoning in Language Models with Layer-Selective Rank Reduction",
    "Main Contributions": "Introduces LAyer-SElective Rank reduction (LASER), a novel post-training intervention that significantly improves LLM performance by selectively removing higher-order components of weight matrices. Demonstrates that this selective rank reduction, particularly in MLP layers of later Transformer blocks, boosts accuracy on various NLP reasoning tasks and improves robustness to paraphrases. Finds that performance gains are most prominent for information less frequently present in training data, suggesting LASER acts as a denoising mechanism. Analyzes that higher-order components often encode conflicting or generic high-frequency words, which clash with correct responses from lower-order components, and their removal resolves this internal conflict. Shows generality of this finding across different language models, datasets, and in a reinforcement learning domain.",
    "Methodology": "LASER (LAyer-SElective Rank reduction) is a post-training intervention that replaces specific weight matrices in Transformer models with their low-rank approximations. It uses Singular Value Decomposition (SVD) to identify and remove \"higher-order components\" (singular vectors with smaller singular values), retaining \"lower-order components.\" The intervention is defined by a parameter type (e.g., Uin, Uout for MLP layers or attention matrices Wq, Wk, Wv, Wo), a layer number, and a rank reduction percentage (ρ). The rank is reduced to ⌊ρ · d⌋, where d is the original matrix dimension, preserving ρ fraction of the original rank. Interventions can be composed and require no additional training or parameters.",
    "Experimental Setup": "Models evaluated include GPT-J (6B parameters, 27 layers), Roberta, Llama2 (7B parameters, 32 layers), and a 6-layer Decision Transformer for reinforcement learning. Datasets for NLP tasks include CounterFact, HotPotQA, FEVER, Bias in Bios (Gender, Profession), TruthfulQA, BigBench-Epistemic Reasoning, and BigBench-WikidataQA. The PILE dataset was used to analyze fact frequency in training data and model perplexity. For the non-text domain, the Sokoban game was used with the Decision Transformer. Evaluation metrics included top-1/top-k accuracy, generation accuracy, classification accuracy, log-loss, perplexity, robustness to paraphrases, and task completion rate for RL. A 20% validation set was used to select optimal LASER hyperparameters, and results were reported on the remaining 80% test set.",
    "Limitations": "The exact mechanisms behind the success of LASER are not fully understood. Specifically, it is unclear why higher-order components accumulate noisy answers during training, how model architecture and other structural choices affect this phenomenon, and why benefits are particularly observed in later MLP layers. While factuality improves, the model's perplexity on its original training data (PILE) slightly increases, though this might be fixable by temperature calibration.",
    "Future Research Directions": "Future work includes investigating why higher-order components accumulate noisy answers during training, studying the effect of model architecture and other structural choices on the LASER phenomenon, and exploring why LASER is specifically effective for later layers in the Multi-Layer Perceptron (MLP) blocks. Further exploration of combining LASER interventions, a thorough analysis of how absolute weight pruning (which shows similar benefits) relates to LASER, and understanding the implications of the slight worsening in model perplexity are also suggested.",
    "Experiment Code": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# Helper functions for abs weight pruning\ndef sorted_mat(matrix):\n    temp = list(abs(matrix).flatten())\n    temp.sort()\n    return temp\n\n\ndef prune(matrix, mat_sort, to_prune):\n    if to_prune != 0:\n        alpha = mat_sort[int(to_prune * 0.1 * len(mat_sort))]\n        matrix[abs(matrix) <= alpha] = 0\n    return matrix\n\n\ndef rank(matrix):\n    np_matrix = np.array(matrix)\n    return np.linalg.matrix_rank(np_matrix)/min(list(np_matrix.shape))\n\n\n# What percentage can be pruned by weight\ndef sparsity(matrix, alpha):\n    abs_matrix = abs(matrix)\n    filtered_matrix = abs_matrix[abs_matrix < alpha]\n    return len(filtered_matrix)/matrix.size\n\n\ndef viz_rank_change(rank_list,name):\n    fig = plt.figure()\n    plt.plot(rank_list)\n    plt.savefig(name)\n\n\n# Helper functions for rank reduction\ndef do_low_rank(weight, k, debug=False, niter=2):\n    assert weight.ndim == 2\n\n    max_rank = min(weight.shape[0], weight.shape[1])\n    desired_rank = int(max_rank * k)\n\n    if debug:\n        print(f\"Shape is {weight.shape} and shape is {weight.dtype} => desired rank {desired_rank}\")\n\n    results = torch.svd_lowrank(weight,\n                                q=desired_rank,\n                                niter=niter)\n    weight_approx = results[0] @ torch.diag(results[1]) @ results[2].T\n\n    if debug:\n        print(f\"New matrix has shape {weight_approx.shape}\")\n\n    assert weight_approx.shape[0] == weight.shape[0] and weight_approx.shape[1] == weight.shape[1]\n    weight_approx = torch.nn.Parameter(weight_approx)\n\n    return weight_approx\n\nimport torch\n\nfrom copy import deepcopy\n# from laser.abstract_laser import AbstractLaser # Assuming AbstractLaser class is defined elsewhere for context\n# from laser.matrix_utils import do_low_rank, sorted_mat, prune # These are defined above or imported in original code\n\nclass AbstractLaser:\n\n    # Helper functions for matrix update\n    @staticmethod\n    def get_parameter(model, name):\n        for n, p in model.named_parameters():\n            if n == name:\n                return p\n        raise LookupError(name)\n\n    @staticmethod\n    def update_model(model, name, params):\n        with torch.no_grad():\n            AbstractLaser.get_parameter(model, name)[...] = params\n\n    @staticmethod\n    def get_edited_model(model, lname, lnum, rate, intervention=\"rank-reduction\", logger=None, in_place=True):\n        raise NotImplementedError()\n\n\nclass GPTJLaser(AbstractLaser):\n\n    def __init__(self):\n        super(AbstractLaser, self).__init__()\n\n    @staticmethod\n    def convert_name(name):\n        \"\"\"\n        Convert a given generic transformer layer name to a model specific layer name(s)\n        :param name: generic name\n        :return: model specific layer name(s)\n        \"\"\"\n\n        ''' \n            For a given layer, we can modify the following type individually or all at onces\n    \n            transformer.h.26.ln_1.weight\n            transformer.h.26.ln_1.bias\n            transformer.h.26.attn.k_proj.weight     -> k_proj\n            transformer.h.26.attn.v_proj.weight     -> v_proj\n            transformer.h.26.attn.q_proj.weight     -> q_proj\n            transformer.h.26.attn.out_proj.weight   -> out_proj\n            transformer.h.26.mlp.fc_in.weight       -> fc_in\n            transformer.h.26.mlp.fc_out.weight      -> fc_out\n        '''\n\n        if name == \"k_proj\":\n            converted_names = \"attn.k_proj.weight\"\n        elif name == \"q_proj\":\n            converted_names = \"attn.q_proj.weight\"\n        elif name == \"v_proj\":\n            converted_names = \"attn.v_proj.weight\"\n        elif name == \"out_proj\":\n            converted_names = \"attn.out_proj.weight\"\n        elif name == \"fc_in\":\n            converted_names = \"mlp.fc_in.weight\"\n        elif name == \"fc_out\":\n            converted_names = \"mlp.fc_out.weight\"\n        elif name == \"None\":\n            converted_names = \"None\"\n        elif name == \"mlp\":\n            converted_names = [\"mlp.fc_in.weight\", \"mlp.fc_out.weight\"]\n        elif name == \"attn\":\n            converted_names = [\"attn.k_proj.weight\", \"attn.q_proj.weight\", \"attn.v_proj.weight\", \"attn.out_proj.weight\"]\n        elif name == \"all\":\n            converted_names = [\"attn.k_proj.weight\", \"attn.q_proj.weight\", \"attn.v_proj.weight\",\n                               \"attn.out_proj.weight\", \"mlp.fc_in.weight\", \"mlp.fc_out.weight\"]\n        else:\n            raise AssertionError(f\"Unhandled name {name}\")\n\n        return converted_names\n\n    @staticmethod\n    def _modify_layer(name, lnum_to_modify, lname_to_modify, converted_names):\n\n        # Check for layer number match\n        # If must be either -1 meaning modify all layers, or must match the given layer number\n        if lnum_to_modify != -1 and not name.startswith(f\"transformer.h.{lnum_to_modify}.\"):\n            return False\n\n        # Check if layer type needs to be modified.\n        #      'all', 'mlp', 'attn', 'k_proj', 'q_proj', 'v_proj', 'out_proj', 'fc_in', 'fc_out'\n        # If all, then modify all\n        # If mlp, then only MLP\n        # If attn, then only attn\n        # Otherwise, update a given layer type\n\n        if type(converted_names) == list:\n            modify_flag = any([name.endswith(f\"{converted_name}\") for converted_name in converted_names])\n        elif type(converted_names) == str:\n            modify_flag = name.endswith(f\"{converted_names}\")\n        else:\n            raise AssertionError(f\"Type should be list or str. Found {type(converted_names)}.\")\n\n        return modify_flag\n\n    @staticmethod\n    def get_edited_model(model, lname, lnum, rate, intervention=\"rank-reduction\", logger=None, in_place=True):\n\n        if in_place:\n            model_edit = model\n        else:\n            model_edit = deepcopy(model)\n\n        if lname == \"dont\":\n            print(f\"Not intervening at all\")\n            return model_edit\n\n        converted_names = GPTJLaser.convert_name(lname)\n        num_update = 0\n\n        for name, param in model.named_parameters():\n\n            modify_flag = GPTJLaser._modify_layer(name=name,\n                                                  lnum_to_modify=lnum,\n                                                  lname_to_modify=lname,\n                                                  converted_names=converted_names)\n\n            if modify_flag:\n                if logger is not None:\n                    logger.log(f\"Updating Layer: {name}\")\n                print(f\"Updating Layer: {name}\")\n            else:\n                continue\n\n            if intervention == 'dropout':\n                # For the sparsity analysis\n                mat_analysis = param.detach().numpy().copy()\n                mat_sort = sorted_mat(mat_analysis)\n\n                mat_analysis = prune(mat_analysis, mat_sort, rate)  # pruned_mat\n                mat_analysis = torch.from_numpy(mat_analysis)\n\n            elif intervention == 'rank-reduction':\n                # Do rank reduction\n                mat_analysis_tensor = deepcopy(param)\n                mat_analysis = do_low_rank(mat_analysis_tensor.type(torch.float32), (10 - rate) * 0.1)\n\n            elif intervention == 'zero':\n                mat_analysis_tensor = deepcopy(param)\n                mat_analysis = 0.0 * mat_analysis_tensor.type(torch.float32)\n            else:\n                raise AssertionError(f\"Unhandled intervention type {intervention}\")\n\n            GPTJLaser.update_model(model_edit, name, mat_analysis)\n            num_update += 1\n\n        assert num_update > 0, f\"Must update some parameters GPTJ: {lnum}, {lname}\"\n\n        if logger is not None:\n            logger.log(f\"Total number of parameters updated is {num_update}\")\n\n        if lnum != -1 and lname not in [\"all\", \"mlp\", \"attn\"]:\n            assert num_update == 1, f\"Was supposed to make 1 update to the model but instead made {num_update} updates.\"\n\n        return model_edit",
    "Experiment Result": "Model: GPT-J 6B (\"EleutherAI/gpt-j-6B\", revision=\"float16\", torch_dtype=torch.float16).\nDataset: BigBench Hard (BBH) tasks, default split is \"causal_judgement\". The dataset is split into 80% for training/validation (implicit) and 20% for validation/testing, where `GPTJExperiment.validate` uses a 0.2 split for validation and test. The `intervene` function uses the whole `dataset` provided to compute predictions, and then `validate` splits these predictions.\nIntervention Type: \"rank-reduction\" is the default and primary focus (with \"dropout\" as an alternative).\nLayer Types for Intervention (`lname`): Experiments iterate through specific layer types for MLP (`fc_in`, `fc_out`) and attention (`k_proj`, `q_proj`, `v_proj`, `out_proj`). The experiment iterates `[\"fc_in\", \"fc_out\"]` for active layers and `[\"dont\"]` for the baseline (`lnum = -1`).\nLayer Numbers for Intervention (`lnum`): For GPT-J, there are 28 layers (0 to 27). Experiments iterate through layers `[-1, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17]`. `lnum = -1` indicates a baseline run with no intervention (handled by `lname = \"dont\"`).\nRank Reduction Parameter (`rate`): The `rate` argument (e.g., `1.0, 2.0, 4.0, 6.0, 8.0, 9.0, 9.5, 9.9, 9.95`) controls the fraction of rank preserved (ρ). Specifically, the fraction of rank preserved (`ρ`) in `do_low_rank` is calculated as `(10 - rate) * 0.1`. A higher `rate` value corresponds to a stronger reduction (less rank preserved). For instance, `rate=1.0` preserves 90% of the rank (ρ=0.9), while `rate=9.95` preserves 0.5% of the rank (ρ=0.005).\nEvaluation Metrics: Accuracy (0-1 correctness) and log-loss (mean negative answer log probability per token). These are computed for both validation and test sets based on predictions, and aggregated using `DatasetMetrics`.\nOther Settings: Evaluations are `case_sensitive=False` and `strip=True`. Experiments are run on a `cuda` device if available, otherwise `cpu`. The iteration order for experiments is: `lnum` from highest to lowest (including -1 for baseline), then `lname` (`fc_in`, `fc_out`), then `rate` in reverse order (strongest reduction first). Best results are tracked across these iterations based on validation accuracy and then log-loss."
}{
    "Title": "Risk-Averse Fine-tuning of Large Language Models",
    "Main Contributions": "This paper addresses the challenge of mitigating negative or toxic content generation by Large Language Models (LLMs) in response to certain prompts, particularly rare but significant events. The main contribution is the integration of risk-averse principles into LLM fine-tuning, specifically optimizing the Conditional Value at Risk (CVaR) measure. The proposed Risk-Averse Reinforcement Learning with Human Feedback (RA-RLHF) methodology trains LLMs to exhibit superior performance in avoiding toxic outputs while maintaining effectiveness in generative tasks, outperforming traditional RLHF, especially for challenging prompts.",
    "Methodology": "The methodology adapts the Risk-Averse Reinforcement Learning (RARL) approach to the standard Reinforcement Learning with Human Feedback (RLHF) pipeline, optimizing the Conditional Value at Risk (CVaR) of the return instead of the expected value. It employs a soft-risk scheduling mechanism, inspired by the CeSoR algorithm, to gradually introduce risk-aversion. This involves an initial training phase where the full batch of episodes is used, followed by a gradual reduction of the batch size to focus on trajectories with the lowest returns (worst-case scenarios). The approach starts with a supervised fine-tuned (SFT) base policy for positive episode recognition and regularizes against it using a KL-Divergence term in the reward function. Proximal Policy Optimization (PPO) is used for policy updates, operating in an Actor-Critic setting with a base transformer extended by a language modeling head (actor) and a value function head (critic).",
    "Experimental Setup": "The research used GPT2 (117M parameters) and GPT-J (6B parameters) as base LLMs. Experiments were conducted on generative versions of three established classification tasks: IMDB-Gen (sentiment modification), Jigsaw-Gen (toxicity mitigation), and RealToxicityPrompts-Gen (toxicity mitigation). Datasets included IMDB (25k train/test reviews), Jigsaw (36,973 train/7,708 test samples), and RealToxicityPrompts (57,983 train/8,698 test samples). Reward models included `lvwerra/distilbert-imdb` for sentiment and `unitary/toxic-bert` for toxicity. Baselines for comparison were Base LLM, Prompted base LLM, DExperts, SFT, RLHF, and Quark. Evaluation metrics included task performance scores (sentiment/toxicity scores), Perplexity (linguistic coherence), and Distinct-n (textual diversity). Tail sentiment/toxicity scores were specifically evaluated on worst-case prompts. GPT-J experiments utilized Low-Rank Adaptation (LoRA) and bfloat16 precision due to memory constraints.",
    "Limitations": "The effectiveness of the risk-averse fine-tuning strategy may vary across different domains and languages, requiring further investigation and adaptation. The work primarily focused on generative tasks and did not explore Question-Answer (Q&A) formats, although the foundational work aims to enable more complex applications like conversational AI. While RA-RLHF showed superior performance, a marginal increase in model perplexity was observed, likely attributed to more aggressive adjustments to satisfy sentiment modification and toxicity mitigation goals. Additionally, ethical considerations regarding potential biases and unintended consequences of LLMs remain paramount and warrant continued attention, acknowledging that any alignment strategy could potentially be reversed to engineer LLM outputs for elevated toxicity.",
    "Future Research Directions": "Future research directions include further investigation and adaptation of the risk-averse fine-tuning strategy across different domains and languages. Extending the methodology to more complex applications, such as conversational AI and Question-Answer formats, is also a key area. Continued attention to ethical considerations regarding potential biases and unintended consequences of LLMs is emphasized. The work also establishes groundwork for exploring additional risk measures and criteria, such as the Entropic Value at Risk (EVaR), in the context of LLM safety and uncertainty quantification.",
    "Experiment Code": "import torch\nimport math\nimport numpy as np\n\n# Assuming trl.core.masked_whiten is available, or implemented as follows:\ndef masked_whiten(values, mask, shift_mean=True):\n    # Simplified for extraction, actual implementation may vary slightly.\n    # Computes mean and variance only over masked elements before whitening.\n    mean = (values * mask).sum() / mask.sum()\n    centered_values = values - mean\n    variance = (centered_values**2 * mask).sum() / mask.sum()\n    if mask.sum() > 1:\n        variance = variance * (mask.sum() / (mask.sum() - 1)) # Bessel's correction\n    else:\n        variance = torch.ones_like(variance) # Avoid division by zero if only one element\n    whitened = (values - mean) * torch.rsqrt(variance + 1e-8)\n    if not shift_mean:\n        whitened += mean\n    return whitened\n\n# --- From examples/IMDB/training/soft_risk_ppo_auth1_2.py ---\n# This function dynamically calculates the number of worst-case trajectories (alpha_N_ceil) to consider in a batch\n# based on the current training epoch, total epochs, and specified risk parameters.\n# It implements a soft-risk scheduling mechanism, gradually adjusting the effective batch size for policy updates\n# to focus on lower-return trajectories as training progresses.\n# Parameters like args_risk_alpha, args_risk_rho, etc. simulate access to a ScriptArguments object.\ndef get_current_risk_level(\n    alpha_cfg: float,\n    rho_cfg: float,\n    m: int,\n    M: int,\n    args_risk_alpha: float,\n    args_risk_rho: float,\n    args_risk_n: int,\n    args_ppo_config_batch_size: int,\n    args_risk_scheduler: str\n) -> int:\n    alpha = args_risk_alpha  # User-defined risk alpha from ScriptArguments (e.2, 0.2)\n    rho = args_risk_rho      # User-defined risk rho from ScriptArguments (e.g., 0.8)\n\n    if args_risk_scheduler == \"old\": \n        if m <= args_risk_n:\n            return args_ppo_config_batch_size # Use full batch size initially\n        else:\n            # Gradual reduction based on the 'old' schedule\n            return math.ceil(args_ppo_config_batch_size * max(alpha, 1 - (1 - alpha) * (m - args_risk_n) / (rho * M)))   \n    else: # This branch is specifically targeted by the 'new_schedule' aspect of the original file path\n        if m <= args_risk_n: # Initial phase (e.g., if current_epoch <= risk_n)\n            val = args_ppo_config_batch_size # Use full batch size\n        elif m >= math.ceil(rho * M): # Final phase (e.g., if current_epoch >= rho * total_epochs)\n            val = math.ceil(alpha * args_ppo_config_batch_size) # Fixed minimum batch size\n        else: # Intermediate gradual reduction phase\n            K = (1 - alpha) / (math.ceil(rho * M) - args_risk_n) # Slope for linear decrease\n            val = math.ceil(args_ppo_config_batch_size * max(alpha, 1 - K * (m - args_risk_n)))\n        return val\n\n# --- From trl/trainer/ppo_trainer.py (modified for soft-risk PPO) ---\n# The following functions implement the Conditional Value at Risk (CVaR) mechanism\n# by selectively masking out higher-return trajectories during policy optimization.\n\n# This method identifies a CVaR threshold and updates the batch mask.\n# 'risk_alpha' (an integer, here 'alpha_N_ceil') specifies the number of lowest-return\n# trajectories to keep. The mask is modified to zero out elements of trajectories\n# whose total return is above the calculated threshold.\ndef find_alpha_return(\n    traj_returns: torch.FloatTensor,\n    mask: torch.FloatTensor,\n    risk_alpha: int,\n) -> tuple[torch.FloatTensor, torch.FloatTensor]:\n    # Sort returns to identify the threshold for worst-case scenarios\n    sorted_returns, _ = torch.sort(traj_returns)\n    # The threshold is the (risk_alpha-1)-th smallest return in the batch.\n    # All trajectories with returns less than or equal to this are considered \"worst-case\".\n    q_risk_alpha = sorted_returns[risk_alpha-1]\n\n    # Apply the mask: trajectories with returns greater than the threshold are excluded\n    for ii in range(traj_returns.shape[0]):\n        if traj_returns[ii] > q_risk_alpha:\n            mask[ii,:] = torch.zeros_like(mask[ii,:], dtype=int)\n\n    return q_risk_alpha, mask\n\n# This method calculates generalized advantage estimation (GAE) but integrates\n# the risk-averse selection logic. After computing initial returns, it calls\n# `find_alpha_return` to modify the mask, ensuring only specified worst-case\n# trajectories contribute to the final advantage calculation.\ndef compute_advantages(\n    values: torch.FloatTensor,\n    rewards: torch.FloatTensor,\n    mask: torch.FloatTensor,\n    risk_alpha: int, # This is 'alpha_N_ceil' from get_current_risk_level\n    config_gamma: float, \n    config_lam: float,\n) -> tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:\n    lastgaelam = 0\n    advantages_reversed = []\n    gen_len = rewards.shape[-1]\n\n    values = values * mask\n    rewards = rewards * mask\n\n    for t in reversed(range(gen_len)):\n        nextvalues = values[:, t + 1] if t < gen_len - 1 else 0.0\n        delta = rewards[:, t] + config_gamma * nextvalues - values[:, t]\n        lastgaelam = delta + config_gamma * config_lam * lastgaelam\n        advantages_reversed.append(lastgaelam)\n    advantages = torch.stack(advantages_reversed[::-1]).transpose(0, 1)\n\n    returns = advantages + values\n    # Assuming the last token index (63) represents the end of a response in a 64-token sequence for total return.\n    traj_returns = returns[:,63] \n\n    # Modify the mask based on the calculated CVaR threshold, filtering out non-worst-case trajectories.\n    _, masks = find_alpha_return(traj_returns, mask, risk_alpha)\n\n    # Whiten advantages using the *newly modified* mask to focus on selected trajectories.\n    advantages = masked_whiten(advantages, masks)\n    advantages = advantages.detach()\n    return values, advantages, returns, masks\n\n# Simplified representation of how these functions are integrated into the training loop:\n# In the main training loop (e.g., 'for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):')\n#   current_epoch = epoch\n#   total_training_epochs = 194 # This value is specifically used in the example script.\n#   # ScriptArguments values would be passed for args_risk_alpha, etc.\n#   # For this specific example, let's assume:\n#   # args_risk_alpha = 0.2\n#   # args_risk_rho = 0.8\n#   # args_risk_n = 240\n#   # args_ppo_config_batch_size = 128\n#   # args_risk_scheduler = 'new_schedule'\n#   # alpha_N_ceil is calculated here, which represents the effective number of samples to consider.\n#   alpha_N_ceil_computed = get_current_risk_level(\n#       alpha_cfg=0.05, rho_cfg=0.98, m=current_epoch, M=total_training_epochs,\n#       args_risk_alpha=0.2, args_risk_rho=0.8, args_risk_n=240, args_ppo_config_batch_size=128,\n#       args_risk_scheduler='new_schedule'\n#   )\n#\n#   # Then, this alpha_N_ceil_computed value is passed as 'risk_alpha' to the ppo_trainer's step method.\n#   # stats = ppo_trainer.step(query_tensors, response_tensors, rewards, alpha_N_ceil_computed)\n",
    "Experiment Result": "The methodology adapts the Risk-Averse Reinforcement Learning (RARL) approach to the standard Reinforcement Learning with Human Feedback (RLHF) pipeline, optimizing the Conditional Value at Risk (CVaR) of the return instead of the expected value. It employs a soft-risk scheduling mechanism, inspired by the CeSoR algorithm, to gradually introduce risk-aversion. \n\n**Key Experimental Settings:**\n- **Base Policy Initialization**: The approach starts with a supervised fine-tuned (SFT) base policy. The specific model checkpoint used for this base is `/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/sft_final/2024-01-24_21-17-41/checkpoint-100`.\n- **Reward Model**: A sentiment analysis pipeline `sentiment-analysis:lvwerra/distilbert-imdb` serves as the reward model.\n- **Optimization Algorithm**: Proximal Policy Optimization (PPO) is used for policy updates. The policy operates in an Actor-Critic setting with a base transformer extended by a language modeling head (actor) and a value function head (critic).\n- **KL-Divergence Regularization**: The `kl_penalty` is set to `\"kl\"` in the `PPOConfig`, which adds a KL-Divergence term to the reward function to regularize against the base policy.\n- **Risk-Aversion Mechanism (CVaR)**:\n    - **Soft-Risk Scheduling**: The system uses a `\"new_schedule\"` for introducing risk-aversion, dynamically adjusting the effective batch size used for policy updates based on training progress. This schedule is controlled by the following parameters:\n        - `risk_n`: `240`. This is the number of initial training steps/epochs where the full batch size is utilized, delaying the introduction of explicit risk-aversion.\n        - `risk_alpha`: `0.2`. This target risk level determines the percentage of worst-performing trajectories (e.g., 20% of lowest returns) that will be selected for policy updates during the fully risk-averse phase.\n        - `risk_rho`: `0.8`. This parameter dictates that the soft risk level will reach its target `risk_alpha` after 80% of the total policy training iterations.\n        - The total policy training steps (`M`) for the scheduling function is `194` (as observed in the example script `soft_risk_ppo_auth1_2.py`).\n    - **Dynamic Batch Size**: The `get_current_risk_level` function calculates `alpha_N_ceil` for each training step. This value starts at the full `batch_size` (`128`) for the first `240` epochs (`risk_n`). It then gradually decreases, reaching `alpha * batch_size` (approximately `0.2 * 128 = 26` trajectories) around `0.8 * 194 = 155` epochs.\n    - **Trajectory Selection (CVaR Optimization)**: The `find_alpha_return` method sorts trajectories by their total returns (specifically, the return at index 63, implying a fixed response length of 64 tokens) and identifies a threshold `q_risk_alpha`. Subsequently, the training `mask` is modified to exclude (set to zero) all trajectories whose returns are strictly greater than this `q_risk_alpha`. This procedure effectively prunes the training batch to focus policy updates exclusively on the `alpha_N_ceil` worst-performing trajectories (lowest returns), directly optimizing for CVaR.\n- **General Training Parameters**:\n    - `learning_rate`: `1.41e-5`\n    - `mini_batch_size`: `128`\n    - `batch_size`: `128` (This is the logical batch size before dynamic adjustment)\n    - `gradient_accumulation_steps`: `1`\n    - `early_stopping`: `False`\n    - `target_kl`: `6.0`\n    - `seed`: `0`\n    - `steps`: `128000` (Total PPO training steps configured, although the `M` parameter for risk scheduling is set to 194 in the example script, suggesting a shorter effective scheduling horizon or an inconsistency.)\n    - `logging_dir`: Typically `/mnt/shared-scratch/Anon1/auth1/rlhf/trl/logs/imdb_final_1/sft_p_new_schedule/{date_n_time}`."
}

# Instructions
Following the instructions below, please provide an evaluation of the new method.
Since I aim to pursue research of high academic significance, I request that the assessment be conducted with rigorous standards.
- output
    - novelty_reason
        - Determine whether the new method has novelty, and output the reason.
        - The reason should be as specific as possible.
        - Carefully review the content of the studies provided in "Related Works" before outputting.
    - novelty_score
        - Score the novelty of the new method on a scale of 1 to 10, where 1 means no novelty at all and 10 means extremely high novelty.
    - significance_reason
        - Determine whether the new method is significant, and output the reason.
        - Significance includes both academic and societal importance.
    - significance_score
        - Score the significance of the new method on a scale of 1 to 10, where 1 means no significance at all and 10 means extremely high significance.
Output:
{'novelty_reason': 'Among the prior works, several papers propose to 1) replace the log-loss with an alternative strictly-proper score (Brier, Spherical, α-power, convex-composition, DEMD, etc.) or 2) blend two preference-learning losses (e.g. DiscoPOP) or 3) dynamically weight MLE against another objective (EMO’s MLE + DEMD).  None of them, however, targets the specific failure mode that bounded scores give too-small early gradients when training from scratch.  HyProS’ contribution is to keep the bounded score but (i) add the unbounded log-loss in parallel and (ii) drive a smooth time-based annealing from “pure log” to “pure Brier”.  This very small but precise modification is absent from the literature: the earlier “strictly-proper scoring rules” paper merely observes the slow-start problem and leaves it for future work; no schedule or hybrid loss is suggested there.  Likewise, convex-learning, EMO and DiscoPOP use other auxiliary losses or adaptive weightings but not a two-phase sigmoid schedule that restores early gradient magnitude for bounded proper scores at the token level.  Therefore the method is moderately novel as an instantiation of “hybrid proper scoring” for language modelling.', 'novelty_score': 6, 'significance_reason': 'The method is conceptually simple (<10 lines of code), requires no extra model parameters and solves a clearly documented practical pain-point: training from scratch with bounded proper scores stalls.  The reported gains are modest (+0.5 BLEU over standard log-loss on WMT14 En-De) but they come with better calibration and without any optimisation-tuning, so they could be attractive for practitioners who care about probability quality.  Academically, it refines our understanding of optimisation dynamics for strictly-proper scores yet does not open an entirely new research direction—the idea of mixing losses and scheduling weights already exists in other sub-topics (RLHF, EMO).  Societal impact is limited to incremental quality/cost improvements rather than a new capability or risk.  Hence the overall significance is moderate.', 'significance_score': 5}
